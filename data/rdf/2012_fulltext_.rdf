<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:dcterms="http://purl.org/dc/terms/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:ical="http://www.w3.org/2002/12/cal/ical#"
    xmlns:swrc="http://swrc.ontoware.org/ontology#"
    xmlns:bibo="http://purl.org/ontology/bibo/"
    xmlns:swc="http://data.semanticweb.org/ns/swc/ontology#"
    xmlns:led="http://data.linkededucation.org/ns/linked-education.rdf#"
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" >
	<swc:ConferenceEvent rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012">
		<swc:completeGraph rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/complete"/>
		<swc:hasAcronym>LAK2012</swc:hasAcronym>
		<swc:hasRelatedDocument rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<rdfs:label>Learning Analytics and Knowledge 2012</rdfs:label>	<ical:dtend rdf:datatype="http://www.w3.org/2001/XMLSchema#date">2012-05-02</ical:dtend>
		<ical:dtstart rdf:datatype="http://www.w3.org/2001/XMLSchema#date">2012-04-29</ical:dtstart><foaf:homepage rdf:resource="http://lak12.sites.olt.ubc.ca/"/>
	</swc:ConferenceEvent>
	<swrc:Proceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings">
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/1"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/2"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/4"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/5"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/7"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/8"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/9"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/10"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/11"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/12"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/13"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/14"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/15"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/16"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/18"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/19"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/20"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/21"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/22"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/23"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/24"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/25"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/26"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/27"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/28"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/29"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/30"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/31"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/32"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/34"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/35"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/36"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/37"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/38"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/39"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/40"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/41"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/42"/>
		<swc:relatedToEvent rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012"/>
		<swrc:booktitle>Proceedings of 2nd Learning Analytics and Knowledge (LAK2012), Apr 29 - May 2, 2012</swrc:booktitle>
		<swrc:month>May</swrc:month>
		<swrc:series></swrc:series>
		<swrc:year>2012</swrc:year>
	</swrc:Proceedings>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/1">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Modelling Learning &amp; Performance: A Social Networks Perspective</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/1/authorlist"/>
		<swrc:abstract>Traditional models of learning using a sociological perspective include social learning, situated learning and models of connectivisim and self-efficacy. While these models explain how individuals learn in varying social dimensions, very few studies provide empirical validation of such models and extend them to include group learning and performance. In this exploratory study, we develop a theoretical model based on social learning and social network theories to understand how knowledge professionals engage in learning and performance, both as individuals and as groups. We investigate the association between egocentric network properties (structure, position and tie), ‘content richness’ in the social learning process and performance. Analysis from data collected using an online eLearning environment shows that rather than performance; social learning is influenced by properties of social network structure (density, inter-group and intra-network communication), relations (tie strength) and position (efficiency). Furthermore, individuals who communicate with others internal rather than external to the group show higher tendencies of social learning. The contribution of this study is therefore two-fold: a theoretical development of a social learning and networks based model for understanding learning and performance; and the construction of a novel metric called ‘content richness’ as a surrogate measure for social learning. In conclusion, a useful implication of the study is that the model fosters understanding social factors that influence learning and performance in the domain of learning analytics. It also begs the question of whether the relationship between social networks and performance is mediated or moderated by learning and whether assumptions of the model hold true in non-educational domains.</swrc:abstract>
		<led:body><![CDATA[ 


 1. INTRODUCTION.
 During the last two decades, advances in information technologies, and particularly the evolution of the Internet and the World Wide Web have changed dramatically the way we live, interact, and learn. New ways of social interaction and participation have been heavily influenced by the appearance of different web-based and mobile technologies; especially social web applications (such as Facebook, YouTube, and so on). These platforms provide stimulating and interactive channels of communication that foster the creation and exchange of user-generated content for learning. Learning, understood as a social process of progressive knowledge acquisition, is shaped by individuals and their interaction with others who can contribute new ideas, opinions and experiences [44]. Thus, social networks within which people interact play an important role in the learning process by expanding the possibilities of learners to reach new sources of information, and by providing (latent and potential) channels for open collaboration among individuals [22, 24]. Despite the attractive advantages presented by scholars about the impact of technology in the learning process, there is still a lack of understanding of the dynamics of social interaction within learning communities. Therefore, the motivating questions that inspire this study are (i) Is there an interplay between social networks, learning and performance? (ii) If so, what is the role of social learning in the inherent relationship between properties of social networks and performance? (iii) how does one quantify and measure learning within a social context? (iv) how does one account for social network properties of structure, relations and position in modeling learning for the purpose of learning analytics? In this exploratory study, we develop a theoretical model based on social learning and social network theories to understand how knowledge professionals engage in learning and performance, both as individuals and as groups. The study also focuses on how an individual’s levels of participation and depth of engagement in the learning process are impacted by social interactions. The following section conducts a review of relevant learning theories and social network theories to arrive at a social networks model for understanding learning and performance. The model is then tested within an eLearning domain as described in section 3. Section 4 discusses the findings of the study followed by a discussion of the results in light of theory in section 5. Implications of the study, limitations and avenue for future research are provided in the conclusion in section 6. 
 2. CONCEPTUAL FOUNDATIONS.
 There are many different theories describing the general aspects and perspectives of human learning [2, 9, 18, 26, 27, 32, 37, 41, 47, 49-51]. However, there is no single theory that can explain by itself how people learn. The generally consensus tends to describe learning as a highly complex process, which involves cognitive, affective, individual and social dimensions [48]. In formal settings, learning takes place in controlled environments where educational bodies are restricted by their curricular structure, programs design, internal policies, resources and student demand [48]. However, learning is also a result of human interactions in informal activities related to work, family, or leisure, where there are no constrains for information exchange and collaboration. To this end, social learning theory suggests that individuals learn through the observation of different situations in a social context, and they change their behavior as influenced by those they observe [2]. This aligns with experiential learning where individuals explore and adapt according with their personal needs. Therefore, in knowledge intensive environments, to whom an individual is connected is critical for reaching resources and for obtaining information in order to solve determined necessities [14]. Social interaction enables the formation and maintenance of relationships with a specific purpose. Through those relations, individuals can obtain mutual benefits and create opportunities in the short and long term [15]. The value that individuals and groups ascribe to their contacts and relationships is instrumental in to maximizing the learning outcomes. However, beyond social interaction, there are fundamental contextual factors that facilitate learning [32] in a dynamic process that involves the exploitation of human and non-human resources in order to create and develop knowledge [46]. These concepts are discussed further below. 
 2.1 Models of Learning.
 People have a predisposition to imitate past behavior of others and the consequences of their actions [31]. Each individual is free to decide which alternative is better for his or her reality in order to solve a given problem. However, analyzing options is a time consuming process, and as a consequence, people have a tendency to rely on the information provided by others. This notion of social influence is core to Situated Learning theory [32], which states that apart from the influence of human interaction, there are contextual factors that impact on the individuals’ learning process. 
 2.1.1 Situated Learning.
 The principles of contextual cognitive development were first introduced by Lev Vygotsky [50]. In his work he argues that individual development cannot be understood without referring to the sociological and contextual aspects within which such development is embedded. Lave and Wenger [32] built on Vygotsky’s theory to describe the basis of their Situated Learning Theory (SLT). SLT argues that learning takes place in highly social situations where individuals develop skills by interacting with others that can provide them with insights about existing knowledge and previous personal experiences within a “community of practice”. Wenger [52] suggests that there are three fundamental factors that constitute a community of practice: (1) a shared domain of interest, (2) mutual engagement within the community, and (3) a common set of resources, practices and beliefs. According to this notion, human relations are based on the mutual interest of two or more individuals that interact in many different ways in order to acquire and share knowledge. Lave and Wenger [32] argue that knowledge is acquired in a context that normally involves the practical use of that knowledge, in what we commonly know as “learn by doing”. Thus, social interactions play a fundamental role in order to involve learners in a community that has a determined behavior and beliefs. Lave and Wenger [32] introduced the concept of Legitimate Peripheral Participation to describe the process through which a new learner that becomes part of the community, moves from the periphery to a more central position in the group by getting more and more engaged on the knowledge creation and sharing processes. As a result of this participative evolution and the progressive accumulation of different competencies, the learner creates a reputation in the community through social interactions and eventually becomes a referent of the group assuming a more leading role as a subject matter expert. Since the definition of its principles, SLT has been the focus of an important number of studies that provide well-documented evidence of cognitive phenomena that have not fully covered for other learning theories. However, the arguments of the theory have been criticized by some scholars arguing that “while cognition is partly context-dependent, it is also context independent; while there are dramatic failures of transfer, there are also dramatic successes; while concrete instruction helps, abstract instruction also helps; and while some performances benefit from training context, others do not” [1]. Apart from these limitations, some others state that situated learning programs are often very time-consuming to develop, they require the active participation of the learner who must be motivated enough to learn, and while situated learning may be an interesting option for some learning scenarios, it is not the most efficient methodology for teaching abstract subjects that involve more complex contents and information [35]. At the same time these limitations are opportunities to go beyond and look for a deeper understanding of the social dynamics that influence learning and learners’ performance through the study of individual and group social networks. In enabling situated learning, social technologies in particular have great potential and educational value due to their inherent capacity to increase learners’ motivation and engagement through participation and knowledge creation [22]. Due to its notion of community, SLT presents an interesting perspective to analyze learning and performance from a social networks point of view. Despite the fact that the theory has been based on contextual and cultural factors that influence learning, it does not consider non-relational sources of information (e.g. databases, webservers, blogs, wikis, discussion forums, and so on) as inputs for knowledge creation. The capacity to establish connections among different human and non-human sources of information to create knowledge is one of the principles of connectivism [46]. 
 2.1.2 Connectivism.
 Siemens’ concept of Connectivism [46] argues that existing learning theories (e.g. behaviorism, cognitivism, and social constructivism) do not provide an adequate explanation for the new characteristics of learning. At a glance, behaviorism provides instructors with insights and information that can be used to manipulate the learning environment, and then stimulate certain behavior to promote learning among students. On the other hand, cognitivism focuses on the study of the inner mechanisms of the human mind and the various processes involved in knowledge acquisition. For its part, social constructivism states that individuals’ learning process is directly affected by social interactions and builds on personal experiences and beliefs [3]. By incorporating ideas from chaos, social networks and complexity theories, connectivism tries to explain the dynamics of learning and how knowledge is permanently affected for new conditions (and new knowledge) in the environment [46]. Siemens [46] describes connectivism through eight principles: “(1) Learning and knowledge rests in diversity of opinions; (2) Learning is a process of connecting specialized nodes or information sources; (3) Learning may reside in non-human appliances; (4) Capacity to know more is more critical than what is currently known; (5) Nurturing and maintaining connections is needed to facilitate continual learning; (6) Ability to see connections between fields, ideas, and concepts is a core skill; (7) Currency (accurate, up-to-date knowledge) is the intent of all connectivist learning activities; (8) Decision-making is itself a learning process. Choosing what to learn and the meaning of incoming information is seen through the lens of a shifting reality. While there is a right answer now, it may be wrong tomorrow due to alterations in the information climate affecting the decision”. The status of theory assigned by some scholars to connectivism has been criticized by others [28]. However, despite such arguments, the constructs over which connectivism is conceptualized are worth studying given the scope and research questions of the study. The principles of connectivism are aligned with situated learning theory through the concept of communities of practice. In times where knowledge is considered a central stone of the society, individuals are required to work continually on their self-actualization processes where the potentials of connections cannot be longer dismissed [40]. According to Siemens [46], “learning is a process that occur within nebulous environments of shifting core elements not entirely under the control of the individual. Learning defined as knowledge patterns on which we can act, can reside outside ourselves (within an organization or a database), is focused on connecting specialized information sets”. Consequently, connectivism can be also described as the application of the dynamic properties of social networks to learning and knowledge acquisition. The role of network technologies and the quality of connections within these networks, is critical for a connectivist-oriented analysis. However, the importance of the quality of dialogues has been just partially covered by the connectivist literature. In his article Ravenscroft [42] presents an interesting perspective of how connectivism can benefit from prior social constructivist theories by focusing on dialogues. Ravenscroft centers his attention on how dialogues support the development and exploitation of connections for enhancing the learning experience. The Social Web [43] is based on the open participation of individuals and groups, with dialogues being the mean through which communities create knowledge. In his work, Siemens [46] remarks the importance of redesigning educational programs in order to improve reasoning, and critical thinking skills among learners towards a more meaningful collaboration in social environments. The importance that Ravenscroft [42] assigns to dialogue as a source of knowledge creation, is fundamental for this study. By understanding dialogue as unit of social interaction is possible to conduct an analysis of how network connections and meaningful content interchange influence individuals’ learning process, and consequently, their overall performance improvements. 
 2.2 Understanding Learning and Performance through Social Networks.
 Social network analysis (SNA) enables the study of social systems from a structural perspective through the identification of behavioral patterns based on node and tie attributes [17]. One of the fundamental principles leading the studies about the relationship among learning, performance and social networks is that an individual’s social structure can influence an individual’s access to valuable resources [5, 8, 14, 25, 33]. Those resources are rich on new information and knowledge, and depending on the level of engagement of the individual, this can be translated into considerable improvements in performance and learning [12, 13]. Some scholars have performed empirical studies in order to provide a better understanding of the role of social learning in individual, team and organizational performance. In Cross et al. [16], social networks analysis (SNA) was used to determine the information flow among top level executives in an organization. Using some of those findings, Borgatti and Cross [6] defined a formal model to explain social behavior during the learning process and the importance of relations to facilitate the search of information. Hence, it is interesting to examine theories that explain how information is disseminated through networks, and how network structures are conducive to learning and performance. 
 2.2.1 Strength of Weak Ties Theory.
 Perhaps one of the most seminal work in information diffusion is the classical paper of Granovetter on the strength of weak ties [21]. Considering strength of a tie as “a combination of the amount of time, the emotional intensity, the intimacy (mutual confiding), and the reciprocal services which characterize the tie”, he interviewed his research subjects focusing on how they found jobs by relying on either strong or weak ties..The remarkable finding was that most of the subjects reported finding a job not through strong ties (family or friends) but rather through weak ties (recent acquaintances). Granovetter rationalized that as individuals connect with closely knit groups bound together by strong ties (that is, the denser the network becomes) information quickly becomes redundant. New information therefore must come from weak ties, which connects one to new groups of people. More formally, Granovetter considers a bridge as the unique path that provides connectivity between two separated parts of the network where “all the bridges are weak ties”. These structural bridges in social networks are the only alternative for some contacts to reach determined information resources held by members in other areas of the structure. The importance of weak ties is that more people and resources are accessible through them. 
 2.2.2 Structural Holes Theory.
 In a network structural and positional perspective (as opposed to Granovetter’s relational view) towards information diffusion, Burt [10] describes how social capital, the set of relationships formed by and among individuals in different social contexts [14], can be effectively and efficiently optimized to obtain information and control benefits by exploiting one’s social network position. Burt argues that there is a tradeoff between the number redundant network contacts one has and the novelty of the information in the information diffusion process. Although the more contacts and individual has, the more valuable information he or she can reach and more opportunities he or she will have, however, by increasing one’s network’s size, the cost of maintaining relationships increases, and without and effective diversity, individuals are soon plagued by redundant information. Burt postulates that one can optimize one’s network by efficiently maintaining one’s ties in non redundant contacts such that one is effectively connected to a diverse variety of groups of closely-connected contacts (e.g. a clique), which (the groups) themselves are not connected. The bridging of such holes within the network structure is termed structural holes, which yields information and control benefits. Thus, two network design principles in play here are: Efficiency -maximizing the number of non-redundant contacts to maximize the gains through structural holes, and Effectiveness - the preservation of primary contacts in the network considering also the contact’s diversity principle. As will be seen in section 3, the measure of efficiency incorporates effectiveness as well. 
 2.3 Towards a Social Networks Model for Learning and Performance.
 While most social networks studies in the context of learning have generally been associated with individual and group performance or medium of communication, very few have incorporated learning as an important construct in their model [11, 23, 53]. Furthermore, given the unprecedented advancement in and adoption of social technologies, this study provides much needed evidence in the eLearning domain to help understand how networks interact with technology to foster learning and performance in an era of digital natives [4]. It is anticipated that the theoretical model depicted below based on network and social learning theories would address these research gaps. Therefore, based on the discussion above, the following hypotheses are proposed: H1: Density of an individual’s network is negatively associated with learning H2: Efficiency of an individual’s network is positively associated with learning H3a: The extent to which an individual engages in communication within the network (intra-network communication) is positively associated with learning H3b: The extent to which an individual contributes internally and externally to his group (inter-group communication) is positively associated with learning H3c: Weak ties within an individual’s network is positively associated with learning H4: Learning is positively associated with performance 
 Figure 1: Social Networks Model for Understanding Learning and Performance. 
 3. CONTEXT AND METHODOLOGY.
 In this section, the domain of the study is described, followed by a description of the process of data collection, extraction, storage and analysis using social network measures and attribute data. 
 3.1 The e-Learning Environment.
 The domain for this study is an eLearning environment where an online project management course was delivered during the last semester of 2009 for the Project Management Graduate Program in a leading ‘Group of Eight (Go8)’ university in Australia. Being a postgraduate program that is similar to a MBA, the course was undertaken by 36 full-time working industry professionals ranging from healthcare to banking, information technology and engineering. Students were based locally, nationally as well as overseas. While the online mode of study suited the student cohort relatively well due to their working hours, it required a high level of engagement with the course activities and peers. The course material consisting of lecture, tutorial, laboratory exercises and useful videos is accessible through the university’s eLearning site WebCT in order to support the students’ learning process. In addition, the eLearning platform provides a channel for synchronous communication via chat, and asynchronous communication via group discussion boards. The discussion board is further classified into varying forums including “Public” and “Group” forums (which are private to the group). Within each forum, students can post and reply to messages. Assessments included an individual assignment (15%), one online quiz (10%), one group assignment (25%), and a final exam (50%). For the purpose of this study, group assignment marks were left out of the analysis that the primary focus is on individual learning and performance. Group interactions, however, were considered given that group communications added further insight into an individual’s level of engagement. Groups ranged from two to four members, with 12 groups in total. As the entire learning took place online, analyzing communication structures of such ‘virtual’ groups is another boon given that coordinating and collaborating with team members located in different time zones is one of the challenges of virtual collaboration and one of the capabilities expected to be developed by individuals as part of learning. 
 3.2 Data Collection, Storage and Extraction.
 The main collaborative tool provided for group coordination and collaboration were the discussion forums. As mentioned earlier, there were two kinds of forum. The public forums were focused upon solving general questions about the course, assignments and so on. The private forums, or group forums were created exclusively for internal group coordination and collaboration. Every student had the chance to post messages to the lecturer, other students using the public forum or directly to a group member using the private forums. In total there were 845 messages in the public forum, and a total of 722 across the private forum. Given that the purpose of this study is to model learning and performance of students only, interactions to and from the lecturer were discarded. The information of the messages was extracted directly from the online discussion forums. Due to the unstructured nature of the message logs, a preliminary process of data preparation was executed. After this first step the information was loaded into a MySQL database. Once the data was loaded, a small Java application was written to extract the information required to generate the node data and tie data required for analyzing with UCINet, and Netdraw to generate the network sociograms and statistics [7]. UCINet and Netdraw were chosen over other tools such as Pajek, ORA and Gephi, because of the small nature of the dataset for analysis and it because it remains the most popular tool of choice for many social network analysts. 
 3.3 Message Content Classification.
 As discussed before, given the lack of evidence of measures of social learning using a structural perspective, it is arguable that the main source of information about social learning in virtual teams resides in the richness of their dialogues. A meaningful exchange of dialogue among team members and classmates is instrumental to enhancing their learning process. Thus, to develop a surrogate measure of social learning, for each message in the forums, we analyzed its content in order to identify behavioral patterns and sought to classify such patterns. The classification was based on past research that defined several methods to categorize different types of communication based on specific message features such length [34], channel of dissemination [34, 39], content [34, 38], and meaning [19], among others. In this study, we have defined a classification method based on message content and meaning in order to categorize each message sent in the public and private forums. We term such categories “content richness”. The five categories of content richness, with non-exhaustive descriptions, are: - Empty Message: Inexistent content, file exchange without dialogue, greeting messages. - Team Building Message: Team member personal introductions and very basic coordination. Final group closing activities, congratulations for group achievements and recognition for mutual cooperation. Team building messages support the creation of a sense of common goal and a shared set of beliefs and values. - Dissemination Message: Information about group submissions and notifications about new document versions. Information dissemination is fundamental to keep all team members on track and aware of the status of project activities. - Coordination Message: Team meeting coordination, a very important factor considering the time zone difference issue that some groups have to face. - Collaboration Message: Messages that add value to the group work in terms of knowledge creation. Problem-solving dialogues, different insights about group work issues and project activities. 
 Thus, in terms of content richness, we consider the last category the most significant, however, in terms of social learning all the other categories are also important as indicators of evidence of social learning. Every category was assigned a different weight that represents the content richness of the messages classified in each category (see Table 1). This parameter is considered later to calculate the content richness scores for an individual as explained further below. 
 Table 1: Content categories and their assigned weights, and some examples. 
 3.4 Measures.
 We have adopted an egocentric approach to collect data from the e-learning course social network. Ego networks are formed by a focal node called “ego”, the nodes to which that ego node is directly connected (alters), and the connections or ties among alters [45]. 
 3.4.1 Measures of Network Structure.
 Density: Density D is described as a measure of network cohesiveness and is defined as the relation of the existing number ties to the maximum number of ties possible in a directed graph of n nodes: 
 (FORMULA_1).
 where xij is the value of the connection from i to j. 
 3.4.2 Measures of Network Position.
 Efficiency: Effective size is a measure of the number of alters minus the average degree of the alters within the ego network [10]. The effective size of an ego’s network has been defined as: 
 (FORMULA_2).
 where i is the ego, j is a primary contact, and q is also a primary contact who has strong ties with the ego i (represented by piq) and j (represented by mjq). Efficiency is measured then by dividing the effective size by the number of alters in the ego’s network. 
 3.4.3 Measures of Engagement.
 Contribution Index: Gloor [20] introduces the notion of Contribution Index (CI) to measure the level of participation in social learning settings. The contribution index is defined as: 
 (FORMULA_3).
 where i is the contributor (ego), Σsi is the total of messages sent by i and Σri is the total of messages received by i. The contribution index value can be in a range from -1 to +1. If the learner mostly received messages, then his or her contribution index will be close to -1. On the other hand, if the learner mostly sent messages the contribution index will be close to +1. In terms of social learning we are looking for highly interactive dialogues. A contribution index near to 0 is indicative of a balanced dialogue of the learner with his or her team colleagues. External-Internal Index: Krackhardt and Stern [30] described a measure of interaction for both intra and inter work teams. External-Internal (E-I) index is a measure that compares the number and average strength of external ties to internal ties within different sub-groups in a network. The E-I index has been defined as follows: 
 (FORMULA_4).
 where q is the ego, Σeq represents the total number of external messages sent by q, and Σiq represents the total number of messages sent within the team. Similarly to the contribution index presented before, the E-I index ranges from -1 (only internal group communication) to +1 (indicating interaction only with external contacts rather than with members of the same team). Content Richness Score: As explained above, every message of the dataset was classified according to the level of richness of its content. Each class was assigned with a weight that reflects the meaningfulness of the message in the dialogue. This is a novel contribution of the study - the Content Richness score. Content Richness (CR) is a measure of learning engagement in a dialogic context where the meaningful information exchange among team members drives the individual and group learning process and is thus evidence of learning. The CR score is defined as: 
 (FORMULA_5).
 where q represents the ego, mci represents the message content value of the message i, n is the total number of messages sent by q, and max(mc) is the maximum possible value of message content quality. The CR score range from 0 to 1. If a learner’s CR score is 0 that means that his or her level of contribution richness to the discussions was inexistent. On the other hand, a CR score of 1 means a highly meaningful participation and engagement in course and group discussions. The importance of the CR score for this study is that we hypothesized that the learner’s performance is directly related to his or her level of engagement in a social learning environment, and this metric can help us to determine if there is such relation between these two indicators. 
 3.4.4 Measures of Relationship.
 Average Tie Strength: In keeping with social network literature [36], we used frequency of contact to calculate average tie strength. Therefore, for the purpose of this study, a learner’s tie strength has been measured as the average of all his or her tie strength (frequency of contact) to all other actors in the network. 
 3.4.5 Measures of Performance.
 As stated previously, during the semester the students were assessed on three individual components - individual assignment (15%), online quiz (10%), and final exam (50%). 
 4. RESULTS.
 According to the descriptive statistics obtained (see Table 3), CI is well balanced (CI = .015, tends to zero), which is indicative of an equally distributed rate of messages sent and received. In terms of E-I index, the mean value (E-I = -.872) indicates that there was more internal communication rather than dialogues external to the groups, which was expected given the large number of messages of internal group communication in the dataset. The CR score is quite high (CR = .667) which is indicative of meaningful content exchange within the groups. 
 Table 2: Descriptive Statistics. 
 Pearson’s correlation analysis was used to test the hypotheses (see Table 3 for a detailed description of the correlations). 
 Table 3: Pearson’s Correlation (n=36). 
 In summary, the most significant results are suggesting the following: - Network Structure: There is a significant negative relationship between network density and CR score, r = -.406, p (one-tailed) < 0.01. In terms of social learning, this result makes sense because is better to have a few high level meaningful dialogues, rather than many meaningless conversations. Given this result, we find support for H1. - Network Position: Network efficiency is significantly correlated with CR score, r = .394, p (one-tailed) < 0.01. Highly efficient learners are connected with contacts that are expected to provide good quality of content (high CR), so they can fulfill the informational needs of the learner without having to look for other sources [10]. This result provides us with evidence to support H2. Thus, when a learner has many sources of information, the cost of maintaining those connections increases as well as the level of information redundancy in the network. As a consequence of this, his or her level of network efficiency decreases. Even though theoretically, a decrement in efficiency should affect learner’s performance, the results obtained do not allow us to conclude such phenomena (i.e. no statistically significant relationship between any of the performance variables and efficiency).. - Engagement: There exists a significant positive correlation between CR and CI (r = .344, p (onetailed) < 0.05), thus lending support to H3a. According to the Gloor’s definition of CI [20], an optimal contributor would present a balanced rate of messages sent compared to the number of messages received. Therefore, the CI value should tend to be zero for optimal communication. The conclusion that we can make from this result is that learners with higher CR score send more messages than they receive, and as a consequence, there is no reciprocity in terms of meaningful content exchange for social learning. In addition, CR score is significantly negatively correlated to E-I index, r = -.354, p (one-tailed) < 0.05, thus allowing us to reject H3b. In the case of E-I index we are also looking for a balanced rate of internal and external communication [30]. A high E-I index indicates a relatively higher communication by an individual to those outside his group relative to those internal to his group. This is beneficial for avoiding ‘group think’. According to the results obtained, those who communicate more frequently internally within groups relative to externally outside groups are also engaged in higher or richer levels of communication. This can be attributed to the fact that the large number of internal group messages in comparison to the external ones influences the E-I index. In fact, there were more interactions between group members, rather than among external contacts, which indicates that learning as evidenced by content richness took place within groups rather than outside of groups. This result is very likely due to the large number of internal dialogues. - Network Ties: There is a significant positive relationship between the average strength of ties and CR score, r = .422, p (one-tailed) < 0.01. Therefore, there is sufficient evidence to reject H3c. The stronger the tie the more frequently the contacts occur. This implication means that contacts with high level of interaction tend to mutually exchange valuable information. These dialogues are rich in content and provide more in depth insights about the topics of learning. Although Granovetter’s theory may not hold true in this circumstance, other researchers have claimed that strong ties are symbol of closeness and trust, which are two determinant components for social learning [29, 32]. - Performance: There is a significant positive relationship between CR score and the individual assignment marks, r = .311, as well as between CR score and the quiz mark, r = .341, both p (one-tailed) < 0.05. However the exam result, which has the highest assessment weight in terms of learning outcome, does not seem to be significantly associated with CR score and for none of the engagement measures proposed in this study. Taken altogether, we consider that these results are somewhat indicative enough of how meaningful dialogic exchange among contacts can enhance learners’ performance. Therefore, we find partial support for H4. 
 5. DISCUSSION.
 According to the results obtained after the correlation analysis we can argue that rather than performance, social learning is influenced by social network properties such structure, relations and position. The results overall are also suggestive of the fact that the inherent relationship between social networks and performance is mediated by social learning. Therefore, a better understanding of how individuals organize themselves and interact with others can prove useful to to improve learning programs that directly influence performance. The findings of this study provide further evidence of the importance of social networks in affecting learning. The results are aligned with the arguments presented by Lave and Wenger [32] in their situated learning theory, and with the principles that provide the basis of Siemens’ theory on connectivism [46]. Content Richness (CR) was shown to be a good predictor of social learning due to the interesting findings that connect the measure with most social network characteristics modeled here. It makes sense that an individual with a less dense but more efficient network shows higher levels of engagement when interacting with his or her contacts [10]. At the same time and according to Granovetter [21], although information tends to be diffused through weak ties, meaningful information exchange involves trust and among contacts, which is directly related to the strength of the ties that connect them. In terms of limitations, first of all, as with many other studies on social networks, the size of the dataset raises the concern about the level of representation and generalization of the results. However, given the central limit theorem’s rule of thumb of a minimum of 30 samples and the fact that this study is an exploratory one, although not generalizable to the entire population, the results are indicative of the power of social networks in influencing learning and indirectly, performance. In addition, we acknowledge that the classification method for classifying content richness is still in its early stage, and may be likely to be subject to criticisms. To this end, we believe that the construction of a taxonomy or vocabulary for group communication in studies of linguistics and semantic data mining could allow the automatic identification of significant keywords in a message, and by using this information, the messages could be automatically classified in a predetermined set of categories. Content mining and information retrieval techniques can provide important benefits in this area of analysis. Another limitation of this study is that most of the dialogues analyzed took place within groups, and as a consequence there is not clear evidence of interaction beyond groups. It would be interesting to study the relation between the meanings of internal and external dialogues. However, when group formation is compulsory, according to our results we can state that the level of quality of the dialogues is significant. One important factor that should not be discarded is that group discussion boards are just one channel through which learners interacted during the semester. The messages in the discussion forum may have been followed up through use of other types of communication channels such as chat, videoconferences, emails, face-to-face meetings and phone calls. These interactions, although nice to have for the study, have not been accounted for. In future, a holistic approach that accounts for such interactions can be considered for further research. Finally, one may argue that excluding messages to and from the lecturer is a poor motive because lecturers (teachers) are also members of the learning community and often have the role of the mediator of the group discussions; they also frequently contribute to the building of learning groups/teams and stimulate collaboration among students. While in terms of social learning, this is quite true, however, the reason why the lecturer’s interactions are excluded is because we wanted to model social learning that takes place amongst students only. In reality, it is possible to include the lecturer’s interaction with the student (and vice versa) as well (but exclude the lecturer in computing his/her content richness score) and this remains an area of future work. 
 6. CONCLUSION.
 Although there are many theories that advocate the importance of social interaction in influencing learning and performance, empirical evidence is relatively scant. In this article we presented the development of a theoretical model for understanding the impact of social networks in learning and performance. The novelty of this research is driven by the construction of content-based measure called “content richness” which provides a new approach for measuring the level of engagement of learners in a social learning environment by exchanging meaningful information. We analyzed the communicational patterns of students and groups located in different cities, countries, and time zones. They were part of an online course that relied heavily on using the discussion forums as an important communication tool. The results show that rather than performance, social learning is highly influenced by the learners’ network of contacts. The implications of this study can be understood from a theoretical, methodological and practical standpoint. Although exploratory, this study challenges the theoretical notion of how social networks directly impact performance and questions the existence of antecedents and other confounding variables in the networks-performance equation. Methodologically, as described above, this study offers a novel approach to measure social learning. As an avenue to further this study, it would be useful to run regression models with a larger dataset to test whether networks and performance is significantly mediated by social learning in a single predictive framework. Practically, it is suffice to claim that such a model would allow for educators, professional development leaders, managers and academics to enhance learning analytics and make informed decisions and estimations of learning outcomes for both teaching and learning.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Modelling Learning &amp; Performance: A Social Networks Perspective</rdfs:label>
		<dc:subject>social learning</dc:subject>
		<dc:subject>situated learning</dc:subject>
		<dc:subject>connectivism</dc:subject>
		<dc:subject>social networks</dc:subject>
		<dc:subject>performance</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/walter-christian-paredes"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/walter-christian-paredes"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kon-shing-kenneth-chung"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kon-shing-kenneth-chung"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/1/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/walter-christian-paredes"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/kon-shing-kenneth-chung"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/2">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Deriving Group Profiles from Social Media to Facilitate the Design of Simulated Environments for Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/2/authorlist"/>
		<swrc:abstract>Simulated environments for learning are becoming increasingly popular to support experiential learning in complex domains. A key challenge when designing simulated learning environments is how to align the experience in the simulated world with real world experiences. Social media resources provide user-generated content that is rich in digital traces of real world experiences. People comments, tweets, and blog posts in social spaces can reveal interesting aspects of real world situations or can show what particular group of users is interested in or aware of. This paper examines a systematic way to analyze user-generated content in social media resources to provide useful information for learning simulator design. A hybrid framework exploiting Machine Learning and Semantics for social group profiling is presented. The framework has five stages: (1) Retrieval of user-generated content from the social resource (2) Content noise filtration, removing spam, abuse, and content irrelevant to the learning domain; (3) Deriving individual social profiles for the content authors; (4) Clustering of individuals into groups of similar authors; and (5) Deriving group profiles, where interesting concepts suitable for the use in simulated learning systems are extracted from the aggregated content authored by each group. The framework is applied to derive group profiles by mining user comments on YouTube videos. The application is evaluated in an experimental study within the context of learning interpersonal skills in job interviews. The paper discusses how the YouTubebased group profiles can be used to facilitate the design of a job interview skills learning simulator, considering: (1) identifying learning needs based on digital traces of real world experiences; and (2) augmenting learner models in simulators based on group characteristics derived from social media.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 1.1 Facilitating Learning Simulator Design.
 Simulated environments for experiential learning, or learning simulators, create a practical and social context in which new skills can be learned, applied and mastered. These environments are increasingly popular as a means to turn experience into knowledge, and are being applied in a variety of domains and learning contexts. A key success factor for learning simulators is the ability to connect learners to the real job-world, helping them to recognize what they need to learn. However, existing learning simulators suffer from the lack of such ability. This is because they incorporate a limited understanding of the learner based on skills and knowledge acquired only within the simulated world and disconnected from the learners’ real job experiences. The poor understanding of the learner needs is mainly due to the limited scope of the initial learner model, where the simulator scope for observing the learner is constrained within the particular application. Such a limited learner model leads to missing key learning aspects. These aspects may include learning domain concepts within a real world situation, which the learner, or people who share similar characteristics with him, are either aware of, or need to know more about. For example, the interpersonal skills and body language signals that job applicants should be aware of during job interviews. Deriving user profiles that include these learning aspects may help in facilitating the design of the learning simulators in two main aspects: 1. The derived profiles can provide a means for the trainers or the content providers to identify key learning needs for the simulator learners. 2. The learning domain concepts derived from the profiles can form a rich resource for augmenting the limited learner model to overcome the classic ‘cold start’ problem in user adaptive environments. 
 1.2 Motivation: Mining Social Media to Support Learning.
 Social media stand for a new culture of participation on the Web. 
 Figure 1. Generic Framework to Derive Group Profiles from Social Media Resources to Facilitate Learning Simulators Design. 
 In the last decade, people have been more and more involved into contributing and shaping content on the Web. In social media, people may comment on multimedia objects like YouTube1 videos and Flickr1 images, share their thoughts on micro-blogging systems like Twitter1, publish their bookmarks on Delicious1, or use services like CiteULike1 to organize scientific publications they are interested in. With the increasing popularity of social media resources, it is likely that people leave authentic digital traces of their profiles and real life experiences in the domain of their interests on social media sites [1]. Mining such digital traces promises to be very beneficial for various applications [5]. The motivation for this work arises from the interesting challenge of mining social media resources to bridge the gap between the simulated world in learning simulators and the actual experiences in the real job-world and better understand the learner’s needs in the context of the learning domain of interest and within the real world community the learner belongs to. This interesting challenge leads to several research questions that we would like to answer with this work, namely: - How to mine the digital traces in social spaces to derive profiles of user groups? - Can the derived group profiles be used to identify key learning needs for potential learners? - Can the derived group profiles be used to augment the learner model with useful learning domain concepts? 
 To answer the questions, this work presents a novel hybrid framework by combining knowledge representation as provided by semantics with knowledge discovery as exploited by machine learning, for mining the user-generated content retrieved from social media resources to derive profiles of user groups that can be further exploited to facilitate the design of learning simulators. The framework is perceived as a key contribution to mining social media in learning analytics to support learning simulators design. The rest of the paper is organized as follows: In the next Section, we introduce a novel, semantically-enriched machine learning framework that derives social group profiles from social media to facilitate design of learning simulators. In Section 3, the framework is instantiated within a case study to derive group profiles from mining user comments on YouTube videos. In Section 4, an experimental study is conducted to evaluate the application of our framework within the YouTube-based content. In Section 5, we position our work in the relevant literature on mining social media to support learning. The paper concludes in Section 6, pointing at requisites for replicating the study and future work. 
 2. DERIVING GROUP PROFILES FROM SOCIAL MEDIA: GENERIC FRAMEWORK.
 To address our first research question, we introduce a hybrid framework that integrates supervised machine learning represented by classification, unsupervised machine learning represented by clustering, and semantics. Figure 1 depicts the generic framework processes and the flow of data processing. The processes are described as follows: 1. User-Generated Content Retrieval: The digital traces that users create on the selected social media resource are retrieved using a content search process. Search is tailored toward retrieving content that falls within the context of the learning domain of interest. This is dependent of the social media resource selected. For example, to retrieve the user-created discussions on job interviews from YouTube, a collection of videos on job interviews can be selected by domain experts (e.g expert job interviewers) and the comments on these videos can be retrieved using the YouTube API. 2. Semantically-enriched Classification Model: Because social media content is full of noise, such as content that is irrelevant to the learning domain of interest, spam, and abuse content. We introduce a supervised machine learning process that classifies the content into relevant and noisy and selects the relevant content for subsequent processing. The process employs a semantically expanded Bag of Words (BoW) and a content scoring mathematical model described in [3] to score and label the training data set used for building the classification model to filter the noisy social media content. 3. Semantic-driven Individual User Profiling: using the filtered user-generated content, this process constructs a social user profile for every content author by merging all the content written by the author into a term weight vector representation. A semantically-enriched filtration layer consisting of domain concepts relevant to the learning domain is used to represent the content author. Using the semantically-enriched layer, the domain-relevant terms that the author used in the content written by him are extracted and included into his social profile. Furthermore, available demographic information that the author may have input in his social media user profile is retrieved and integrated into his user profile. 4. Group Clustering Model: An unsupervised machine learning process is exploited to cluster the social user profiles generated in process 3 into groups of users based on the similarity of their term weight vector representation. Clustering validity measures are used to determine the number of distinct groups that are mostly representative to the users based on the content they wrote. 5. Group Profiling: The learning domain concepts that the content authors who belong to each derived group are interested in or aware of are extracted from the cluster centroid of that group. In addition, statistical analysis is performed on the demographic information of the authors. The profile of each group is derived using the extracted domain-relevant concepts and statistical distributions of the authors’ demographics. 
 Figure 2. Instantiated Framework to Derive Group Profiles from User Comments on YouTube Videos to Facilitate Design of Simulated Environments for Learning. Interpersonal Skills in Job Interviews. 
 In the next section, we illustrate how the generic framework can be exemplified within the context of a specific social media resource, namely the YouTube video sharing site. For this, we present a YouTube-driven instantiation of the framework to derive social group profiles from YouTube-based corpus. 
 3. DERIVING GROUP PROFILES FROM YOUTUBE: INSTANTIATED FRAMEWORK.
 YouTube has become the most successful Internet website providing a new generation of short video sharing service. Since its establishment in early 2005, more than 100 million videos are being watched every day on YouTube, making it to rank second in traffic among all the websites in the Internet by the survey of Alexa [6]. YouTube provides several social tools for community interaction, including the possibility to comment on published videos. The analysis of comments constitutes a potentially interesting data source to mine for obtaining implicit knowledge about users, videos, categories and community interests [17]. YouTube allows the batch retrieval of these comments with its public API. In addition to comments, YouTube enables registered users to create individual user profiles on the site. A YouTube user profile contains information about a user, such as the user's age, gender, country of residence, hobbies, occupation, or favorite books, music and movies. Any personal information that appears in a user profile feed will have been entered by that user for publication on YouTube. The YouTube Data API allows you to retrieve user profiles. Motivated by the data acquisition and analysis opportunities that mining YouTube corpus can bring, we present a YouTube-driven instantiation of our framework to derive group profiles purely based on YouTube corpus. The derived group profiles are expected to provide two main functions: (i) help training professionals to identify key learning needs that can be considered in the storyboarding design of learning simulators, and (ii) identify learning domain concepts that can be augmented in the model of a learner who shares similar demographics with all users in one group. Figure 2 illustrates the components of this YouTube-driven instantiation. To demonstrate the process flow of the framework instantiation, we aim to derive group profiles that facilitate the design of learning simulators that train users to be aware of the various interpersonal communication skills for the job interview. Here, users can be either inexperienced job interviewers, or job applicants. Both job interviewers and job applicants need to know more about the different interpersonal communications during the interview session in order to improve their job interviewing skills, or increase their chances of getting hired after conducting the job interviews, respectively. The framework process flow consists of 5 main stages: Stage 1 - Video Selection: Uploaded YouTube videos about Job Interviews are selected for the retrieval of their public comments. Selection of the videos can be based on Domain expert selection, or keyword-based search of the YouTube API. The YouTube API provides many search ‘filters’ to improve the relevance of the search results. This includes filtering search results to show only videos that match a given set of categories and/or user-defined keywords. Each video can have many keywords but can only be associated with one YouTube category Stage 2 - Comment Retrieval: all the public comments on the videos selected in Stage 1 are retrieved using the YouTube API. Stage 3 - Filtration of Noisy Comments: Using the semantically-enriched machine learning technique described in [3], a YouTube noisy comment filtration component is implemented to predict the noisy comments and filter them out from the retrieved comment set. The predictive model is trained using corpus retrieved from YouTube classified into two classes; relevant and noisy, using a novel mathematical scoring model and a semantically expanded Bag of Words that consist of concepts highly relevant to the job interview domain. Noisy comments are those that (i) contain too little job interview-related concepts; (ii) are totally irrelevant to Job Interviews; or (ii) are spam or abusive comments. The remaining comments are those that contain a considerable rate of Job Interview-related concepts. Table 1 shows four example comments on the left that have been predicted as noisy by the noise filtration component. Obviously, the first three ones do not comment on the job interview video being watched, whereas the fourth one is a spam. The machine learning-based component aligned with that, classifying them as noise. The two comments on the right clearly contain concepts that are relevant to the job interview activity. Therefore, the component classified them as relevant. 
 Table 1. YouTube Comments, with their Scores and Labels Determined by the Machine Learning Component. 
 Stage 4 - Retrieving YouTube User Profiles: Given the subset of comments that have been filtered using the noise filtration component, the YouTube Data API is utilized for each comment in order to retrieve the demographic characteristics of the comment author. Firstly, given the comment entry element in the comments feed, the comment author username is retrieved for that comment. Secondly, given the comment author username, the demographic characteristics are retrieved from the YouTube user profile, which contains information that the user lists on his YouTube profile page. The author’s age, gender, and location are retrieved. Stage 5a - Clustering-based Group Profiles: Groups of comment authors are derived based on content similarity of the video comments they write. A semantically-enriched clustering algorithm is employed to derive the group profiles. The objective of the derived groups is to support trainers in identifying key learning needs to embed in the design of the learning simulator storyboarding design. The algorithm is further described in Section 3.1. Stage 5b - Demographic-based Group Profiles: Groups of comment authors can also be constructed based on user predefined demographics. The objective of the derived group is to identify job interview-related concepts that users who share specific demographics with a potential learner are interested in or aware of. These identified concepts can be used to augment the model of that learner. For example, given two adult female learners who live in the United States and Great Britain, stage 5b in Figure 2 shows synthetic key Job Interview Concepts derived from the comments of female adults who live in US & GB. The demographic-based group profiling method is further described in Section 3.2. 
 3.1 Clustering-based Group Profiling.
 The methodology of the Clustering-based Group Profiling consists of the following steps: 
 3.1.1 Semantic filtration of the Comments Content.
 The textual content of each comment output by the noise filtration component is represented by the terms that exist in a semantically enriched Job Interview-related Bag of Words (BoW). This BoW is derived from an experimental study documented in [9]. Pre-processing the experimentally-controlled user comments includes two main steps: Text Preprocessing: includes NLP techniques for text analysis using the Antelope NLP framework2, i.e. sentence splitting, tokenization, Part of Speech tagging and syntactic parsing using the Stanford parser for linguistic analysis. This enables the extraction of a structured form text representation to empower further analysis using semantics. Semantic Enrichment: representing Ontology-based word sense disambiguation and linguistic semantic text expansion. The first filter applied concerns the selection of specific lexical categories implemented within the WordNet3 Lexicon English language thesaurus to directly exclude non-significant terms for the job interview activity. For the words remained, the Suggested Upper Merged Ontology (SUMO) [7] has been exploited to provide direct mappings of WordNet English word units. WordNet Lexicon queries were then performed to retrieve synonyms, antonyms and word lexical derivations to expand the word set. Furthermore, DISCO [13] has been exploited to retrieve distributionally similar words from the Wikipedia corpus, and the filters discussed above have been applied, i.e. lexical category and SUMO concept mapping. 
 Table 2. Sample YouTube comment in original form (top) and reduced form (bottom). 
 Figure 3. Text Clustering Process in RapidMiner to Cluster the YouTube Comment Authors into User Groups based on Learning Domain Concept Similarity in Their Comments. 
 Each relevant YouTube comment is represented by only the terms that exist in the semantically-enriched Bag of Words. Table 2 depicts a sample YouTube comment in its original form (top) as well as in its reduced, annotated form (bottom). 
 3.1.2 Representing Authors by Their Filtered Comments.
 Given the semantically-filtered YouTube comments derived according to the method Section 3.1.1 and the author usernames retrieved in Stage 4, each comment author is represented by a term vector that consists of an aggregation of all the comments he wrote. Each derived term vector corresponds to one author. Table 3 depicts a term vector representation of one comment author, which is derived from combining two different annotated comments. The original two comments are shown on the right side of Table 3. 
 Table 3. Term vector representation of Comment Authors. 
 As can be seen in table 3, the vector terms derived from each comment are depicted in a color similar to the original comment. This vector representation of the author allows us to infer the overall awareness of that author in the job interview domain. For example, the author’s first comment indicates that the author is already aware of body language aspects in business, whereas his second comment indicates that the video enabled him to become aware of proper handshaking. The overall awareness can be seen by the author’s term vector representation. 
 3.1.3 Clustering Comment Authors.
 A RapidMiner4 process has been built to perform text clustering of the comment authors, where each unique author is represented by a term vector derived according to the method described in Section 3.1.2. Figure 3 shows the flow of operators used in the text clustering process. The functionality of each operator in the RapidMiner process is described as follows: - Read Database: Reads the term vector dataset from the database into the RapidMiner process. - Set Role: Sets the role of the author username as a row identifier and assigns the term vector attribute as an input for text clustering. - Extract Docs: Builds a text document representation for each term vector from the input dataset. This step is required for text clustering in RapidMiner. - Pre-Processing: Performs a number of text pre-processing sub-operators on the input text documents. These include: (i) tokenization, (ii) transforming to lower cases, (iii) stop word removal, (iv) filtering too short terms, and (v) generating bigram phrases. - Clustering: Clusters the comment authors using the Feature Weighting K-Means Algorithm, which has been previously used to perform text clustering with good performance results [11]. The optimum value of K (the number of clusters) is selected after performing a clustering evaluation strategy in RapidMiner. The clustering evaluation strategy is described in Section 4.2. - Centroids: Extracts the cluster centroids from the resulted derived clusters. Each centroid represents a vector of the TFIDF weights of the terms in each cluster. These are written to a CSV file using the Write CSV operator. - Select Attribs: Extracts the cluster membership (cluster number) that each comment author belongs to. This is then written back to the database using the Write Database operator. 
 3.1.4 Deriving the Group Profiles.
 A Profile for each group of comment authors is derived in this step, where each profile consists of the following elements: - Percentage of the comment authors in the group. - List of the job interview-related terms the authors of the group are interested in or aware of. These concepts are retrieved from the cluster centroid elements having maximum TFIDF weights. - Percentages of the gender distribution. - Percentages of the age groups in years (11-20, 21-30, 31-40, etc.). - Percentages of the location (country) distribution. - Sample comments written by authors who belong to the group. These comments are selected according to their relevance scores. The scores are computed based on the mathematical model presented in [3]. The top n comments having the maximum relevance scores are shown in the profile, where n can be a fixed number (e.g 1 comment) or a proportion of the comments written by the group authors (e.g 10%). 
 Figure 4. Statistical Distribution of Three Demographic Properties for the YouTube Comment Authors. 
 3.2 Demographic-based Group Profiling.
 As discussed in Section 1, the initial learner models in simulated learning environments do not contain sufficient information about the learner. This creates a user-adaptive problem for learning simulators to well adapt the learner needs. On the other hand, users who share demographic characteristics may have similar interests or can be aware of the same concepts. For example, people who are of certain age groups may be interested of the same kinds of songs. Collaborative filtering techniques in recommender systems are based on similar concept, where users are recommended items based on how their user preferences are similar to each other [20]. To address the limited scope problem of the initial learner model, a group profile for the YouTube users who share demographic characteristics with the learner can be derived by aggregating all the comment author vectors that meet specific demographic criteria. Then, the key job interview concepts that the users in this group are interested in can be identified as the list of terms having the maximum n TFIDF weights. These concepts can then be augmented into the model of the learner. 
 4. EXPERIMENTAL STUDY.
 To evaluate the framework instantiation in YouTube, an experimental study has been conducted. In the following Subsections, we describe the YouTube data that has been used in the study; the clustering validation techniques that have been employed to determine the number of groups to derive by the group clustering component of the framework; and present sample group profiles, demonstrating in examples how the derived group profiles can be used to identify key learning needs and augment initial learner models. 
 4.1 Data Description.
 Prototypical versions of the clustering-based and demographicbased group profiling components in the framework have been implemented to illustrate how the framework can be exploited to facilitate the design of a simulated environment for learning interpersonal skills and good practices in job interviews. Table 4 provides a statistical description of the experimental data. 
 Table 4. Overview of the YouTube Corpus. 
 Seventeen YouTube videos that show teaching examples of interpersonal skills in job interviews have been selected by training professionals who are expert in the job interview domain. As can be noticed from table 4, Most of the public comments (68%) on those videos are irrelevant to the job interview domain although they are within the comments on videos that are highly relevant to job interviews. This shows that filtering out noisy comments from high traffic social media resources, such as YouTube, is important to remove the comments that are not valuable in deriving group profiles, which can describe the awareness of the comment authors in the job interview domain concepts. This noise removal results in producing better clusters and reduces the computational time of the clustering process. Figure 4 depicts the gender distribution (a); the user age groups in years (b); and the 10 most frequent countries the authors are located in (c). These demographic statistics suggest that most comment authors who write relevant comments about job interview videos on YouTube are adult males who are located in Western countries. However, the figures also show that female users, elderly users, and users who live in different parts of the world, such as Turkey, Canada, Asia, and Australia, write relevant comments on YouTube job interview videos. Moreover, it would be interesting to compare the distribution of the demographic characteristics for each derived group with a generic profile for the overall YouTube community, provided such a generic profile exists. 
 4.2 Clustering Validation.
 In order to select the right number of clusters for the clustering based group profiling to derive the YouTube groups, 9 feature weighting k-means clustering models are trained using the input dataset, where each model has a unique number of clusters (k). For each model trained, two cluster validity measures implemented in RapidMiner are computed: (i) Davies Bouldin index [8], and (ii) Cluster density performance measure. Davies Bouldin index is the ratio of the sum of within-cluster scatter to between-cluster separation. This ratio has smaller values for a model that derives clusters which are more compact and farer from each other than other models. The cluster density performance calculates the average distance between the YouTube comment authors in each cluster and multiplies the result by the number of comment authors in that cluster minus 1. The Euclidean distance is used as the distance measure. The smaller the density value is, the more similar the comment authors are to each other in the cluster, and thus the more compact the cluster is. Figure 5 shows the Davies Bouldin index (top) and average cluster density values (bottom) for the 9 feature weighting k-means clustering models, with (k) ranging between 2 and 10. 
 Figure 5. Davies Bouldin Indices (top) & Cluster Density Values (bottom) for 9 Clustering Models. 
 Figure 5 (top) shows that the model having 9 clusters produce the minimum Davies Bouldin index. Because RapidMiner negates the density values, the best cluster density values are those closest to zero. As can be seen in Figure 5 (bottom), the cluster density values become closer to zero as the value of (k) increases. However, the difference of the increase in density after the model with k = 8 clusters becomes insignificant with the subsequent models. The consideration of the two models suggests that clustering the YouTube comment authors into 8, 9, or 10 clusters can derive groups of users who are relatively close to each other inside each group and far from each other from one group to another. In the sext subsection, we base our analysis on the model that derives 9 groups, as suggested by the Davies Bouldin index results. 
 4.3 Example Group Profiles and Usage.
 4.3.1 Identification of Learning Needs.
 We illustrate in the example how clustering-based group profiles can be used by training professionals to identify learning needs for group of learners who meet the demographic criteria of the users who belong to the example groups. Table 5 shows the details of an interesting sample group which contains around 10% of the comment authors. The key job interview-related terms in this group suggest that the users who have been associated to this group by the framework express anxiety (e.g “anxious”, “nervous”, “worried”). Other bigram phrases detected by the framework, such as “good_words”, “question_interviewer”, and “job_experience” suggest that these topics could be the reason for the anxiety expressed in the comments. In the GUI interface, the identified concepts can be used by training professionals to browse the comments written by the users who are associated to this group to better understand the reason behind their anxiety. We simulated that by showing sample comments written by authors who are associated to this group by the clustering algorithm. As the first sample comment reads, the author is a potential job applicant who is worried that the interviewer might ask him about his little previous job experience. Hence, he seeks an advice on proper answers (good words) that he can say to well justify such a potential question during his job interview. Similarly, the second comment author is nervous because he is not sure what good questions he may ask the interviewer if he has been asked to do so during the interview. The trainers can identify these learning needs by depicting the identified learning concepts and use them to browse and read the authors’ comments that are linked to these concepts. Furthermore, based on the distribution of the demographic characteristics of the group users, the trainer may link these learning needs to adult applicants who live mostly in the US and Europe, as the age group and the location distributions suggest for this group. 
 4.3.2 Learner Model Augmentation.
 As described in Section 3.2, group profiles may also be derived by aggregating all the individual user profiles of the comment authors – represented by their semantically-filtered comment term vectors – whose demographic characteristics, such as their age groups and locations, match a potential learner for a learning simulator. The learning domain concepts extracted from the group are augmented into the model of the learner to overcome the cold start problem, providing the learning simulator with more learning domain-relevant information about the learner. To evaluate the effect of having different demographic properties on the learning domain concepts, we assume an artificial example of having three adult learners with different known locations: Great Britain (GB), United States (US) and Asia. Given their ages and locations as input to the framework, three demographic-based group profiles are derived by aggregating the term vectors of the YouTube individual comment authors who meet the demographic properties of each of the learners. Table 6 depicts the three different demographic properties. As can be seen, the key learning concepts that the leaners are interested in or aware of considerably differ from a group to another. Differences are illustrated in three job interview aspects: - Body Language Signals: the frequent body language signal that adults in GB talk about is the “eye contact”. However, for US adults. On the other hand, other body parts used in body language signals, such as “fingers” and “hands”, frequently exist in the comments written by US users. Extracted concepts from content written by users in Asia do not suggest that they are interested in or aware of body language signals. - Emotions: US adult job candidates are more inclined to express their anxious emotional states when talking about job interviews. This can be sensed from the “nervous” concept being only in the group of US adults. GB adults on the other hand tend to show more confidence by using terms like “hope” and “helpful”. - Interests: Asian users show more interests in watching interview and job hunting guides than users in US and GB. Interest by users in Asia and US in the financial aspect can also be seen in relevant terms such as “money” (Asia, US) and “pay” (Asia). US users show more interests in “companies” and “education” in addition to money. Both GB and US users tend to mention the “interviewer” more frequently than mentioning the interviewee, as opposite to users in Asia who tend to mention the “candidate” more frequently. Mentioning humans, as can be seen in terms like “people” and “girl” is more apparent in comments written by authors who live in the US. 
 Table 5. A Sample Clustering-based Profile for a YouTube Group having 10% of Comment Authors. 
 Table 6. Demographic-based Group Profiles. 
 5. RELATED WORK.
 In this Section, we position our work within the literature works in three main relevant aspects, namely: (i) Augmenting user models for personalization and adaptation in simulated environments, (ii) Mining user-generated content in social media for learning support and learner modeling, and (iii) Exploiting machine learning techniques to mine social media content. Augmenting user models by deriving characteristics of other similar users for personalization and adaptation in digital environment is well founded in the literature. [16] presented a machine learning framework that addresses the lack on interoperability between different multiple electronic systems offering recommendations to users. The output is a cross system personalization that supports augmenting user models from learning about what other users know in different electronic systems. In this framework, a set of examples from one system is used to train a machine learning model and the model is then applied to augment user models in another system. Likewise, [22] described the design of an extension to a lifelong learning system, called “people like me”, which supports personalized searching for timelines of people who share similar characteristics with the learner. In our framework, we further extend the concept of augmenting user models with characteristics mined from digital traces written by other users, involving social media as the resource for these traces. Learner model development and augmentation to support learning is also discussed. [12] presented a method based on fuzzy sets to describe the uncertainty of learner knowledge used for learner modeling and identification of the learner’s needs for adaptive educational systems. A fuzzy user model is proposed to deal with vagueness in the user’s knowledge description. However, in learning simulators, the main problem lies in the limited scope of information about the learner that exists in his initial learner model, and not in the imprecision and vagueness of the learner’s descriptions of his knowledge. Similar fuzzy techniques have been applied in [14] to use tracking information generated by a Web Course Management System to build student, group, and class models that generate advice to the course instructors and facilitators. This can be effective when the limitation of the learner model is due to psychological barriers, such as the students' feeling of isolation and the instructors' communication overhead and difficulty to address the needs of each individual student. The lack of sufficient information about the learner due to the limited scope of the learning application has not been addressed. [23] presented an activity-visualization prototype extended to the Moodle virtual learning environment. The prototype supports personalization of learning by enabling learners to visualize their virtual learning activities in Moodle and mirroring them back to the learners using Web2.0 services. However, these activities are constrained within the capabilities that Moodle provides, or more specifically, what Moodle plug-ins the learning service provider chooses to install. Our framework on the other hand, is not tied to digital traces that users may produce in one platform. For example, the platform can extend the YouTube-based corpus used for deriving the group profiles by adding user tweets about Job Interviews retrieved from twitter, user blog posts in blogs that address interpersonal skills, and professional discussions about Job Interview skills from LinkedIn. Researchers have also focused on the involvement of training professionals in the construction of learner models. Examples include the works in [19], [18], and [15]. In [19], the structure of the student model has been specified by domain experts and then the parameters were mined from the data obtained in transcripts of problem-based learning sessions. In [18], a Bayesian student model was constructed based on a combination of elicitation from the domain experts and automated methods. In [15], training professionals compared the performance of two different knowledge student model structures for a virtual learning environment, where evaluation parameters are revealed in the model data. In our work, we built on this concept by enabling training professionals to augment learner models by identifying key learning needs for learners based on machine-derived profiles of user groups with which the learner has a potential to share similar characteristics. Literature on mining content written by users in social media resources to support learning environments is also relevant to our work. [10] presented a method to mine the micro digital traces, or ‘tweets’, that the learner writes on twitter to extract knowledge about the learner and use them to augment the learner model, which can be helpful to overcome the “cold-start” problem in elearning systems. Other works have also analyzed learners’ tweets to support the learning process. [4] and [21] analyzed the usefulness of twitter micro-blogging activities in second language learning. For that, students have been required to write certain number of tweets per week and then analysis of the communication patters between students using their twitter activities has been conducted to investigate any improvements in many learning competences that students acquire, including Sociolinguistic, strategic, and cultural competences. However, the discussed methods can be useful only if the learner has an active twitter account. This is not a controlled feature for learners in simulated learning environments. Moreover, tweets written by users who share similar characteristics with the learner, which may also imply additional features about the learner, have been overlooked. Machine learning techniques have been recently considered to mine social media content for many purposes. [2] reviewed many machine learning-based works to analyze blogs and blog posts to discovery communities in social media; detect influential and trustful blogs and bloggers; and filter spam blogs. Both cluster analysis and classification techniques have been utilized in these works. However, semantic enrichment techniques to improve clustering results and predicting rates have been clearly overlooked. From a technical perspective, the extension of our approach to these works is perceived in providing a semantic enrichment layer to the machine learning techniques in order to improve, firstly, the noise filtration capability, by training the classifier to filter out content irrelevant to the learning domain, and secondly, the clustering capability of the individual profiles to user groups based on their similarity of the learning domain content they provide. Furthermore, the approach presented in our work is tailored toward supporting the learning domain by employing the machine learning outputs to facilitate learning simulator design. 
 6. CONCLUSIONS.
 Mining user-generated content and Modeling users from social media resources are becoming increasingly important thanks to the huge available content that users generate. However, the focus on simulated environments for learning has not been addressed extensively by researchers. Learning simulators may not have sufficient information about learners and thus adaptation to leaner needs becomes problematic. Most of the works that mine social media content to support learning environments assume that learners have active social media accounts. In many cases, learners may not have social media accounts or may choose not to reveal them. In this paper, we introduced a novel framework to facilitate the design of user-adaptive learning simulators to better meet the learners’ needs. The framework intuitively aims to utilize the collective intelligence aspect employed in collaborative filtering techniques. This is achieved by deriving social group profiles and extract key learning domain-relevant concepts that can reveal many interesting findings about those who are demographically similar to the simulator learners. We have exemplified the generic framework within the context of YouTube, illustrating how it can be instantiated to a specific social media resource. We evaluated how this can support learning simulator design by helping trainers to identify key learning needs as well as extracting learning domain concepts from the group profiles that can be used to augment an initial learner model that has a cold start problem. 
 6.1 Requisites to Run Similar Experiments.
 The following requisites to replicate the similar experiment on same or different contexts should be taken into consideration: 1. Semantic enrichment relies on Bag of Words extracted from domain ontologies. However, vocabularies that describe different learning domains may be absent. To address this requisite, ontologies such as SUMO and DBPedia as well as lexical resources such as WordNet Domains can be used to extract domain-relevant Bag of Words. 2. Expert selection is needed to both select the domain-relevant YouTube videos and to evaluate how relevant the used domain vocabulary to determine comments relevant for deriving user profiles. 3. The dataset used to derive the group profiles needs broadening to well represent the YouTube community. This can be addressed by retrieving more domain-relevant videos using the search capabilities of the YouTube data API, such as search by keyword, categories, and developer tags. 
 6.2 Future Work.
 A user-based evaluation study involving learners and training professionals will be conducted in the ImREAL project. Learning interpersonal skills for job interviews is the first targeted use case. Other use cases involve learning social signals in different cultures to help learners involved in student exchange programs to be aware of the various cultural communication signals. 
 7. ACKNOWLEDGMENTS.
 The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/20072013) under grant agreement no ICT 257831 (ImREAL project).]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Deriving Group Profiles from Social Media to Facilitate the Design of Simulated Environments for Learning</rdfs:label>
		<dc:subject>augmented user models</dc:subject>
		<dc:subject>data mining for learning</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>learning needs</dc:subject>
		<dc:subject>mining social media</dc:subject>
		<dc:subject>social profiles</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ahmad-ammari"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ahmad-ammari"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/lydia-lau"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/lydia-lau"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/vania-dimitrova"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/vania-dimitrova"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/2/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ahmad-ammari"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/lydia-lau"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/vania-dimitrova"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Learn-B: A Social Analytics-enabled Tool for Self-regulated Workplace Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3/authorlist"/>
		<swrc:abstract>In this design briefing, we introduce the Learn-B environment, our attempt in designing and implementing a research prototype to address some of the challenges inherent in workplace learning: the informal aspect of workplace learning requires knowledge workers to be supported in their self-regulatory learning (SRL) processes, whilst its social nature draws attention to the role of collective in those processes. Moreover, learning at workplace is contextual and on-demand, thus requiring organizations to recognize and motivate the learning and knowledge building activities of their employees, where individual learning goals are harmonized with those of the organization. In particular, we focus on the analytics-based features of Learn-B, illustrate their design and current implementation, and discuss how each of them is hypothesized to target the above challenges.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 In the last few years, the growing emergence and acceptance of social software tools, social media and Social Web (Web 2.0) paradigm have brought forth a new perspective to the concept of learning [4][14][7], demonstrating a transition from conventional pedagogical approaches to a more social and collective knowledge paradigm of learning, in that creativity, social-embeddedness, and the capacity to gain knowledge from a sea of collective are highly expected and valued [9][13]. Such a perspective to learning is especially important in the context of workplace [5], where learning is social, affects and is affected by the social context and the available collective knowledge. To keep up with and adapt to the contextual needs of workplace settings, learning at workplace mostly happens as a by-product of work. This “on-demand” and informal approach to learning [1] requires contemporary knowledge workers to have SelfRegulatory Learning (SRL) skills in identifying their learning needs and conducting appropriate learning strategies to attain them [8]. The majority of conventional interpretations of SRL are based on an individualistic perspective, where the impact of the collective is often assumed less significant than individualbased factors [6]. Such perspectives contradict the nature of the workplace, where individuals’ work and learning activities are highly social and collective-centred. The recent research on workplace learning clearly stresses the role of the collective and other forms of social exchange in both individual learning and organizational development [4][1]; findings on patterns of defining learning goals in the workplace show that in the process of setting and managing their learning goals, individuals draw from and contribute to the collective knowledge in their organization [8]. To support users’ in their SRL processes in modern workplaces as well as scaffolding organizational learning, there is a need for systems that collect learning–related contributions, re-aggregate and analyse them to create further new knowledge, and make this new knowledge available to users. Such new knowledge can be beneficial to users in every step of their learning process from identifying their learning needs and setting their goals (e.g. they can get aware how other employees with similar organizational positions have defined their goals), to monitoring their learning progress and comparing it with that of their colleagues who hold the same position or work in the same project, and sharing and documenting their learning experiences (e.g. by observing how actively their colleagues are sharing their learning experiences and comparing it with their own sharing activities, or to see how their shared knowledge has been useful to other members of the organization). Designing systems that unlock the collective knowledge, and the collective intelligence in higher levels of inference for the purpose of scaffolding learning, however, is not a straightforward task [4]. Semantic technologies and Linked Data paradigm could provide the required technical backbone for tackling this challenge. Today’s knowledge workers often use diverse tools and services in their everyday working and learning practices; therefore, the traces and outcomes of their activities are dispersed among different tools/services that often lack the capability of interchanging and/or integrating user’s data. If properly applied, the Linked Data paradigm and the associated Semantic Web technologies would enable meaningful data integration and knowledge structuring. Needless to say, to be successfully deployed and to lead to the expected results, these advanced technologies need to be supported by proper pedagogical and motivational approaches. We base the foundations of our pursued pedagogical approach on a well-known organizational knowledge building model proposed by Nonaka and Takeuchi [10] (to address the challenge of harmonizing individual and organizational learning), and extend it with SRL practices (to support users’ in initiating and conducting their individual learning processes) [11], and motivational elements [12] (to address the challenge of motivating users to share their personal knowledge and learning experiences, and contributing to the collective knowledge in their organization). For this pedagogical framework to work effectively, we hypothesize that Learning Analytics (i.e. collecting users’ contributions, aggregating them, analyzing them and reporting back to the users and the organization) play an important role: it allows for the organization to better align its learning objectives with those of its employees by knowing about their learning practices; it supports users’ SRL processes by providing them with the necessary input from the social context of the workplace; and it enhances the motivation of individuals to take part in learning and knowledge building activities and sharing their experiences by providing them with feedback from the collective. In this design briefing, we introduce the Learn-B environment, our attempt in designing and implementing a research prototype to support workplace learning that addresses the above challenges. Learn-B stands for Learning Biosis (“biosis” meaning a way of life), i.e. learning as a way of life. In particular, in this design briefing we report on the learning analytics aspects presently supported by Learn-B. 
 2. THE LEARN-B ENVIRONMENT.
 The design of the Learn-B environment was driven by the requirements for effective learning and knowledge building in organizational settings. It is designed to integrate different tools that employees often interact with during their everyday (working and learning) practices. In particular, so far we have integrated a wiki (MediaWiki), a social networking and collaboration platform (Elgg), and a bookmarking tool (Tagging tool – implemented within this research as a Firefox plugin). Learn-B serves as the central hub for this integrated environment, and relies on an interlinked set of ontologies as its underlying (linked) data model. These ontologies are available at: http://goo.gl/Saui4. A current demo of the main functionalities of the Learn-B environment is available at: http://goo.gl/RaiIm. Figure 1 illustrates the multi-layer architecture of Learn-B which can be adapted to and applied in a wide range of organizations. There is no strong boundary between the layers and components defined within each layer. In this design briefing, we only focus on the Analytics-enabled functionalities provided within the Processing Service Group. This service group is responsible for tracking all the events that happen in Learn-B, and other integrated tools (i.e. MediaWiki, Elgg and the Tagging Tool), processing and analyzing the gathered data, and providing users with the resulting feedback and analytics. In particular, Event Dispatcher (Figure 1.K) is responsible for processing all of the events occurring in the Learn-B environment, storing them into the RDF repository (Figure 1.A) and distributing them to other services. Analytics Service (Figure 1.L) is responsible for processing and analyzing the data about users’ learning activities and their interaction with diverse kinds of learning resources. It makes use of the interaction data stored in the RDF repository to provide users with feedback, primarily through different kinds of visualizations, to support them in planning, performing and monitoring their learning process. 
 Figure 1. The architecture of the Learn-B Environment. 
 Usage Information is one type of the provided analytics which comes in the form of statistics, Social Waves or the collective stand. Derived from the collected knowledge within the system, this functionality supports the recommender services (Figure 1.H-I) and more importantly, provides users with analytics representing the collective knowledge around a resource and assists them in planning their learning processes. Statistics and Social Wave analytics are implemented as a set of various visualization charts, each conveying the intended feedback/analytics data. The feedback reflecting the collective stand about a learning resource comes in diverse forms such as annotations, reflections (e.g. comments and notes), ratings and tags of other users. For instance, Figure 2 illustrates how each organizational objective, defined in terms of competences, is accompanied by statistical analytics such as the number of users who have acquired that competence and their roles in the organization, and the Social Wave stream of that competence showing the activities performed on or events happened around it over a certain period. Such analytics represent the “popularity” of a given competence, indicating whether and to what extent it is (socially) alive. The comments of other users can be viewed under the Comments tab in Figure 2. The recommendation of a learning path, via the Learning Path Recommender service (Figure 1.H), to achieve a competence (in this research, each learning path is comprised of one or more learning activities that lead to the attainment of a specific competence at a specific level) is further augmented with analytics such as the number of users who have successfully finished this learning path or a revision of it, or are still working on it, or have abandoned it. Users can also see the organizational positions of users in each of the above categories (i.e. active, finished, abandoned). Similar to the competences, each learning activity is also accompanied by Social stream and collective stand analytics. 
 Figure 2. Analytics - Usage Information provided for each organizational objective a) Statistics b) Social Waves. 
 Progress-o-meters represent another type of the provided analytics; they aid users to monitor their learning progress in the organizational context, by showing them their progress flow in achieving their defined learning goals and the competences included within those goals, and are implemented as a set of line charts (Figure 3). Moreover, Progress-o-meters provide users with a comparison of their progress flow with that of their colleagues who have the same learning goal (e.g., a goal shared by the members of a project), or are working on the same competence. We hypothesize that observing oneself within the social context of the organization helps users to monitor their progress toward achieving their goals, thus also assisting them in further regulating their learning strategies. 
 Figure 3. Analytics – Progress-o-meters. 
 Knowledge Sharing Profiles inform users of their reflections, in terms of sharing their learning resources, within an organization. Via this type of provided analytics, users can see how actively they are sharing each of their learning resources, and also compare their sharing activities with the average within their organization (Figure 4). As a factor targeting individuals’ extrinsic motivation [12], we hypothesize that such feedback can help users to regulate their knowledge sharing activities. 
 Figure 4. Analytics – Knowledge Sharing Profiles. 
 Motivational Messages are another type of provided analytics which aim to support users’ stronger engagement with the system. Generally, a user (learner) model represents user knowledge, goals, interests, and other features that allow for better recommendations or provided adaptivity by the system. Opening the learner model may bring additional benefits to users, allowing them to take charge of their own learning experience. However, collecting explicit data from users is often challenging and a strong motivation is needed on learners’ part to provide explicit feedback about their learning [1]. Motivational Messages aim to tackle this challenge by providing users with personalized messages indicating to what extent the collective has opened their models in terms of sharing their personal preferences and learning experiences. For instance, Figure 5 shows a set of motivational messages related to the degree of completeness of a user’s ‘preferences’ compared to other users, where these preferences are used to adjust the recommendations generated for the user. 
 Figure 5. Analytics – Motivational Messages. 
 Last but not least, the Analytics Service supports the harmonization of individual and organizational learning objectives. Browsing the different forms of Analytics available for a certain competence, updates the managers of an organization on, for instance, how frequently this specific competence has been used within the organization, in the context of which learning goals it has been used, by users of what organizational positions, and what the main issues regarding this competence are. This allows managers to apply any necessary modifications in the definition of the competence itself or the learning paths associated with it, to better harmonize organizational and individual learning needs. Also discovery of emerging competences or other learning resources can be learned via this service. On the other hand, if some user-created competences are frequently being re-used by members of an organization; the managers might consider them as ‘emerging’ organizational goals. As can be seen, organizational goals are also dynamic and can evolve via the contributions of the community members. Accordingly, this targets individuals’ intrinsic motivation for knowledge sharing by giving them the feeling of being competent in contributing to the organization’s goals and objectives. 
 3. CONCLUSIONS.
 In this design briefing, we demonstrated the analytics-based features of Learn-B, which are built on Semantic technologies and Linked Data paradigm, and backed with an extended pedagogical and motivational framework. In our empirical work with Learn-B, we aim to answer if and to what extend these features, along with the other functionalities provided within Learn-B, address the existing challenges in supporting workplace learning. In particular, to support users’ self-regulatory practices in the context of workplace learning, we hypothesize that the usage information analytics accompanying each learning resource in Learn-B (i.e. statistics, Social Waves and the collective stand) assist users in planning their learning goals; Progress-o-meters, on the other hand, provide users with feedback on their progress flow and thus help them with monitoring and evaluating their learning progress. Knowledge Sharing Profiles inform users of their reflections, in terms of sharing their learning resources and experiences within an organization. Accordingly, we propose that these profiles support users to align their reflections and sharing of their learning resources and experiences. Motivational Messages are another analytics-based means designed to foster users’ contributions and to motivate them to provide higher quality inputs to the system. 
 4. ACKNOWLEDGMENTS.
 This demonstration was partially supported/co-funded by NSERC, Athabasca University, and the European Community under the Information and Communication Technologies theme of the 7th Framework Program for R&D. This document does not represent the opinion of NSERC, Athabasca University, and the European Community, and NSERC, Athabasca University, and the European Community are not responsible for any use that might be made of its content.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Learn-B: A Social Analytics-enabled Tool for Self-regulated Workplace Learning</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>workplace learning</dc:subject>
		<dc:subject>self-regulated learning</dc:subject>
		<dc:subject>collaborative learning</dc:subject>
		<dc:subject>semantic technologies</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/melody-siadaty"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/melody-siadaty"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dragan-gasevic"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dragan-gasevic"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jelena-jovanovic"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jelena-jovanovic"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nikola-milikic"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nikola-milikic"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/zoran-jeremic"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/zoran-jeremic"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/liaqat-ali"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/liaqat-ali"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksandar-giljanovic"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksandar-giljanovic"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/marek-hatala"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/marek-hatala"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/melody-siadaty"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/dragan-gasevic"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/jelena-jovanovic"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/nikola-milikic"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/zoran-jeremic"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/liaqat-ali"/>
		<rdf:_7 rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksandar-giljanovic"/>
		<rdf:_8 rdf:resource="http://data.linkededucation.org/resource/lak/person/marek-hatala"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/4">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Probability estimation and a competence model for rule based e-tutoring systems</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/4/authorlist"/>
		<swrc:abstract>In this paper, we present a student model for rule based e-tutoring systems. This model describes both properties of rewrite rules (diﬃculty and discriminativity) and of students (start competence and learning speed). The model is an extension of the two-parameter logistic ogive function of Item Response Theory. We show that the model can be applied even to relatively small datasets. We gather data from students working on problems in the logic domain, and show that the model estimates of rule diﬃculty correspond well to expert opinions. We also show that the estimated start competence corresponds well to our expectations based on the previous experience of the students in the logic domain. We point out that this model can be used to inform students about their competence and learning, and teachers about the students and the diﬃculty and discriminativity of the rules.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Students of natural sciences learn to solve various types of standard problems. Many of these problems are solved by rewriting some kind of formula or expression, step by step, until the problem is solved. Each problem domain (e.g. algebra, matrix calculus, or logic) has its own set of rewrite rules. Students start practicing these types of problems early in their school careers: already at primary school they learn how to calculate with fractions. Learning to solve problems using rewriting is often a time-consuming and labor-intensive process, both for students and teachers. Students have to practice a lot, and often require a lot of feedback, which a teacher may not always be able to give immediately. E-tutoring systems can be used to alleviate this situation by providing feedback automatically. In recent years the Open University the Netherlands has developed an e-tutoring system based on rules and strategies: the ideas framework1 . Heeren et al. [?] show how to use rewrite rules and strategies in e-tutoring systems to provide feedback. Feedback can be provided for every (correct or incorrect) application of a rewrite rule, but not about how diﬃcult and discriminative rules are, how competent students are when they start working, and how fast they learn. In this paper we answer the question: How can we describe student behavior in a rule based e-tutoring system with a student model, and estimate the probability of a student applying a rule correctly, the next time he/she tries to apply it? The model describes the behavior of the students (whether or not a student applies the rewrite rules correctly) in relation to the rules. Therefore, it includes the properties of a rewrite rule, as well as properties of a student. Previous research on student models in the context of e-tutoring systems suggests that Item Response Theory (IRT) is a good starting point. Johns et al. [?] apply IRT to an e-tutoring system that oﬀers multiple choice questions for mathematics tests. However, multiple choice questions are single static questions, while we want to model recurring rewrite rules. We therefore have to adjust the model to incorporate rewrite rules, and to allow a student to apply the same rule multiple times. Cen et al. [?]present a student model based on logistic regression, as we will do, but with more parameters than we use. They use a simplifying assumption that students learn at the same rate, which is of course invalid in general. This paper is organised as follows. Section ?? discusses the method we use in our research. Section ?? presents our student model, and shows the results of testing this model by simulation and using real life data. Section ?? concludes and discusses what needs to be done to further evaluate this model, and how the model can be used in practice. 
 2. METHODS.
 This research consists of two phases: a construction phase, in which we create a student model and test it on simulated data, and a validation phase in which we use real-life data from students working on an e-tutoring system to learn the model parameters and compare these learned parameters to expert opinions. To create a student model for rule based e-tutoring systems we use the IRT framework, and more speciﬁcally the two parameter logistic ogive function (2PL), as a basis. One reason why we select 2PL, is because it is a well-established model in psychology and education (e.g. used to validate tests). An even more signiﬁcant reason is that it operationalizes the desired properties both for rules (items in IRT) and student: diﬃculty and discriminativity, and competence. It relates these properties to the probability of a correct application of a rule (a positive response in IRT) [?]. We have investigated four alternative models: 1. The start competence per student per rule may be different, as well as the learning speed. 2. The start competence per rule may be diﬀerent, but the learning speed is always the same for one student. 3. The start competence is the same for all rules for a student, but the learning speed may vary. 4. The start competence and the learning speed do not depend on the rule, but are properties of a student only. Our domain experts discard option 3, because the learning speed is more likely to be a constant parameter over all rules for a single student than the start competence. We test the other options using simulated data. We run several simulations to discover how the model and the learning algorithms we use (see next section) behave in terms of: approximating the true parameter value, variance in the estimated parameters, sensitivity to the amount of data (number of rules, number of students, number of instances per rule per student), and the accuracy with which the outcomes (applying a rule either correctly or incorrectly) are predicted. For validating the model we collect data from students using a rule- and strategy-based e-tutoring system to learn to rewrite boolean expressions into disjunctive normal form (DNF). This subject is often taught in Computer Science programmes, and we have a number of domain experts available at the two universities where we collect data. We collect data at Utrecht University (UU) and Utrecht University of Applied Sciences (HU). We ask the domain experts to rank the rewrite rules necessary for the task twice: once for diﬃculty and once for discriminativity. We compare these rankings to the rankings based on the estimated values for the parameters from the student data returned by our learning algorithm. The rewrite rules for this domain can be found in Table ??. We also compare what we know about the previous knowledge of the students to the output values of the starting competence parameter of the student model. There are three groups of student participants: 4 UU students with recent training in solving DNF problems, which we presume to have most previous knowledge; 5 HU second-year technical computer science students, who have had less training in manipulating logic expressions, but do have one and a half years of programming experience; and 5 HU ﬁrst-year business informatics students, who have had little training in manipulating logical expressions and little programming experience, who we assume to have least previous knowledge. 
 3. RESULTS.
 The results section is divided into two subsections: construction and validation. In the construction subsection we describe how the model is constructed, and how it performs on simulated data. In the validation subsection we show the results of applying the model to real data.2 
 3.1 Construction.
 As a starting point for model construction we choose the 2PL IRT model. 2PL relates the probability of a correct answer for a number of items (e.g. problems on a test) for a number of students, to the latent variables of student competence and item diﬃculty and discriminativity, through the following probability function: 
 (FORMULA_1).
 where the binary outcome oi,j is the outcome for student i attempting item j, aj and bj are the discriminativity and diﬃculty of item j, and θi is the competence of student i. The data is a binary matrix, containing the outcomes for each student for each exercise. The parameter values can be learned through an iterative scheme, alternating between optimizing the likelihood by changing aj and bj , and optimizing the likelihood by changing θi , using gradient descent [?]. The 2PL model assumes that the competence is constant for each student. The data we gather from an e-tutoring system however, is diﬀerent from a binary matrix. A student does several exercises in which each rule can be attempted several times, with the purpose to increase his or her competence. We obtain data in the form of a sequence of tuples of student ID, rule ID and the outcome (0 or 1). We therefore know how many times a student has attempted a rule before when (s)he applies a rule. We extend 2PL, by adding a learning parameter. The current competence for a rule for a student is now the start competence plus the learning parameter times the number of times the rule has been attempted before. As described in the methods section we have to choose whether the starting competence and learning speed are parameters of the student, or whether they can also vary per rule. After testing the diﬀerent models, we conclude that the variance in the learned parameters is too high to be used for prediction or to be informative to teachers and students when they depended on both the student and the rule. We therefore let the starting competence and the learning parameters be attributes of a student only. The following probability function describes our model: 
 (FORMULA_2).
 where r is the rule ID, s the student ID, θ0,s is the starting competence of the student, ηs the learning speed of the student, and tr,s the number of times student s attempted rule r before. We run several simulations using this model. For n students, the starting competence θ0,s is chosen randomly from a uniform distribution between −3 and 1, and the learning parameter ηs is chosen randomly from a uniform distribution between 0 and 0.5. For m rules the parameter values are also drawn from uniform distributions, for ar between 0.8 and 1.5 and for bj between −3 and 3. We choose a value tmax for the number of outcomes per student per rule. Using the randomly chosen rule and student parameter values we simulate data, by drawing outcomes stochastically, with the probability of a positive outcome given by equation (??). Using the simulated data we learn the parameters back: we generate parameter values, with these generated parameters as the original parameters we generate outcomes, and (ignoring the original parameters) we estimate the parameters using the data. This process we call parameter recovery. When recovering the parameters for small data sets we cannot use gradient descent. Gradient descent cannot be applied when there is linear separability of the data, or when there are either no positive or no negative outcomes for a rule. The probability that one of these issues occurs is high when the dataset is small. We therefore replace gradient descent by attempting a discrete set of parameter values (with intervals of 0.01) and determine which combination of item and student parameters yields the highest likelihood. When we use parameter recovery simulations we ﬁnd that our model and learning algorithms are unbiased by calculating the average diﬀerence between the original randomly chosen parameter values, and the recovered parameter values (which should be close to 0). We can also show that the learning is reliable by calculating the mean absolute difference between the original and the recovered parameter values. For a simulation with 50 students, 25 rules, and 20 instances per rule, the average diﬀerence between the recovered and the original parameter values is around 1% of the parameter value range. The mean absolute diﬀerences between the original and recovered parameters are: 0.015 for ηs , 0.15 for θ0,s , 0.07 for ar , and 0.09 for br . These “errors” are acceptable. The simulation closest to the real data obtained is 15 students, 23 rules, and 8 instances per rule. Here the mean absolute diﬀerences between the original and recovered parameters are: 0.09 for ηs , 0.38 for θ0,s , 0.21 for ar , and 0.34 for br . For θ0,s and br , these errors are still acceptable, but for ηs it is almost one ﬁfth of the parameter range, and for ar it is two ﬁfth of the range. We therefore conclude that the parameter value estimates of ηs and ar are unreliable for this data size. The average accuracy of predicting the outcomes is determined as follows. We have original and recovered parameter values. We use the original rule parameters and new randomly drawn student parameters to generate more data. The data comes in as a stream of outcomes. At each point in time a random rule is selected, a prediction is made whether this outcome will be positive or negative, and then an outcome is generated with the right probabilities (based on equation (??)). After each outcome the estimated student parameters are adjusted. Using all data until time t we make a prediction (0 or 1) for the outcome at time t + 1. The accuracy is the number of correct predictions. We start with adding an outcome to all rules before starting prediction. The accuracy for diﬀerent values of the student parameters (θ0,s and ηs ) is between 0.75 and 0.80. 
 3.2 Validation.
 To validate our model against reality, we use data from students working on solving problems in the domain of boolean algebra. As mentioned before there are 3 diﬀerent groups of students. We estimate the parameters of the model on the basis of this data. The program produces the diﬃculty and discriminativity of the rules, and the starting competence and the learning parameter of the students as output. As we know from our simulations, the parameter estimates for ηs and ar are unreliable. We can therefore only draw conclusions for the parameter value estimates of br and θ0,s . The estimates for starting competence can be compared to what we know about the diﬀerent student groups. We expect the UU students to have most previous experience, and therefore the highest start competence. The HU business informatics students (HU-BI) are expected to have the lowest start competence, and the HU technical computer science students (HU-TCS) are expected to be in between. The means and median competences learned from data are: UU mean 1.27 and median 1.36, HU-TCS mean −0.19 and median −0.50, and HU-BI mean −0.90 and median −1.20. This conﬁrms our hypothesis. The domain experts rank the items by putting them in their perceived order of diﬃculty. In Figure ?? these rankings are shown together with the ranking calculated from the diﬃculties estimated from data. We observe that the expert rankings look similar to each other, and to the ranking learned from data. The main “surprise” is rule number 10: rewriting something or true to true. We use Spearman’s rank correlation on the diﬀerent rankings. The correlations between expert rankings 1, 2 and 3, and the ranking estimated from data can be found in table ??. The estimated model parameters of diﬃculty and the diﬃculty rankings provided by experts are highly correlated. Also, the expert opinions do not deviate much more from each other than from the estimated ranking. This means that the model makes a similar estimated ranking of the difﬁculty of the rules as experts do. The model produces a this similar ranking, even though the amount of data is small. Another important measure of the quality of the model is accuracy. We measure the accuracy by cross-validation. We set 1 student apart and estimate the rule parameters with the rest of the students. For the one student we set apart, we estimate the start competence and learning parameter after the ﬁrst 25 outcomes. Then, for each outcome that follows, we predict whether it will be positive or negative, and determine whether our prediction is accurate. After the prediction we update the estimates for the start competence and learning parameters, before we predict the next outcome. There are 13 students with enough outcomes to warrant this prediction procedure. The average of the total number of outcomes per rule for these students is 6.1. The average accuracy of prediction for the 13 students we predicted the outcomes for is 0.78. For students with mostly correct outcomes the accuracy is not much higher than the percentage of correct answers. Around a proportion of correct outcomes of 0.5, the algorithm performs better. A typical example is student 12, with 52% correct answers, for who the accuracy was 0.71. 
 Table 1: The rewrite rules that can be applied while performing the DNF task, and the estimated diﬃculty rankings of these rules by experts 1 (Utrecht University of Applied Sciences), 2 (Utrecht University of Applied Sciences) and 3 (Utrecht University) and by maximum likelihood estimation. 
 Table 2: The Spearman’s rank correlations between experts 1, 2 and 3, and ranking based on the estimated model parameters. 
 4. DISCUSSION.
 The purpose of this research is to create a student model for rule based e-tutoring systems. This model describes and predicts student behavior: whether or not a student applies a rule correctly. For this purpose, we have constructed a probability model based on 2PL IRT (equation ??). We have used simulations to prove that the parameters of the model can be reliably learned; when we generate the data from a certain set of parameters, these parameters can be adequately recovered. For a small data set of 50 students, 25 rules, and 20 instances per rule, we show the parameters can be estimated with small error. For a data set of 15 students, 23 rules, and 8 instances per rule however, the estimates for the discriminativity of a rule, and the learning parameter of a student become unreliable. The accuracy of prediction for this small simulation however, is still between 0.75 and 0.80. Using data from students working on rewriting logic expressions to DNF, we estimate the student and rule parameters from data obtained from 14 students, 23 rules and an average of 6.1 instances per rule. We compare the estimated diﬃculties of the rules to the rankings provided by domain experts, and show that the ranking based on the learning from data is as close to the rankings of the experts as the expert rankings are to each other, based on their Spearman’s correlations. We show that the initial competence, θ0,s , of the students as learned from data, is what we expect based on what we know about the diﬀerent groups of students who participated. We cannot draw any conclusions about the result values of rule discriminativity (ar ) or student learning speed (ηs ) because the real-life dataset is too small. We know from simulations however, that all model parameters are recoverable. To have reliable estimates for all parameters, the dataset should be obtained from around 50 students who apply each rule around 20 times. We selected the domain of rewriting logic expressions because it is relatively well-known and often taught. In less well-known domains the model could be used to inform the teacher about the rule diﬃculty and, as hopefully further research will show, discriminativity. The model can also be used to report the competence to the students working on a rule based e-tutoring system. We conclude that extended IRT is a promising model, which has the potential to provide more information to the users of rule based e-tutoring systems. More extensive tests with real-life data are required, but the results we obtained for the logic domain are promising.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Probability estimation and a competence model for rule based e-tutoring systems</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>student model</dc:subject>
		<dc:subject>data mining</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/diederik-m-roijers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/diederik-m-roijers"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/johan-jeuring"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/johan-jeuring"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ad-feelders"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ad-feelders"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/4/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/diederik-m-roijers"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/johan-jeuring"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/ad-feelders"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/5">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Cyberlearners and Learning Resources</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/5/authorlist"/>
		<swrc:abstract>The discovery of community structure in real world networks has transformed the way we explore large systems. We propose a visual method to extract communities of cyberlearners1 in a large interconnected network consisting of cyberlearners and learning resources. The method used is heuristic and is based on visual clustering and a modularity measure. Each cluster of users is considered as a subset of the community of learners sharing a similar domain of interest. Accordingly, a recommender system is proposed to predict and recommend learning resources to cyberlearners within the same community. Experiments on real, dynamic data reveal the structure of community in the network. Our approach used the optimal discovered structure based on the modularity value to design a recommender system.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Nearly 20 years ago, it seemed that the Worldwide Web (WWW) played a considerable role in facilitating the way people share information. Today, it is obvious that the Web is not only about sharing information, but it is a place where people create, share, interact and learn. If we view the Web purely, like a natural phenomenon, and if we study, say, the behaviors of learners using the social media networks to learn how people learn, this could answer fundamental questions about human behaviors and the impact of the Internet on the social process. Using analytics to discover hidden patterns in big data started a long time ago in ﬁelds such as business intelligence and predictive marketing. Nevertheless, recently, big organizations, such as EDUCAUSE2 and the Bill & Melinda Gates Foundation3 , are playing an essential role in bridging higher education and data analytics [17]. 
 2. BACKGROUND AND RELATED WORK.
 The term Learning Analytics occurred the ﬁrst time in Berk [3], but it was more related to business intelligence as stated in Shum et al.[22]. Noticeably, over the last two years, more research related to learning analytics occurred. The following summary does not exhaust the vast amount of research done in this area, but it gives a short summary. For example, Gaˇeviˇ et al.[11] experimented with LOCOs c Analyst 4 on two master’s level courses for learning analytics with feedback; their goal was to discover the semantic knowledge based on community and accordingly update the learning process of this community. Johnson et al.[14] mentioned some learning analytics tools that have emerged in higher education: i) Northern Arizona University uses an academic early alert and retention system5 to improve students’ retention and success; ii) Purdue University utilizes Signals 6 , an analytical data mining tool that identify students who need help; iii) Ball State University designed a visualized collaborative writing system to help better evaluate student performance; iv) the University of Wollongong in Australia uses SNAPP 7 to visualize data from discussion boards to ﬁnd patterns in students’ behaviors. The last example is the closest to our system. The similarity between both systems is twofold: goal and method. The goal is to collect data about students to better understand their behaviors and accordingly provide a better understanding on how to help these students. There is a major diﬀerence, however between our approach and theirs; in our system this help comes as a recommendation based on similar a community of learners. We consider the similarity in both context and in communities in social networks; whereas, SNAPP is based only on context. Both systems use visualization as a method to discover patterns. As we know, our cognitive system is not able to deal with vast amounts of information [6]. The importance of communities in social networks has long been recognized. In 1999, Kleinberg et al.[16] used the concept of a bipartite core to identify Web communities. Flake et al.[8] introduced one of the most attractive deﬁnitions for a community, both because of its intuitive appeal and its computational simplicity. There, a community is deﬁned as a set of web pages in which each member page has more links (in either direction) inside the community than outside. The exact proportion of inside to outside links can be varied as required. Girvan and Newman [12, 19] devised a method for community determination based on betweenness centrality by generalizing it to edges and ﬁnding communities by deleting edges from the network in order of decreasing betweenness (and recalculating the betweenness between deletions). Modularity, introduced in Newman and Girvan [20], has become a standard method for measuring the success of community decomposition in a network. This measure was turned into a fast and eﬀective community identiﬁcation mechanism by using a greedy algorithm to approximately optimize the modularity values. Even this fast algorithm was improved in Clauset et al.[5]. For a recent and much more comprehensive survey see Fortunato [9]. 
 3. METHODOLOGY.
 3.1 Data Collection.
 The data set used in this study is part of HyperManyMedia’s (HMM) Logﬁle. We extracted and used only the last six months from the Logﬁle. In our previous research Zhuhadar et al.[23], we found that i) the proﬁles of our users are evolving (users’ interests change over time; i.e., a user might register for courses in chemistry, but after three months, the same users switch to courses in biology) and ii) our platform is an evolving domain (new courses are added to the platform each semester). Accordingly, we provide recommendations based on this dynamic change of students’ interests. Also, we argue that building a dynamic recommender system based on a social network needs to be scalable to accommodate current and new users. If we considered using the whole Logﬁle which consists of activities of 750,000 users so far, the time needed to extract recommendations from the best candidates in the Logﬁle, on the ﬂy, would be impractical. The data set used in this research consists of users’ logs during the following period (2/1/20118/1/2011). Each entry has the following ﬁelds: user name, visited resource, number of visits. The number of visits is used as a Weighted Degree in the graph. The more the user (learner) visited a learning resource, the closer the learner is considered to the hub (learning resource); therefore, users who are close to hubs are considered as authorities in that speciﬁc domain. Our assumption is built upon the theory of reinforced learning. A very old concept that was introduced in 1913, by Ebbinghaus, in [7]. Ebbinghaus found that if learners are introduced to a problem over many trials, an exponential learning curve is produced. Finally, we visualized our Logﬁle using a graph analysis tool called Gephi [2]. 
 3.2 Evaluation Method (Categorization Criteria and Determining the Energy Levels) 
 As we discussed in section 2, there is a variety of measures and methods for ﬁnding communities in social networks. In this research, a modiﬁed version of the modularity measure proposed by Blondel et al. [4] is utilized to compare the quality of clusters (Equation 1) for measuring the success of a community decomposition of a network. This measure is considered fast and eﬀective to identify communities by using a greedy algorithm to approximately optimize the modularity values. 
 (FORMULA_1).
 where Aij represents the weight of the edge between i and j, ki = j Aij is the sum of the weights of the edges attached to vertex i, ci is the community to which vertex i is assigned, the d-function d(u, v) is 1 if u = v and 0 otherwise and m = ij Aij . 
 3.3 Detecting Communities.
 In this section, we identify the methods used to detect communities of similar users from our extracted Logﬁle. First, we deﬁned a set of various force laws to recognize communities in the network structure. Once the communities have been recognized, we ensured that each community had its own energy state which determines the relevance level of that particular community within its range of proximity. We used force directed methods to discover the similarity between users. Three types of methods were used: (i) the Yifan Hu Algorithm [13], (ii) the Fruchterman and Reingold Algorithm [10], and ﬁnally, the Force Atlas 2 Algorithm [18]. 
 4. EXPERIMENTAL ANALYSIS.
 We deployed the three algorithms (i) Yi Fan Hu Algorithm, (ii) Fruchterman and Reingold Algorithm, and (iii) Force Atlas 2 Algorithm on HyperManyMedia’s Logﬁle extracted during the period of (2/1/2011- 8/1/2011). After ﬁltering out some data based on the conditions (user’s visits>= 10 & length of accumulated sessions>= 30 minutes) and deleting outliers, our network consisted of 8, 510 Nodes (# of users) and 23, 079 Edges (# of edges between users and learning resources). Each edge connects a user (learner) to a learning resource. In this small portion of the Logﬁle, the number of learning resources is ∼ 10, 000 learning objects. First, we noticed that the Yifan Hu Algorithm [10] proved to be eﬃcient. It seems to overcome the localized nature of the Kernighan-Lin Algorithm [15] and also the local minima of the Fruchterman and Reingold’s Algorithm [10]. We also deployed the Force Atlas 2 Algorithm, calculated the modularity for each force directed method, and visualized the network. 
 4.1 Force Directed Method (Fruchterman Reingold Algorithm).
 We used three parameters in the Fruchterman Reingold’s [10] force directed method: (i) Area (which deﬁnes the number of nodes in the graph); (ii) Gravity (it works to attract all nodes to the center to avoid dispersion of disconnected components); and (iii) Speed (convergence speed). We ran 20 trials and the best results obtained in trial 6 had a modularity of 0.606 and number of communities (clusters) of 14. Accordingly, we present the social network structure in Figure 1. 
 Figure 1: Social network structure for HMM’s Logﬁle (Fruchterman Reingold). 
 Figure 2: Social network structure (Force Atlas 2). 
 Figure 3: Social network structure (Yi-fan Hu). 
 
 4.2 Force Directed Method (Force Atlas 2 Algorithm).
 The Force Atlas 2 Algorithm [18] uses a classic forcevector, similar to the Fruchterman Rheingold. This algorithm beneﬁts from Barnes-Hut optimization techniques [1] and its own repulsive and tolerance levels [1]. We ran 20 trials and the best results obtained in trial 18 had a modularity of 0.610 and number of communities (clusters) of 14. Accordingly, we present the the social network structure in Figure 2. We noticed that we got the best results in this method when there is (i) a little repulsive force given by Scaling and (ii) a higher attractive force given by Gravity. 
 4.3 Force Directed Method (Yi fan Hu Algorithm).
 The Yifan Hu Algorithm [13] overcomes local minima by using Barnes and Hut’s [1] octree technique which approximates the short-and-long range force eﬃciently. It uses the adaptive cooling schemes and general repulsive force models to develop the set of forces to be applied on the data set for formation of the communities. We ran 20 trials and the best results obtained in trial 12, had a modularity of 0.607 and the number of communities (clusters) of 15. Accordingly, we present the the social network structure in Figure 3. 
 4.4 Discovering the Best Communities Structure in HMM’s Logﬁle Social Network 
 To summarize, by running multi-trials, we discovered the best combinations of parameters for each algorithm, as shown in Table 1. However, we noticed a slight diﬀerence in the results that could be inferred as follows: We found more clusters using the Yi fan Hu Algorithm; whereas, we obtained a better modularity measure using the Force Atlas 2 algorithm. Therefore, we decided to use the Force Atlas 2 algorithm for clustering. Figure 2 presents the notion of detecting communities of users in the Social Web. 
 Table 1: Evaluation of the three Force-directed methods. 
 4.5 Designing a Social Recommender System.
 We considered that providing recommendations to a learner based on similarity metrics between the users and him/her and extracted from the social network would answer this question. We propose adding a social recommender system to HMM repository where recommendations provided to a user (learner) is based on detecting triangles in the community [21], refer to Figure . 
 Figure 4: Adding personalized recommendations to a user proﬁle based on three users (Triangle). 
 5. CONCLUSIONS.
 We consider our research is diﬀerent than any other research that has been done on detecting community using graph-based methods for the following reasons: i) our research is an applied study on a real platform visited by thousands of users on a daily basis; ii) we used data collected from HyperManyMedia’s Logﬁle to discover communities in social networks; iii) ﬁnally, we proposed the triads concept for recommendations, keeping in mind that our current experiments are based on triangles of nodes. In our future work, we plan to experiment and compare the results based on learners’ feedback. In addition, we plan to complete our evaluation of the visual recommender system, using objective metrics as well as user testing.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Cyberlearners and Learning Resources</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>e-learning</dc:subject>
		<dc:subject>social web</dc:subject>
		<dc:subject>social network analysis</dc:subject>
		<dc:subject>visual analytics</dc:subject>
		<dc:subject>modularity</dc:subject>
		<dc:subject>social recommender system</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/leyla-zhuhadar"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/leyla-zhuhadar"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rong-yang"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rong-yang"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/5/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/leyla-zhuhadar"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/rong-yang"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Using agglomerative hierarchical clustering to model learner participation profiles in online discussion forums</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6/authorlist"/>
		<swrc:abstract>Online discussion forums are a key element in virtual learning environments. The way learners participate in discussion boards can be a very useful source of indicators for teachers to facilitate their tasks. The use of a two-stage analysis strategy based on an agglomerative hierarchical clustering algorithm is proposed in this paper to identify different participation profiles adopted by learners in online discussion forums. Different parameters are used to characterize learners’ activity (amount of posts, rhythm, depth of threads, crossed replies, etc). Participation profiles are identified and analyzed in terms of behavior and performance.</swrc:abstract>
		<led:body><![CDATA[ 


 1. INTRODUCTION.
 Online discussion forums (or boards) are one of the most common tools in web-based teaching-learning environments. Online learner participation has been defined as a complex and intrinsic part of online learning [6]. In fact, a high level of interaction is desirable and increases the effectiveness of distance education courses [5]. Thus, discussion boards can be a relevant source of information in order to provide teachers with useful indicators of learners’ activity and to facilitate their monitoring, guidance and feedback tasks. The purpose of the present work is to present a two-stage analysis strategy in order to model and identify learners’ participation profiles in online discussion forums. Since clustering learners has proved to be a proper way to find similar learning behaviors [11], learners with similar activity patterns are clustered together in the first stage and resultant clusters are combined in the second stage to identify participation profiles. This paper is structured as follows. The working framework is introduced in Section 2; the clustering algorithm used in the experiments is proposed in Section 3; the data set is described in Section 4; the modeling strategy to identify participation profiles and the obtained results are shown in Section 5; and, finally, conclusions and future work are presented in Section 6. 
 2. WORKING FRAMEWORK.
 Relevant contributions can be found in literature on modeling learner behavior in online asynchronous environments. [1] deals with identification of lurkers (in a discussion board, a lurker is the one who reads but never writes). This kind of behavior makes impossible a visible and active interaction both with other learners and teacher in virtual environments. In order to investigate lurking, [8] carried out a study on lurking using indepth semi-structured interviews with members of online groups. The analysis reveals that lurking is a strategic activity involving more than just reading posts and a model to explain lurker behavior is proposed. Finally, three significant participation patterns in accessing and contributing to an online discussion board are defined in [10]: workers (proactive participants that are continuously involved in discussions), lurkers (peripheral participants that regularly access to the board and participate in the discussions in read-only mode) and shirkers (parsimonious participants that barely access to the board). A different approach is provided by social network analysis techniques, in order to show interactions between learners in online discussion threads (learner network) and evaluate their participation [9]. Moreover, a great diversity of indicators (depth of threads, rhythm, reciprocal readings, cross replies, etc.) is used from this approach in order to define effective interaction models capable of giving an immediate picture of the effectiveness level of a collaborative group [2]. Finally, interesting contributions on participation profiles in discussion boards of general topics –not strictly educational– can be found as well. Several online forums of different topics are classified in [3] regarding their predominant user roles rather than their topics. Eight different user roles (popular initiators, popular participants, joining conversationalists, supporters, taciturns, grunts, elitists and ignored) are identified through an analysis method based on PCA (the most dominant feature in the largest component is selected in order to define three bands of users and discard the lowest and middle ones –marginal participation profiles–) and agglomerative hierarchical clustering (the optimal number of clusters is selected after an inspection of the solutions provided by different validation techniques). 
 3. CLUSTERING ALGORITHM.
 The modeling strategy in the present work is based on identifying participation profiles from the different activity patterns conducted by learners in online discussion forums. In order to group learners with similar activity patterns together, a clustering algorithm is used [11]. Due to the number of relevant patterns (i.e., the number of relevant clusters) is a priori unknown, we use an agglomerative hierarchical clustering algorithm [3]. The outcome of an agglomerative hierarchical clustering algorithm is not a data partition, but a dendrogram-type graph [7]. A dendrogram is a hierarchical tree structure formed by links that join couples of clusters together in a new cluster (i.e., each link defines a possible cluster of data) from the beginning (singleton clusters –i.e., one cluster per learner–) to the end (a unique cluster including the whole set of data –i.e., all learners grouped together–) of the tree. The heights of the links correspond to the distance between the couple of clusters joined as a new cluster under the link. The similarity measure between clusters depends on the linkage function defined in the algorithm: the Single Link (nearest neighbor) and Complete Link (furthest neighbor) are the most popular linkage functions [7]. A dendrogram is a useful tool for both visually exploring similarities between data (data exploratory analysis) and obtaining data partitions (clusters of data). Classical strategies to get a data partition consist in cutting the dendrogram at any defined threshold height and dismiss the links above the cut [7]. More interesting is to evaluate links in terms of their inconsistency instead of their height and define a threshold inconsistency in order to dismiss the most inconsistent links [12]. Finally, more versatile strategies try to isolate clusters separately as the dendrogram grows, for the sake of flexibility and to be able to detect both sparse and dense clusters [4]. The agglomerative hierarchical clustering algorithm used in this paper combines the strategy of isolating clusters separately (instead of getting a final data partition in one go by a single cut in the dendrogram) with a modified version of the inconsistency criterion defined in [12]. Our algorithm builds the whole dendrogram and isolates its best cluster in terms of a consistency criterion (the best cluster is the one defined by the most consistent link). Once the best cluster is isolated, this process is iterated until there is no remaining data to be isolated. Thus, taking the gap concept (height increment between consecutive links) proposed in [4], we define: 
 (FORMULA_1).
 as the consistency of the i-th link in the dendrogram, being:.
 (FORMULA_2).
 where gapi is the gap above i-th link, µ i and σ i are the mean and the standard deviation of the population formed by gapi and the gaps above all the links nested under the i-th link (zi is the standard score of gapi), ui is the set of standard scores of the gaps above all the links nested under the i-th link, NTOT is the total amount of elements (i.e., learners) in the data set, ni is the amount of elements within the cluster defined by the i-th link and k(ni) is an exponential correction applied to avoid isolating too small size clusters (k(ni) is less than γ when ni is less than the β % of NTOT). 
 4. DATA SET.
 The experiments conducted in this paper analyze the activity carried out by learners within the online discussion forums of three different subjects in a virtual Telecommunications Degree (Electronic Circuits, Linear Systems Theory and Mathematics) and throughout three complete semesters (from February 2009 to July 2010). All the courses took place in an asynchronous web-based teaching-learning environment and the participation of learners in discussion boards was not mandatory, but strongly recommended. Thus, the whole dataset involves a total amount of 672 learners (NTOT) distributed in eighteen different virtual classrooms and a total amount of 3842 posts. Total withdrawal and passing rates are 36.31% and 52.23%, respectively. 
 5. MODELING ACTIVITY AND FINDING PARTICIPATION PROFILES.
 The analysis strategy conducted in the present work consists of two main stages. In the first stage, learners’ activity in online discussion forums is characterized in two different domains (writing and reading) and learners with similar activity patterns are grouped together in each domain separately. The activity carried out by learners is differently characterized in each domain (different parameters are used depending on the domain). In writing domain, each learner is characterized according the following four parameters (all of them are ratios over learner’s specific virtual classroom and semester): ratio of threads –weighted by their respective depths– initiated by learner over total amount of threads –weighted, as well– (depth), ratio of reply posts written by learner over total amount of reply posts (reposts), ratio of learners replied –at least, once– by learner over total amount of learners (recross ) and ratio of days when learner writes at least one post over total amount of days (wrrhythm). 
 Figure 1. This figure shows the resulting dendrograms from clustering learners in terms of (a) writing and (b) reading. Each dendrogram legend indicates the assigned label to each cluster, which can be identified by its color. According to the criterion defined by the algorithm, resultant clusters are nested under the most consistent links in the dendrogram. Each cluster top height corresponds to the distance between the furthest learners within the cluster (Complete Link). 
 Table 1. This table shows how the participation profiles are identified. Combining the resultant clusters from the first stage of analysis, the final set of clusters is obtained (i.e., learners belonging to WRi and RDj clusters belong now to the new WRi–RDj cluster). In the table, final clusters are represented in rows, first column show the final clusters labels, N column (% over NTOT) indicates the amount of learners per cluster, Withdrawal and Passing columns (% over N) indicate withdrawal and passing rates at the end of semester, Average Representatives (Centroids) columns show each final cluster’s average representative (in terms of the eight different parameters used to characterize learners’ activity), and the last two columns describe the participation profiles represented by each final cluster (by identifying the different participation profiles proposed in [10]). 
 Other four different parameters are used in the reading domain (self-readings are excluded): ratio of posts read by learner over total amount of posts (rdposts), ratio of threads where learner reads at least one post over total amount of threads (rdthreads), ratio of learners read –at least, once– by learner over total amount of learners (rdcross) and ratio of days when learner read at least one post over total amount of days (rdrhythm). Learners are separately clustered in both domains by using the agglomerative hierarchical clustering algorithm described in Section 3 with the following configuration: Normalized Euclidean Distance, Complete Link, β = 10 and γ = 0.9 . Results obtained in both domains are shown in Figure 1. Finally, the second stage of the analysis strategy consists in grouping together those learners belonging to the same clusters in both writing and reading domains. Thus, the final set of clusters that completely defines the different activity patterns and allows to identify the participation profiles of learners in online discussion forums is obtained (see Table 1). Participation profiles are mapped to final clusters by observing and comparing the values of the parameters that characterize the learners’ activity patterns in each cluster. Final clusters’ centroids allow to confirm the suitability of this mapping and to describe and characterize the participation profiles in more detail. Some interesting remarks can be made from the obtained results. Regarding the agglomerative hierarchical clustering algorithm performance, it allows to find clusters of different size and density in both different domains (e.g., in Figure 1 (a), WR2 cluster is larger and denser than the smaller and sparser WR3). Participation profiles like the ones describe in [10] can be easily identified by observing final clusters’ centroids (see Table 1): shirkers (inactive learners) are grouped within WR1-RD1 and WR2-RD1 clusters (centroids with no kind, or negligible, activity at all); lurkers (only readers), within WR1-RD2/…/-RD5 clusters (centroids with no kind of reading activity and different patterns of writing activity); and workers (active learners), within WR2-RD2/…/-RD5 and WR3-RD4/-RD5 clusters (different patterns of both writing and reading activity). Furthermore, specific sub-profiles for lurkers (low- and mid-level lurkers) and workers (low-, mid- and high-level workers) have been defined depending on differences between centroids’ values of reading and writing parameters, respectively. Empty possible combinations (WR3-RD1/…/-RD3) are also useful to deduce some –pretty logical– conclusions: writing involves reading, but not the other way around (reading does not necessarily involve writing –lurking behavior–). Besides, differences between centroids’ values can be useful to identify other kinds of participation profiles as well (e.g., the different user roles proposed in [3]: popular initiators, popular participants, joining conversationalists, supporters, taciturns, elitists, grunts and ignored). Finally, some interesting conclusions regarding on performance differences between profiles can be pointed out: the withdrawal rates of shirkers and low-level lurkers are the top highest and the passing rates of high-level lurkers are comparable with the ones of high- and mid-level workers’ (which are logically the top highest). 
 6. CONCLUSIONS AND FUTURE WORK.
 In this paper, a two-stage strategy in order to model learner participation profiles in online discussion forums is proposed. The presented agglomerative hierarchical clustering algorithm successfully isolates the more relevant activity patterns in different domains (writing and reading). The obtained final clusters actually group learners with similar activity patterns and allow to satisfactorily identify different participation profiles in online discussion forums. In terms of future work, the number of domains in first analysis stage will be increased (rhythm domain, neighboring domain, etc.) and the impact of this increasing on both the presented clustering algorithm suitability and the identification of participation profiles accuracy will be checked.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Using agglomerative hierarchical clustering to model learner participation profiles in online discussion forums</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>educational data mining</dc:subject>
		<dc:subject>learner behavior modeling</dc:subject>
		<dc:subject>hierarchical clustering</dc:subject>
		<dc:subject>online discussion forums</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/german-cobo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/german-cobo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-monzo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-monzo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-melenchon"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-melenchon"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/german-cobo"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-monzo"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-melenchon"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/7">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Network Awareness Tool – Learning Analytics in the workplace: Detecting and Analyzing Informal Workplace Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/7/authorlist"/>
		<swrc:abstract>This paper aims to contribute to the understanding of informal workplace learning in contemporary face-to-face and virtual environments. Informal learning is an important driver for professional development and workplace learning. However powerful informal learning may be, there is a problem when it comes to making it a real asset within organizations: Informal learning activities are mostly invisible to others, sometimes the learners themselves might not even be aware of the learning that occurs. As a consequence informal learning in organizations goes undetected, remains off the radar of HR departments and is therefore hard to asses, manage and value [1]. This problem poses an interesting challenge for the field of Learning Analytics, namely finding ways to capture and analyse traces of (social) informal learning in every day life and work networks. Therefore empirical research and tools are needed that can raise awareness about informal learning activities to make it surface the radar, amplify the benefits of it and strengthen the social relations through which it occurs. In this paper we introduce a tool that aims to facilitate exactly this and we hope to stimulate to widen the discussion on Learning Analytics by expanding the field from a predominantly educational focus to informal and workplace learning. In this paper we will discuss methodologies that Learning Analytics can draw upon to make informal learning more explicit and accessible to analyse and to share amongst professionals.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Organizations, when thinking of teacher professional development, often rely on refreshment courses given by experts, in-service training, or personalised learning trajectories. These formal training opportunities provided for professionals represent just the tip of the iceberg when imagining all learning that takes place triggered by the challenges professionals face in their daily practice. The more spontaneous and informal ways of learning are largely overlooked in organisations and the effects of it remain therefore implicit. At the same time there is a large body of literature that convincingly shows that these forms of spontaneous work related learning are important drivers for ongoing professional development [2], [3], [4]. However powerful informal learning may be, there is a problem when it comes to making it a real asset within organizations: Informal learning activities are mostly invisible to others and sometimes the learners themselves might not even be aware of the learning that occurs. As a consequence informal learning in organizations goes undetected, remains off the radar of HR departments and is therefore hard to asses, manage and value [1]. This problem poses an interesting challenge for the field of Learning Analytics, namely finding ways to capture and analyze traces of (social) informal learning in every day life and work networks. Learning Analytics is about harvesting and analyzing information about learners, with a main focus on learners’ behavior in virtual environments, focusing on the collection of information about learners behavior in formal learning activities, like completing assignments and taking exams and in more informal settings like posts on discussion forums and online social interactions [5]. Extracting and evaluating patterns of participation and interaction through examination of online conversations is a major component in this area of work. To get a full insight of the informal learning activities of workplace learners, Learning Analytics needs to go much further then this [5], but we feel that this field holds a lot of promise in doing so. In this paper we will contribute to this development by discussing methodologies that Learning Analytics can draw upon to make informal learning more explicit and accessible to analyse and to share amongst professionals and other organizations. Our current research is aimed at the design of an awareness tool that will make explicit how professionals within and between organizations (and beyond) create network connections to populate the social space in which learning occurs. To do so we created a holistic methodology to capture and analyse informal learning activities in the workplace. In our research practice our target group are teaching professionals working in school organizations. The informal learning activities mostly take place face-to-face and in the schools themselves [6]. In the future we want to translate this methodology again to use it in the virtual world and replace the data provided by our target group with data mining technologies to automatically build up our dataset. Here we represent our methodology of building our Tool, and we would like to invite experts in the field to debate how we can transfer this methodology to the virtual world again and complement it with possible data mining solutions. 
 2. THEORETICAL BACKGROUND. 
 The Network Awareness Tool is founded on theories Networked Learning, Social Capital, Communities of Practice and Social Network Theory and Social Network Analysis. On the one hand these theoretical orientations provide a rationale for why such a tool is important when raising awareness about informal networked learning, on the other hand it provides crucial design elements such a tool should include to make it work. 
 2.1 Networked Learning Theory.
 In schools innovative teacher professional development involves opportunities for teachers to share their expertise, learn from peers, and collaborate on real-world projects [7]. This approach to learning embraces the participation metaphor [8], [9] where learning is seen as situated, embedded and maintained in the daily culture of (shared) practices and professional standards. A network in this sense can be regarded as a web of social relationships among teachers that reflects the flow of resources among them. People use their networks as a social infrastructure to gain access to what it is they are looking for. Key aspect in this paper is to use these network structures to study a relational approach to learning [10], where the emphasis is on the interaction between people. To know how people learn and create knowledge through these networks is very important, especially in the increasingly connected world of today. Understanding these networked processes helps to indicate areas of knowledge creation and problem solving and how this (new) expertise is being shared, but it also helps to identify networks of creativity and innovation within organizations. To study informal learning activities from this social network perspective we can build on Networked Learning Theory. Networked Learning Theory is an emerging perspective that tries to understand learning by asking the question how people develop and maintain a ‘web’ of social relations used for their own and reciprocal learning and professional development. Networked Learning is a form of informal learning situated in practice, where people rely strongly on their social contacts for assistance and development [11],[12]. Recent research has provided evidence linking Networked Learning to an array of positive outcomes like student performance and school improvement [13], [14], [15], [16]. Networked learning theory is useful for our analysis, moreover because it is closely linked to and uses methodologies of Social Network Theory. 
 2.2 Social Network Theory and Social Network Analysis.
 According to Moreno [17] Sociometric tests, the forerunner of Social Network Theory, show “in a dramatic and precise fashion that every group has beneath its superficial, tangible, visible, readable structure an underlying intangible, invisible, unofficial structure, but one which is more alive, real and dynamic than the other.” (p. 268). To investigate the dynamics of informal learning it is exactly this invisible and informal structure we want to bring to light. Social Network Theory asserts that the constitution of a network may influence the accessibility of information and resources and that the social structure may offer potential for the exchange of resources [18], [19], [20], [21], [22], [23], [24], [25]. Understanding the network structure can reveal important evidence on the information flow and shared knowledge within an organisation [26]. Teams with the same skill composition can act differently depending on the structure of relations within the team and similarly an individual can act differently depending on their position in a network. Therefore the teachers’ social environment will be looked at for explanations, rather than individual characteristics like in traditional social research [27]. Social Network Theory tries to explain both the antecedents and the consequences of social networks. Following Social Network Theory we can investigate for example if teachers with a central role in a network learn more from their colleagues (consequence) or investigate if teachers who learn a lot, get a central position in a network (antecedent). The structural dimension of a network can be investigated by using Social Network Analysis. According Social Network Analysis a network consists of nodes and ties. Nodes are the individual actors within a network and ties are the relationships between the actors. The impact of the structure of social networks can be studied on three levels: first the positions people have in a network (individual dimension), the relational level (ties dimension) and finally the overall network structure (network dimension). Frequently used network concepts in social science that Social Network Analysis draws upon to are: network brokers, network isolates, gatekeepers, actor centrality and structural holes [21]. 
 2.3 Social Capital.
 While Social Network Theory helps us to highlight the structural dimension informal networks, we use Social Capital Theory to frame Social Network studies from a ‘content’ perspective. Networks are always about something. Social Capital Theory provides a lens to look more closely at the relational resources embedded in social ties and how actors interact to gain access to these resources [24]. The first systematic analysis of social capital was produced by Bourdieu [28], who defined the concept as the aggregate of the actual or potential resources existing within the relationships of a durable network. According to Lin [29] the common denominator of all major Social Capital Theories is summarized as ‘’The resources embedded in social relations and social structure which can be mobilised when an actor wishes to increase the likelihood of success in purposive action’ (p. 24). This differentiates Social Capital from Human Capital which refers to the stock of competences, knowledge and personality attributes of the individual actors. Social Capital Theory asserts that a node’s position in a network determines in part the opportunities and constraints of the node, and in this way affects the nodes outcomes. 
 2.4 Communities of Practice: The social learning dimension from an individual and collective perspective 
 While the learning network perspective deals mostly with the personal aspirations, attitudes and strategies used for of learning in networks the collective advancement of knowledge and the development of shared identities comes together in the community aspect of social learning, which we base on the well known concept of communities of practice [30], referring to the development of a shared identity within a network of people and the collective development of a particular domain. A shared identity represents a collective intention, mostly related to a certain practice. Theories argue that networks, to be fruitful and active, need a shared framework of values and norms. Learning in communities is a process where both individual and collective learning goals and agenda’s are carefully and constantly being negotiated, around a topic or domain that is of interest to each participant. In this way communities enable the learners to develop a space for a shared activity in which their learning is situated. Here they connect ideas, share problems and insights in a constructive way, and connect with concepts with which they are already familiar, using new knowledge that is collaboratively constructed through their dialogues and social interactions. If professionals engage in networked learning from personal frameworks that are too different, they will be unable to understand the norms of each other. As a result, trust and reciprocity in the network are too low, and learning does not take place.’’ 
 2.5 Individual (demographic) characteristics influencing professionals’ informal learning 
 Although we mainly focus on network characteristics, the individual demographics are also important indicators and need to be taken into account. According to a study conducted by Moolenaar [31] findings indicate that differences relationships were associated with differences in gender, grade level, working hours, formal position, and experience. Age and years of experience can also have an impact on teachers’ professional development. Senior employees tend to take less initiatives in their professional development [32]. Hence to investigate the informal social activities within an organisation we need to investigate several dimensions: the actors and their attributes, the relations or ties between the actors, the content and resources within these ties and the overall network structure. Highlighting these dimensions of dynamic relationships helps us to understand how professionals engage in informal learning relationships and the value it produces[1]. 
 3. BUILDING A TOOL TO ANALYSE INFORMAL LEARNING.
 To gather and analyse the required data we needed to built a tool: The Network Awareness Tool [33]; This Tool serves different goals. First of all we built it to collect data from our target group, focusing on informal learning happening in the workplace. This differentiates the tool from a general learning analytics approach, because we do not use already existing data in an existing database. We would like to refer to our article submitted for Computers in Human Behavior, that focuses on the strategies describing how to collect the data. Here we will focus on the second goal of the tool: How will we analyse the data: The tool will be used to gain insight into the dimensions of informal networks in organizations (see figure 1). Third, the tool generates instant feedback to our target group to get insight into their own informal learning activities. The second goal (analysis) is be relevant for this conference, because we think this approach could help learning analytics to translate data mining technologies to a framed and theoretically based research methodology. 
 3.1 How the Network Awareness Tool works.
 In short the tool works as follows: Users can register and create a profile page, with a focus on their work and field of expertise. Within the learning networks page users can create a learning network based on a theme or subject users are exploring. Then users can add persons they interact with on this theme to the learning network. Users can also explore existing learning networks within their organisation and link themselves to a learning network they are active in within their organisation (see figure 2). The Network Awareness Tool can be used on different mobile devices like smart phones, android tabs and i-pads. Next to the mobile devices, the Network Awareness Tool can also be used on a desktop computer using a standard web browser. The data will be gathered through the REST protocol and gathered in a central MySQL database. 
 3.2 What data we collect.
 First of all we have the actors within the informal learning network so we want to collect the individual (demographic) characteristics. To gain in depth information about the individual participants we included a personal profile page for all the actors to fill in, including gender, age, the organisation(s) they work, their profession and role in the organisation and their specific areas of expertise. This profile can be updated at all times. Extra features can be added, if specific research data is required. After creating a profile, participants can create a network on a certain topic or join an existing network (by existing we mean here, already set-up by another user on our Network Awareness Tool). Within the network they indicate the people they interact with. We also gather data on the quality or nature of the ties. For each person that the participant indicates as a learning relation in other words a ‘’learning resource’’ there is the possibility to indicate the frequency of their interactions, and the quality of the learning interaction with that person. We add this feature in the tool by means of a rating system. Actors can rate the frequency of their contacts (1 = daily, 2 = weekly, 3 = monthly, 4 = less than monthly) using stars and rate the quality for learning: What is the value of this tie in relation to the topic of this learning network, using stars as well (ranging from a low quality (1 star) to high quality 5 stars). To gather data about the content of the ties we added a box where the participants can fill in meta-tags about the issues they are talking about and share information on. People can describe the content of their interactions, by filling in keywords in the tool. 
 3.3 How we analyse the data: Social Network Analysis and Semantic Analysis 
 To analyse the data we use the methodology based on the theoretical theories described in the theoretical background. We gather data on the components visualised in fig. 1. Through Social Network Analysis we can visualise the structural dimension of the learning networks. We used UCINET [27] to conduct Social Network Analysis. But we are building a feature to automatically visualize the data gathered in our database. Network analysis aims at finding out who is talking to whom with respect to a particular theme. This step visualizes existing informal networks in which people talk about the theme in question and shows the extent to which they are (or are not) connected throughout the entire organization. We analyze data on the three dimensions of the network: Individual level, Tie level and Network level. First we look at the ego-perspective. We start with calculating the out-degree centrality (number of ties that an actor directs to others) which indicates the extent to which an individual interacts with other members in the network [21]. The user profile with different attributes about the participant (like area’s of expertise) gives the opportunity to create informed network visualizations allowing the organization to see how professionals with a specific expertise are connected within the network, or how they are accessible by other colleagues f.e. Within organisations it is often interesting to see the gatekeepers between different locations or between different departments. This could be interesting to track the information flow within an organisation. To get more insight into the Tie level, we combine the data about frequency and quality to the social network analysis. In this respect we can investigate the role of strong and weak ties in a learning network. Combining data on the frequency and the quality can of course be very valuable. This data is also needed to draw the right conclusions about the learning outcomes of a learning network. If the network structure is very dense for example, but the frequency of all relations is very low, we can see this network is not so active. Therefore learning outputs can be low, although the network density is high f.e. The network data is used to measure the density of a network, the centrality of persons within a network, detect key persons and investigate the structure of the network [27] [20]. By calculating the density of the network, this is the proportion of ties within the network, we know how well the networked learning activities is distributed within the team. The centrality of a learning network is interesting to see if knowledge and learning is distributed over the whole network, or if it is centralized around a few people. The Network Awareness Tool will then automatically visualize the overall structure of the network to the users. Actors can not only see the list of people they indicated as being connected to, but they can also see a list of people within the network who mention him or her as a learning connection. A graphical representation of the ego- and overall network structure will be visualized reflecting the current state of the network. The visualisations and network data will be used to automatically compute SNA results about the density of a network, the centrality of persons within a network, the structure, cliques, etc, in real time or a specified period (to be able to go back in time to be able to study the network dynamics). With this methodology we can detect multiple (isolated) networks in the organization, connect ideas and foster collaboration beyond existing boundaries. Using the Network Awareness Tool organizations can link in with existing informal networks of practice and unlock their potential for organizational learning by giving them a voice and make their results more explicit within the organization. To incorporate the resources and the quality of the relations we will look at solutions in the field of semantic analysis. If we combine the tags of all the ties within a network we can investigate the overall content of the network, represented by a tag cloud of keywords, or another graphical presentation indicating the main topics the participants are learning about within their organizations. The use of tags can also make it possible to investigate if there is a shared language and possible a shared identity within a network and if the members are working towards a shared goal, tackle shared problems and address similar, different or compatible solutions. To gain more insight into this aspect we also included a quality rating on the level of the overall network. By asking all members of a particular network to rate the impact this network has on individual growth, developing their practice and the organizations as a whole, we can make explicit the informal learning potential of a certain network and see how this changes over time using repeated measurements. When presenting a visualization of the whole network, participants can see not only ‘who talks to whom’, but also ‘what they are talking about’ [34]. 
 4. CONCLUSION.
 As more and more people participate in multiple networks – learning by observation in some, and participating strongly in others – extending the scope of learning to include lean and rich engagement in social networks is becoming more important for understanding individual experience in a multi-dimensional, multi-membership, and multi-identity world. Informal Learning thus engages with a wider view of the influences and impacts on individual’s ideas and knowledge acquisition, a view that is synergistic with the greater availability of information and social contacts accompanying developments on the Internet in an increasingly networked society [35]. Building the Network Awareness Tool we not only assist learners in making their connected world more visible but will also assist strategic networked learning by providing insight in what possible networks to join. This tool emphasises the relational approach to learning through which we can gain further insight about who learns from whom, what they learn from each other, what kind of interaction happens between people who learn together, how frequently learning interactions happen over time. In our research on social professional development networks among teachers in and between schools, we find that working with these visualizations stimulates a networking attitude amongst teachers in the school towards learning. They become aware that they are not alone in their classroom and that professional development is also a social activity; one that is spontaneous and deeply connected to day-to-day challenges in the workplace. Another advantage of these visualizations is that they serve as very concrete artefacts for the teachers to help them reflect on how they act as networkers building a social space for informal learning. This research shows that the presented methodology is a useful research driven intervention tool to detect, connect and facilitate informal networked learning. With this methodology we can detect multiple (isolated) networks in the organization and connect ideas and stimulate participants to think of solutions to support their own professional development in certain domains. Using this approach, organizations can link in with existing informal networks of practice and unlock their potential for organizational learning by giving them a voice and make their results more explicit within the organization. 
 5. DISCUSSION.
 In the future we would like to apply this methodology to the virtual world and investigate online teacher professional environments like Open Education Resource groups, Online discussion for a and other initiatives. We believe that if we apply this methodology to the virtual world, we can collect even more data, including data form communication via email, online discussion, blogs commentaries, or twitter streams, as well as hyperlink analyses of connectivity across sites. Semantic analysis can be conducted for creating tag clouds dealing with the content of the networks. Social networks analysis can be applied to datasets who interacts with whom and who downloads resources, etc. If we use this set-up a more holistic and full story can be created about the online informal learning activities of people and organisations can therefore analyse their users and see how to support and encourage online communities to share and learn from each other. It is tools like these we believe that can extend the discussion on the application of Learning Analytics and this paper is an attempt to stimulate a discussion amongst researchers coming from Technology Enhanced Learning, Networked Learning, Data Mining, Artificial Intelligence and Learning Analytics about technological solutions and methodologies to gather and analyse relational data on learning to create a holistic view of peoples off- and online informal life long learning activities in education, work and society.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Network Awareness Tool – Learning Analytics in the workplace: Detecting and Analyzing Informal Workplace Learning</rdfs:label>
		<dc:subject>workplace learning</dc:subject>
		<dc:subject>social network analysis</dc:subject>
		<dc:subject>networked learning visualizations</dc:subject>
		<dc:subject>informal learning networks</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/schreurs-bieke"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/schreurs-bieke"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/de-laat-maarten"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/de-laat-maarten"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/7/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/schreurs-bieke"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/de-laat-maarten"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/8">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Monitoring Student Progress Through Their Written “Point of Originality”</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/8/authorlist"/>
		<swrc:abstract>This paper describes a new method for the objective evaluation of student work through the identiﬁcation of original content in writing assignments. Using WordNet as a lexical reference, this process allows instructors to track how key phrases are employed and evolve over the course of a student’s writing, and to automatically visualize the point at which the student’s language ﬁrst demonstrates original thought, phrased in their own, original words. The paper presents a case study where the analysis method was evaluated by analyzing co-blogging data from a reading and writing intensive undergraduate course. The evidence shows that the tool can be predictive of students’ writing in a manner that correlates with their progress in the course and engagement in the technology-mediated activity. By visualizing otherwise subjective information in a way that is objectively intelligible, the goal is to provide educators with the ability to monitor student investment in concepts from the course syllabus, and to extend or modify the boundaries of the syllabus in anticipation of pre-existing knowledge or trends in interest. A tool of this sort can be of value particularly in larger gateway courses, where the sheer size of the class makes the ongoing evaluation of student progress a daunting if not otherwise impossible task.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Today, it is not uncommon practice for faculty to deploy an online instructional environment in order to promote student conversations over course materials and ideally to heighten comprehension and retention of course readings. Co-blogging is an example of a technology-mediated learning activity of this sort. The “blogging” part requires students to explain course material in their own words. The “co-” part makes the students exchange ideas and interact regarding the course material by discussing the alternate viewpoints that emerge within the blogosphere [26]. One of the trends in the American university system over a number of years has been the migration towards larger so-called “gateway” courses [28]. These introductory courses are typically a student’s ﬁrst exposure to collegiate work. However, these large classes have a negative impact on the student’s learning process. Large lectures are useful for certain things, but less useful for fostering higher order thinking [31, 10]. Technology like co-blogging, however, can be used to make the class seem smaller, and provide the foundation for students to learn more and better. For most if not all learning activities, a substantial amount of an instructor’s time and eﬀort is devoted to evaluating and monitoring the quality of students’ work, and thus, hopefully, the depth of their learning [11]. The purpose of this monitoring, however, is not merely the determination of grades; part of the instructor’s work is entirely self-reﬂective, enabling the instructor to concurrently, or ideally even preemptively, intervene to make adjustments to course pedagogy based on students’ engagement or understanding [29]. While assigning grades might be facile, some diﬃculties complicate this second objective: how might an instructor intuit when, precisely, students have understood the material sufﬁciently? Making this determination manually, speciﬁcally in larger gateway courses, would prove an intensely laborious and time-consuming process, far more complicated than simple reading and re-reading of any single student’s work. When supporting learning using technology, however, a positive by-product is that students produce their work in an electronic form, which enables the creation of computerassisted instructional aids [22]. This paper describes an automated solution that can be used by educators to help resolve these tensions. Through the application of lexical analysis to student writing, we have implemented an analysis tool that allows an instructor to track how a student’s written language migrates from mere paraphrase to mastery, isolating the moment when the student’s understanding of core concepts best demonstrates an ability to place that concept in his or her own words, a moment that we’ve chosen to call the “point of originality.” This process recreates the same cognitive activity that educators might ordinarily undergo, yet in an automatic manner that is less labor-intensive. Ultimately, the resulting data is presented to the instructor by way of a custom visualization, which allows for continuous self-monitoring with minimally expended effort. In order to demonstrate the utility and validity of the analysis method, this paper explores the potential of the point of originality to preemptively predict likely student success or failure. The remainder of the paper is organized as follows. Section II explores the beneﬁts of co-blogging in education. Section III and IV provide background on the particular problems of, and potential solutions for, evaluation in larger gateway courses. Section V and VI introduce the theory behind, and the speciﬁcs of, the point of originality analysis method. Section VII and VIII report on a case study where the point of originality method was used to evaluate student co-blogging data. The paper ends by discussing future work. 
 2. CO-BLOGGING AND EDUCATION.
 Co-blogging is an example of a social computing activity that can be very conducive to learning [13, 26]. Overall, blogging provides a platform that promotes individual expression, enables students to establish their own “voice” and yields a richer conversational interactivity within a community [48, 47]. Each student has a blog, composed of multiple blog posts. Students can read each other’s blog posts and comment on them. Because blogs are easy to use, they can promote students’ digital ﬂuency [23] and encourage students to explore and publish their own nascent ideas under less pressure than in the rough-and-tumble of in-class discussions [3]. Writing a blog forces students to become analytic and critical as they contemplate how their ideas may be perceived by others [47]. Being able to review older contributions affords reﬂection and enables students to revisit and revise their artifacts, further developing their own viewpoints in the context of each other’s writing as they sense how others understand the material similarly or diﬀerently [37]. Conversations emerge when students read, and then comment on, each other’s blog posts, thus enabling them to exchange, explore, and present alternate viewpoints on the course material [17]. This type of social explanatory discussion can beneﬁt learning [12, 9]. Alternative spaces such as asynchronous discussion forums are another example of a technology that is sometimes used to mediate online discussions between students. However, as predominantly shared community spaces, forums give students voices that are heard but are without a distinct, individual identity [14]. Critical thinking may emerge for individuals, but the organization does not promote the coherent and interactive dialogue necessary for conversational modes of learning in the same way that co-blogging does [44]. A blogosphere can function as a repository of information, opinions, monologues and dialogues about course content, where students participate, and leverage each other’s contributions in other educational activities (e.g., when writing term papers) [2, 1]. Blogging enables students to gather their thoughts and come better prepared for class [24, 12] and can be predictive of student performance in a course [13]. Even non-active student bloggers can beneﬁt from the blog’s educational value as it exposes them to diﬀerent views of the material without necessarily participating directly [47]. Overall, having students discuss and/or “argue” about course readings has signiﬁcant educational utility [5, 4, 38]. Some discussion might take place during class, however, class time is a limited resource. This is particularly true for larger classes or so-called “gateway courses” that typically enroll large numbers of undergraduates where there is simply not time for everyone to speak up. Using co-blogging, students can both express individual voices and continue conversing with their peers outside the conﬁnes of the physical classroom. Unfortunately, the sheer size of these courses presents several challenges. 
 3. PROBLEMS WITH LARGER GATEWAY COURSES.
 The ability to monitor and respond to student progress is ever more imperative given the realities of the modern classroom. As noted even a decade ago, the political economies of American universities increasingly mandate large class sizes, particularly in the introductory or “gateway” courses that are typically a student’s ﬁrst exposure to collegiate work [28]. These large classes have a negative impact on students and instructors alike. There is, for example, an abidingly inverse correlation between class size and student achievement [20, 42]. The large lecture, while useful for reinforcing rote facts, is less successful in fostering higher-order thinking [31, 10], or in encouraging students to construct their own understanding of core concepts [28]. Such a sizable student population further constrains instructors’ abilities to familiarize themselves with students’ individual learning styles [28], thereby forcing instructors to assume that their audience consists of uniform types of learners [10]. Although the extent of feedback that students receive is one of the most powerful predictors of student achievement [46, 40], instructor feedback in large lecture courses is often slow and sporadic; students typically need to wait weeks - from, for example, one midterm assessment to the next - to put their course-related skills into practice, and even longer than that to have their assignments evaluated by an instructor [10]. Pedagogical adjustments, in other words, become both more unwieldy and more unlikely in the precise environment where they would be most necessary. Given the problems inherent to large lectures classes, but given also their entrenched status within the American university system, it would thus logically be prudent to ﬁnd a way to minimize their most pernicious consequences. Any broader attempt to remedy the problems of larger gateway courses should thus aspire to ﬁrst, foster higher-order thinking; second, to suit multiple types of learning styles; and third, to provide students with feedback as rapidly as possible. These ﬁrst two objectives are inherent virtues of the co-blogging process (e.g. [24, 12, 37, 9, 5, 4, 38]); the ﬁnal objective is the focus of this paper. 
 4. PRIOR EFFORTS.
 Several attempts to minimize the unintended consequences of large gateway courses exist. Almost all efforts call for resizing the large class group, either by literally subdividing the class or else by designing activities to make the large class “seem” small. This latter method, it might be argued, is the one already pursued by student participation in a coblogging environment, where conversations take place in an ad hoc and freeform manner. Known interventions can be roughly classiﬁed into two major groups: those interventions that are speciﬁcally meant for in-class use, and those interventions that are intended to take place between classes. Those activities that take place during class typically interrupt the lecture itself [32, 35, 8], asking students, for instance, to respond to a series of prompts which they answer through remote devices. These same activities, however beneﬁcial, generally disrupt the actual process of knowledge transmission and tend to reward rote memorization rather than higher-order thinking; what feedback students receive reﬂects only whether or not they got a prompt right or wrong, and not how well or how comprehensively they understood the material. Since the activities take place in the classroom, and in front of the entire student population, the activities themselves moreover treat all students in exactly the same manner regardless of learning style. Interventions intended for use between classes are roundly invested in providing instructors with observable statistical modeling in near real-time [39, 8, 19], which can then be referred to before the next session. These activities typically attempt to breed higher-order thinking by forcing students to reﬂect on their own learning, asking, for instance, that students rate their level of conﬁdence before responding to prompts [8], or that they engage in a collaborative peer review of one another’s written work [39, 19, 21]. The beneﬁts of this type of activity are directly analogous to the beneﬁts of co-blogging. 
 5. ORIGINALITY IN STUDENT WRITING.
 When students engage in a writing activity, the ﬁnal evaluation of their work cannot only assess whether or not the student has provided the most closely correct answer. Process is just as relevant to student writing as content [43]. Student writing that exhibits exceptional higher-order thinking is generally seen as that which demonstrates a mastery of the course material in new, profound or statistically unusual ways [33]. The ideal is not only for students to conﬁrm that they’ve understood lectures, but to do so in ways that even the educator might not have thought of. This process of mastery need not take place all at once. As a student is continually exposed to the same material, or is given the independent opportunity to rethink, reframe, or revisit that material [45], their writing on the subject has the chance to evolve, from rote regurgitation to wholly original expression [34]. At the level of language, this evolution is reﬂected through recasting. Recasting is the learning process whereby a student reﬁnes his or her understanding of a concept found in course lectures or readings by putting that concept into his or her own words [41]. In the acquisition of new languages especially, this process can be useful, because it allows students to acquire new vocabulary using the assortment of words already available to them [41, 30]. Even where the student’s understanding of a language is not an explicit concern, recasting can mark a student’s attempts to graduate to more sophisticated or professionalized terminology, or, inversely but to the same end, to place new concepts into terms that are nearer to what the student would naturally be more likely to say [15]. “Originality,” fully deﬁned, can of course take numerous forms. The concept of recasting, however, spans a number of theoretical orientations, with an inﬂuence on theories of schema formulation [25], the sensemaking process known as “scaﬀolding” [18], as well as the express principles of educational constructivism [27]. “Originality,” as deployed here, does not therefore strictly refer to a student’s creativity or capacity for non-conformity as might be suggested by more colloquial uses of the term. Rather, what the Point of Originality tool seeks to gauge is students’ ability to interpret, to place core concepts into new and diﬀuse usages. This deﬁnition of originality straddles the tiers of learning that Bloom’s taxonomy [6] associates with “understanding” and “application.” By interpreting core concepts and extrapolating to diﬀerent terms, students demonstrate their understanding of the material, and when putting those concepts into play through iterative exercises like co-blogging, they begin to apply that knowledge is newer and more diversedomains. For an instructor, the simple identiﬁcation of recast terminology within a student’s written work can provide an eﬀective barometer for pedagogical self-reﬂection. If a subset of terms or concepts are deemed vital to the syllabus, repetitions and recast iterations of those same terms will at least suggest that those terms are being acknowledged and reﬂected upon. Although the presence of recast terminology is not the only metric representative of a student’s mastery, the central role that recasting plays in a host of pedagogies (e.g. [25, 18, 27]) suggests that writing demonstrating high or low levels of recasting will reﬂect other aspects of performance within the course. Yet if the instructor hopes not only to identify instances where key concepts are deployed, but to determine how comprehensively the concepts are being internalized, it is ﬁrst necessary to possess a method of scoring how original any given recast might be. In order to do this, we have developed a metric for isolating a speciﬁc point of originality within student writing. 
 6. EVALUATION OF CO-BLOGGING.
 The process of computer-assisted evaluation of student writing is primarily composed of two parts: the analysis method, and a custom-made visualization depicting each student’s “originality” at any given time throughout the duration of the semester. 
 6.1 Analysis Method: Theoretical Background.
 WordNet is a lexical database that arranges nouns, verbs, adjectives, and adverbs by their conceptual-semantic and lexical relationships [16]. Whereas a simple thesaurus would be able to identify any two words as synonyms or antonyms of one another, WordNet is able to note the similarity between two words that don’t have literally identical meanings. These relationships are ideally meant to mirror the same lexical associations made by human cognition. WordNet’s arrangement is hierarchical, which is to say that certain terms are more closely related than others. Within WordNet, these relationships are displayed as “synsets,” clusters of terms that fork, like neurons or tree branches, from more speciﬁc to more and more diﬀuse associations (see Figure 1). If two words are found within one another’s synset tree, it stands to reason that these terms are, in some way, related, be it closely or distantly. As discussed in the next sub-section, these distances between two terms can be calculated, and assigned a value commensurate with their degree of semantic relatedness [7]. 
 Figure 1: Model synset tree (by hyponym relation). 
 The hierarchical arrangement inherent to WordNet provides one method of determining the relationship between two terms. If the synset tree of one term encompasses another term, it is simple enough to note how many synset jumps it takes to move from one to another. In Figure 1, a “Dalmatian” is a type of “dog,” which itself belongs to the subcategory of “domestic animals;” thus there are two tiers of associations between the concepts of “Dalmation” and “domestic animals.” Unfortunately, however, just how closely any two terms might be related is not a purely linear relationship. WordNet organizes related terms by their precise lexical entailment, such that nouns might be categorized as synonyms, hypernyms, hyponyms, holonyms and meronyms, as seen in Table 1. These possible entailments provide a rudimentary roadmap for all the ways in which two words might be related. Since WordNet attempts to map the cognitive associations automatically formed between words [16], a student’s evocation [36] of the holonym or hypernym of a given noun instead of the noun itself is more likely to form an associative recast of the original term. To put these abstract concepts into the same terms used to describe the original problem, if an instructor in a large lecture course in animal biology wanted to monitor how students had grappled with the (admittedly basic but obviously fundamental) concept “animal,” he or she would want to know not only that the students had deployed the literal term “animal,” but were also conversant in the other associated concepts found (in immensely abbreviated form) in Figure 1. This association is consistent with the pedagogical principle of recasting, and with the concept of “original” expression as deﬁned here. Where an instructor wants students to know one thing - in this case, about “animals” that those students can deploy more diﬀuse and more concrete examples of animals would be the readiest evidence that they actually understand. Yet while this simple index displays just how any two terms might be related, all the possible relationships noted are not necessarily equal. Some relationships, like that between synonyms smile and grin, are obviously bound to be more strongly associated than that between mammal and dog. Following a method ﬁrst noted by Yang & Powers [49], it is possible to install a series of weights that can best calculate the semantic distance between any two terms. This method in particular is useful because of all known methods, it bears the highest correspondence between its own distance calculations and the intuitions of actual human respondents (at 92.1 percent accuracy). 
 Table 1: Possible lexical entailments for nouns in WordNet. 
 6.2 Analysis Method: Implemention.
 Determining the point of originality of a student’s blog post depends upon the manual input of a speciﬁc query term by the instructor. The term relates to a key course topic and manual input of the topic reinforces the pedagogical utility of the process. For the query term, the process generates a WordNet synset tree. Words within the tree are then compared to the body of words extracted from a student’s blog post. Where matches are found, a summation of distance calculations between the original query term and the matches is performed as follows: Let q be a query term supplied by the instructor. Then, let W = {w0 , w1 , ..., wn } be a set containing all synset word matches (w ) from the WordNet database for q. Let B = {b0 , b1 , ..., bn } be a set of all words composing a blog post by a particular student and let S = {s0 , s1 , ..., sn } be a set of stopwords, a list of common words in English usage (like “the” or “and”), to be omitted to speed up processing time. Then, M = {m0 , m1 , ..., mn }, the set of synset term matches found in a blog post for query term q can be deﬁned as: 
 (FORMULA_1).
 WordNet stores synset matches in a tree structure with q as the root node. Then, δ, the distance (depth) for any given synset match (m ∈ M ) from the root node (query term q) is deﬁned as: 
 (FORMULA_2).
 WordNet also supplies the lexical entailment of each synset term. Thus, t, the “word type” of any given synset term match m ∈ M , is deﬁned as: 
 (FORMULA_3).
 Then α, the weight of any given synset term match is calculated as: 
 (FORMULA_4).
 The depth for any given synset term is multiplied by a constant value of 0.7, which reﬂects the diminished associations between two terms the farther separated they are along the synset tree. This value is selected because it corresponds with the calculation of distance between terms that yields the nearest match with human intuition [49]. Then, C, the cumulative originality score for a given query term q in a student’s blog post, can be deﬁned as: 
 (FORMULA_5).
 The point of originality for a particular course topic is in many cases deﬁned by the presence of several related query terms, or in other words, the synset matches for those terms. By deﬁning Q = {q0 , q1 , ..., qn } as the set of query terms supplied by the instructor at any one time, then P, the overall point of originality of a given student’s blog post for a particular course topic (deﬁned by Q), is: 
 (FORMULA_6).
 Finally, repeating the point of originality calculation (Equation 6) for each blog post written by a particular student, and plotting all instances of originality on a horizontal timeline, allows for an optimal instruction comprehension whereas the instructor can see recasts of a particular course topic (deﬁned by Q) across the entire body of a student’s writing throughout a single course. Although this paper focuses on the analysis of blog posts as students’ writing examples, given some additional programming work, any electronic form of student writing could be made compatible with the tool for subsequent analysis. 
 6.3 Visualization for the Point of Originality.
 The timeline visualization, as seen in Figure 2, displays a horizontal timeline that represents the time interval for the writing activity of any student for the duration of a particular semester. The numbered components of Figure 2 correspond to the following features. 1. This drop-down menu allows the instructor to select which student’s writing samples are currently being displayed. 2. This is where query terms (Q) are input by the instructor. 3. This timeline displays the date/times of each of the students’ writing samples. Each marker is color-coded, from colder to warmer colors along the ROYGBIV spectrum, the higher the value of the point of originality (P ) score for any given writing sample. These color assignments present an intuitive way for the instructor to quickly recognize that the sample has been assigned a higher originality value. 4. If a writing sample marker is selected in the timeline window (see inset 3), the text of that writing sample is displayed here. This assortment of visualization options allows the point of originality calculation to be displayed in a number of intuitive ways: both within chronology (inset 3) and in context (inset 5). 
 7. CASE STUDY.
 This section reports on a case study that explores the capability of using the Point of Originality tool to assess the originality of student writing in a semester-long co-blogging activity. More speciﬁcally, the study focuses on correlating originality scores assigned to students’ blog posts to their activities in the blogosphere during the semester and the ﬁnal grades assigned to a term paper covering the same topics. Although primarily aimed at testing the validity of the point of originality method, this study models a likely use case. By demonstrating how low point of originality values correspond to poor performance in other aspects of the course, the Point of Originality tool could provide instructors with an early, near-instantaneous diagnostic of which students might require additional help. The tool might thus ideally streamline the process of conducting targeted pedagogical adjustments or interventions. The co-blogging data was collected from a course taught in the Fall of 2008 in the Computer Science Department at Brandeis University. The course is an introductory course, an elective, focused on exposing students to topics such as the social life of information, virtual communities, privacy, intellectual property and peer-to-peer computing. In the co-blogging activity, each student has a blog where he or she writes opinions on the course readings. Students can read each other’s posts and comment on the posts of their peers. The blogosphere, provides several features focused on increasing students’ awareness of recent activity, and enabling them to ﬁnd interesting blog posts to read and conversations in which to participate. 
 7.1 Participants.
 There were 8 female and 17 male students, all undergraduates, enrolled in the class. There were 3 science majors and 1 science minor in the class. There were 12 students majoring in the social sciences and 8 minoring in the social sciences. The remainder of the class was either in the humanities or ﬁne arts. Three students were omitted from the data set because they did not begin blogging until the end of the semester following a warning from the instructor. As an introductory course, open to non-majors, the technical requirements for enrollment were few. No formal evaluations were done to assess the students’ computer literacy or prior domain knowledge. In class discussions, most of the students expressed moderate or advanced technical skills. The instructor and teaching assistant did not design, or implement, the co-blogging activity in such a way that it pre-assigned students into particular authoring roles in the online blogosphere, thus potentially inﬂuencing the students choice of writing topics or styles. 
 Figure 2: The Point of Originality timeline visualization. 
 7.2 Procedure.
 At the beginning of the semester, an in-class tour and exercise introduced the students to the important features of the co-blogging environment. The students were required to blog at the pace of one post per lecture: there were two lectures per week. A typical post was 1 or 2 paragraphs in length. The students were also required to read and comment on other contributions to the blogosphere. The coblogging work of each student counted for 35% of the ﬁnal grade. During the semester, the students read four books and wrote a paper on one of these books. The focus of the analysis presented in this paper is on the co-blogging work that the students did during the time the class read the book for which they wrote their papers. 
 7.3 Metrics.
 Lectures were presented using slides that summarized the key points of the presentation. At the beginning of each lecture, hard copies of the slides were handed out to support student note taking. We used the lecture slides as a basis for identifying the inputs to the blogosphere. For each set of slides, a set of key topics that were covered by the lecture ultimately became the query terms used for analysis. 
 7.4 Method.
 All of the students’ online work was automatically recorded in a transcript and analyzed using the Point Originality tool. Originality scores were generated for all blog posts and papers, which were then correlated to students’ ﬁnal paper grades and to statistical data summarizing their reading and writing activities during the co-blogging part of the semester. 
 8. RESULTS.
 The analysis was composed of two principle parts. The ﬁrst part compared the degree to which the tool indicated the originality of the students’ blog posts and how well the originality scores related to the grades that the instructor assigned their papers. In the ideal situation, given that the instructor graded their papers based on how well the students expressed higher order understanding of the course material, or in other words their writing reﬂected original thought, the tool should provide scores where higher originality values would correspond to higher paper grades. The second part sought to explore to what degree the students’ interactivity in the blogosphere inﬂuenced their understanding of the course readings, and in what way their immersion in the co-blogging community positively or negatively impacted their levels of originality when writing papers. Ideally, students would ﬁnd suﬃcient impetus to become deeply involved in the co-blogging learning community and their exposure to alternate or similar viewpoints of the same materials would help them develop their own viewpoints or to strengthen existing ones, thus leading to more original thought and better papers. Since the analysis was primarily concerned with ensuring that the tool could be used during a course to preemptively diagnose likely student success, the blog post dataset was ﬁltered to only include blog posts written in what was deﬁned as the “lead-in” period of co-blogging. During this period, the students were writing blog posts and comments on the topics that they eventually wrote their papers on, but at the time were unaware which speciﬁc topics they would have to address in those papers. The paper grades were assigned during the fall of 2008, roughly two years prior to the study described in this paper. Furthermore, grading was done by the course instructor, who is not a participant in the Point of Originality project. 
 8.1 Originality in the lead-in period.
 We began by collecting the originality scores calculated by our system for the blog posts written by each student on the paper topics and the actual grades that each student received for his or her paper. The average grade for student papers was 80.00 with a standard deviation of 16.83. The highest grade assigned was 95 and lowest was 40 on a scale from 0 to 100. The students’ blog posts received on average an originality score of 10.61 with a standard deviation of 4.29. The highest originality score assigned by our system was 18.30 whereas the lowest score was 3.92. Soon, a pattern emerged indicating that the more original the students’ co-blogging work, the higher the paper grades assigned by the instructor. While this is to be expected, the importance here is that the Point of Originality tool is automatically producing results that potentially correlate to standard approaches to pedagogy. A chi-square distribution test conﬁrmed that there was indeed a positive correlation between the two factors. As students’ blog post originality scores increased, their ﬁnal paper grades covering the same topics increased as well. In other words, as their blogging activity became more original, the students wrote better papers: 
 (FORMULA_7).
 To further conﬁrm the potential relationship between originality while initially learning the course materials (during the lead-in period) and how well that work transformed into mastery of course content as reﬂected by paper writing, students were divided into two groups based on their paper grades. Students whose paper received a grade above the average (80.00) were assigned to one group, the upper group, whereas students who scored below the average were assigned to the lower group. 
 Table 2: Originality variance and paper grades for two different groups of students. 
 As shown in Table 2, the students in the upper group received an average grade of 90.63 on their papers whereas the students in the lower group received an average grade of 66.79. What is more interesting, however, is what can be deﬁned as the originality variance: the diﬀerence between how original the students’ blog posts were compared to their ﬁnal papers. While the lower student group had an originality variance of 21.49, the variance for the students in the upper group was -6.10. Because the variance for the upper group is negative, those students’ blog posts, written during the lead-in period, were on average more original than their ﬁnal papers. It might seem then that those students were not necessarily more original than the students in the lower group, however, that is not the case. The fact that the variance is negative for the upper group is indicative of the fact that those students were at the height of their understanding of the materials even during the lead-in period. These students had mastered the materials in such a way that they had an easier time of writing their papers, whereas the students in the lower group were only ﬁrst beginning to wrestle with this content after the papers were assigned. This is suggested by the fact that the originality variance for the lower group was a positive value of 21.49, a value more than twice as great as the students’ average originality score during the entire period. A t-test of independent samples conﬁrmed that the originality variance between the upper and lower groups was indeed statistically signiﬁcant. Students who had received higher grades for papers wrote blog posts that were more original in the lead-in period: 
 (FORMULA_8).
 Similarly, a t-test also conﬁrmed that the students’ distribution of grades was by itself signiﬁcant: 
 (FORMULA_9).
 The key observation is whether or not students’ retention of course materials was equal for both groups. Students that master materials when taking exams don’t necessarily have the ability of applying that knowledge after the course ends because their “grasping” of the content was short lived. These students knew the material well enough to pass the exam but not necessarily well enough to be able to easily apply that knowledge later on. If students can get “into the game” earlier in the semester, they have greater opportunities to participate in discussions, reﬁne their understanding and “lock it down deep” so that they leave the course with a higher degree of mastery. In a large reading- and writing-intensive course, where a bulk of the work towards mastery might take place in machine-readable form, it goes without saying that it would be advantageous for the instructor to be able to use technology to monitor each student’s progress. Speciﬁcally in larger gateway courses, where the odds are already stacked against student achievement and the need for interventions is more diﬃcult to spot, students who fail to integrate completely with the class community - either because their experience comes from another discipline, or because they simply aren’t accustomed to the speciﬁc class environment - are likely to suﬀer poor performance. Having the ability to assess students’ mastery of the material, however, would enable the instructor to identify those students who are perhaps struggling or only falling behind, and to intervene to correct the students’ performance. 
 8.2 Interactivity in the blogosphere.
 In an online technology-mediated community like the one described in this paper, students beneﬁt from the exposure to both similar and contrasting viewpoints of the same course material. If the students’ deep emersion in the coblogging activity has a positive impact on their learning, one can assume that the originality score would correlate with the degree to which each student participates online. In other words, for those students that take advantage of the technology-mediated activity, frequently reading other students’ viewpoints and partaking in thoughtful conversations about the course readings, then originality scores should correlate with positive student outcomes. To assess student participation in the blogosphere, each student’s exposure (reading blog posts and comments by others) and contributions (writing blog posts and comments oneself) were measured. These activities were then correlated with the originality scores assigned to each student’s paper. Table 3 summarizes these metrics. Overall, the student papers received an average originality score of 53.49, with a standard deviation of 14.53. The highest originality score was 93.76, whereas the lowest score was 31.66. In terms of exposure in the blogosphere, the average number of times that a student was exposed to other students’ contributions was 4.36, with a standard deviation of 3.93. 
 Table 3: Originality and interactivity in the blogosphere. 
 The highest number of contributions read by a student in the blogosphere was 14, whereas one student read no contributions by the class at all. A chi-square test was used to explore the potential correlation between the originality of student papers and the degree of each student’s exposure in the blogosphere. As shown in Equation 10, there is a statistically signiﬁcant positive correlation between the two factors. In other words, higher exposure in the blogosphere led to more original papers. 
 (FORMULA_10).
 In terms of contributing in the blogosphere, each student made on average 4.18 contributions during the lead-in period, with a standard deviation of 2.17. The highest number of blog posts and comments written by a student was 9, whereas the lowest number of contributions was 1. As before, a chi-square test conﬁrmed that there was a statistically signiﬁcant positive correlation between the number of contributions a student makes in the blogosphere and the eventual originality of his or her paper. 
 (FORMULA_11).
 9. CONCLUSION.
 Integrating technology into higher education curricula to extend the physical boundaries of the classroom can be of signiﬁcant value, as it enables students to interact and learn outside of class time. This is particularly true in larger gateway courses, where there are fewer opportunities for students to engage in higher order thinking and to construct their own understanding of core concepts. While the introduction of technology like co-blogging can create a successful learning experience, the large number of students creates additional noise that makes it harder for instructors to isolate the students most in need of help. This paper described a method and tool by which student writing can be automatically analyzed to determine whether or not students have reached a point of originality in their writing, reﬂecting mastery of the course content. The paper presented a case study where the tool was used to analyze co-blogging data collected from an interdisciplinary reading- and writing-intensive course. The evidence showed that the tool was generating originality scores for students’ blog posts that correlated both with the degree to which they participated in the online activity as well as the ﬁnal grades that they received for their term papers. In other words, students who were more original during their co-blogging wrote better papers, and students who took advantage of the technology were more original. Although the paper was primarily aimed at conﬁrming the validity of the Point of Originality method, these ﬁndings suggest a likely use case for the technology. In a large class, where students engage both in iterative writing assignments like co-blogging and in summative writing assignments like midterm essays, an instructor might employ the Point of Originality tool at regular intervals throughout the semester to see which students are utilizing recast terminology in their work. Given the correlation seen here between a student’s ability to recast key concepts and their eventual performance on an assessment of those concepts (see Equation 7), the instructor could essentially use the tool to identify students with low point of originality values, and thus those most likely to do poorly on the assessment. Especially in large gateway courses, with potentially hundreds of students producing iterative assignments during the lead-in period, the process of identifying student learning styles and responding to student work is unwieldy [28, 10]. This strategy would allow an instructor to identify problems before it is too late: to determine which students might be struggling, to begin to isolate why, and to implement adjustments to pedagogy accordingly. It goes without saying that any tool of this sort might give a skewed measure for some students. For example, in any given class, some students simply learn best through faceto-face participation in class discussions while others accrue the highest learning beneﬁt through solitary reﬂective writing outside lecture hours. However, this does not reduce the merit or applicability of learning analytics tools such as the Point of Originality. On the contrary, if the “intangibility” of a technology-mediated learning activity is what makes any kind of evaluation or monitoring diﬃcult, even in smaller classes, then the production of tools that can assist the teacher in performing these activities would be a signiﬁcant boon. It is perhaps better to consider tools of this sort to be part of a larger “arsenal of assistive devices”, where one can pick-and-choose the tools most appropriate for the needs of a particular instructor, student, learning activity or course, whether it be for exploring textual content, activity logs or other types of data. It is the ability to be able to conduct any diagnoses at all, with a tailor-made analytics set, which is the primary beneﬁt. 
 10. FUTURE WORK.
 The tool is currently being used to analyze even larger gateway courses that typically enroll over 90 students. Additional features are also being developed to combat two not necessarily common but plausible anomalies where the composition of the writing examples themselves can produce false positives of originality. First, within the same writing sample, a student might (inadvertently) repeatedly use a word that not only is a synset match, but a match that yields a particularly high α value (see Equation 4). Therefore, an unreasonable degree of originality might be suggested for a particular writing sample. Currently, development is under way for a “decay factor,” that once enabled will gradually decrease the weight of the α value for a particular synset match, given how many times it has appeared before in the sample. The ﬁrst mention gets the maximum weight, where the nth mention receives the relative lowest possible weight. Second, where the evaluation of “originality” of a particular course topic depends on the presence of synset matches of multiple query terms within the same writing sample, the set Q (see Equation 6), then the distance between those matches within the text may also be signiﬁcant. For example, in response to a query for the compound term “color blindness,” the occurrence of a synset match for the word “color” in the ﬁrst paragraph of a writing sample may be otherwise unrelated to a synset match for “blindness” four paragraphs later. By implementing a “distance factor,” it will be possible for the instructor to specify a maximum distance (in terms of character, word, or paragraph count) between any two related synset matches in order for their α values to be included in the ﬁnal originality calculations for a given writing sample. 
 11. ACKNOWLEDGMENTS.
 Special thanks to Thanya Rajkobal for her contributions to the Point of Originality analysis tool and to the students in the course for providing data for this research project.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Monitoring Student Progress Through Their Written “Point of Originality”</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>evaluation of student writing</dc:subject>
		<dc:subject>recasting</dc:subject>
		<dc:subject>pedagogical adjustment/intervention</dc:subject>
		<dc:subject>lexical analysis</dc:subject>
		<dc:subject>information visualization</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/johann-ari-larusson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/johann-ari-larusson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/brandon-white"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/brandon-white"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/8/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/johann-ari-larusson"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/brandon-white"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/9">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Learning Analytics and Educational Data Mining: Towards Communication and Collaboration</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/9/authorlist"/>
		<swrc:abstract>Growing interest in data and analytics in education, teaching, and learning raises the priority for increased, high-quality research into the models, methods, technologies, and impact of analytics. Two research communities – Educational Data Mining (EDM) and Learning Analytics and Knowledge (LAK) have developed separately to address this need. This paper argues for increased and formal communication and collaboration between these communities in order to share research, methods, and tools for data mining and analysis in the service of developing both LAK and EDM fields.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 In education, the emergence of “big data” through new extensive educational media, combined with advances in computation [1] holds promise for improving learning processes in formal education, and beyond as well. Increasingly, very large data sets are available from students’ interactions with educational software and online learning - among other sources - with public data repositories supporting researchers in obtaining this data [2]. Two distinct research communities, Educational Data Mining (EDM) and Learning Analytics and Knowledge (LAK), have developed in response. The first workshop on Educational Data Mining was held in 2005, in Pittsburgh, Pennsylvania. This was followed by annual workshops and, in 2008, the 1st International Conference on Educational Data Mining, held in Montreal, Quebec. Annual conferences on EDM were joined by the Journal of Educational Data Mining, which published its first issue in 2009, with Kalina Yacef as Editor. The first Handbook of Educational Data Mining was published in 2010 [7]. In the summer of 2011, the International Educational Data Mining Society (IEDMS) (http://www.educationaldatamining.org/) was formed to “promote scientific research in the interdisciplinary field of educational data mining”, organizing the conferences and journal, and the free open-access publication of conference and journal articles. The EDM community brings together an inter-disciplinary community of computer scientists, learning scientists, psychometricians, and researchers from other traditions. A first review of research in EDM was presented by Romero & Ventura [3], followed by a theoretical model proposed by Baker & Yacef [4]. A very comprehensive review of EDM research can be found in [6]. The Learning Analytics and Knowledge conference series was initiated in early summer, 2010, with the development of global steering and program committees (https://tekri.athabascau.ca/ analytics/node/5). The conference explicitly emphasized its role as bridging the computer science and sociology/psychology of learning in declaring that the “technical, pedagogical, and social domains must be brought into dialogue with each other to ensure that interventions and organizational systems serve the needs of all stakeholders.” The first conference, held in Banff, Canada attracted over 100 participants, with proceedings published in ACM [5], validating interest in inter-disciplinary approaches to analytics in learning. In summer of 2011, the Society for Learning Analytics (SoLAR -- http://www.solaresearch.org/) was formed to provide oversight for the conference, develop and advance a research agenda in learning analytics, as well as advocate for, and educate in the use of, analytics in learning. With growing research interest in learning analytics and educational data mining, as well as the rapid development of software and analytics methods, it is important for researchers and educators to recognize the unique attributes of each community. While LAK and EDM share many attributes and have similar goals and interests, they have distinct technological, ideological, and methodological orientations. As schools, university, and corporate learning and curriculum organizations begin to adopt data mining and analytics, both LAK and EDM can benefit from building off work occurring in the other community. This paper details the overlap between these different communities and discusses the benefits of increased communication and collaboration. 
 2. SIMILARITIES BETWEEN COMMUNITIES.
 The EDM and LAK communities are defined in relatively similar ways. The International Educational Data Mining Society defines EDM as follows: “Educational Data Mining is an emerging discipline, concerned with developing methods for exploring the unique types of data that come from educational settings, and using those methods to better understand students, and the settings which they learn in.” The Society for Learning Analytics Research defines Learning Analytics as: “… the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs.” EDM and LAK both reflect the emergence of data-intensive approaches to education. In sectors such as government, health care, and industry, data mining and analytics have become increasingly prominent for gaining insight into organizational activities. Drawing value from data in order to guide planning, interventions, and decision-making is an important and fundamental shift in how education systems function. LAK and EDM share the goals of improving education by improving assessment, how problems in education are understood, and how interventions are planned and selected. Extensive use by administrators, educators, and learners of the data produced during the educational process raises the need for research-based models and strategies. Both communities have the goal of improving the quality of analysis of large-scale educational data, to support both basic research and practice in education. 
 3. KEY DISTINCTIONS BETWEEN COMMUNITIES.
 The similarities between EDM and LAK suggest numerous areas of research overlap. Additionally, the organizational deployment of EDM and LAK requires similar data and researcher skill-sets. However, these two communities have different roots and some distinctions are important to note. Table 1 shows some of the key differences between the communities. It is important to note that these distinctions are meant to represent broad trends in the two communities; many EDM researchers conduct research that could be placed on the LAK side of each of these distinctions, and many LAK researchers conduct research that could be placed on the EDM side of these distinctions. By identifying these distinctions, we hope to identify places where the two communities can learn from each other, rather than defining the communities in an exclusive fashion. Certainly, communities that grow organically as these two communities have done will not have rigid edges between what work appears in the two communities. One key distinction is found in the type of discovery that is prioritized. In both communities, research can be found that uses automated discovery and research can be found that leverages human judgment through visualization and other methods. However, EDM has a considerably greater focus on automated discovery, and LAK has a considerably greater focus on leveraging human judgment. Even in research which combines these two directions, this preference can be seen; EDM research which leverages human judgment in many cases does so to provide labels for classification, while LAK research which uses automated discovery often does so in the service of informing humans who make final decisions. This difference is associated with another difference between the two communities: the type of adaptation and personalization typically supported by the two communities. In line with the greater focus on automated discovery in EDM, EDM models are more often used as the basis of automated adaptation, conducted by a computer system such as an intelligent tutoring system. By contrast, LAK models are more often designed to inform and empower instructors and learners. A third difference, and an important one, is the distinction between holistic and reductionistic frameworks. It is much more typical in EDM research to see research which reduces phenomena to components and analyzing individual components and relationships between them. The “discovery with models” paradigm for EDM research discussed in [4] is a clear example of this paradigm. By contrast, LAK researchers typically place a stronger emphasis on attempting to understand systems as wholes, in their full complexity. The debate between reductionist and holistic paradigms has often paralyzed discussion between education researchers from different “camps”; encouraging discussion between EDM and LAK researchers is a key way to prevent this common split from reducing what EDM and LAK researchers can learn from one another. Two other differences are in the most common origins and methods of researchers in these two communities. Researchers’ origins tend to drive the preferred approaches discussed above, and these preferred approaches in turn drive preferred methods. Greater detail on these issues is given in Table 1. 
 Table 1: A brief comparison of the two fields. 
 4. CALL FOR COMMUNICATION AND COLLABORATION: EDM and LAK.
 There is a positive value to having different communities engaged in how to exploit “big data” to improve education. In particular, different standards and values for “good research” and “important research” exist in each community, allowing creativity and advancement that might not otherwise occur in a single, monolithic research culture. For example, EDM researchers have placed greater focus on issues of model generalizability (e.g. multi-level cross-validation, replication across data sets). By contrast, LAK researchers have placed greater focus on addressing needs of multiple stakeholders with information drawn from data. Each of these issues are important for the long-term success of both fields, a key opportunity for the two communities to learn from one another. Friendly competition between the two communities will keep both communities vigorous, and is generally beneficial for science. This type of competition has occurred in the past, such as in the split between the International Conference on the Learning Sciences and the International Conference on Artificial Intelligence in Education in 1992. Research networks are increasingly global, as reflected by the multi-national executive committees of IEDMS/EDM and SoLAR/LAK, but reflect different nations to a significant degree. Hence, the existence of both communities broadens the number of researchers working and collaborating in the broader area of data-driven discovery in education. At the same time, it is very important to keep competition healthy. Healthy competition requires that both communities disseminate their research to each other through their respective conferences and journals to ensure awareness of important ideas and advances occurring in the other community. The two communities must communicate, in order to bring the greatest possible benefits to educational practice and the science of learning. 
 5. CONCLUSION.
 Given the overlaps in research interests, goals, and approaches between the EDM and LAK communities, the authors of this paper recommend that the executive committees of SoLAR and IEDMS formalize approaches for dissemination of research and enacting cross-community ties. A formal relationship will allow each community to continue developing their specialized and distinct research methods and tools, while simultaneously increasing opportunities for collaborative research and sharing of research findings between the communities. This alliance would also strengthen our opportunities to influence non-academic research and practice. A particular concern now facing both EDM and LAK is the rapid development of analytics and data mining tools by commercial organizations that do not build off of either community’s expertise, algorithms, and research results. To give one example, there is increasing consensus in the EDM community that cross-validation needs to be conducted at multiple levels (in particular the student level, but also the classroom and lesson/unit levels). However, there is not direct support for this goal in many of the data mining/analytics tools now emerging. To the extent that EDM and LAK can jointly articulate quality standards for research in this area, it may be possible to more effectively communicate these standards to the wider community of tool-developers and analytics practitioners, as well as the broader research community. As such, both communities would be facilitated in communicating their vision for data-driven science and practice in the field of education. Both the LAK and EDM communities anticipate that the impact of data and analytics within education will be transformative at primary, secondary, and post-secondary levels. An open, transparent research environment is vital to driving forward this important work. As connected, but distinct, research disciplines, EDM and LAK can provide a strong voice and force for excellence in research in this area, guiding policy makers, administrators, educators, and curriculum developers, towards the deployment of best practices in the upcoming era of data-driven education. 
 6. ACKNOWLEDGMENTS Our thanks to Jaclyn Ocumpaugh, and the anonymous reviewers for their valuable input and assistance on this paper.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Learning Analytics and Educational Data Mining: Towards Communication and Collaboration</rdfs:label>
		<dc:subject>educational data mining</dc:subject>
		<dc:subject>learning analytics and knowledge</dc:subject>
		<dc:subject>collaboration</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/george-siemens"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/george-siemens"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/9/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/george-siemens"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/10">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>The Pulse of Learning Analytics Understandings and Expectations from the Stakeholders</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/10/authorlist"/>
		<swrc:abstract>While there is currently much buzz about the new field of learning analytics [19] and the potential it holds for benefiting teaching and learning, the impression one currently gets is that there is also much uncertainty and hesitation, even extending to scepticism. A clear common understanding and vision for the domain has not yet formed among the educator and research community. To investigate this situation, we distributed a stakeholder survey in September 2011 to an international audience from different sectors of education. The findings provide some further insights into the current level of understanding and expectations toward learning analytics among stakeholders. The survey was scaffolded by a conceptual framework on learning analytics that was developed based on a recent literature review. It divides the domain of learning analytics into six critical dimensions. The preliminary survey among 156 educational practitioners and researchers mostly from the higher education sector reveals substantial uncertainties in learning analytics. In this article, we first briefly introduce the learning analytics framework and its six domains that formed the backbone structure to our survey. Afterwards, we describe the method and key results of the learning analytics questionnaire and draw further conclusions for the field in research and practice. The article finishes with plans for future research on the questionnaire and the publication of both data and the questions for others to utilize.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Despite the great enthusiasm that is currently surrounding learning analytics, it also raises substantial questions for research. In addition to technically-focused research questions such as the compatibility of educational datasets, or the comparability and adequacy of algorithmic and technological approaches, there remain several ‘softer’ issues and problem areas that influence the acceptance and the impact of learning analytics. Among these are questions of data ownership and openness, ethical use and dangers of abuse, and the demand for new key competences to interpret and act on learning analytics results. This motivated us to identify the six critical dimensions (soft and hard) of learning analytics, which need to be covered by the design to ensure an appropriate exploitation of learning analytics in an educationally beneficial way. In a submitted article to the special issue on learning analytics [5], we developed the idea of a conceptual framework encapsulating the design requirements for the practical application of learning analytics. The framework models the domain in six critical dimensions, each of which is subdivided into sub-dimensions or instantiations. Figure 1. below graphically represents the framework. In brief, the dimensions of the framework contain the following perspectives: - Stakeholders: the contributors and beneficiaries of learning analytics. The stakeholder dimension includes data clients as well as data subjects. Data clients are the beneficiaries of the learning analytics process who are entitled and meant to act upon the outcome (e.g. teachers). Conversely, the data subjects are the suppliers of data, normally through their browsing and interaction behaviour (e.g. learners). In some cases, data clients and subjects can be the same, e.g. in a reflection scenario. - Objectives: set goals that one wants to achieve. The main opportunities for learning analytics as a domain are to unveil and contextualise so far hidden information out of the educational data and prepare it for the different stakeholders. Monitoring and comparing information flows and social interactions can offer new insights for learners as well as improve organisational effectiveness and efficiency [23]. This new kind of information can support individual learning processes but also organisational knowledge management processes as describe in [25]. We distinguish two fundamentally different objectives: reflection and prediction. Reflection [14] is seen here as the critical selfevaluation of a data client as indicated by their own datasets in order to obtain self-knowledge. Prediction [11] can lead to earlier intervention (e.g. to prevent drop-out), or adapted activities or services. 
 Figure 1. The learning analytics framework. 
 - Data: the educational datasets and their environment in which they occur and are shared. Learning analytics takes advantage of available datasets from different educational systems. Institutions already possess a large amount of student data, and use these for different purposes, among which administering student progress and reporting to receive funding from the public authorities are the most commonly known. Linking such available datasets would facilitate the development of mash-up applications that can lead to more learner-oriented services and therefore improved personalization [24]. However, most of the data produced in institutions is protected, and the protection of student data and created learning artefacts is a high priority for IT services departments. Nevertheless, similar to Open Access publishing and related movements, calls for more openness of educational datasets have already been brought forward [13]. Anonymisation is one means of creating access to so-called Open Data. How open educational data should be, requires a wider debate but, already in 2010, several data initiatives (dataTEL, LinkedEducation) began making more educational data publicly available. A state of the art overview of educational datasets can be found in [15]. - Method: technologies, algorithms, and theories that carry the analysis. Different technologies can be applied in the development of educational services and applications that support the objectives of the different educational stakeholders. Learning analytics takes advantage of so-called information retrieval technologies like educational data mining (EDM) [20] [17], machine learning, or classical statistical analysis techniques in combination with visualization techniques [18]. Under the dimension ‘Methods’ in our model, we also include theoretical constructs by which we mean different ways of approaching data. These ways in the broadest sense “translate” raw data into information. The quality of the output information and its usefulness to the stakeholders depend heavily on the methods chosen. - Constraints: restrictions or potential limitations for anticipated benefits. New ethical and privacy issues arise when applying learning analytics in education [16]. These are challenging and highly sensitive topics when talking about datasets, as described in [13]. The feeling of endangered privacy may lead to resistance from data subjects toward new developments in learning analytics. In order to use data in the context of learning analytics in an acceptable and compliant way, policies and guidelines need to be developed that protect the data from abuse. Legal data and privacy protection may require that data subjects give their explicit and informed consent and opt-into data gathering activities or have the possibility to opt-out and have their data removed from the dataset. At the same time, as much coverage of the datasets as possible is desirable. - Competences: user requirements to exploit the benefits. In order to make learning analytics an effective tool for educational practice, it is important to recognise that learning analytics ends with the presentation of algorithmically attained results that require interpretation [21] [22]. There are many ways to interpret data and base consecutive decisions and actions on it, but only some of them will lead to benefits and to improved learning [25]. Basic numeric and other literacies, as well as ethical understanding are not enough to realise the benefits that learning analytics has to offer. Therefore, the optimal exploitation of learning analytics data requires some high level competences in this direction, but interpretative and critical evaluation skills are to-date not a standard competence for the stakeholders, whence it may remain unclear to them what to do as a consequence of a learning analytics outcome. To further substantiate the currently dominant views on the emerging domain, we turned to the wider education community for feedback. The aim was to extrapolate the diverse opinions from different sub-groups and roles (e.g. researchers, teachers, managers, etc.) in order to see: (a) what the current understandings and the expectations of the different target groups are and (b) if a common understanding of learning analytics has already been developed. The mentioned framework was used to structure the questionnaire in order to avoid as much as possible bias toward a single perspective of learning analytics, e.g. the data technologies, and in order to get a balanced overview of the field as a whole. The questionnaire took concrete aspects into focus in the following way: The ‘stakeholders’ dimension inquired about the expected beneficiaries; ‘objectives’ tried to highlight the preference between reflective use of analytics and prediction; It also checked for the development areas where benefits are most likely or are expected; the ‘data’ section looked into stances on sharing and access to datasets; ‘methods’ investigated trust in technology and algorithmic approaches; ‘constraints’ focused on observations on ethical and privacy limitations (so-called soft-barriers); and, finally, ‘competences’ looked into the confidence for exploiting the results of analytics in beneficial ways. Although we won’t go into this issue in this article, we are aware that there may be cultural, organizational, and personal differences that influence the subjective evaluation of the dimensions of learning analytics. In section two below, we go on to describe in more detail the setup of the questionnaire, the participants and the distribution method. In section three, we present and discuss results and statistical effects. 
 2. EMPIRICAL APPROACH.
 2.1 Method.
 To evaluate the understanding and expectations of the educator and research community in learning analytics, we decided to use a questionnaire for reasons of ease of distribution and world-wide reach. In a globally distributed learning analytics community, this promised the best effort-return ratio, as opposed to other deeper, but more restrictive and effort intensive methods such as interviews. However, we anticipated the questionnaire as a first exploratory step toward more refined questioning and deeper analysis that would follow. 
 Table 1: Overview of question items and answer types. The full questionnaire and the cleaned dataset are available at http://dspace.ou.nl/handle/1820/3850. This includes also the tested statements of the multiple-choice and rank order questions that can not be mentioned here due to space issues. 
 For a more representative study the dissemination of the questionnaire should be supported as well over public bodies like school foundations and governmental institutions. To give the questionnaire an organized structure that would capture the domain in its entirety, where pedagogic and personal perceptions would have equal attendance to technical challenges or legalistic barriers, we divided the instrument into the six dimensions as indicated by the framework (see above). For each dimension, we asked the participants two-three questions and offered the opportunity for open comments. Questions were formulated in a variety of types, including prioritization lists (rank order), Likert scales, matrix tables, and multiple and single choice questions. Another criterion we felt necessary to adhere to in our evaluation of the current perception of learning analytics as a domain was openness. Rather than selecting a handful of renowned experts in the field, or to involve a particular education sector or even a local school (which would most probably have just revealed a widespread ignorance about this developing research domain), we wanted to compile an overview cutting across national, cultural, sectorial boundaries, and even roles of people involved in learning analytics. Although this would unavoidably lead to a much fuzzier picture, we felt the benefits to our understanding of the interest and hesitations toward learning analytics, in what is a general trend to much wider open educational practices, outweighed such concerns, allowing us to better assess the potential impact of learning analytics to education. Before publishing it, the questionnaire was validated in a small internal pilot with two PhD students and two senior staff members within the newly founded Learning Analytics research unit in our institution. In order to reach a wide network of a globally distributed Community of Practice, we designed and hosted the questionnaire online, using the free limited version of Qualtrics (qualtrics.com). This online tool is pleasantly designed and easy to use. It provides several sophisticated question-answer types with more being available for premium users. The data and the questionnaire are exportable in a number of popular formats including MS Excel and SPSS. The free version came with a limitation of 250 responses. All excess answers were recorded, but discarded in the analysis and data export. In our case, with a small sampling community, the free version proved to be sufficient. 
 2.2 Reach.
 We first promoted the questionnaire in a learning analytics seminar at the Dutch SURF foundation, a national body for driving innovation in education in the Netherlands. We then went on to distribute the questionnaire through the JISC network in the UK and via social media channels of relevant networks like the Google group on learning analytics, the SIG dataTEL at TELeurope, the Adaptive Hypermedia and the Dutch computer science (SIKS) mailing lists and to participants in international massive open online courses (MOOCs) in technology enhanced learning (TEL) using social network channels like facebook, twitter, LinkedIn, and XING. This distribution method is reflected in the constituency reached, in that there is, for example, a limited response rate from Romance countries (France, Iberia, Latin America) against a high return from Anglo-Saxon countries. The lack of responses from countries like Russia, China or India, maybe due to a number of factors: the distribution networks not reaching these countries, the language of the questionnaire (English), or a general lack of awareness of learning analytics in these countries. Still, we found that with the numbers of returns, we received a meaningful number of people interested in the domain. The survey was available for four weeks, during September 2011. After removal of invalid responses we analysed answers from 156 participants, with 121 people (78%) completing the survey in full. In total, the survey now covers responses from 31 countries, with the highest concentrations in the UK (38), the US (30), and the Netherlands (22) (see Figure 2. below). 
 Figure 2. Geographic distribution of responses. 
 2.3 Participants.
 Although we tried to promote the questionnaire equally to schools, universities and other education sectors, including elearning companies, we received a significantly higher response from the tertiary sector (further and higher education) with 74% (n=116). It is probably fair to say that learning analytics as a topic is not yet popular or well-known in other educational sectors with the combined K-12 sector amounting to 9% (n=13) and some 11% (n=17) coming from the adult, vocational, and commercial sectors. The remaining 6% (n=9) in the ‘other’ subgroup includes cross-sector and other individuals, such as retirees from the education sector. 
 Regarding the prioritisation of the stakeholder of learning analytics, the majority of respondents agreed that learners and teachers were the main beneficiaries of learning analytics where 1 was the highest score on the Likert scale. The weighting of the 155 responses shows that learners were rated highest at 1.9 mean rank, followed by teachers with 2.1. However, the ranking distribution and standard deviation for learners was higher (1.12) than for teachers (0.88). Institutions came in third place with an average rank of 2.6. There was also substantial contribution to the ‘other’ category with suggestions for further beneficiaries. Among those and most prominent were government and funding bodies, but also employers and support staff were mentioned. 
 The only other demographic information we collected from participants was their role in the home institution. Here we received a broad variety of stakeholder groups that deal with learning analytics. Multiple answers were possible, taking into account that people may have more than one role in their organisation. The three largest groups of our test sample were teachers with 44% (n=68), followed by researchers with 36% (n=56) and learning designers with 26% (n=41). With 16.1% (n=32) senior managers too were identified as a representative group of which two thirds (65.6%) came from HE institutions. 40.4% of the 156 participants claimed more than one role in their institution, of which again 40.3% were teacher/researchers (16.7% of the total sample). Next, we’ll present the most relevant results from the online questionnaire regarding expectations and understanding of learning analytics. 
 3. RESULTS.
 Our report on the results is organized along the lines of the six dimensions of the learning analytics framework (cf. section 1 above). We paid special attention to mapping opinions against institutional roles in order to identify any significant agreement or discord in each of the dimensions. One uncertainty underlying the outcomes is the lack of an established domain definition and/or established domain boundaries through practice. The term “learning analytics” is still rather vague, shared practice in the area is only just emerging and a scientifically agreed definition lacking. From on-going research and development work we know that some researchers subsume for example educational business analysis or academic analytics [8], or action analytics [7] under learning analytics [2]. Thus, the domain name itself carries a highly subjective interpretation, which almost certainly influenced the answers in the survey. We have no doubt that as the domain matures further, this interpretation will be narrowed down, leading to a better graspable scope and possibly more congruencies in the responses. 
 3.1 Stakeholders.
 In this section, we wanted to know: (a) who was expected to benefit the most from learning analytics, and, (b) how much will learning analytics influence specific bilateral relationships? 
 Graph 1. Relationships affected (1). 
 Graph 1 above illustrates the outcomes of question (b) and confirms the findings of question (a) above. The peaks identify the anticipated intensity of the relationship. Relationships with parents are not seen as majorly impacted, which is probably due to the fading influence parents have in tertiary education. It would be interesting to complete this picture with more responses from the K-12 domain. The highest impact is seen in the teacher - student relationship (83.5%, n=111, of respondents emphasised this), whereas the reverse student - teacher connection is strengthened slightly less (63.2%, n=84). Only less than half the participants see peer relationships as being strengthened through learning analytics: learner - learner by 45.9% (n=61), and teacher - teacher by 41.4% (n=55). At roughly the same level comes the relationship between institution and teachers (46.6%, n=62). 
 Graph 2. Relationships affected (2). 
 In the spider diagram (graph 2 above), the area indicates that it is the relationships of teachers that are expected to be most widely affected, followed by learners, institutions, and parents at a minimal level. 
 3.2 Objectives.
 In this section, we asked participants in which way learning analytics will change educational practice in particular areas. Of the total answers given in all 13 areas (n=1543), collected from 119 participants, only 10.8% of responses anticipated no change at all. On the other hand, the remaining responses left it open whether the expected changes will be small (43.8%) or extensive (45.4%). 
 Graph 3. Objectives for learning analytics. 
 Looking at the individual areas (cf. graph 3 above), the highest impact was expected in more timely information about the learning progress (item 2), and better insight by institutions on what's happening in a course (item 8). On the bottom end were expectations with respect to assessment and grading (items 6 and 5), where the least changes were anticipated. Further, we contrasted the importance of three generic objectives for learning analytics: (a) reflection, (b) prediction, (c) unveil hidden information. 47% (n=61) of the respondents felt that stimulating reflection in stakeholders about their own performance was the most important goal to achieve, while 37% (n=48) expressed the hope that learning analytics would unveil hidden information about learners (cf. graph 4). Both are not necessarily in contradiction to each other, since insights into new information can be seen as motivator for reflection. However the case may be, only 16% (n=20) favoured the prediction of a learner’s performance or adaptive support as a key objective. 
 Graph 4. Generic preference. 
 When looking at these objectives from the perspective of the different roles of participants, we find that teachers show a fairly equal interest in unveiling hidden information 44.6% (n=25), and in reflection 37.5% (n=21). This is a reasonable finding as many teachers expect learning analytics to support them in their daily teaching practice by offering additional indicators that go beyond reflection processes. On the other hand, 60.4% (n=29) of researchers indicated a clear preference for reflection. Translated into technological development, the expectations favoured more adaptive systems (highest rank), followed by data visualisations of learning, and better content recommendations in third place. Further interesting suggestions were “learning paths/styles adopted by students”, the clustering of learning types, and applications for the acknowledgement of prior learning. A further question surveyed the perception of learning analytics being a formal or less-formal instrument for institutions. In two intermixed sets of three options, one set represented formal institutional criteria: mainstream activities, standards, and quality assurance, all relating to typically tightly integrated domains that are governed by institutional business processes and strategies. The other set contained three less-formal and less monitored areas of pedagogic creativity, innovation, and educational experimentation. All three items represented individual choice of staff members to be innovative, experimental, and creative in their lesson planning and teaching activities. As indicated in graph 5 below, among the 129 responses, there was a noticeable preference towards less formal institutional use of learning analytics at a ratio of 55:45 per cent. Quality assurance ranked highest in importance among the formal criteria, whereas innovation was seen as most important aspect of all criteria. 
 Graph 5. Formality versus innovation. 
 One participant summed up the situation of these findings in the following statement: “It would be easy for learning analytics to become a numbers game focused on QA, training/instruction and rankings charts, so promoting its creative and adaptive potential for lifelong HE/professional-life learning is going to be key for the sector - unless learning analytics people want to spend all their lives doing statistical analysis?” 
 3.3 Educational data. 
 The section on data investigated the parameters for sharing datasets in and across institutions. The potential of shareable educational datasets as benchmarking tools for technology enhanced learning is explicitly addressed by the Special Interest Group (SIG) dataTEL of the European Association of Technology Enhanced Learning (EATEL) and has been demonstrated in [11]. Sharing of learning analytics data is impeded by the lack of some standard features and attributes that allows the re-use and reinterpretation of data and their applied algorithms [3]. For researchers, the most important feature was the availability of added context information (n=43, means 3.42) with a maximum value of 4 on the Likert scale. Perhaps, equally unsurprising was that for the manager group sharing within the institution (n=16, means 3.63) and anonymisation (n=19, means 3.53) were the most important values. Teachers, on the other hand, valued context (n=52, means 3.42) and meta-information (n=47 means 3.47) the most. At the other end of the spectrum, version control was the least important attribute across all constituencies (n=106, means 2.93). However, despite ‘version control of educational datasets’ was ranked the lowest, we still believe that this will play an important role in an educational data future. Version controlled datasets will offer additional insights into reflection and improvements through learning analytics by comparing older and newer datasets. Graph 6 illustrates the importance of the given data attributes. Note that the notion of “important” outweighs the “highly important” overall, which results in a lower means value. 
 Graph 6. Data attributes. 
 To get an idea of existing educational data, we asked participants about their institutional IT systems. For learning analytics, the landscape of data systems will play an important part in information sharing and comparison between institutions. In the tertiary education sector alone (Further and Higher Education), 93.9% (n=92) reported an institutional learning management system, which made this the most popular data platform by far. This was followed by a student information system 62.2% (n=61) and the use of third-party services such as Google Docs or Facebook 53.1% (n=52). Table 2 below shows a summary inventory of institutional systems in use across all sectors of education covered in our demographics. We assume that the more widely available a type of system is, the more potential it would hold for inter-institutional sharing of data, which could be utilised for comparison of educational practices or success factors. However, such sharing would depend on the willingness of institutions to share educational datasets with each other. When asked this question, a majority of people (86.6%, n=71) were happy to share data when anonymised according to standard principles. 
 Table 2. Data systems. 
 What is slightly contradictory is that people who indicated before that anonymisation was not an important attribute for data are less inclined to share (n=18, 83.3% yes : 16.7% no) than people who felt that it was highly important (n=40, 92.5% yes : 7.5% no). 
 3.4 Methods. 
 Learning analytics is based on algorithms (formulas), methods, and theories that translate data into meaningful information. Because these methods involve bias [1], the questionnaire investigated the trust people put into a quantitative analysis and in accurate and appropriate results. Within the 100% rating range, where 100% would indicate total confidence and 0% no confidence at all, the responses were located at mid-range. Among the given choices, slightly higher trust was placed on the prediction of relevant learning resources. This may be due in analogy to the amazon.com recommendation model, which is well-known and widely trusted. Other recommendations, such as predictions on peers or performance were rated rather low. The percentage on the horizontal axis in graph 7 below shows the level of confidence. 
 Graph 7. Confidence in accuracy. 
 One comment criticised that it was “disappointing that you included institutional markers, rather than personal ones for the learners, e.g. while learning outside the institution, which in my view are much more important and interesting”. We are not aware that the questions actually reflected an institution-centric perspective. At the same time, we still remain sceptical that analytics might currently be able to seamlessly capture learning in a distributed open environment, but mash-up personal learning environments are on the rise [12] and may soon provide suitable opportunities for personal learning analytics as has recently been presented in [6], and [9]. 
 3.5 Constraints.
 The constraints section focuses on the mutual impact that wider use of learning analytics may have on a variety of soft barriers like privacy, ethics, data ownership, data openness, and transparency of education (see graph 8 below). It should provide more detailed information on potential restrictions or limitations for the anticipated benefits of learning analytics. Most of the participants agree that learning analytics will have some or very much influence on the mentioned characteristics. Only a few did not expect any effects on privacy (10.4%, n=96) and ethics (8.8%, n=102). The majority of the responses believe that learning analytics will have the biggest impact on data ownership (66.4%, n=107) and data openness (63%, n=108) followed by more transparency of education (61.3%, n=111). 
 Graph 8. Problem areas of learning analytics. 
 After the general weighting of the expected impact on these constraints, we explicitly asked the participants how they estimated the influence of learning analytics and automated data gathering on the privacy rights of individuals by further describing what we mean with privacy rights in four statements (see graph 9 below). From 123 responses it appears that there is much uncertainty about the influence of learning analytics on privacy rights (cf. graph 9). The answers are widely spread from ‘no effect at all’ until ‘very much effect’. But the majority of participants believe that learning analytics will influence all four privacy dimensions at least a little. By recoding the given answers into a negative voting (will have no effect) and a positive voting (will have an effect) we got a clearer picture of the expectations of the participants. Regarding statement 1, about two thirds (65.8%, n=81) believe that learning analytics will affect ‘privacy and personal affairs’. Equally, in statement 3 - ‘ownership and intellectual property rights’ -, we can again see a clear majority (60.1%, n=74) convinced that these will be affected by learning analytics. Statement 2 - ‘Ethical principle, sex, political and religious’ and statement 4 - ‘Freedom of expression’ are close together, but with the majority in the negative, thus expressing they do not think that learning analytics affects these privacy domains (statement 2 negative effect size 54.5%, n=67; statement 4 negative effect size 53.7%, n=66). Taking further into account the large presence of ‘don’t know’ answers, we conclude that to most participants, the impact on privacy is not yet fully determinable. 
 Graph 9. Soft issues. 
 To get further information on these pressing soft barriers, we wanted to know if the participants have already (a) an ethical board and guidelines that regulate the use of student data for research. Further, we wanted to know (b) if they trust anonymisation technologies, and finally (c) how they rate a concrete example for data access in their own organisation to test the two answers before. Regarding (a), the majority of the participants 61% (n=75) indicated that they have an ethical board in place. Another 18% (n=22) said that they did not have such a body in place, whereas 21% (n=26) were unsure. Yet to us, such an organisational infrastructure represents an important starting point for more extended learning analytics research that is ethically backed up through proper procedure. With respect to (b), we went on to ask the participants whether they thought a (national) standard anonymisation process would alleviate fears of data abuse. With 49% (n=60), the majority of the 123 participants showed high trust in anonymisation technologies, whereas 24% (n=29) did not believe that anonymisation would be effective to reduce data abuse. 21% (n=26) indicated they know too little to answer this question. This leads us to the interpretation that in case learning analytics utilises data that is protected by legislation, participants expect further development of effective anonymisation techniques to deal with this issue. After having asked participants about ethical guidance and their trust in anonymisation, we tested with question (c) how the participants estimated the use of educational data within their organisation. We asked them whether institutions should allow every staff member to view student data internally in the organisation. In this, we received a significant negative response from the participants. 43% (n=53) did not want to allow all staff members to view student data, only 30% (n=37) did not see any problem with shared access. We also received 15% open text responses to this question that mainly emphasised the need for levelled access to student data in compliance with the law and ethical regulation and the strong need to anonymise data. The tenor in the comments strongly pointed to a “need to know” rational. That is to say that participants felt that only people who had good reasons to see such data should be permitted to access them. As one commentary phrased it: “Only if legitimately necessary and only for those who have a need to know”. 
 3.6 Competences.
 In our section on the competences dimension, we wanted to identify the key competences connected with learning analytics. We also asked for the confidence experts have in the independence of learners to exploit learning analytics for their own benefit. According to the learning analytics framework we suggested the following seven key skills: 1. Numerical skills, 2. IT literacy, 3. Critical reflection, 4. Evaluation skills, 5. Ethical skills, 6., Analytical skills, 7. Self-directedness. We wanted to know which of these skills the participants find important to benefit from learning analytics? Graph 10 shows the spider diagram of the answers. 
 Graph 10. Competences. 
 The participants found all mentioned skills rather important for learning analytics. By way of means ranking with a maximum value of 4 on the Likert scale, participants identified selfdirectedness (means 3.53) and critical reflection (means 3.42) as the most important competences required from beneficiaries. These were rated as ‘highly important’ by 59.3% (n=67) and 48.7% (n=57) respectively. Numerical skills (means 2.83) and ethical thinking (means 2.95) were on the bottom end of the scale. We consider this in line with the previous answers to ethical aspects of learning analytics. In addition to the required skills, we wanted to know whether participants thought that learners were competent enough to independently learn from learning analytics reports. It turned out that a significant majority did not think that learners would be able to deal with learning analytics reports without additional support (70.2%, n=85). Only 21% (n=26) believed that learners were competent enough to do so. We have to admit that we did not ask the same question with respect to skills required of teachers. This would have been an interesting comparison at this point. 
 4. SUMMARY AND CONCLUSIONS.
 The current article reported the results of an exploratory community survey in learning analytics that aimed at extracting the perceptions, expectations and levels of understanding of stakeholders in the domain. Divided up into six different dimensions we came to a number of conclusions which we are going to present below. - Stakeholders: Participants identified the main beneficiaries in learning analytics as learners and teachers followed by organisations. Furthermore, the majority of respondents agreed that the biggest benefits would be gained in the teacher-to-student relationship and that learners would almost certainly require teacher help to learn from an analysis and for taking the right course of action. This is rather surprising as learning analytics is seen by many researchers as an innovative liberating force that would be able to change traditional learning by reflection and peer support, thus strengthening independent and lifelong learning. This latter opinion on independence could be seen in the ‘objective’ section of the survey (cf. chapter 3.2 above) where the majority expressed a preference for learning analytics to pay special attention to non-formalised and innovative ways of teaching and learning. Yet, respondents expect less potential impact on the student-to-student and the teacher-to-teacher relationships. This current perspective may be affected by the scarcity of learning analytics applications that demonstrate the innovative possibilities for learning and teaching. Thus people may not have a clear point of reference as, for example, is the case for ‘social networks’ where an established group of competitive platforms already exists. - Objectives: The survey concludes further that research on learning analytics should focus on reflection support. The attained results clearly emphasized the importance of ‘stimulating reflection in the stakeholders about their own performance’. This goal could be supported by revealing hitherto hidden information about learners, which was the second most important objective. At the same time more timely information, institutional insights, and insights into the learning context were other areas of interest to the constituency. - Data: Our institutional inventory in chapter 3.3 gives an overview of the most widespread IT systems. These could be prioritised by learning analytics technologies to gain an institutional foothold. They also provide the best ground for interinstitutional data sharing. Anonymisation can perhaps be seen as the most important enabler for such sharing to happen. It is emphasised in a number of responses as the second most important data attribute and confirmed in the willingness of people to share if data is anonymised. For a clear majority anonymisation also reduces fears of privacy breaches through sharing (cf. chapter 3.5). On the other hand, when it comes to internal sharing with departments and operations’ units of the same institution, the use of available data will continue to be an uphill struggle, and, according to participants, require good justification. Here, perhaps, a clearer mandate to ethical boards may help. These are already widely in place. - Methods: Chapter 3.4 on methods revealed that trust in learning analytics algorithms is not well developed. We interpret the midrange return levels as hesitation towards “calculating” education and learning. What seems interesting to us is that the widely interpretable hope for gaining a comprehensive view on the learning progress was given the highest confidence, but perhaps this shows wishful thinking rather than a real expectation. Overall rather low was the expectations of impact on assessment. A majority of people did not see easier or more objective assessments coming out of learning analytics (cf. chapter 3.2). They were also not fully convinced that it would provide a good assessment of a learner’s state of knowledge (cf. chapter 3.4). - Constraints: A large proportion of respondents thought learning analytics may lead to breaches of privacy and intrusion. Yet, they ranked privacy and ethical aspects as of lesser importance to consider (cf. chapter 3.5) or as belonging to further competence development (cf. chapter 3.6). However, data ownership was expressed as highly important. This may be interpreted in that way that if ownership of data lies with the learners themselves, there is no perceived risk for privacy or ethical abuse. In any case, it seems that many organisations have ethical boards and guidelines in place. These may come to play an increasingly important role for institutional data exploitation since a large number of respondents trust that anonymisation of educational data is possible but not necessarily sufficient to enable full internal exploitation of the educational data within an organisation. - Competences: In the area of competences, participants mainly stressed the importance of self-directedness, critical reflection, analytic skills, and evaluation skills. On the other hand, few believe that students already possess these skills. This indicates to us a need to support students in developing these learning analytics competences. In conclusion of this section we can say, that the results suggest that there is little faith that learning analytics will lead to more independence of learners to control and manage their learning process. This identifies a clear need to guide students to more self-directedness and critical reflection if learning analytics should be applied more broadly in education. This interpretation is quite in contrast with some suggestions made with respect to empowerment of learners through providing graphical reflection of the learning process and further access to additional information regarding their learning progress [4]. 
 5. LIMITATIONS.
 We are aware of several limitations to both the questionnaire and the presented results. As has been mentioned in the introduction, there is a dominance of responses from the Higher Education sector that makes the study only partly representative for other educational domains. Another limitation is the virtual absence of students (undergraduate or secondary) although we did receive a tiny fraction of responses from lifelong learners. This makes the results of the survey biased toward a top down perspective on learning analytics. In an open environment, these shortcomings may still be interpreted as representative for the expressed interest and the pervasiveness of the topic in particular constituencies. Or, they may simply be due to the limited reach social networks like facebook, linkedin, twitter, etc. have with respect to the distribution of the questionnaire. Furthermore, the survey only represents a select few Western cultures. We need to be aware that substantial differences exist in educational cultures and that learning is always local. It would perhaps provide for interesting future research to compare these results with other dominant education cultures. Furthermore, for a more comprehensive study, the dissemination of the questionnaire should be supported by public bodies like school foundations and governmental institutions as it can be expected that the used dissemination channels reached a more technically interested group of the stakeholders. One hopefully time-restricted limitation is the low awareness of learning analytics among the target survey group and learners especially. With the rise of useful and popular learning analytics applications, we hope that this limitation will ease away over time and thus yield more concrete insights into the field of attitudes in learning analytics. As such, this survey can only be taken as an indicative insight of innovators and early adopters. To address these limitations and to gain more valid insights into expectations from and understanding of learning analytics we intend to do further research and plan to target the K-12 and adult education sector more specifically. Additionally, investigating the student perspective more intensely might reveal interesting contrasts to the above reports. 
 6. FUTURE RESEARCH.
 As announced in the beginning, this short survey was conceived as a first step toward more refined research into perceptions and potential for learning analytics. After collecting a more representative and selective dataset, we plan to apply more advanced analysis methods like the Group Concept Mapping [10] to further analyse different stakeholder groups and to identify consensus about particular issues in learning analytics among them. Group Concept Mapping will allow identifying thematic groups within learning analytics and it allows making a clear distinction between different aspects of learning analytics. Finally, we want to announce that the underlying datasets of this article and the planed extended version of this dataset will be made publicly available at the dspace.ou.nl repository (at http://dspace.ou.nl/handle/1820/3850). In that way, we would like to encourage the learning analytics community to gain additional insights from our dataset for the fast evolving of the learning analytics research topic. 
 7. ACKNOWLEDGMENTS.
 We would like to thank all participants in the survey and those who further disseminated it to their networks. Further, we want to thank the NeLLL funding body and the AlterEgo project that sponsored part of the authors’ efforts.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>The Pulse of Learning Analytics Understandings and Expectations from the Stakeholders</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>survey</dc:subject>
		<dc:subject>understanding</dc:subject>
		<dc:subject>expectations</dc:subject>
		<dc:subject>attitude</dc:subject>
		<dc:subject>privacy</dc:subject>
		<dc:subject>learning technologies</dc:subject>
		<dc:subject>innovation</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/wolfgang-greller"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/wolfgang-greller"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/10/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/wolfgang-greller"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/11">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Bridging the Gap from Knowledge to Action: Putting Analytics in the Hands of Academic Advisors</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/11/authorlist"/>
		<swrc:abstract>This paper presents current findings from an ongoing design-based research project aimed at developing an early warning system (EWS) for academic mentors in an undergraduate engineering mentoring program. This paper details our progress in mining Learning Management System data and translating these data into an EWS for academic mentors. We focus on the role of mentors and advisors, and elaborate on their importance in learning analytics-based interventions developed for higher education.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 This paper reports on the first iteration of a design-based research project focused on developing an early warning system (EWS) for a mentoring program that supports undergraduate engineering students. The EWS described in this paper represents an application of learning analytics that is gaining popularity across colleges and universities—the near real-time aggregation and analysis of students' use of campus information technology (IT) systems to support the immediate identification of students in need of academic support (e.g., [1; 2]). The mentoring program for which our EWS was developed, called the M-STEM Academy (Michigan Science, Technology, Engineering, and Mathematics Academy) [3], provides an integrated student development program for first- and second-year undergraduate engineering students. The M-STEM Academy is aimed at increasing academic success and retention of students who, for reasons of socioeconomic status, first generation college status, racial or gender bias, or lack of rigor in their high school preparation, might not be successful at a highly competitive, elite research university. For the first two years of their undergraduate career, students enrolled in the M-STEM Academy participate in a variety of core activities including a summer transition program, a program living community, peer study groups, and monthly meetings with their academic advisor, or "mentor". The M-STEM Academy is an academic support program modeled on the Meyerhoff Scholars Program at the University of Maryland-Baltimore County [4] and the Biology Scholars Program (BSP) at the University of California Berkeley [5]. The EWS developed in this project provided Academy mentors with frequent updates on students' academic progress and streamlined the presentation of data to allow immediate identification of students in need of support. Research on "Data Driven Decision Making" conducted on in K-12 schools has shown that the access to data does not automatically lead to improvements in organizational functioning or student learning [6]. In our research program, we viewed mentors as critical to increasing students' learning. We provided EWS data to mentors on a weekly basis, allowing mentors to engage students to different degrees or in different ways as a result of having access to the EWS data. Based on more frequent interactions with better informed mentors, students can engage with academic support services in a more timely manner and by doing so may positively affect their course performances. With this model, we provided mentors' with access to data but allowed them to determine the frequency and intensity of connecting students to academic support services provided by the M-STEM Academy and the College of Engineering. 
 2. BACKGROUND.
 Our design-based research project draws from previous research on utilizing data from Learning Management Systems (LMSs) for the purposes identifying "students at risk" of academic failure. Below, we describe recently deployed solutions that aim to leverage LMS data to improve various stakeholders' decision-making, and ultimately, retention and academic success of students. 
 2.1 Leveraging LMS Data.
 The vast majority (over 95%) of postsecondary institutions support, augment, and facilitate classroom instruction using a LMS [7]. Given the penetration of LMSs throughout higher education and the breadth of use on most campuses, these systems offer a ready source of near real-time data that can be analyzed to support a multitude of academic decisions for an increasing number of students. Our research lies at the intersection of learning analytics-based interventions and mining LMS data, where the use of LMS data, in particular, has been demonstrated to provide a useful stream of data to support just-in-time decisionmaking around students' academic performances [9]. Prior efforts to create "early warning systems" that alert students and/or faculty to sub-par student performance have generated useful proofs of concept. For example, Morris, Finegan, and Wu [10] found positive correlations between how long students remain on an LMS course site and the degree to which they participate in online discussions with students' likelihood of persisting in an online course. Focusing on online courses, Goggins, Galyen, and Laffey found that students were able to use LMS feedback to identify what their peers were doing, and what they, in turn, might need to accomplish in order to catch up to the "herd" [11]. Macfayden and Dawson [9] attempted to identify specific online activities—supported by an LMS—that have unique predictive power; using a variety of statistical techniques, these authors used LMS data to correctly identify 81% of students who failed the course. Two recently deployed systems are examples of scalable learning analytics products in higher education. At Purdue University, Course Signals [1] analyzes students' use of the campus LMS to provide students feedback on how their use of the system corresponds to their possible future grade in a course. At the University of Maryland-Baltimore, the Check My Activity [2] tool supports students' awareness of how their use of the LMS and their current grades compares to that of their peers. The Course Signals project represents the most robust EWS using LMS data; however, this project has yet to pair data visualizations with specific interventions, relying instead on generating notifications to students and instructors without identifying how instructors or students can best use this information. The study presented here addresses this gap between identification of a problem and defining specific interventions. 
 3. METHODOLOGY.
 Our research agenda is organized around the principles of design-based research [e.g., 12]. Design-based research involves the iterative production of theories, artifacts, and practices that can potentially impact teaching and learning. A distinguishing feature of design-based research is that the development of learning tools and environments is a collaborative effort among researchers and practitioners [13]. 
 3.1 Participants.
 Four cohorts, roughly 50 students each, have been recruited and admitted into the 2-year M-STEM Academy. Data from these four cohorts show that after participating in the Academy, the MSTEM students have GPAs at or above those of their peers in the College of Engineering (CoE), and that they outperform the average CoE student in the three entry-level math courses [3]. Although this represents overall success for the M-STEM Academy, there are still individual students who "fall between the cracks" in the mentoring program—especially after the first year—and their failure to perform well is not discovered until final course grades are submitted. The EWS described in this paper tracked approximately 200 M-STEM students across 165 unique courses. 
 3.2 Procedure.
 Our work with the M-STEM Academy began in the Winter 2011 term. We provided grade information stored in the LMS to mentors on a weekly basis during the middle weeks of the term. Also during this time, we conducted multiple collaboration sessions with mentors to determine their information requirements and to get feedback on the earliest versions of the EWS. From these sessions, we developed a plan for displaying data, clarified ways in which data visualizations would be helpful to mentors, and identified how mentors used the initial data visualizations before and during mentoring sessions with students. Below, we discuss the specific development of the EWS, and the mentors' reactions to and use of the system. 
 4. FINDINGS.
 Data to support the development of the EWS were drawn from the campus LMS. Up-to-date grades from the LMS's Gradebook and Assignments tools were used to track students' performance. Further, we integrated a proxy for student "effort" in a course through the use of LMS course website login events. While each tool on the LMS tracks user actions, the number of times that a student logs into a course website provides a parsimonious metric that can be used across all course websites. These data sources were aggregated and translated into a variety of visualizations. Data visualizations included figures displaying students' developing grades and use of the LMS in comparison to their peers, students' performances on specific assignments, and weekto-week classifications of students. These week-to-week classifications were the primary features of the EWS, designed to provide mentors with an easy-to-interpret assessment of students' current academic performances. Based on specific combinations of students' grades and course site login frequency, the EWS displayed whether mentors should take one of three actions: "encourage" students to keep doing well, "explore" students' progress in more detail, or immediately "engage" students to assess possible academic difficulties. To develop the classifications of encourage, explore, and engage, we worked closely with M-STEM Academy faculty and mentors. Classifications of encourage, explore, and engage were generated using three rules: (1) whether a student's percent of points earned was at or above the thresholds of 85%, 75%, or 65%; (2) whether a student was 10% or 5% below the course average in percent of points earned; and (3) whether a student was below the 25th percentile in number of logins. These rules put a premium on students' absolute performances as well as their performances relative to their classmates'. Each student's percentile rank for course site logins was used to classify students close to the borderline on the first two rules. One of the difficulties of using system log data to gauge student performance is the variation in how LMS tools are used course-tocourse. There are two general strategies for overcoming this variability. First, create a composite metric that integrates some combination of tools used on the course site whereby this metric is in units that locate students within an intra-course distribution. Second, identify system events that are shared across all course sites. We chose to work with only the events that are shared across all course sites given the transparency of what these events track; the mentors interpreted this as an acceptable proxy for student effort. Classifications were aligned to a single student enrolled in a single course; for example, a single student enrolled in four courses would receive four classifications. Thus, all comparisons between students were intra-course comparisons and the statistics used, such as percentile ranks, were appropriate for such comparisons. Using a variety of data mining techniques as well as feedback from mentors, we continuously modified the thresholds for each of the above rules and are currently testing the accuracy of the rules across larger samples of courses and students. Data visualizations and classification schemes were compiled and released to mentors on a weekly basis. The EWS was comprised of a summary sheet and multiple, individual student display sheets (see Figure 1). The summary sheet provided a broad overview of all students and the student detailed sheets provided figures depicting an individual student's academic progress. These detailed individual student displays supported mentors in examining students classified as "explore" or "engage". 
 Figure 1. Screen Shots of Data Displays for Academy Mentors. Summary Display (Left) and Detailed Display (Right) Examples. 
 Because students transition to a less structured participation in the M-STEM Academy during their second year followed by a transition to the general engineering student population in their third year, the mentors became interested in tracking upper division students. Almost immediately upon receiving the first set of performance indicators from the EWS, mentors noticed multiple third- and forth-year students who had fallen behind. They then contacted these students to schedule an advising session where the student received guidance and suggestions for how to improve their grades. Currently, mentors are immediately contacting any student classified as "engage", regardless of their year in the program. The mentors report scheduling meetings with these students to discuss their academic performance and to identify the most appropriate course of action. In our future work, we will review the current version of the EWS with academic mentors. We also plan to use this version of the EWS as a template for a web-based version that will allow us to more easily merge various datasets and mathematical functions, as well as integrate data from the university's larger data warehouse. Lastly, we intend to closely monitor the types of services recommended by mentors and the degree to which students follow up on mentors' recommendations to utilize those services. 
 5. DISCUSSION.
 Our goal with the M-STEM Academy design-based research project is not simply to design data displays for Academy mentors, but to use this unique group of faculty, staff, and students as a test bed for scaling the EWS to the College of Engineering and, eventually, to the entire university. By beginning the research project with an eye towards broad scale, we hope to avoid the difficulties that can develop when tailoring analytics to a small population that cannot then be implemented with a wider group [e.g., 8]. An important aspect of learning analytics research that surfaced in our investigations of the MSTEM Academy concerns organizational capacity. Though there are multiple definitions (e.g., [14]), here we define organizational capacity as the resources and routines that both enable and constrain access and application of learning analytics tools and related services. Resources include the actual support services available to (all) students, as well as the availability of dedicated mentors or advisors who have time to meet with and assess students' needs. More generally, the culture of data use (e.g., whether data is valued for supporting decision-making as well as that which is considered to be valid data) within an academic unit, such as a school or college, can be an important resource that enables the use of learning analytics. A culture of data use, moreover, can be instantiated in specific policies that incentivize data use or restrict key actors' access to data. Thus, factors such as values and policies are not solely barriers, though often framed as such, but rather possible mechanisms that can be targeted to improve the effectiveness of learning analytics-based interventions. Thus, one of the important resources for supporting student success is ensuring their access to academic mentors and advisors. Faculty members may be too burdened with teaching, research and service to effectively interpret the complex data that is available from the Learning Management System without significant training efforts that have proven difficult at a large research institution like Michigan. Furthermore, while competitive students are typically motivated to continually check on their own progress and standing within their courses (hence the pattern of use seen in Fritz's work [2]), students who are already falling behind are less likely to engage in help-seeking behavior and can therefore become "stuck" in a pattern that they do not know how to appropriately confront. Designated academic advisors are uniquely positioned to discuss course requirements with individual faculty and apply that knowledge when engaging with students. Helping these advisors to know which students to contact during the term will allow the students who have the greatest need to get connected with the resources already available and improve their chance of success before the final course grade is ultimately determined. The M-STEM Academy model has already received acclaim and support throughout our university administration and has spawned new initiatives in biology and other STEM disciplines. With that commitment has come an acknowledgement that in order to scale appropriately, the staff and faculty supporting these academies need data about their assigned students in order to have timely interventions with students before they are in serious jeopardy of academic failure. We believe that our model of presenting data to academic mentors about students' performance and effort is not only scalable, but also particularly appropriate given the complexities involved in course planning, locating academic services, and cultivating positive academic behaviors. In our future work, we are exploring several additions to our system that can make the task of identifying students who need assistance easier by leveraging appropriate data sources and technologies. First, we plan to include displays of admissions and registrar data that can help inform students who might be predisposed to greater difficulty in particular courses. We believe that this type of information could be valuable at the start of term as well as when students are planning their course schedules. We plan to test this hypothesis as our design-research project progresses. We will also investigate the potential to integrate additional LMS data (e.g., file downloads) into our existing model. Additionally, we are planning to construct tailored messages to students. These messages will encourage students to be pro-active about seeking advising services and utilizing the support mechanisms available to them that are most appropriate for each course (e.g., the science learning center for physics courses, the writing center for English courses). This type of message will supplement, rather than replace, the role of the academic advisor. Our overall goal is to put the most timely and helpful data in the hands of individuals who can analyze, interpret, and translate the presented data into actionable strategies to support student success. Learning is complex and so too, justifiably, is the task for leveraging appropriate analytics to serve that endeavor. 
 6. ACKNOWLEDGMENTS.
 Our thanks to the continued participation and feedback from our partners in the M-STEM Academy including the academic mentors Darryl Koch, Mark Jones, and Debbie Taylor and staff members Cinda-Sue Davis and Dr. Guy Meadows. Also thanks to Gierad Laput for recent efforts for future work in this project and to Dr. Timothy McKay for his feedback about the application of this research program at the University of Michigan.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Bridging the Gap from Knowledge to Action: Putting Analytics in the Hands of Academic Advisors</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>design-research</dc:subject>
		<dc:subject>undergraduate engineering</dc:subject>
		<dc:subject>higher education</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/steven-lonn"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/steven-lonn"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-e-krumm"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-e-krumm"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/r-joseph-waddington"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/r-joseph-waddington"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/stephanie-d-teasley"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/stephanie-d-teasley"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/11/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/steven-lonn"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-e-krumm"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/r-joseph-waddington"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/stephanie-d-teasley"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/12">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Learning Analytics for Collaborative Writing: A Prototype and Case Study</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/12/authorlist"/>
		<swrc:abstract>This paper explores the ways in which participants in writing intensive environments might use learning analytics to make productive interventions during, rather than after, the collaborative construction of written artifacts. Specifically, our work considered how university students learning in a knowledge work model—one that is collaborative, project-based, and that relies on consistent peer-to-peer interaction and feedback—might leverage learning analytics as formative assessment to foster metacognition and improve final deliverables. We describe Uatu, a system designed to visualize the real time contribution and edit history of collaboratively written documents. After briefly describing the technical details of this system, we offer initial findings from a fifteen week qualitative case study of 8 computer science students who used Uatu in conjunction with Google Docs while collaborating on a variety of writing and programming tasks. These findings indicate both the challenges and promise of delivering useful metrics for collaborative writing scenarios in academe and industry.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 The successful deployment of learning analytics holds much promise for educators and professionals who wish to make productive interventions into knowledge work as it happens; these interventions may be seen as a kind of formative assessment- regular, real time feedback about a specific deliverable as it is being developed. In recent years, several germinal studies of computer-supported cooperative work have emerged from the overlapping fields of technical and professional communication [see, for example, 1, 2, 3, 4, and 5]. Related studies of networked writing technologies in particular have contributed to the understanding of writing's formidable role in knowledge work, the cross-disciplinary, collaborative, often distributed work ubiquitous in post-industrial developed nations [see 6]. Indeed, researchers have cogently argued that “knowledge work often looks like writing,” leading to a focus on what writing does in collaborative work environments—how writing work both represents and moves knowledge assets [7, 8]. Building upon this body of research, we developed and studied a system that generates visualizations of real time metrics about the contribution and edit histories of collaboratively written documents as a means for fostering formative assessment in both peer-to-peer and instructor-to-student modes. The system we developed, known as Uatu, interoperates with Google Docs to better trace and represent the complexity of collaboratively written documents. Our project was driven by two conceptual research questions: first, do learning analytics related to collaborative writing foster greater metacognition among student participants?; and second, does such analytic data promote both instructor and peer opportunities for real time interventions into ongoing collaborations as formative assessment? In the remainder of this paper, we address these questions by first describing the technical details of our system; we then report the methods, data, and initial findings from a systematic qualitative case study conducted with eight computer science undergraduates who used Uatu over fifteen weeks; and finally, we explore the implications of our findings for the learning analytics community. Our research revealed a strong preference for co-located collaboration among participants, a practice that directly reduced the efficacy of our system since many co-located collaborative writing practices are ephemeral and thus do not produce measurable data. These findings suggest, therefore, that learning analytic systems designed to represent collaborative writing are perhaps better suited to fully online learning environments and to distributed teams in industry or other professional domains. 
 2. DEVELOPING THE UATU PROTOTYPE.
 Our work was motivated by conceptual interest concerning the ways in which writing functions as an arbiter of knowledge work in organizations and academe, as well as our recognition of the growing adoption and use of networked writing technologies such as Google Docs. A key impetus for this project, therefore, was our interest in providing real time data about collaborative writing histories to key stakeholders—students and instructors in academe, and knowledge workers and project managers in industry. We hypothesized that making such data readily visible to all participants would lead to more frequent and robust opportunities for metacognition. In this process, distributed collaborators might gain a better sense of ongoing text development [9] accompanied by a more thorough understanding of their own contributions to the knowledge and practice of a collaborative team. Provided with these metrics, students, instructors, knowledge workers, and project managers would also be equipped with data to better make formative assessments that might help shape final collaboratively written documents. Our focus, then, was on learning analytics for collaborative writing in both industry and academe. We chose the name Uatu for our prototype application in homage to the Marvel Comics character “The Watcher,” a fictional being tasked with watching over the earth. Uatu is a visualization system that continuously watches documents created in Google Docs and shared with a special user account that collects and stores edit and revision data (about user contributions, changes in document size, and time) in a MySQL database located on a local server. As a visualization system, Uatu is comprised of three interrelated modules—Watcher, Vis, and the Data Table Servlet—that interoperate with the Google Docs GData application programming interface and the Google Visualization application programming interface (see Figure 1). 
 Figure 1. Uatu Module Architecture. 
 In addition to the backend architecture, we created a front end web site for users; when logged in, Uatu produces basic visualizations of the overall collaborative writing activity for a requested document (see Figure 2). 
 Figure 2. Uatu Visualizations. 
 These visualizations are generated from document revision histories stored in the local Watcher database. As such, these values are not truly representative of the entire document, but instead reflect a very close approximation based on frequent polling (currently in one-minute intervals) of the document hosted on Google's servers. In Figure 2, the top visualization includes document revisions as they occurred over time, denoting who made a particular revision, when the revision was saved (noted by the horizontal axis of the graph), and the size of the contribution (noted by the vertical axis of the graph). Users may adjust the visualization by manipulating the left-hand side of the graph to highlight specific revisions. The bottom visualization in Figure 2 is a horizontal bar chart displaying each user who has contributed a revision and the number of saved revisions they have made to the ongoing document. Despite several early challenges encountered when interacting with Google's application programming interface, Uatu is presently a fully functioning prototype that faithfully watches and logs revision data from any documents shared with a special user account. Moreover, the front end web site delivers simple yet effective visualizations of collaborative writing activity generated in Google Docs. In addition to building the Uatu system, however, a major goal of this project was testing and researching the system with small groups working on authentic collaborative writing tasks. In order to meet the latter objective, we conducted a systematic qualitative case study of computer science undergraduates who used Google Docs and Uatu during a project-based advanced programming course over fifteen weeks. 
 3. METHODS AND DATA.
 Work on the technical development of the Uatu prototype began in July of 2010, while our systematic qualitative case study of Uatu, Google Docs, and learning analytics about collaborative networked writing activity began in January, 2011; data collection concluded in May, 2011. We used a systematic qualitative case study methodology conducted with ethnographic methods of field research [10, 11]. Following Dourish, we were interested in determining not only what Uatu can do as a system, but what Uatu does for participants in the course of their everyday work [12]. Consequently, we conducted a series of classroom observations, usability observations followed by stimulated recall interviews, observations of pair/group programming and group presentations, and semi-structured interviews with participants. These methods improved the reliability of our data, since thorough triangulation across data types and instances led to a deeper understanding of the collaborative behaviors we observed. In total, our fieldwork consisted of: twenty different classroom observations; twenty-four semi-structured and stimulated recall interviews spaced evenly over 15 weeks; fourteen observations of student writing and collaboration behaviors conducted outside of the classroom and accompanied by talk-aloud protocols; over seventy photographs; and the collection of nineteen participant-produced artifacts written in Google Docs, with granular revision history data captured by Uatu. Our study generated rich qualitative data about collaborative writing strategies among novice computer programmers. There were eight participants in total, and we collected in-depth and well-triangulated data from six of those participants, all of whom were undergraduate students at a mid-sized public research university in the Midwestern region of the United States; most were computer science majors, though two were computer science minors. The initial findings detailed in the following section were developed inductively through analysis of the granular data listed above. We used a combination of qualitative coding methods, including in vivo, descriptive, and process oriented approaches in order to derive superordinate categories and themes from our data [13]. Because of space limitations, we report these themes in narrative form. Reliability was fostered by our collection and analysis of data across multiple types (semi-structured interviews, talk-aloud protocols, stimulated recall interviews, observational fieldnotes) and instances (repeated observations and interviews over 15 weeks—from the first week of class until the final exam). 
 4. FINDINGS.
 In this section, we begin by briefly describing the project-based pedagogical model in which our research participants worked over the course of our study. We then detail three interrelated themes to emerge from our analysis—two minor themes which feed into our key conceptual theme. First, we describe our participants' workflow practices and their overwhelming preference for co-located collaboration. Next, we explore a key outcome of this collaborative preference, one that is especially relevant for learning analytics: much of our participants' workflow practices in co-located collaborations are ephemeral and thus do not render metrics that can be easily captured and measured. These two themes lead directly into our third theme: what “counts” as writing work for participants? We describe how certain forms of writing collaboration are often rendered invisible by common approaches to the assessment of writing assignments. This is the key conceptual finding of our study, one that has significant implications for the use of learning analytics in networked collaborative writing environments. Though ostensibly a course in advanced programming at the sophomore level, in actual practice our participants were immersed in a curriculum that more closely resembled an upper division software engineering course. Students learned methods of agile software development, Scrum, risk matrices, UML diagramming, and concepts in code reading and documentation that were all modeled and carried out in a project-based learning environment. Many students in the course were introduced to version control (using Mercurial) for the first time; they learned concepts in paper prototyping and usability, and were engaged in a variety of collaborative writing tasks that supported their programming practices (including project planning, requirements gathering and analysis, and learning assessments). The course was divided into two major units: a nine-week portion with two smaller group projects and several individual assessments, and a six-week major group project that involved designing, developing, and testing a working prototype application in Java. For this latter project, the three student teams of five were required to maintain and develop a learning standards document which eventually served as a major component of our study (while also providing much of the data that drove Uatu visualizations). Our first minor theme concerns the collaborative practices of participants. As they worked on the large six-week group project, our participants displayed a strong preference for co-located collaboration, despite access to distributed, networked programming and writing environments (Mercurial and Google Docs). In this sense, the actual collaborative practice of our participants varied greatly from our expectations. Many students in the course had never used Google Docs previously, and it was our expectation that using the application would ease collaborative writing tasks for groups since they could work asynchronously or even in real time from distributed locations, removing the potential barrier of coordinating school and work schedules to complete face-to-face meetings. However, participants in our study showed an overwhelming preference for both ad hoc and planned collaboration sessions that occurred face-to-face. In fact, two teams scheduled three to four hour-long meetings each week during the large project, collaborating on both programming and writing tasks during these sessions. In short, participants strongly preferred co-located verbal and gestural collaboration rather than distributed programming and writing collaboration, but they used Google Docs to develop their learning standards documents nonetheless. The primary motivation for this preference appears to be centered on alleviating programming anxiety among novice software developers. For example, one participant noted that pair or group work helped alleviate his programming anxiety since he could receive immediate feedback from peers on a given challenge. Another factor is convenience: distributed writing is labor intensive, and participants simply found writing as a group to be more efficient and less individually taxing. But this preference for face-to-face writing collaboration caused us to focus on exactly how our participants generated prose in such sessions, our second minor theme. When he was asked who contributed edits and saves to the major written component of the six-week development project, Terry, one of our research participants, responded: “um, I think just me and Roy. Roy copied down the stuff on the whiteboard.” Roy also logged meeting notes from the oral interactions of team members in face-to-face development sessions. Roy, it turns out, was the designated “typist” for his four-person group, both in the IDE and in Google Docs. When we examined the edit histories of the ongoing learning standards documents in Uatu, we noticed that often only one or two members of a group made edits and saves to a work that was to be explicitly written collaboratively. We naïvely assumed that the combination of a version control system (Mercurial) and a distributed writing application (Google Docs) would lead to more distributed and asynchronous group collaboration, and consequently, more group members contributing commits and saves. Our assumption, however, was diametrically opposed to observed student practices. Face-to-face collaborations included practices such as verbal interaction, whiteboarding and sketching activties, paper prototyping, and ad hoc ideation (for example, via the Google Docs embedded chat feature). In short, many important participant contributions were ephemeral, and thus invisible to Uatu. Because Uatu couldn't capture these more dynamic learning and collaboration activities, we were forced to re-examine our perspective on what “counts” as collaborative writing work during the six-week projects. For example, even though Roy was the designated “typist” for his group, he certainly didn't contribute a corresponding proportion of the actual writing of the learning objectives document. Typing does not equal writing. Examining learning analytic data captured by Uatu would indicate that Roy and Terry, to take one example, overwhelmingly “wrote” the final learning standards document. But this would not reflect the actual collaborative construction of that document, which was far more complex and nuanced. For example, other group members sketched ideas on white boards and paper, and they meaningfully contributed to the writing of the document orally in face-to-face sessions. We repeatedly observed dynamic collaboration sessions in which all members contributed in meaningful (but not necessarily equal) ways to the final document. However, because groups preferred face-to-face meetings, gestural, oral, and non-digital contributions that were integral in the collaborative writing of the learning standards document were rendered invisible in the document's edit history. What “counts” as collaborative writing work in such group sessions, therefore, cannot be accurately measured with a traditional approach to assessment or analytics. 
 5. IMPLICATIONS AND OPPORTUNITIES.
 The limitations for visualizing collaborative writing activity with Uatu are obvious: if group members are engaged in complex writing work without writing, their individual contributions will remain invisible to the system. This is actually an encouraging finding, despite being contrary to our expectations and hypothesis. Our participants collaborated on complex knowledge work projects in creative and productive ways, using the systems they had been given to best effect the inquiry-driven work they had undertaken. In the process, we recognized that Uatu's utility is limited for collaborative writing situations in which participants are likely to work in shared space, and where input tasks are delegated to one or two specific individuals. Another limitation of Uatu concerns shorter documents that are collaboratively written by a few (under 5 or 6) participants. An individual participant's sense of the text is strong in these scenarios; they readily apprehend the extent of their fellow team members' contributions at any given stage in the project, so Uatu's visualizations yield a representation of something they already implicitly understand. In such scenarios, the utility of learning analytics for formative assessment seems particularly limited. While Uatu's metrics weren't especially useful for our research site, the use of Google Docs as a real time collaborative writing application was valued across participants. For example, when participants contributed in ephemeral ways to collaboratively written documents, they could see their respective contributions reflected in real time via the operations of a given group's designated “typist.” In this way, several participants noted that seeing such contributions develop in real time caused them to become more metacognitive about how their own feedback played a part in the evolution of the learning standards document. Perhaps more importantly, our key theme should lead us, as educators, to re-evaluate how we assess collaborative writing and learning scenarios. By broadening our understanding of what “counts” as writing work, we can more readily account for meaningful contributions across learning styles. The major challenge for the learning analytics community, therefore, is capturing data about more ephemeral forms of collaboration. The real utility of a system like Uatu, it seems, is in visualizing large, complex documents that are collaboratively written by several distributed participants over an extended period of time, where any individual's sense of text is likely to be overwhelmed by the number of contributors, revisions, and saves. A project manager tracking the development of a complex policy document, for example, would greatly benefit from periodic Uatu visualizations. Rather than laboriously sifting through pages and pages of electronic text, the project manager could simply produce a daily visualization that details ongoing development. This is certainly no substitute for closer inspection; instead, it can help a project manager determine when and how to more closely and strategically investigate the developing text, a scenario in which learning analytics might foster robust formative assessment. Another clear application of Uatu is in online education, where visualizations of ongoing writing activity may help instructors provide more productive formative feedback and assessment, helping students learn as they work. Where collaborative writing assignments may be seen as onerous in contemporary online courses, a system such as Uatu could actually facilitate better integration of such assignments. Because fully online students are less likely to collaborate face-to-face, Uatu visualizations might better reflect the collaborative construction of knowledge occurring in and through writing work. Yet even in this scenario, our study suggests that important ephemeral forms of ideation and development would still contribute to final deliverables. These are forms of thinking and doing that remain difficult to capture with current learning analytic systems. 
 6. ACKNOWLEDGMENTS.
 This project was supported by a grant from the Indiana Space Grant Consortium, a Ball State University Emerging Media Initiative Innovation Grant, and the Ball State University Honors College.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Learning Analytics for Collaborative Writing: A Prototype and Case Study</rdfs:label>
		<dc:subject>writing</dc:subject>
		<dc:subject>collaboration</dc:subject>
		<dc:subject>programming</dc:subject>
		<dc:subject>knowledge work</dc:subject>
		<dc:subject>distributed work</dc:subject>
		<dc:subject>metacognition</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/brian-j-mcnely"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/brian-j-mcnely"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/paul-gestwicki"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/paul-gestwicki"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/j-holden-hill"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/j-holden-hill"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/philip-parli-horne"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/philip-parli-horne"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/erika-johnson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/erika-johnson"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/12/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/brian-j-mcnely"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/paul-gestwicki"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/j-holden-hill"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/philip-parli-horne"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/erika-johnson"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/13">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Course Signals at Purdue: Using Learning Analytics to Increase Student Success</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/13/authorlist"/>
		<swrc:abstract>In this paper, an early intervention solution for collegiate faculty called Course Signals is discussed. Course Signals was developed to allow instructors the opportunity to employ the power of learner analytics to provide real-time feedback to a student. Course Signals relies not only on grades to predict students’ performance, but also demographic characteristics, past academic history, and students’ effort as measured by interaction with Blackboard Vista, Purdue’s learning management system. The outcome is delivered to the students via a personalized email from the faculty member to each student, as well as a specific color on a stoplight – traffic signal – to indicate how each student is doing. The system itself is explained in detail, along with retention and performance outcomes realized since its implementation. In addition, faculty and student perceptions will be shared.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 The first year of college is arguably the most critical with regard to the retention of students into subsequent years of study [2, 3, 8, 9]. Noel and Levitz indicate that retention, or the lack of attrition from college, is a by-product of student success [4]. Tinto has spent much of his career investigating the necessary conditions for student success, and notes that academic support is among the primary pieces necessary to ensure success. In his 1993 book, Leaving College, Tinto proposed three necessary conditions for student persistence. First, an institution needed to put programs into place that placed the welfare of the students higher than that of the university. Second, programs and solutions should be focused on all students at an institution, not just a specific subpopulation. Finally, solutions implemented to enhance student success, and therefore persistence, needed to help integrate a student academically into the institution [6]. Helping a student become academically integrated to the institution is key, as Course Signals helps to promote integration in several ways. First, it allows faculty members to send personalized emails to students that contain information about their current performance in a given course. Second, faculty members can encourage students to visit various help resources on campus or office hours – activities that contribute to a student becoming more fully integrated into the institution. Third, it employs learner analytics to allow for the integration of real-time data on student performance and interaction with the LMS with demographic and past academic history information. This combination creates an intentionally created environment for the students that does “not leave learning to chance,” something Tinto noted was necessary to ensure that a solution would be broadly effective in helping students persist to graduation [7]. The remainder of this paper will describe Course Signals in detail, including its development and outcomes realized as a result of its implementation. In addition, faculty and student perceptions will be shared. 
 2. COURSE SIGNALS OVERVIEW.
 2.1 Description of Course Signals.
 Course Signals (CS) is a student success system that allows faculty to provide meaningful feedback to student based on predictive models. The premise behind CS is fairly simple: utilize the wealth of data found at an educational institution, including the data collected by instructional tools, to determine in real time which students might be at risk, partially indicated by their effort within a course. Through analytics, large data sets are mined and statistical techniques are applied to predict which students might be falling behind. The goal is to produce “actionable intelligence” —in this case, guiding students to appropriate help resources and explaining how to use them. [1] A predictive student success algorithm (SSA) is run on-demand by instructors. CS works by mining data from multiple university sources and subsequently transforming the data into a generated risk level with supporting information for each student [1]. The algorithm that predicts students’ risk statuses has four components: performance, measured by percentage of points earned in course to date; effort, as defined by interaction with Blackboard Vista, Purdue’s LMS, as compared to students’ peers; prior academic history, including academic preparation, high school GPA, and standardized test scores; and, student characteristics, such as residency, age, or credits attempted. Each component is weighted and pulled into the proprietary algorithm, which then calculates a result for each student. Based on results of the SSA, a red, yellow or green signal is displayed on a student’s course homepage. A red light indicates a high likelihood of being unsuccessful; yellow indicates a potential problem of succeeding; and a green signal demonstrates a high likelihood of succeeding in the course. Instructors then implement an intervention schedule they create, possibly consisting of: - Posting of a traffic signal indicator on a student’s LMS home page; - E-mail messages or reminders; - Text messages; - Referral to academic advisor or academic resource centers; or, - Face to face meetings with the instructor. [1] With Course Signals, students are not placed at risk due to one single factor; risk is determined by a contextualized landscape that varies from student to student based on the data comprising the four components of the SSA. The SSA transforms both static and dynamic data points into a single score, improving the reliability of the prediction. Since a course-specific risk indicator is created for each student based on performance, peer-based behavior, and educational preparation data, instructors can intervene early and give students a realistic opportunity to adapt their behavior to be more specific in a given course. 
 2.2 History of Course Signals.
 Facing challenges of under prepared students, budget crises, decreasing retention and longer graduation periods, higher education is working to provide solutions to these challenges while at the same time balancing the demands of providing exceptional student service to foster student success. In an attempt to ease these mounting pressures, CS was developed to help identify students potentially at risk of not reaching their full potential in a course. Once identified, instructors have the ability to deliver meaningful interventions suggesting behaviors a student may wish to change in order to improve her chances of success. [1] In 2007, Purdue University piloted Course Signals. According to Pistilli and Arnold, the system “was built from the ground up using empirical data at every stage to ensure the most predictive student success algorithm” [5]. Course Signals became automated in spring 2009 and partnered with SunGard Higher education in October 2010 in order to help other institutions harness the power of learning analytics. Today, nearly 24,000 students have been impacted by the CS project, and more than 145 instructors have used CS in at least one of their courses. 
 3. ACADMIC PERFORMANCE AND RETENTION OUTCOMES.
 3.1 Impact on Academic Performance.
 Undeniably, one performance measure of student success is final course grade. Research indicates that courses that implement CS realize a strong increase in satisfactory grades, and a decrease in unsatisfactory grades and withdrawals. Individual courses see variable success with: an increase in As and Bs ranging from 2.23 to 13.84 percentage points; a decrease in Cs ranging from 1.84 to 9.38 percentage points; and a decrease in Ds and Fs ranging from 0.59 to 9.40 percentage. Combining the results of all courses using CS in a given semester, there is a 10.37 percentage point increase in As and Bs awarded between CS users and previous semesters of the same courses not using CS. Along the same lines, there is a 6.41 percentage point decrease in Ds, Fs, and withdrawals awarded to CS users as compared to previous semesters of the same courses not using CS. 
 3.2 Impact on Student Retention.
 With increased student success in individual courses comes an expected increase in retention to the University as well, and the data indicate this nicely. Course Signals has been employed at Purdue since 2007, and its use for each beginning cohort at the institution is described in detail below. 
 3.2.1 Methodology.
 The fall 2007, 2008, and 2009 beginner cohorts were compared to a master list of all Course Signals participants to determine who from those entering cohorts took courses utilizing Course Signals or not, and, if applicable, the number of times students took courses with Course Signals. From there, students were put through the retention module in the University’s data reporting system. They were analyzed based on the number of times they had a course with CS – a number ranging from zero to five. The beginner cohort for each year consists of the students who are both first-time in college and carrying full-time credit loads. The students who comprise each cohort is determined the second week of each fall semester, and this data set is frozen; once created, students do not leave the cohort for any reason. Each semester, every student in each cohort has some form of retention or leaving behavior – from simply remaining enrolled, to graduating, to being academically dropped or withdrawing voluntarily. The retention rate is calculated by adding those who are still enrolled and who have graduated and dividing that sum by the total number of first-time full-time students in the original cohort. 
 3.2.2 Results.
 As indicated below, the students who began at Purdue in fall 2007 (Table 1), 2008 (Table 2), or 2009 (Table 3) and participated in at least one Course Signals course are retained at rates significantly higher than their peers who had no Course Signals classes but who started at Purdue during the same semester. Further, students who have two or more courses with CS are consistently retained at rates higher than those who had only one or no courses with Signals. The analysis detailed in Tables 1, 2, and 3 does not account for when a student had a course with CS, only that at some point during their academic career they did. Tables 4, 5, and 6 examine that aspect. For the 2007, 2008 and 2009 cohorts, instances of CS use across successive semesters was compared to students’ retention behavior for the following semester. This analysis asked if, within a set of semesters, if a student had at least one course with Course Signals. So, for example, for the 2007 cohort, the first row looks at whether or not a student had CS in a course during either the Fall 2007 or Spring 2008 semester, then determines if they were retained into the Fall 2008 semester. The comparison is against students who did not have a course with CS during the same time period. In short, there is a noticeable impact on students having a course with CS early in their academic career; basically, the earlier a student encounters CS the better. Combined with the first analysis, the earlier and the more occurrences, the greater the likelihood students will be retained. 
 Table 1. Retention Rate for the 2007 Entering Cohort. 
 Table 2. Retention Rate for the 2008 Entering Cohort. 
 Table 3. Retention Rate for the 2009 Entering Cohort. 
 Table 4. Analysis of Retention by Semester of Course Signals Use for the 2007 Entering Cohort. 
 Table 5. Analysis of Retention by Semester of Course Signals Use for the 2008 Entering Cohort. 
 Table 6. Analysis of Retention by Semester of Course Signals Use for the 2009 Entering Cohort. 
 In addition to the previous analyses, it should be noted that in every case for the students from the 2007, 2008, and 2009 cohorts, students in courses with CS have a lower average standardized test scores than those in non-CS courses. While this aspect needs to be further investigated, early indications show that lesser-prepared students, with the addition of CS to difficult courses, are faring better with academic success and retention to Purdue than their better-prepared peers in courses not utilizing Course Signals. 
 4. FEEDBACK FROM STUDENTS AND INSTRUCTORS.
 While the quantitative data provide evidence that there is an impact on students’ grades and retention behavior, there exist additional data that support the use of CS. The instructors who have employed CS, as well as students who have benefited from the system, have provided information via surveys, focus groups, and interviews that continue to warrant the usage of the system. 
 4.1 Student Perception and Feedback.
 One of the major objectives of academic analytics is to identify underperforming students and intervene early enough to allow them the opportunity to change their behavior. For this reason the Course Signals development team has closely tracked the student experience with Signals since the pilot stage. At the end of each semester, a user survey gathers anonymous feedback from students, with more than 1,500 students surveyed across five semesters. In addition, several focus groups have been held. Students report positive experiences with Course Signals overall (89% of respondents stated CS provided a positive experience and 58% said they would like to use CS in every course). Most students perceive the computer-generated e-mails and warnings as personal communication between themselves and their instructor. The e-mails seem to minimize their feelings of “being just a number,” which is particularly common among first-semester students. Students also find the visual indicator of the traffic signal, combined with instructor communication, to be informative (they learn where to go to get help) and motivating (74% said their motivation was positively affected by CS) in changing their behavior. Of the roughly 1,500 student responses, only two wrote of becoming demoralized by the “constant barrage” of negative messages from their instructor. While this perception should not be downplayed, negative feedback from instructors, especially for students who might not be prepared for the rigors of higher education, can be difficult to receive. Aside from these two instances, however, the remainder of the negative feedback concerned faculty use of the tool. For example, many students spoke of over penetration (e-mails, text messages, and LMS messages all delivering the same message), stale traffic signals on their home pages (an intervention was run but not updated, giving a false impression of a student’s status), and a desire for even more specific information. This information notwithstanding, the overwhelming response from the students is that Course Signals is a helpful and important tool that aids in their overall academic success at Purdue. Faculty believe this as well, as indicated in the following section. 
 4.2 Faculty Perception and Feedback.
 While the student success algorithm predicts which students might be in jeopardy of not doing as well as they could in a course, it is the faculty and instructors who use the information provided by Course Signals to intervene. It could certainly be argued that these instructors, armed with the data provided by learner analytics, are the most important weapons against student under performance in the classroom. To wit, one instructor asserted that “I want my students to perform well, and knowing which ones need help, and where they need help, benefits me as a teacher.” Faculty have easy access to CS data via the faculty dashboard. Using learner analytics, faculty can provide action-oriented and helpful feedback much earlier in the semester, which students appreciate. This particularly benefits students early in their academic careers, as they often are not fully aware of the behaviors they must exhibit or actions they must take in order to be successful. Faculty also say that students tend to be more proactive as a result of the Course Signals interventions. While students still tended to procrastinate, they began thinking of big projects and assignments earlier. Instructors and TAs also noticed that students posted more questions about assignment requirements well before the due dates. Because of the ability of academic analytics to assess risk early and in real time, the instructors consistently indicate that students are benefiting from knowing how they are really doing in a course and, moreover, understand the importance of completing assignments, and performing well on quizzes and tests. In general, faculty and instructors have a positive response to CS but many approached the system with caution. Before using Course Signals, faculty initially expressed concern about floods of students seeking help; however, few actually reported problems after they began using the system. The most commonly reported issue being an excess of e-mails from concerned students. In addition, faculty reported concerns about creating a dependency in newly arrived students instead of the desired independent learning traits. A final faculty concern was the lack of best practices for using CS, demonstrating that instructors and students share the same concern about the lack of best practices. There is little that can be done to mitigate the first concern, since one of the goals of CS is to have students take a more active role in their success. The second concern is mitigated by the strong retention results discussed in this paper. The final issue was addressed by creating and posting best practice tips at http://www.itap.purdue.edu/learning/tools/signals. 
 5. CONCLUSION.
 The use of learner analytics through the application of Course Signals to difficult courses has shown great promise with regard to the success of first and second year students, as well as their overall retention to the University. To date, over 23,000 students across 100 courses have been impacted by Course Signals and over 140 instructors have utilized the system. Plans call for the expansion of CS to include as many as 20,000 students a semester within the next 18 months, and the upper administration the institution is in strong support of this goal. While this analysis is not without its limitations or areas for continued improvement of the algorithm behind CS or the use of CS, the outcomes are such that continued use of Course Signals as a means of helping instructors provide detailed feedback to their students, and to ultimately assist students in their academic endeavors is highly warranted. 
 6. ACKNOWLEDGMENTS.
 Our thanks to Kyungmin “Mike” Ahn for his work analyzing the data associated with the retention numbers.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Course Signals at Purdue: Using Learning Analytics to Increase Student Success</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>college student success</dc:subject>
		<dc:subject>early intervention</dc:subject>
		<dc:subject>retention</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kimberly-e-arnold"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kimberly-e-arnold"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/matthew-d-pistilli"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/matthew-d-pistilli"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/13/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/kimberly-e-arnold"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/matthew-d-pistilli"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/14">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>First Steps Towards a Social Learning Analytics for Online Communities of Practice for Educators</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/14/authorlist"/>
		<swrc:abstract>Learning analytics has the potential to provide actionable insights for managers of online communities of practice. Because the purposes of such communities and the patterns of activity that might further them are diverse, a wider range of methods may be needed than in formal educational settings. This paper describes the proposed learning analytics approach of the U.S. Department of Education’s Connected Educatorsproject, and presents preliminary applications of social network analysis to the National Science Teachers Association Learning Center as an illustration.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 This paper provides an overview of research and development work being conducted by the American Institutes for Research through the Connected Educators project, under contract from the United States Department of Education’s Office of Educational Technology. In accordance with the National Educational Technology Plan, Connected Educators intends to help educators at the school, district, and state level across the U.S. become more connected with resources and with each other in order to enhance their professional effectiveness [13]. Online communities of practice (OCoPs) are a key means for helping educators connect. The project’s initial environment scan yielded considerable evidence that online communities of practice are becoming increasingly prevalent and effective means for professional learning in education [9]. The research synthesized in our initial report has shown that OCoPs can help educators access, share, and create knowledge and develop professional identity in ways that go beyond what is possible through face-to-face engagement alone. This research also shows that effective leadership and moderation is key to the success of OCoPs in supporting these activities [1, 6, 7]. In small OCoPs, a community manager may be able to read all of the member-contributed content and discussion and come to know most or all of the participants. Once an OCoP reaches a certain scale, however, this coverage becomes impossible. Division of labor is one approach to being responsive to the emerging dynamics of community activity and relationships, but it is also helpful for the manager to have a systematically generated, holistic picture of what is going on. Learning analytics may help provide that picture, drawing on the considerable volume of “data exhaust” generated by online community activity. Beyond basic Web analytics, these data are largely an untapped resource for practitioners. COCP is eager to explore ways in which pioneering work in the field can be applied to OCoPs for educators. However, most learning analytics work of which we are aware has so far focused on structured learning experiences for students, such as semesterlong online courses or discussions on a blog. Learning within OCoPs likely differs from learning in these contexts in several ways. First, the learning experience is not time bounded. Second, participation is usually voluntary, with individual participants coming to the community with different needs that correspond with a range of styles of engagement [10]. Third, the primary motivation for engaging with OCoPs is often solving problems of practice rather than learning for its own sake. Finally, experience has shown that the value OCoPs offer is multi-dimensional, that causal relationships between different types of value are challenging to establish, and that understanding of the success of the shared enterprise may change over time [15]. All these differences suggest that it may be more difficult to determine which outcomes of collective activity are valuable and what patterns of activity are more significant in OCoPs than in formal educational settings. Learning analytics for OCoPs may, therefore, need to be more exploratory than for formal educational experiences. Learning analytics should help community managers see a range of potentially notable patterns rather than simply tracking progress towards pre-defined indicators of success. Precisely because the range of potentially valuable patterns of content and activity are so diverse, learning analytics is likely to be powerful in enabling community managers and moderators to invest their expert interpretive attention more efficiently. 
 2. PLANNED APPROACH.
 Making visible the wide range of patterns of activity that are potentially actionable likely requires multiple methods. Initially, we plan to utilize learning analytics in two of the five categories proposed by Buckingham Shum and Ferguson [3], social learning network analysis and social learning context analysis. Social network analysis is our chosen method in the first category, and we are employing pre-hypothesis narrative analysis in the second. In future work, we hope to also explore content and discourse analysis. 
 2.1 Social Network Analysis.
 Our work with social network analysis (SNA) is furthest along, and an example of its application is presented in the next section. SNA has been used in previous research on OCoPs in education, but primarily to analyze discrete discussions or “friend” networks [2, 8]. In contrast, we are using SNA to explore as wide a range of relationships between community participants and content objects as is possible for a given community, beginning from whole-network maps such as those presented in the next section and only narrowing the scope of the analysis when potentially significant patterns or individuals have been identified. By representing OCoP platform usage data as a unimodal network of relationships between individual community members—the approach common in learning analytics applications of SNA, such as Social Networks Adapting Pedagogical Practice (SNAPP) [5]—we are seeking to identify community members who are particularly significant to the health of the community, either because they are highly influential, as signified by metrics such as eigenvector centrality, or because they frequently connect subgroups, as signaled by high betweenness centrality. These individuals may merit additional support or recognition from the community managers. In addition, we are visualizing the data as bimodal social network diagrams that explore the relationships between people and content objects, as illustrated in the next section. This alternative representation allows us to see patterns in subgroup activity that might not otherwise be detectable. For selected individuals identified as significant in either representation, we will create egocentric maps of their usage. If permission can be obtained, we will also create egocentric maps of their usage of Facebook and Twitter to obtain a fuller picture of how they connect with other professionals online. Patterns in this usage may suggest strategies individuals can use to increase the impact of their participation, as well as criteria for identifying potentially effective participants to recruit into the OCoP. 
 2.2 Pre-Hypothesis Narrative Analysis.
 Analysis of usage data can yield a rich representation of the dynamic of online activity, but understanding the impact of that activity on offline professional practice can be furthered through the collection of self-report data. To collect and analyze self-report data, we are employing Snowden’s [12] narrative analysis approach to identify patterns in the context of OCoP members’ experiences. In this method, narrative fragments—brief stories— are collected online using the CognitiveEdge SenseMaker Collector software, which also asks the respondent a series of closed survey questions about the context of the narrative, called filters. Community members recording and classifying narratives are given unique identifiers, making it possible to add their centrality metrics from SNA. Snowden calls this approach “pre-hypothesis narrative” research because it is designed to help analysts identify emergent patterns—“weak signals” in the terminology of complex systems theory—that might be missed if the impact of their cognitive biases—in this case, the beliefs they hold about what aspects of community context are significant enough to attend to—are not minimized. Techniques for minimizing bias include using prompting questions that encourage both strongly positive and negative stories, developing filter questions that mask the desired outcomes, and soliciting stories from a large, diverse group of respondents. Most significantly, analysts begin analysis by using the SenseMaker Explorer’s powerful visualization capabilities to look for patterns in the quantitative filter question data (including, in our case, SNA metrics) prior to analyzing the content of the narratives themselves. This abstraction helps to reduce the influence of the analysts’ interpretive predilections. Once a pattern is identified, researchers can drill down into the content of the narratives associated with it, which can become the subject of content analysis. This method of selecting samples of stories to analyze has the added benefit of making narrative analysis more time efficient as a formative evaluation strategy. 
 2.3 Content and Discourse Analysis.
 Eventually, we also hope to systematically analyze the content of members’ contributions to the communities, such as discussion posts and blog entries, as well as the narratives collected from them. We are particularly interested in extracting significant semantic concepts using automated tools such as Open Calais. Such automated analysis has the potential to help community managers discover emerging topics of interest to the community that could be incorporated into its editorial calendar. We are also evaluating tools for discourse analysis. These tools have the potential to help community managers understand what styles of discourse are most often associated with which community outcomes, enabling them to encourage the style most likely to help the community achieve its purpose. 
 3. CURRENT WORK.
 At present, we have begun SNA work in three OCoPs. Here, we present some very preliminary findings on the National Science Teachers Association Learning Center (NSTA LC) to illustrate our direction in working with these communities. The NSTA LC, launched in April 2008 through the efforts of Dr. Al Byers and his colleagues, aims to support science teachers in increasing their knowledge of science and of pedagogy [4]. It provides a rich source of (mostly free) learning materials and experiences for science teachers. The NSTA LC also hosts an online community through its “Community Forums.” NSTA members can initiate topics within any of a number of forums, or post to existing topics. 
 Figure 1: One year of 6978 posts made by 492 NSTA members to 557 topics within 21 forums. 
 We received NSTA LC forum posts for the full year, from 9/24/2010 to 9/28/2011. As a preliminary analysis of this forum activity, we used NodeXL [11], an open-source template for Microsoft® Excel®, to create bimodal network diagrams of the 6,978 posts made by 492 members to 557 topics within 21 forums during that time. Figure 1 is a low-resolution depiction of the patterns of these posts as edges between member nodes (black triangles along the left) and topic nodes (diamonds, along the right, colored according to their forums; 13 forums were private, labeled PrvF<n>). Member nodes are sized according to the number of different topics to which they posted; topic nodes are sized according to the number of posts made to them. Topic nodes are grouped by forum; within their forum group they are placed left to right by number of posts made to them. The member nodes and the forum groups are ordered top to bottom by their total number of posts. Edge color (black to yellow) and opacity are logarithmically proportional to the number of times a member posted to the topic. A frequency analysis (not shown) indicates greatly skewed distributions within the post data: a few forums received most of the posts; a few members initiated most of the topics and made most of the posts; and a few topics received half the posts. The figure captures this, with dense dark edges in the upper portion of the network, between the relatively few members and topics. However, the distribution of edges is not smooth from the upper portions of the network to the lower, and there appear to be different concentrations of edges in some regions. There might be some interesting activity by members in those regions, but with this static view, it is difficult to see what that could be. To tease out this information, we separated the data into 5 contiguous periods, Q1–Q5, each containing one-fifth of the posts, and created network diagrams for each period. These diagrams are shown in Figure 2. Nodes are placed exactly as in Figure 1 (i.e., according to total annual number of posts), but their sizes are relative to the number of posts made during the quintile. Likewise, edge color and opacity are relative to the data in the quintile. During the initial period, Q1, the activity is mostly by a very few active members, and there is very little activity in the lower part of the figure. During Q2 there are many new members, but their posting activity is fairly light. In Q3 something interesting develops: very heavy posting to the private forum Prv18 (pink, mid-diagram), mostly by moderately active posters, but also from a number of new members. The PrvF18 posts all but disappear in Q4, but the mid-active members remain somewhat active during this period. By Q5, they are posting quite heavily, and now to the more “standard” forums of Life Science, Earth and Space Science, and Physical Science. We need to examine these data further, but it is possible that this time series of network views has highlighted something that could prove useful to community managers. It might show how time-bounded activity targeted at some subgroup could be leveraged into more sustained and general engagement. The next steps in our analysis will include using the topic initiator information to create initiator-topic and member-initiator networks; transforming the bimodal data to create unimodal, member-member diagrams; obtaining additional member information, such as which online seminars they are attending, who is a member of a “cohort” (a district-wide NSTA LC professional development plan), and the points and badges they have attained for their activity in the Learning Center. We will also examine the social network analysis metrics produced by NodeXL, such as the centrality measures discussed in the previous section. 
 Figure 2. Network diagrams for five quintiles. Nodes are placed exactly as in Figure 1 (i.e., according to total annual number of posts), but their sizes are relative to the number of posts made during the quintile period. Likewise, edge color and opacity are relative to the data in the quintile. 
 4. FUTURE WORK.
 Overall, our work with learning analytics has two goals. The first is a traditional goal of research, to increase our knowledge of how OCoPs work and how to use them effectively. Our second, and perhaps ultimately more important, goal is to give community leaders and participants tools and techniques that can help them make better choices about leadership of and participation in such communities as part of their routine professional practice. By the conclusion of the Connected Educators project, we hope to offer tools that community managers themselves can use to analyze usage data analogous to what Social Networks Adapting Pedagogical Practice (SNAPP) offers for teachers using learning management systems. The NSTA example illustrates both goals. The general pattern of participation in a time-bounded subgroup transforming into more general sustained participation, should it also be observed elsewhere within the NSTA LC and in other OCoPs, may help us understand one way that individuals become persistently engaged in OCoPs. The specific pattern of the PrvF18 forum contributors becoming active in other popular forums may help NSTA managers identify other subgroups within the current membership that could be supported in making the same transition. Connected Educators might offer managers a set of NodeXL data providers, settings files, and macros that make such identification less complicated. Although our current primary focus is on learning analytics in the service of effective management and moderation of OCoPs for educators, we share Buckingham Shum and Ferguson’s [3] conviction that learning analytics should also be put into the service of helping individual learners. Learning analytics has the potential to guide individual educators in choosing how to connect with others online in support of their professional goals. For this potential to be realized, however, data about online participation and resource use need to be shared across community and network contexts. We are encouraged that the Learning Registry—an open, distributed infrastructure for learning resource sharing and discovery that launched in November 2011—enables sharing not just metadata but also paradata about resources [14]. Paradata capture how resources are used and evaluated, representing the contexts of learning. If privacy issues can be addressed, in the future the usage and self-report data we are analyzing within OCoPs could be shared across them as paradata via an infrastructure similar to the Learning Registry. Enabling educators to use learning analytics to examine distributed records of OCoP engagement at scale could help educators connect with each other much more powerfully and efficiently than is possible today.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>First Steps Towards a Social Learning Analytics for Online Communities of Practice for Educators</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>online communities of practice</dc:subject>
		<dc:subject>professional development</dc:subject>
		<dc:subject>social network analysis</dc:subject>
		<dc:subject>education</dc:subject>
		<dc:subject>paradata</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/darren-cambridge"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/darren-cambridge"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kathleen-perez-lopez"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kathleen-perez-lopez"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/14/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/darren-cambridge"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/kathleen-perez-lopez"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/15">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>APPLYING ARTIFICIAL INTELLIGENCE TO THE EDUCATIONAL DATA: AN EXAMPLE OF SYLLABUS QUALITY ANALYSIS</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/15/authorlist"/>
		<swrc:abstract>Developing new courses and updating existing ones are routine activities for an educator. The quality of a new or updated course depends on the course structure as well as its individual elements. The syllabus defines the structure and the details of the course, thus contributing to the overall quality of the course. This research proposes a new AI based framework to manage the quality of the syllabus. We apply AI methods to automatically evaluate a syllabus on the basis of such characteristics as validity, usability, and efficiency. We provide user trials to show the advantages of the developed approach against the traditional human-based process of syllabi verification and evaluation.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION: THE IMPORTANCE OF SYLLABUS STRUCTURE ANALYSIS.
 How to increase the efficiency of teaching is one of the top problems in this new information century. For years, the best educators have been developing new approaches to make the process efficient while keeping it affordable. From the beginning of the computer era, educators believe that informational technologies could significantly improve the outcomes that students get from the courses. Some of these expected advantages are now experimentally proven: -increase of student productivity [10] as the result of implementation of new teaching tools [3] and more relevant computer adaptive testing [18]; -increase of learning efficiency with the help of new forms of information presentation. For example, dual-coding approach [2] with use of multimedia allows students to show better performance on tests in comparison with those who learn from just animation or text-based materials. In exploring these advantages, many authors pay special attention to the structural changes in the learning process itself. IT changes the traditional roles of an instructor and a student and these changes are not always positive. An educator should strive to accentuate the positive and eliminate the negative elements of the course structure. Among well-known negative effects, we should mention digital plagiarism [19]; loss of learning efficiency due to disruption and information overwhelming [10][11]; visual or mental fatigue [22] and others. This paper considers a syllabus as a “good scenario” in which an instructor and a student play their roles to achieve course objectives. We concentrate on the quality of the syllabus and apply AI methods to eliminate negative elements from the syllabus structure. 
 2. SYLLABUS QUALITY CHARACTERISTICS FOR THE AUTOMATED ANALYSIS.
 Public and private institutions in different national educational systems use a variety of methods to evaluate syllabus or course quality [21]: -standardized tests to evaluate students’ levels of competence; -interview-like exams and final course projects; -student end-of-course evaluations. Many researchers state that such evaluations provide independent opinions that are correlated with students' knowledge and skill sets [5][14]. But some claim that students can misuse them or that they can be misinterpreted by the administration [8][15]. The evaluation of quality depends on the evaluators’ point of view. The European Foundation for Quality Management defines the following groups of evaluators: -the corporate world of potential employers; -students’ families and prospective students, who need information on which to base their choice; -alumni, who may require additional training and others; 
 Thus, it is necessary to align the syllabus structure with the sets of criteria and methods provided by the above-mentioned groups of evaluators. This work is obviously labor intensive and requires some level of automation. In practice the educator adopts the most convenient syllabus prototype from his or her archive or any other accessible source. For example, in computer related disciplines it could be community-driven Microsoft Faculty Connect (www.microsoft.com/education/facultyconnection/) or it could be MIT’s famous OpenCourseWare or some other discipline specific courses. 
 Unfortunately, there is no “search by quality” function in such archives. In recent publications, we can find methods for quality assessment [7][20], but most of these works are based on an expert-centric approach. Community-driven resources in most of the cases have some ranking mechanisms mostly based on user opinions. In this research, we choose a statistical approach because it makes the evaluation process more objective. The following quality characteristics have been studied in this paper: 
 - Validity. Each syllabus has an underlying cognitive model of the subject as a basis for the set of topics to be taught [9]. This model should be valid. To evaluate its validity, we need to check how closely the real outcomes of the course match the expected outcomes, as declared by the course author. - Usability. The problem of syllabus usability has two dimensions: usability for the student and usability for the instructor. The first is mainly about the accuracy of the course description and clarity of the objectives. The syllabus presentation should facilitate the understanding of its content. One of the key factors here is the requirement for a syllabus to be understandable by a student. The issue of instructor usability focuses more on the ease with which the educator can implement and modify / re-use the syllabus. - Efficiency. Because all educational institutions have resource constraints, the syllabus should consume the minimum amount of resources while achieving the course objectives. For example, if one can show that, in a Digital Systems course, students have mastered the same skills using simulation software instead of expensive circuit boards, and then we should recommend the syllabus that uses such simulation software. E.g. one structural element of the course has been substituted with another one. Statistics procedures could be used for syllabus evaluation across the characteristics discussed above. To perform such an evaluation, an institution has to have a database with course outcome metrics, information about student performance, estimates of course implementation costs, faculty evaluations and other. Fortunately, all this information could be found in course management systems such as BlackBoard, Angel, Moodle, etc. 
 3. ARTIFICIAL INTELLIGENCE FOR QUALITY EVALUATION.
 A key feature of the proposed approach is the two-step procedure based upon “a priori” and “a posteriori” syllabus validity and efficiency evaluation. The first evaluation detects a number of latent problems in the syllabus structure with the help of rule-based knowledge base. With such an evaluation, one can correct the faulty course structure before implementing the syllabus in an actual class. The second evaluation (a posteriori, on the basis of students’ examination results and some additional information) allows us to measure how close the real student outcomes are to the expected outcomes. This is an intelligent criterion for step-by-step quality improvement. To evaluate “a priori” validity, we analyze the consistency of the syllabus. Even though the syllabus structure differs from one educational institution to another, some acknowledged patterns could be found in the vast majority of syllabi ([12][6]): 1. basic information (current year and semester, course title, etc); 2. prerequisites for the course; 3. general learning goals or objectives; 4. conceptual structure of the course, textbooks, assignments, term papers, and exams; 5. activities, grading and evaluation criteria, policies and others. 
 These sections are linked to each other and these relations are not homogeneous (see Figure 1). In fact, this is a semantic network. And this network contains some problems in its structure. For example, the Course calendar and the Textbooks sections are linked by the «refers to» relation. Sometimes, the books are listed in one section, and also repeated in another one. This duplicate causes a problem while modifying the syllabus. 
 Figure 1. Syllabus as a semantic network. 
 The “mapping” relation requires explanation. We suggest using it if one element of a syllabus affects the content of another one. For example, the same set of learning objectives might be fulfilled by various conceptual structures, a topic understanding could be assessed with one or more tests (see Figure 2), etc. 
 Figure 2. Mapping of tests to topics. 
 Table 1 shows the most typical relations between elements of a syllabus. This table highlights the fact that almost every type of relation may have problems in real syllabi. For example, some sections might be linked to non-existent elements («dead link»); some elements may be not linked at all («redundant link»). Please note, that we describe these problems in the term of “rules.” A link is “dead” if there is no appropriate book in the “textbooks” section. 
 Table 1. Typical Syllabus Section Relations. 
 To obtain a complete description of the syllabus structure, we also need to consider internal relations within syllabus elements. For example, the internal structure of the «textbooks» section has the one relation type «alphabetical order». Another example of internal relations is shown in Figure 3. 
 Figure 3. An internal structure and two corresponding syllabi. 
 In Figure 3, one can see that two possible pathways on the negotiated syllabus [4] lead to the same final exam. These syllabi are conceptually equal but might demand different amounts of resources (e.g. time, labs, etc). To perform the “a priori” statistical evaluation of the syllabus quality, we need to count numerous problems detected in the structure of the syllabus. Being hardly implemented with a traditional expert-based evaluation process the problem solves easily with the help of ordinal rule-based system and the rules like the following: Count “limit violation problem” if “time for the test is more than the class time.” 
 A posteriori validity evaluation is based on a statistical procedure that compares expected student outcomes and examination results (interviews, tests or practical work). There are several methods to calculate a posteriori validity, and we employ the simplest one. One is expected grade distribution set by an institution. For example: about 15 percent of students get «A» mark, about 20 percent get «B», about 50 percent get «C and D» and up to 15 percent fail a course. If the real results are significantly better (see Figure 4) than the expected distribution, we consider the course to be too simple for the students. If the results are shifted towards poor grades, then the course is considered to be too difficult for them to understand. This could be caused by insufficient methodological supplies, by the professor’s errors, or by some other reasons. We shape some requirements to the statistical distribution of results. 
 Figure 4. Exam results for the very difficult, very simple courses and a course of required difficulty. 
 We believe that the worst results appear when the outcome is far different from the expected statistical distribution. For the above-mentioned requirements, the mean≈3.5 and the dispersion≈0.99. If we consider the worst possible outcome (100% of students got «F» mark), mean=2, and the dispersion=0. Finally, if consider the "best possible" result (in fact this result is very unwanted) the mean=5, and the dispersion=0. We use the chi-square method to determine whether the real course outcomes are close to the declared ones. The course is reliable if it is valid in a number of applications. One of the simplest ways to calculate the reliability of the course is a re-examination. The course is reliable if the result of reexamination repeats the original examination result. If the reexamination result is better, then we believe that a student has obtained some additional materials (not included in the course) that helped him or her to improve the result. So, the course is not reliable (maybe due to the insufficiency). If the re-examination result is worse than the first examination, then the course is also not reliable (perhaps due to the redundancy or because it is too hard for students). To calculate the reliability, a simple sign test method could be implemented. It allows the detection of some typical shifts in data. The following section is dedicated to the practical implementation of the discussed theoretical issues with the help of specially developed software tool, called “Chopin.” 
 4. EXPERT SYSTEM PROTOTYPE.
 System prototype has been created to test the above mentioned statistical and rule-based techniques. Top level system structure is presented in Figure 5. System employs two internal languages to represent all its data: Test Description Language (TDL) and Expert Description Language (EDL). TDL describes adaptive test structure. It is similar to the well-known QTI language by IMS Global Learning Consortium [13] used in modern CSM (e.g. Moodle), but also has some advanced features [17]. EDL describes the syllabus as a mathematical graph (see Figure 6). An instructor can draw a graph with EDL editor or fill it in as a MS Word template which converts itself to EDL automatically. The program called VIPES applies rules, evaluates the syllabus quality and visualizes the syllabus as a graph. When a student starts the VIPES program, it graphically visualizes the syllabus and the student’s achievements. The student chooses the objective for his/her next activities, and the system generates the learning path consisting of virtually any type of activities, from reading to peer-reviewing of submitted assignments. The choice here is to generate either the complete path to the end of the course or just to cover some topics. If the generated plan (“scenario”) does not match the student’s expectations, then she/he can ask the system to schedule an online chat with the instructor. This is a very effective new teaching method that is hardly feasible in the traditional class format. Now instructor concentrates on the core of the student’s problem, while the VIPES system helps to diagnose and visualize the problem. In creating the syllabus, an instructor develops a set of scenarios to match the needs of as many students as possible. Thus, each syllabus is a set of scenarios (in contrast to a traditional single-scenario syllabus), so it has more chances to fit the needs of an individual student. On the other hand, it takes much more time to create and test a multi-scenario syllabus. That is why this approach demands automatic quality correction procedures. Using this system, with data collected over more than seven years, the set of experiments have been performed on syllabus quality evaluation and syllabus structure variations. Results of two experiments are presented in the following section. 
 5. RESULTS AND DISCUSSION.
 To study the effectiveness of the proposed approach and to evaluate the scope of its implementation, we discuss two continuous experiments here. The first one was aimed at evaluating the quality of many syllabi, and, in the second experiment, we monitored the structure changes and evaluations of one syllabus for several years. For the first experiment, we have selected a set of 15 syllabi (Altai State Technical University, MIS program) and performed the “a priori” quality estimation (see Table 2). While performing evaluations, we eliminated the identified problems by restructuring of the syllabi. We also checked the correlation between the number of identified problems and the students' results. The experiment shows the significant correlation between syllabus consistency and students’ outcomes. As can be seen from Table 2, there is a direct relation between the number of consistency violations and the percentage of unsatisfactory student outcomes. Different consistency problems affect student results differently: for example, “insufficient mapping” affects the students' results more than “non-optimal mapping,” etc. In practice, the majority of syllabi which were already checked by experts had some level of consistency problems. 
 Figure 6. A syllabus as an EDL script and its graph visualization. 
 Table 2. A fragment of “a priori” monitoring of the syllabi database. 
 Table 3. “A posteriori” monitoring and optimization of the “E-business” syllabus. 
 The second part of the user trials was to follow the changes in one syllabus. The E-Business course was selected for this part of the research. It was offered several times a year for different majors. The students’ results showed some deviations, the causes of which were not immediately obvious. Sometimes, all the students were very successful in examinations, and sometimes, the majority of students got poor results. The course was originally taught by one professor. Later, the theoretical and practical parts of the course were separated and taught by two different professors. Such a change was implemented to make the students’ evaluations more clear and to exclude any personal preferences from the examination. It was necessary to establish an indicator that would inform the department chair that some problems must be corrected in order to improve the quality of the course. Table 3 shows the results of the validity and reliability monitoring. The table shows how the validity and reliability characteristics could be applied as objective tests to changes in the syllabus. The professors modified the syllabus step by step until its validity and reliability were not stabilized. Later they updated its contents and optimized it with the same characteristics as the goal function. They assumed the elements to be included to the course (such as on-line materials instead of paper-based) and checked the efficiency of these changes with the objective criteria. The users (instructors, administrators and students) agreed that this approach has positive effects on the syllabi database. They stated that it helps them to make the evaluation more objective and to highlight the key factors affecting the quality of teaching. The users also agreed that this AI-based technique is a promising one for accreditation activities. The comparison of syllabi/curricula is one of the most critical stages of this process. This approach contributes also to the theory of “semantic matching” problem. It is also applicable in practice, in the case of a syllabus we need to compare short parts of documents: in particular, the course goals and the description of course content should be compared with appropriate parts of the guide from the accreditation body. This could be considered as a possible new direction for development of these software tools, i.e., as an extension for this project. Another promising extension is related to the development of EDL and TDL languages which promise to be more efficient in their narrow fields than the existing common ones [17][16]. The existence of well defined and statistically proven criteria for the evaluation and optimization of test and syllabus structure gives us a prospect to build up a complete logical system for this particular case of educational data analysis.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>APPLYING ARTIFICIAL INTELLIGENCE TO THE EDUCATIONAL DATA: AN EXAMPLE OF SYLLABUS QUALITY ANALYSIS</rdfs:label>
		<dc:subject>architectures for educational technology system</dc:subject>
		<dc:subject>evaluation methodologies</dc:subject>
		<dc:subject>intelligent tutoring systems</dc:subject>
		<dc:subject>cognitive graphics</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/denis-smolin"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/denis-smolin"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sergey-butakov"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sergey-butakov"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/15/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/denis-smolin"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/sergey-butakov"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/16">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Seeing What the System Thinks You Know - Visualizing Evidence in an Open Learner Model</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/16/authorlist"/>
		<swrc:abstract>User knowledge levels in adaptive learning systems can be assessed based on user interactions that are interpreted as Knowledge Indicating Events (KIE). Such an approach makes complex inferences that may be hard to understand for users, and that are not necessarily accurate. We present MyExperiences, an open learner model designed for showing the users the inferences about them, as well as the underlying data. MyExperiences is one of the first open learner models based on tree maps. It constitutes an example of how research into open learner models and information visualization can be combined in an innovative way.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Learner models [1] are at the core of adaptive learning systems, as they enable a system to adapt to individual learning needs. The accuracy of a learner model is the baseline for the usefulness of the adaptation decision. A variety of learner characteristics (knowledge, interest, learning style, etc.) may be represented in a learner model. Within this paper we concentrate on the learner’s knowledge state. A learner’s knowledge state is highly dynamic: it may increase (through learning) or decrease (through forgetting) from session to session. To make sure that the learning system can adapt to the knowledge of its users, continuous maintenance of the learner model is necessary. In our previous work, we have suggested Knowledge Indicating Events (KIE, related to evidence bearing events as described in [1]) as a means for non-invasively diagnosing user knowledge in an adaptive learning system [2]. Similar approaches to assessing user knowledge from different sources of evidence have been proposed with the Cumulate server of KnowledgeTree [3] and the Personis server [4]. Conceptually, the idea of KIE is in line with evidence-centered assessment design as suggested by [5]. In a nutshell, KIE are naturally occurring user actions (e.g., selecting a link, accessing a learning hint) that are interpreted as evidences for a user’s knowledge state. The main benefit of assessment based on KIE is that the diagnosis of user knowledge happens non-invasively, i.e. no additional interaction with the user is needed. This makes the KIE approach especially useful for systems in other fields than school or university settings such as workplace learning (e.g., [6]) where tests of knowledge and skill are typically not an option. While having obvious benefits, assessment based on KIE bears at least two serious drawbacks. first, the method makes a number of assumptions (e.g., “a person who clicks on the ‘help’-button for a concept has little knowledge of this concept”) that may not be accurate in all cases. The accuracy of the learner model, however, is a key issue in adaptive learning environments. Second, the algorithms for inferring user knowledge are based on aggregations of all KIE that occurred for each user. This leads to complex data and inferences that are not necessarily understandable for the users. Open learner models (for an extensive overview see [7]) have been proposed to improve understandability and accuracy of the learner model and, as a consequence, the adaptation. In an open learner model, the contents of the learner model are made visible to the learners. Some open learner models also allow the user to correct entries or to suggest additional information. Besides their benefits for improving accuracy of the content in the learner model, we regard an open learner model as a powerful means to enhance transparency and understandability in KIE based assessment approaches. There is broad agreement in the open learner model community that users should be offered an overview of the information in their learner model ([7], see also [8]). This is difficult for large and complex data sets, and most of the existing open learner models presented in [7] are not able to provide such an overview as the number of concepts increases. For a different context than learning, visualizations providing an overview of large models exists [8]. However, these visualizations are not applicable to our problem, as the structure of the underlying models is entirely different. In this paper, we present MyExperiences, an open learner model for evidence-centered assessment based on KIE. MyExperiences is implemented by using the design principles of information visualization which are condensed in Shneiderman’s well-known information visualization mantra ‘Overview first, zoom and filter, then details-on-demand’ [9]. The space-filling approach of MyExperiences is applicable to large and complex learner models. The aim of this paper is to present an innovative way of visualizing learner models that are based on the KIE approach. 
 2. MyExperiences: APOSDLE’ S OPEN LEARNER MODEL.
 MyExperiences is the open learner model of APOSDLE, an adaptive work-integrated learning system. The aim of APOSDLE is to improve knowledge worker productivity by supporting learning within everyday work tasks. Within APOSDLE, the learner model is used for ranking learning goals, recommending useful learning content, and for suggesting knowledgeable people (for details see [2]). As an integral part of APOSDLE, MyExperiences has been developed in an iterative three-year participatory requirements and design process. In this process, users from several knowledge-intensive work domains have been involved from the start through such methods as use cases and personas, scenario-centered design, formative and summative evaluations in the lab and the field. These design methods are reported elsewhere (e.g., [10, 11]). MyExperiences allows each user to access his or her own learner model in order to (i) understand recommendations and suggestions within APOSDLE, and (ii) improve the learner model’s accuracy by contributing information to it or altering information in it. 
 2.1 The APOSDLE Learner Model.
 The APOSDLE learner model is designed as a layered overlay of the APOSDLE domain model. APOSDLE can be instantiated to various domains by creating new semantic domain models. Typically, an APOSDLE domain model consists of approximately 100 to 150 domain concepts. For each concept, one of three knowledge levels is distinguished: learner, worker, and supporter. A learner in a topic is a user who uses the system with regard to this topic mainly for learning purposes, e.g. in order to receive in-depth information and hints. If a person is considered a worker in a topic, the person has worked on the topic without requesting further help. A supporter in a topic is a person who provides additional information for that topic, or who is contacted by another person to provide support for the topic. The users receive different recommendations for a topic (different types of resources etc.), depending on the detected knowledge level. The three knowledge levels are automatically diagnosed applying the KIE approach. To define KIE within APOSDLE, we first analyzed possible user actions with regard to how typical they are for one of these three levels. Then, we assigned the most typical events to each of the three knowledge levels. For instance, one learner event is “asking for a learning hint for a concept”. An example for a worker event is “performing a task which requires knowledge about a concept”. “Being contacted by someone else about a concept” constitutes an example for a supporter event. For maintaining the learner model, all KIE of a user within APOSDLE are logged. The inference to one of the knowledge levels is based on the proportion of KIE assigned to the different levels: The knowledge level of a user in a topic is that level for which the highest proportion of KIE has occurred. Clearly, different inference algorithms are conceivable. In the following, to make it easy to understand, we assume the simplest case where the inferred knowledge level of a user in a concept always is the level for which the most KIE have been observed. 
 2.2 MyExperiences: Overview, filter, Zoom and Details-on-demand.
 In order to fulfill the first aspect of Shneiderman’s mantra, ‘overview first’, the data structure of the data needs to be considered. In terms of data structures, the APOSDLE open learner model can be viewed as a forest of trees with the knowledge levels being the roots of the trees. Then, the children of each root node (knowledge level) are all concepts for which the algorithm inferred the corresponding knowledge level. The children of each concept again are the KIE that occurred for the respective concept. Different visualization techniques are conceivable for visualizing tree structures (e.g., node-link diagram, hyperbolic tree, tree map) [9]. Studies reveal that explorer-like visualizations (further referred to as tree view) are superior to all other tree visualization techniques for almost all tasks [12]. The drawback of tree views is that they do not allow a big picture overview if the data set is large and complex - which is the case for the APOSDLE learner model. In contrast, tree maps [13] enable a big picture overview of large hierarchical information structures and have shown to be quickly learnable by users unfamiliar with the visualization [14]. Therefore, we chose to combine the familiar tree view and the powerful tree map as coordinated multiple views (i.e., interactions in one view are instantly reflected in the other view). figure 1 shows an example of the resulting visualization for all three knowledge levels for a user who has been working with the APOSDLE system for several hours. The domain for which the system was created is innovation management. Typical tasks in this domain are writing an offer for an innovation project for a customer, or designing a creativity workshop. MyExperiences is divided into three rows, one for each knowledge level. Each knowledge level has a specific color. These colors are also used throughout the APOSDLE system whenever a visual item refers to that knowledge level. Each row in MyExperiences consists of a tree view (left) and a tree map view (right). Both views are coordinated, i.e. when a user clicks on a concept in one view, the concept is also selected in the other view. 
 figure 1. MyExperiences: The tree map-based open learner model of APOSDLE. 
 Each tree map gives an overview of all the concepts for the specific knowledge level. The number in brackets indicates the number of evidences that occurred for the specific concept. E.g., for the concept “Marketing” (in the topmost map) five KIE occurred so far. These events belong to two different types of KIE indicated by the concept rectangle separated into two parts. The color of such a sub-rectangle is derived from the number of events for this type of event and the total number of events for this concept. The lighter the color, the higher is the relative occurrence of this specific event for the specific concept. In other words, an equal color distribution means that all events occurred equally often, whereas one light color between darker ones indicates that one type of event occurred much more often than the others. The second aspect of Shneiderman’s mantra, ‘zoom and filter’, is realized with the search functionality on the top left. Search has shown to improve usability, if the user’s task is search rather than exploration [12]. With regard to Shneiderman’s third aspect ‘details on demand’, the user has several possibilities to interact with MyExperiences. Common interaction techniques like selection and zooming enable the user to investigate the open learner model, either as an overview or in detail. Zooming one step into the tree map allows the user to understand why the specific knowledge level was inferred for this concept. figure 2 shows this zoomed view for the concept “Kreativitätstechniken” (creativity techniques) of the bottom tree map of figure 1. In this zoomed view, the user is provided with the additional information concerning the frequency of events occurring for this specific concept. Remember that the inference of a knowledge level is a majority voting. Hence, the events that occur for one and the same concept may vote for different levels. This is the case, for instance, for the event “Select Learning Goal” in figure 2 (second from left), which is a learner event. We will address this issue in the discussion in Section 3. The user also can alter the knowledge level for any concept with right-click on the concept in either the tree view or tree map. Concepts with manually altered knowledge levels are then marked with an asterisk, like the concept “Kreativitätstechniken” (creativity techniques) in the bottom tree map in figure 1. This enables the user to differentiate automatically inferred (observation) and manually changed (explicit evidence) levels. Manual changes can be reset at any time either individually or all at once. 
 figure 2: Zooming into a concept reveals the knowledge indicating events. 
 3. DISCUSSION AND FUTURE WORK.
 We have presented MyExperiences, an open learner model for evidence-centered assessment based on KIE. MyExperiences was implemented by using the design principles of information visualization and allows overview and detail for large and complex learner models. The search functionality facilitates direct access to specific areas of interest which may be useful, e.g., for reflection. Users can interact with MyExperiences to improve the learner model’s accuracy by correcting the inferred knowledge level of different concepts. Outcomes of informal interviews with APOSDLE users indicate that MyExperiences helps users to understand the underlying models and inferences, and supports reflection and meta-cognitive processes. Because of its unusual look-and-feel (MyExperiences is one of the first attempts to use tree maps for learner modeling), the users did not understand intuitively at the first glance what the purpose of the tool was. However, once the concept was explained to them, they did not seem to have difficulties to understand and use the interactive visualization. Still, these findings need to be evaluated systematically in usability studies, with control groups where MyExperiences is compared to other visualizations of KIE based learner modeling. Strictly speaking, the current version of MyExperiences visualizes the approach of KIE in a simplified manner: As described above, KIE voting for different knowledge levels might have occurred for one and the same concept. However, this is not represented in the second level of the tree map where all KIE for one concept have the same color (figure 2). The main reason for this design decision was that the visualization should be usable for all different kinds of algorithms underlying the inference of the knowledge level. While in the case of a simple majority voting, the visualization of the second level in the tree map might be straightforward, this would not be the case, e.g., if the inference algorithm uses a weighting scheme. Concerning the visualization, we see three main avenues for follow-up. first, we plan to provide users with visual clues about uncertainty in automatically inferred knowledge levels. Second, it might be helpful for the users to see the development of their knowledge levels over time (history). Third, it may be interesting to take into account the representation for the rest of the users such as the gathered history of transaction data, or group average. In this line of research, an approach for visualizing user models of other users with tree maps (not based on KIE) has been recently presented by Peter Brusilovsky’s group [15]. While being aware of the fact that in-depth usability studies of MyExperiences are still missing, we believe that using tree maps for representing information about users combines research into open learner models and information visualization is an innovative and promising way. 
 ACKNOWLEDGEMENTS.
 The Know-Center is funded within the Austrian COMET Program - Competence Centers for Excellent Technologies under the auspices of the Austrian Ministry of Transport, Innovation and Technology, the Austrian Ministry of Economics and Labor and by the State of Styria. COMET is managed by the Austrian Research Promotion Agency ffG. APOSDLE (www.aposdle.org) has been partially funded under grant 027023 in the IST work programme of the European Community.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Seeing What the System Thinks You Know - Visualizing Evidence in an Open Learner Model</rdfs:label>
		<dc:subject>open learner model</dc:subject>
		<dc:subject>visual learning analytics</dc:subject>
		<dc:subject>knowledge indicating events</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/barbara-kump"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/barbara-kump"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/christin-seifert"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/christin-seifert"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/guenter-beham"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/guenter-beham"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tobias-ley"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tobias-ley"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/stefanie-n-lindstaedt"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/stefanie-n-lindstaedt"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/16/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/barbara-kump"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/christin-seifert"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/guenter-beham"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/tobias-ley"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/stefanie-n-lindstaedt"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Educational Monitoring Tool Based on Faceted Browsing and Data Portraits</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17/authorlist"/>
		<swrc:abstract>Due to the idiosyncrasy of online education, students may become disoriented, frustrated or confused if they do not receive the support, feedback or guidance needed to be successful. To avoid this, the role of teachers is essential. In this regard, instructors should be facilitators who guide students throughout the teaching-learning process and arrange meaningful learner-centered experiences. However, unlike faceto-face classes, teachers have diﬃculty in monitoring their learners in an online environment, since a lot of learning management systems provide faculty with student tracking data in a poor tabular format that is diﬃcult to understand. In order to overcome this drawback, this paper presents a novel graphical educational monitoring tool based on faceted browsing that helps instructors to gain an insight into their classrooms’ performance. Moreover, this tool depicts information of each individual student by using a data portrait. Thanks to this monitoring tool, teachers can, on the one hand, track their students during the teaching-learning process and, on the other, detect potential problems in time.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 In the last years, distance learning has become very popular, especially, its modality of online education. In this regard, online students have diﬀerent characteristics, needs and preferences to the ones who attend face-to-face classes. For instance, online learners are often over 30 years old, which means having learners with diﬀerent professional and educational backgrounds in the same classroom. This heterogeneity entails helping students with diﬀerent needs at the same time. Likewise, because most learners are adult, they have a lot of responsibilities, such as: working, taking care of children, etc. Consequently, it is hardly surprising that they prefer studying at their convenience, i.e. anytime, anywhere and at their own pace. As a result of the aforementioned particularities, online education should avoid using a teacher-centered paradigm, since it treats all learners as if they were the same and forces them to do the same things in the same amount of time [32]. Instead, online education, as any kind of distance learning, involves a student-centered approach in which the instructor is a facilitator and students engage in peer learning [19, 25]. Thus, students are required to take primary responsibility for their learning process [4]. To do this, learners need to have some speciﬁc skills, e.g. self-regulation. However, some studies [23, 26] provide evidence that a lot of students need some help to learn these skills, since most of them are not able to achieve these abilities on their own. Consequently, the role of teachers shifts from a masterful ﬁgure to a facilitator who guides learners throughout the course and arranges meaningful learner-centered experiences [29]. This role is essential in online education, since learners may become disoriented, frustrated or confused if they do not receive the support, feedback or guidance needed to be successful [8]. In order to carry out a good instruction in an online environment, instructors need appropriate means to set up a monitoring process so that they can be aware of the students’ learning process and provide learners with just-intime assistance. In addition, monitoring allows teachers to forecast potential problems (e.g. dropouts) and avoid them in time. In this regard, instructors can use tracking information (e.g. logs) and monitoring tools which are available in learning management systems (LMSs). However, a comparative study on tracking functionalities of various LMSs [10] concluded that none of them oﬀer much tracking ability. One of the reasons of this devastating conclusion may be the fact that these platforms often provide tracking data in a tabular format which is commonly poorly structured, incomprehensible and diﬃcult to understand [21]. To avoid the aforesaid drawbacks, this paper presents a novel graphical educational monitoring tool based on faceted browsing and data portraits. On the one hand, faceted browsing, as an exploratory search technique, helps instructors to narrow the class roster down until ﬁnding those learners who meet teachers’ requirements. On the other hand, data portraits depict summarized information about an individual student in a single image. The rest of the paper is structured as follows: a categorization of educational monitoring tools can be found in the next section. In section 3, an educational monitoring tool based on faceted browsing and data portraits is proposed. Finally, conclusions and future work are in section 4. 
 2. CLASSIFICATION OF EDUCATIONAL MONITORING TOOLS.
 This paper classiﬁes educational monitoring tools into two categories: (1) according to the data processing techniques that they use, and (2) the element which they monitor. They both are described below. 
 2.1 Data Processing Techniques.
 The main goal of any monitoring tool, whether this has an educational purpose or not, is to give users insights on the data at which they are looking. To this end, two diﬀerent data processing techniques can mainly be used. On the one hand, information visualization (IV) techniques and, on the other, data mining (DM) algorithms. Next, they both are explained in sections 2.1.1 and 2.1.2, respectively. At the same time, some relevant educational examples related to each method are described brieﬂy. Finally, a brief discussion about the main diﬀerences between IV and DM is presented in the section 2.1.3. 
 2.1.1 Information Visualization.
 One of the most well-known deﬁnitions of information visualization (IV) was proposed by Card et al. [3]: “the use of computer-supported, interactive, visual representations of abstract data to amplify cognition”. Thus, IV encompasses a set of techniques that transform data into eﬀective graphical representations by taking properties of human visual perception into consideration. Thereby, these visual representations reveal facts and trends which allow users to infer some unknown information by combining the visual inputs with their knowledge of data. To stress that the data processing which is performed in IV is mainly based on simple mathematical and statistical functions, such as sum, calculation of percentages, mean, median, mode, and so on. In the educational context, some proposals based on IV techniques have been suggested. Two of the most relevant tools are CourseVis [21] and, its successor, GISMO [22]. The former is a stand-alone visualization tool that obtains tracking data from WebCT, transforms them into a form convenient for processing and generates graphical representations that can be explored and manipulated by instructors. Thereby, teachers can examine social (it uses a 3D visualization of discussion boards), cognitive (it uses a matrix in which each cell is the grade attained by a speciﬁc student in a particular quiz), and behavioral (it shows information about access and participation in a timeline) aspects of students. CourseVis mainly uses 2D visualization techniques, but it also uses color and shape as a third dimension. As far as GISMO is concerned, this also uses LMS tracking data, but in this case from Moodle, to display graphical representations (e.g. bar charts, matrices, etc.) about overall classroom accesses and detailed information of a speciﬁc student. This was developed as a Moodle block, but its interface is detached from Moodle’s one. Similarly, Zhang et al. [35] designed Moodog, a visual student tracking data plug-in for Moodle which also sends automatic reminder e-mails to students. This displays information about the course, students, resources and access time. Unlike GISMO, the information provided by Moodog is integrated into Moodle’s interface, keeping the original Moodle’s layout as much as possible. There are many other works that have also proposed different representations based on IV techniques. For instance, Hardy et al. [9] constructed, as a graph, the route taken by one student through the course material during a single work session; Hijon-Neira and Velazquez-Iturbide [11] used an interactive graph (for students’ grades) and a data mountain (for access) by using Prefuse API; Juan et al. [14], in turn, proposed scatter plots and quadrants as well as an evolution graph in order to represent students’ performance; and Bakharia et al. [2] proposed SNAPP, a social network analysis tool that displays, as a graph, the evolution of participants’ relationships within an LMS’s discussion forums. 
 2.1.2 Data Mining.
 According to Romero and Ventura [28], data mining (DM) techniques can be classiﬁed into four categories: (1) clustering, classiﬁcation and outlier detection; (2) association rule mining and sequential pattern mining; (3) text mining; and (4) statistics and visualization. However, the latter is not universally seen as a category of DM [1], since the data processing that is carried out is minimum compared to the rest of categories, which are based on artiﬁcial intelligence algorithms. In fact, the fourth category proposed by Romero and Ventura accords with the deﬁnition of information visualization (IV) presented in the section 2.1.1. Hence, in this paper, only the ﬁrst three categories deﬁned by Romero and Ventura are considered as belonging to data mining. DM techniques are able to infer underlying patterns from a large database, generating some new type of valuable information such as student models or predictions. These new data can be delivered in writing or visually. For some time now, more and more educational monitoring tools are based on DM. For example, Kosba et al. [17] developed TADV, a system that builds student, group and classroom models by using fuzzy logic. From these models, TADV gives instructors advice, e.g. to advise a learner to review a concept. Thereby, teachers have extra information to make appropriate decisions during the course. On the other hand, Hung and Zhang [12] used statistical models and machine learning algorithms to analyze patterns of online learning behaviors and, at the same time, to make predictions on learning outcomes. Zorrilla et al. [37], in turn, proposed a decision support system that utilized diﬀerent techniques. First, two clustering algorithms, Expectation-Maximization (EM) and KMeans, were used to characterize, on the one hand, students and, on the other, sessions. The output of EM, a probability distribution, allowed to determine the number of clusters with which K-Means would be executed. Depending on the input data, the clusters obtained from K-Means described either student behavior models or session patterns. In addition to the information provided by the clusters, the proposal of Zorrilla et al. also indicated the resources that were commonly used together. To obtain these data, Apriori, an algorithm for ﬁnding association rules, was employed. More proposals of educational monitoring tools based on DM techniques can be found in [1, 28]. 
 2.1.3 Information Visualization vs. Data Mining.
 One of the main diﬀerences between information visualization (IV) and data mining (DM) is how new information is inferred. Due to the complexity of the algorithms used in DM, these are able to suggest latent information with an explanation based on text, rules, clusters, etc., e.g. “student S is about to drop out because she has not acceded to the classroom for two weeks”. By contrast, in the case of tools based on IV techniques, new information is not inferred by an algorithm, but by users through observing graphics and taking advantage of their knowledge of the domain. As a result of the manner of inferring new information, two issues arise: (1) reliability, and (2) computational cost. With regard to reliability, the users of tools based on DM must rely on the accuracy of the information suggested by the algorithm, whereas the users of tools based on IV infer reliable information based on their own expertise. As far as computational cost is concerned, IV techniques calculates statistical data that require minimum processing, while tools based on DM executes complex artiﬁcial intelligence algorithms which are usually time-consuming. Another important aspect is user-friendliness. Merceron and Yacef [24] claim that it is essential to use techniques and measurements which are fairly intuitive and easy to interpret. In this regard, the explanation provided by the tools based on DM often requires users to master the algorithm so as to understand it. Unfortunately, a lot of users neither have this knowledge nor can make time for acquiring it. On the contrary, tools based on IV seem to meet these requirements better, since they use simple statistical data and focus on displaying information in a visual and eﬀective way. Despite the diﬀerences between information visualization and data mining, they can work together in an application. For instance, a system can ﬁrst infer some latent information from a large database by using a DM algorithm and then display it by using a visual representation based on IV techniques. Thereby, users can beneﬁt from the advantages of each technique: on the one hand, the possibility of inferring new data automatically and, on the other, the capability of visual representations to convey information eﬀectively. This combination of DM algorithms with IV techniques is called visual data mining [15]. There is every indication that this new discipline will become a promising research area. 
 
 2.2 Monitored Element.
 Although the most popular name is student monitoring tools, it would be more correct to call them educational monitoring tools. The reason is that they can monitor other items in addition to learners. In this regard, this section describes the elements that are commonly monitored in an educational context. Finally, to stress that both IV and DM techniques can be used to track any of the following items. 
 2.2.1 Classroom.
 The most common item is the whole classroom, i.e. all students are considered a unique entity. This entity is characterized by the overall information that comes from combining all students’ tracking data (e.g. class’s average grade in each assignment). Thereby, instructors are provided with an overview of their learners’ performance. Thanks to this, teachers can make decisions that aﬀect the whole class. Most educational monitoring tools show data of the classroom. 
 2.2.2 Student Group.
 A particular case of the previous monitoring is the one which focuses on groups. This is useful when group activities are proposed, e.g. tasks that belong to a project-based learning. The supervision of groups allows to obtain information about how students interact each other, who are the most and least participative members of a group, which group has the best and the worst performance, etc. [2, 14, 27] are examples of group monitoring tools. 
 2.2.3 Individual Student.
 Teachers often need to take a closer look at a particular learner or make comparisons between students. Consequently, they need tools that provide detailed information about an individual learner (e.g. how many times a student has accessed the LMS). These data may help instructors to gain understanding of the reasons why a speciﬁc learner has a particular behavior. Thereby, teachers can oﬀer each student a better support and a tailored learning experience. Regarding this, CourseVis [21] and Moodog [35] are two tools that display information of a particular student. 
 2.2.4 Resources.
 The term resource encompasses a wide spectrum of elements, from learning materials (e.g. documents, videos, quizzes, etc.) to educational tools (e.g. forum, chat, LMS’s pages, etc.). Regarding this, there are multiple types of data related to resources that can be tracked, e.g. how many times a document has been read, how many messages a forum has, the path that a learner followed while she was navigating through an LMS, etc. Hence, monitoring resources may provide teachers with valuable information about the instructional design (e.g. to detect a bad design of content pages) and students’ performance (e.g. to detect whether learners are engaging in the course thanks to forums participation). An example of this kind of monitoring is [37]. Likewise, a graphic can simultaneously provide information about various items depending on how this is read. For instance, Mazza and Dimitrova [21] use a matrix to show students’ performance on quizzes. Thereby, if teachers paid attention to a speciﬁc column, they would observe the performance of a particular student in all course quizzes. By contrast, if a row were observed, then teachers would analyze the performance of the whole class in a speciﬁc quiz. 
 3. PROPOSAL.
 From the observations done in the previous section, this paper presents a graphical interactive educational monitoring tool which uses information visualization (IV) techniques. This allows instructors to monitor the class and, at the same time, look details of a particular student. The ﬁrst part, monitoring of the classroom, is based on faceted browsing, a type of exploratory search. As far as the second part is concerned, a novel technique called data portrait is used to depict the information of a speciﬁc learner. A detailed explanation of both parts of the tool is exposed below. Nevertheless, before explaining the proposal of this paper, a brief list of the most common characteristics of educational monitoring tools based on IV is given. This will help to better understand the work presented in this article. 
 3.1 Features of Educational Monitoring Tools Based on Information Visualization 
 3.1.1 Stand-alone vs. Built-in.
 There are tools which have their own interface and collect data from an LMS [21, 22], whereas there are others that, due to the success of LMSs (e.g. Moodle), are integrated into their framework as a plug-in [35]. 
 3.1.2 Representation 2D vs. 3D.
 There are two kinds of representation according to the number of axes: 2D and 3D. In this regard, 2D graphics are ﬁrmly established. They consist of two dimensions (or axes) in which each of them represents a variable (or attribute) of information (e.g. students, grades, etc.). To a lesser extent, 3D graphics has also been proposed, e.g. the scatter plot for discussion boards suggested in [21]. In general, 2D representations are usually more intuitive for instructors than 3D ones. For that reason, most of educational monitoring tools display 2D graphics. 
 3.1.3 Multivariate Data.
 Much more variables than the number of axes of the graphic (i.e. 2D or 3D) can be displayed thanks to the use of diﬀerent techniques. Next, some of them are described brieﬂy: • Single-axis composition [18]: this is a method whereby an axis represents a large set of variables (e.g. content topics, number of accesses, etc.). This is used in [21]. • Visual components: elements such as color, shape and size, are often used as a data dimension. • Quadrants: these allow to organize data into four different groups. These are employed in [14]. • Mouse events: diﬀerent mouse actions, such as click and rollover, are also used to show information (e.g. relationships between elements [11]). • Multivariate representations: unlike most of the proposals that depict data as points, bars, lines and so on, there are tools that use a single representation to display multivariate observations with an arbitrary number of variables. This is the case of star plots [13], which show each observation (e.g. a learner) as a star-shaped ﬁgure with one ray for each variable (e.g. grades, participation, etc.). 
 3.1.4 Manipulation.
 Zoom, rotation (in 3D) and ﬁltering are typical actions that educational monitoring tools allow instructors to do. With regard to ﬁltering, this refers to hide some visual elements so as to emphasize others. Thereby, some values in the graphic act as layers that can be shown and hidden. Take the example of a bar chart that shows the number of posts written by the students of a class. In this case, the graphic has the variables “student” (X-axis), whose values are {S1 ,S2 ,S3 ,S4 ,S5 }, and “number of posts” (Y-axis) with values {4,6,7,5,8}. If the graphic focuses on the students, then this has 5 layers (i.e. S1 -S5 ). Hence, a teacher can indicate that the graphic only shows the bars that belong to the students S2 and S5 . However, she cannot set a general constraint that asks to show those learners who have written between 3 and 9 messages, since the layers 3 and 9 do not exist. Similarly, she cannot ﬁlter the information based on other variables that are not layers (e.g. number of accesses). A lot of educational monitoring tools with ﬁltering options are based on static layers, therefore this limits the exploratory search that an instructor can carry out. 
 3.2 Monitoring of the Classroom.
 3.2.1 What Is Faceted Browsing?.
 Faceted browsing is becoming a popular method to allow users to interactively search and navigate through complex information spaces [16]. This is widely used on a lot of ecommerce websites such as eBay or Wal-Mart. A faceted browser provides users with facet-value pairs that are used for query reﬁnement. Faceted browsing is made up of three stages [34]: opening, middle game and end game. In the opening, the interface shows the whole collection and all facets that can be used. The middle game, in turn, allows users to iteratively narrow down the result set by deﬁning constraints on the values of one or several facets, which reﬁnes the search query. Finally, the end game occurs when the user ﬁnishes the search by selecting an individual item from the result set and its information is detailed. 
 3.2.2 Why Is Faceted Browsing Suitable for Educational Monitoring in an Online Environment? 
 As said in section 1, teachers in online education should be guides. To carry out this role, they need to observe students’ behavior by analyzing any feature associated with learners. Thereby, teachers may detect any kind of problem in time. Unlike the other proposals, which show graphics with predeﬁned attributes or queries, faceted browsing allows teachers to perform an exploratory search strategy by using a wide range of orthogonal variables (i.e. facets). Thereby, instructors gain an insight into the behavior of their students by iteratively submitting tentative queries based on more than one facet at the same time. This iterative process (i.e. the middle game) ﬁnishes when teachers ﬁnd relevant information that they did not know or when the result set meets a speciﬁc set of requirements that teachers wanted. In that moment, instructors can either use the information found to make decisions (e.g. to send an e-mail or start a new search), or click on a student to see detailed information about her. As seen, faceted browsing seems to be good at monitoring a classroom, since it allows instructors to deﬁne any tailored query and ﬁnd relevant information that they did not know while they explore/reﬁne the result set. 
 Figure 1: The opening stage of the faceted browser proposal. Students remain anonymous. 
 3.2.3 Proposal of Faceted Browser.
 This paper presents a built-in faceted browser (see Figure 1) that collects data from an ad hoc learning management system [7]. This uses information visualization techniques in order to eﬀectively display data, leaving knowledge inference in teachers’ hands. Therefore, data mining techniques are not used to obtain underlying information about students. Regarding its interface, 2D graphics are used. Its design is divided into two areas. The main one shows the class roster as a set of cards in which each one includes student’s name and photo. The use of a photo is better than identifying students by looking at points, squares, bars, etc. Actually, Zhao et al. [36] states that there is evidence for the existence of a dedicated face processing system in human’s brain. On the left, there is a menu that has diﬀerent facets whereby teachers can narrow down or sort out the class. Table 1 gathers the facets that have been included along with their possible values. The chosen facets encompass the three aspects studied by Mazza [20]: sociality (i.e. forum participation), cognition (i.e. assignments grade) and behavior (i.e. studying pace). To stress that facets related to resources, such as the number of times that an item has been accessed, were not included because the platform, in which the faceted browser is placed, does not provide these data. As seen in Table 1, facets can have nominal or numeric values. Those facets that allow gradation are represented with sliders. To indicate the number of students in each facet value, numbers in parentheses and histograms are used. Their values are updated depending on the data set that is shown in the main area every time. This helps teachers to set more meaningful queries during the middle game. As said, teachers can sort out the result set by student’s name, studying pace or the average grade of assignments, in ascendant or descendant order. Finally, diﬀerent visual elements are used to represent more variables in the main area. The background color of each card indicates the learner’s studying pace. Thereby, the more orange the background is, the more advanced the student is. Besides the background color, the card can have a red border that means that the average grade of assignments is C- or less and, hence, that student would not pass the course if the term ﬁnished at that moment. Moreover, the border of the photo can be drawn with black dashes. This means that the student is repeating the subject. Likewise and, because the result set can be ordered, the position of the cards is another visual element that gives extra information. Thereby, if a teacher sorted learners by the average grade of assignments in ascendant order, then students with lower grades would be on the top positions. 
 3.2.4 An Example of Using the Faceted Browser.
 From the opening stage shown in Figure 1, a teacher may deﬁne any query, e.g. “to retrieve, ordered by studying pace in a descendant way, those students who, regardless of their participation in forums, are not repeating the course and, at the same time, have achieved 24% of activities as well as they have an average grade of assignments equal or greater than C+”. If the previous query were executed, then the result set would be that shown in Figure 2b. In this regard, Figure 2a shows the transition between the opening (see Figure 1) and the middle game (see Figure 2b). As seen, instructors can see how students change their positions or even they dissapear. Thanks to the possibility of seeing the transition, teachers can gain extra understanding of their learners. Finally, it is worth emphasizing that the result of the Figure 2b may be either an intermediate step of the middle game or the last one. This depends on whether the teacher is satisﬁed with the result and selects one student, or prefers to narrow down the result set by changing some facets. 
 Table 1: Set of facets with their values. 
 Figure 2: Execution of a query in the proposed faceted browser. 
 3.3 Monitoring of an Individual Student.
 So far, only information about the whole classroom has been represented. However, instructors need to know details of a speciﬁc learner quite often. In this regard, the great majority of monitoring tools usually show the same kind of graphic for both the classroom and the learner. Instead, the present paper suggests using a novel technique, called data portraits, to display an individual student’s information. 
 3.3.1 What Is a Data Portrait?.
 According to Donath [5], “data portraits depict their subjects’ accumulated data rather than their faces. They can be visualizations of discussion contributions, browsing histories, social networks, travel patterns, etc. (...). Data portraits depict a person through their digital archive”. In short, the idea behind data portraits is to compactly convey a large amount of information from an individual in a single graphic. An example of data portrait used for discussion forums is PeopleGarden [33]. This is a metaphor in which each user is a ﬂower and, hence, the forum is a garden. Each petal symbolizes a message written by the user. Thereby, the number of petals indicates the user’s posting frequency. Thus, the more petals a ﬂower has, the more active the user is. Likewise, the petal’s color represents if the message is an initial post (in magenta) or a reply (in blue). Moreover, pistil-like circles are used on top of the petals to show how many responses each message has received. To display how old a message is, petals, like in the real life, fade over time. Another similar proposal is daisy maps [13]. A daisy map is a star-shaped glyph that displays the scores received on diﬀerent parts of an assignment (e.g. reading, writing, etc.). Thereby, each student is a daisy map and each ray has a color depending on the score attained. Lexigraphs [6] is, in turn, a group of data portraits in which each user is represented as a face-like outline. Each one is drawn by the words written in the user’s Twitter account. Thus, silhouettes are updated with each new tweet. Finally, Authorlines [31] is an horizontal timeline with vertical monthly dividers that represents the user’s yearly posting behavior in a set of newsgroups. Each month is divided into weeks, and each week is shown as a vertical lineup of circles. Each circle represents a conversation and its size indicates the number of author’s messages in that thread. Authorlines places threads that were initiated by the author above the timeline, whereas the rest of threads to which she contributed are placed underneath the timeline. 
 3.3.2 Proposal of Student Data Portrait.
 The proposed data portrait creates a snapshot of an individual student from the data of her learning process. Each portrait appears when the instructor moves the mouse over a card of the proposed faceted browser. The items shown in the data portrait are the facets of Table 1, except gender. The data portrait (see Figure 3) is a bar that is divided into ﬁve squares: number of initial posts (in dark blue), number of replies (light blue), number of read messages (pink), the average grade of the messages (green) and the number of highlighted messages (orange). Therefore, these ﬁve squares represent the student’s forums participation. Each one of them change its opacity in order to indicate the level of achievement. The lighter the color of the square is, the lower student’s performance in that item is, and vice versa. 
 Figure 3: Diﬀerent examples of student data portraits. 
 Likewise, the border of the bar can be red or black. Red means that the student would not pass if the course ﬁnished at that moment. Moreover, this can be drawn with dashes. This indicates that the learner is repeating the course. Finally, there are two red markers above and underneath the bar. The former represents the studying pace (in percentage), i.e. how many activities the learner has already ﬁnished. Thereby, the top of the bar works as a continuous axis that goes from 0% to 100%. As far as the second marker is concerned, this indicates the average grade of the assignments. In this case, the bottom of the bar works as a discrete axis whose values are: NA/NP, D, C-, C+, B and A. Each value coincides with the ends of each square. 
 3.3.3 Examples of Using the Student Data Portrait.
 Figure 3 shows diﬀerent students’ behaviors by using the proposed data portrait. For example, the learner in Figure 3a participates actively in forums, since she writes (opaque dark blue) and replies (opaque light blue) a lot and, at the same time, she reads most of messages (opaque pink). On the other hand, the student in Figure 3b initiates a lot of conversations (opaque dark blue), but she does not reply to other messages (transparent light blue). Therefore, she only writes when she initiates the conversation. By contrast, the learner in Figure 3c helps classmates by participating in threads initiated by others (opaque light blue). However, she does not initiate any conversation. Moreover, the teacher has highlighted some of her messages (opaque orange). As seen, the students in Figure 3b and Figure 3c participate in discussion forums, but in an opposite way. According to Taylor [30], the learner in Figure 3d would be a lurker. A lurker can be deﬁned as a user that writes occasionally or not at all (transparent dark and light blues), but she regularly participates as a reader (opaque pink). Likewise, lurker students usually obtain similar grades to more active learners. For instance, in Figure 3d, this student has a B as the average grade of her assignments. The last three data portraits show: Figure 3e) a student who is repeating the course (dashed border) and, at that moment, she would pass the course (she has a C+); Figure 3f) a lurker who does activities, but she would fail (red border and bottom marker in the beginning); and Figure 3g) a student who has dropped out (all squares are transparent, her studying pace is low and she has not handed in any assignment) and, moreover, is repeating (dashed border). As seen, thanks to the proposed data portrait, it is possible to compact a student’s learning process in a single image. Thereby, teachers can get an overall idea of a student at a glance and compare learners with each other easily. 
 4. CONCLUSIONS AND FUTURE WORK.
 Online education involves a student-centered approach in which the instructor is a facilitator. Consequently, teachers should be guides who help their students to achieve learning goals successfully. To carry out this role properly, instructors should be provided with the maximum amount of data about both the whole classroom and a particular student. With regard to this necessity, this paper introduces a novel approach of educational monitoring tool whose contribution is twofold. On the one hand, the use of facet browsing to monitor a classroom is proposed. As seen, facet browsers provide a user-friendly way to navigate through data collections. Regarding its use in online education, faceted browsing may help instructors to gain understanding of the behavior of their classrooms, since it allows teachers to explore and analyze the class by requesting iterative tailored queries based on diﬀerent combinations of facets (i.e. attributes). On the other hand, this paper promotes to employ a novel technique called data portrait that allows to compact the user’s information in a single picture. With regard to its use in online education, a data portrait can be a good way to summarize the student’s data visually, allowing teachers to gain understanding of a student’s behavior at ﬁrst glance. In this regard, the data portrait of this paper is a ﬁrst proposal of this kind of visualization in the educational context. As seen, this is in its very early stages and it thus needs to be studied in depth in future research. This paper also proposes a classiﬁcation of educational monitoring tools from two points of view. The ﬁrst one focuses on the data processing technique that is used, i.e. information visualization and data mining, whereas the second one is based on the element that is monitored. Likewise, this paper presents a study on the most relevant features that educational monitoring tools based on IV usually have. As future work, the proposed faceted browser must be integrated into the most popular LMSs, e.g. Moodle. To this end, a plug-in version must be developed. Thus, educational community will be able to take advantage of this tool. Finally, an experiment with teachers throughout a term must be conducted so as to check the eﬀectiveness and usefulness of both the faceted browser and the data portrait. After the experiment, a survey could be sent in order to detect strong and weak points of the proposal. Likewise, a focus group or semi-structured interviews with 4-6 instructors may also be useful to know their opinion in depth. From the results of the survey and focus group/interviews, valuable conclusions might be drawn. Thereby, new features and improvements might be deﬁned and added to the tool.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Educational Monitoring Tool Based on Faceted Browsing and Data Portraits</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>information visualization</dc:subject>
		<dc:subject>student monitoring</dc:subject>
		<dc:subject>instructor support</dc:subject>
		<dc:subject>faceted browsing</dc:subject>
		<dc:subject>data portraits</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/german-cobo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/german-cobo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-monzo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-monzo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-melenchon"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-melenchon"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/german-cobo"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-monzo"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-melenchon"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/18">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Course Correction: Using Analytics to Predict Course Success</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/18/authorlist"/>
		<swrc:abstract>Predictive analytics techniques applied to a broad swath of student data can aid in timely intervention strategies to help prevent students from failing a course. This paper discusses a predictive analytic model that was created for the University of Phoenix. The purpose of the model is to identify students who are in danger of failing the course in which they are currently enrolled. Within the model’s architecture, data from the learning management system (LMS), financial aid system, and student system are combined to calculate a likelihood of any given student failing the current course. The output can be used to prioritize students for intervention and referral to additional resources. The paper includes a discussion of the predictor and statistical tests used, validation procedures, and plans for implementation.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Whereas the holy grail of predictive models in higher education would likely be one that could predict graduation at the time a student applies for admission, the reality is that the elapsed time between the start and end of college is years long, creating the opportunity for a multitude of factors to interfere with a student’s progress. A model to make such a prediction will take years to develop and require data beyond the scope of what is available in an institution’s Student Information System (SIS) and LMS. Nonetheless, there are known reasons students fail to graduate. Work schedules, health problems, child care challenges, transportation, and financial issues comprise some of the reasons students drop out that are largely outside of the institution’s control [1]. One predictor of students’ decision to drop out that is under the institution’s purview, however, is a lack of preparation or effort as reflected in their course grades. Regardless of the reason, a student enrolled in a course will often display signs of course failure before either formally withdrawing or disappearing altogether. Failing to attend class (or in the case of online courses, failing to participate in discussion forums), sloppy or incomplete assignments, or a significant change in the student’s behavior and academic performance are all warning signs that a student may be on the verge of dropping out. It is generally in both the student’s and the institution’s best interest that students remain enrolled or, if they must leave, withdraw via the established process. It is this concern that precipitated the development of a predictive model. Whereas the student may be wrestling with problems that range from personal tragedy to time management or academic under-preparedness, the University can monitor the student’s behavior in a course for warning signals and increase that student’s priority for a call from his or her academic advisor. The advisor can help the student by pointing them to necessary resources, coaching them on time management, or even advising early withdrawal. This paper discusses the rationale for the model; the process through which it was developed, revised and refined; and the validation of the model by the operational team. The next section will provide the context within University of Phoenix and further exposition of the problem the model is intended to address. Following this discussion is a brief overview of the extant literature that guided some of the decisions made about variables included in the model. This section is followed by a description of the methods used to develop the model, including the data elements found to be predictive and those found to be irrelevant to prediction. We then discuss the process used to validate the model within our operational environment. Finally, we offer a brief discussion of next steps and plans for broader implementation. 
 2. INSTITUTIONAL CONTEXT.
 Founded in 1976, University of Phoenix is a regionally accredited, degree-granting institution. Based in Phoenix, Arizona, the University has over 200 campuses throughout the United States and the largest (measured in student enrollment) online campus in North America. In addition to holding regional accreditation, University of Phoenix holds programmatic accreditation in nursing, counseling, business, and education. As of August, 2011 the university enrolled more than 340,000 students in over 100 degree programs, ranging from associates through doctorates. The university was founded with a focus on working adults who wished to complete their degree. These non-traditional learners remain the focus of the university, resulting in a more diverse student population than found in traditional institutions, in terms of racial and ethnic demographics as well as the proportion of first- generation college students [2]. Also, many non-traditional students are employed while pursuing their education. In order to help students complete their degrees in a time-efficient fashion, University of Phoenix adopted a focused academic model in which courses last between 5 and 9 weeks. Non-traditional learners may have been out of school for many years before deciding to pursue a post-secondary degree. Adding school responsibilities to busy schedules and refreshing study skills are challenges that all returning students face. We constantly seek new ways to provide our students with the services they need in order to progress academically, including tutoring and coaching. And, although a number of static triggers (such as those that monitor attendance) exist to monitor students for signs of trouble, there is continued interest in improving the information available to academic advisors in order to direct interventions. 
 3. THEORETICAL FOUNDATION.
 Garman used logistic regression to predict student success in an online database course based primarily upon scores on a reading comprehension assessment [3]. The only other input to the model was the semester in which the student took the course. Whereas this approach is interesting in that it supports the proposed methodology for our model, the study found the semester variable insignificant and the assessment score only minimally predictive. Moore looked explicitly at course participation in both the student’s current and prior courses [4]. This research indicated increased participation to correlate highly with higher performance in the course. Some other variables, such as student expectations, high school rank, and entry exam scores (ACT) were not significant predictors of student achievement. The standard measure for monitoring participation in an online course is student discussion postings, and prior research has found final grades correlated with the number of postings both read and written by students [5]. However, other research has found postings to have an indeterminate relationship with course success [6,7]. Ramos and Yudko found that total page hits were more predictive than discussion board use of online course success [8]. The lack of agreement suggests including post counts in the model until they can be definitively excluded. Regarding demographic variables, Martinez found high school GPA, age, sex, grade in last math class, highest level of math, ethnicity, definite major choice, and work hours planned to predict success in different levels of English courses [9]. In addition, current credit hours, financial aid usage, and program level were predictive of the likelihood of drop out [10]. Where possible these variables were included. Because the goal of this project was to work from an existing data set, studies addressing variables that are unavailable (such as selfdiscipline, motivation, locus of control, and self-efficacy) were not included in our literature review. 
 4. METHODS.
 Based on the literature, the variables in Table 1 were identified as potentially useful and worth examining. A number of key variables were populated for less than 50% of the cases. Logistic regression drops any records for which all of the fields are not populated, resulting in too large a loss of data. Therefore, despite theoretical support for those fields’ inclusion, the decision was made to exclude them from the analysis at this time. 
 Table 1: Variable Disposition. 
 4.1 Model Version 1.
 A consulting company using a limited data set created the initial model. The data included a unique identifier; basic demographic information from a voluntary survey completed by the student at time of admission; and academic history within the University, including number of transfer credits, number of courses taken, and percentage of points earned in these courses. For each course, information was also provided about discussion board postings, points earned by week within the course, and whether the student submitted assignments late. The consulting company used this data to create a logistic regression model. The analysis of the initial data exposed missing data and data quality issues that would have compromised the final model. Fields, such as submission timeliness and discussion board post quality, were found to be either inaccurate or missing too many records to contribute to the model. The final data set was reduced to data reflecting transfer credits, prior academic activity at University of Phoenix, and week-by-week activity (points earned and discussion posts made) for each course. These data elements were further recoded to create interpretable indicators. The data set included all activity for approximately three months2, organized by degree. The SPSS randomization algorithm selected approximately 50% of the data as a hold-out sample, making the remaining 50% available for model development. These data were analyzed using logistic regression, with the outcome variable being an indicator of whether the student passed the course. The model assessed student data through course week 4. Separate models were developed for each degree level. For example, coefficients for bachelor’s degree students through week 2 were as follows: 
 Table 2: Coefficients for Model 1, Bachelor’s degree students. 
 These coefficients allow us to classify each student into one of three tiers in week zero3: high risk, low risk, or neutral risk. Initial percentages that comprised the neutral zone ranged from 41% to 54% of students. Models in weeks 1 through 4 added discussion post information and percentage of assignment points earned. This parsing immediately (by the end of week 1) trimmed the range of the grey zone to between 35% and 40%. By week 2 all master’s degree students were out of the neutral zone. By week 3 all bachelor’s degree students were out of the neutral zone. Results for students not in the neutral zone were accurately predicted on average 94% of the time, with no week below 85%. In other words the prediction of pass (low risk) or fail (high risk) was accurate more than 90% of the time. 
 4.2 Concerns with Model 1.
 The neutral zone was quite large initially, and the team felt that a better way to categorize the students in that zone was to assigned a “score” to each student, ranging from 1 (unlikely to pass) – 10 (nearly sure to pass). That score would provide the prioritization needed to make the output actionable. Also, since some courses are as long as 9 weeks, the time frame needed to be extended. There was also concern about the reproducibility and actionability of the model as developed. The data used came from a variety of different databases and, as such, required significant manual intervention to compile. At least one of the data sources required a programmer to do an ad-hoc query to generate a data set that was not directly accessible to the analytics team. Enhancement of the model was brought in-house. The model was replicated for validation purposes, and then, the process of refinement and further development was started. 
 4.3 Model Version 2.
 One of the first problems addressed was that of data availability and validity. As mentioned, there were critical data elements around discussion board postings that were not easily available. However between the initial data request and the start of model 2 a partial feed of the data was added to the enterprise data warehouse environment. This advancement allowed post count by week to be incorporated into the model. Further, the ability to automatically update all data fields will make implementation easier. One limitation of the new data source is that it has only been populated since June, 2011. Therefore data extraction processes were developed to use data from June through October, 2011, to construct the updated model. Additionally, some variables that had not been included in model 1 were included in model 2. These included military status and financial status. As model 1 showed, some of the variables that the literature suggested were relevant proved not to be when looking across all programs and levels. Gender, age, military status, pell grant receipt and responsible party (whether the student was receiving financial aid, paying through their employer, or paying directly) were not significant or resulted in extremely low weights. There was also some variability by degree level. For example, military status was not significant for associate’s and bachelor’s degree students, but was significant and negative for master’s degree student. One interpretation is that master’s-level military personnel are more likely to be officers and therefore more likely to have substantial responsibilities that could interrupt their studies; however, other explanations could be considered. Because of their lack of predictive power, most of these variables were eliminated from the final version of Model 2. Model 2 was built using a Naïve Bayes algorithm in RapidMiner and validated using 10x cross validation. A sample of information gain weights on a zero to one scale are in Table 3. 
 Table 3: Weights for Model 2, Bachelor’s Online students. 
 The new variables added to model 2 increased the predictive accuracy considerably. Model 2 accurately predicted 85% of all students at week 0 (compared to 50% in model 1), rising to 95% by week 3. Specifically, the ratio of credits earned to credits attempted was a substantial indicator of potential problems, as was a financial status other than current. As might be expected, cumulative points earned remained the most powerful predictor. In addition, whether the point delta between prior and the current course had change by more than 10% was treated as a categorical variable and was also influential. This variable is less about the exact point change than an indicator of a substantive change in behavior. Most of the other variables provided less predictive power than these three, although enough to keep in the model. 
 4.4 Model Version 3.
 Development of model version 3 is awaiting the availability of higher quality posting data. While version 2 allows direct access to the total number of posts made by a student, it fails to provide any distinction between posts made in response to discussion questions, posts made in collaborative forums, and posts made to the private forum provided for discussion with the instructor. Because the literature suggests a link between passing and engagement with the instructor [6], this differentiation between post sources is necessary for the next phase of analysis. Acquisition of this data is underway. Additionally, per Ramos and Yudko [8], there is value in tracking student page views within the learning environment. Currently, only aggregate data is available, but these data are not useful from a predictive analytics perspective. Accordingly, the technical team is working to capture this data at the individual student level. To complement these data elements around discussion board activity, three variables will be added based upon research conducted at another institution: major area of study, time since last course, and participation in an orientation program. These elements will be incorporated into the model while the discussion data is being sourced. One other potential modification will be a review of other types of models. Specifically support vector machines and random forest models will be investigated for improved predictive accuracy. 
 5. MODEL VALIDATION.
 Validation of model 1 involved using the 50% hold-out sample. The risk category percentage differences between estimation and hold-out samples were within two percent, with the majority of cases within one percent. For model 2, a 10x cross-validation procedure was used which provide that the model was highly accurate at predicting students who would pass, but with some room for accuracy improvement on those students who were predicted to not pass. These were under predicted in the model. More important than validation of the model fit, however, is validation of the model’s utility. In order to validate that the model was indeed providing actionable information, a pilot has been initiated to create scores that could be provided to academic advisors. The academic advisors then use these scores for prioritization, calling students with the highest risk score first, even if that student would not normally have received a call, while delaying calls to students who scored lower. The initial pilot of the model is being conducted with only a few academic advisors. These more experienced advisors are looking at the model in terms of both its accuracy (does the information the model provides align with what they learn by talking to the student) and its utility (does it trigger contact with the right students, and are those students then successful?). Statistical validity alone is insufficient; the model must provide actionable information to front-line advisors in a form that can increase student success in order for it to be seen as truly valid. 
 6. NEXT STEPS AND IMPLEMENTATION.
 Providing a score for students that can be used for prioritization is helpful, but too many students remain in the neutral zone during weeks 0 and 1. Refinement will continue, with the objective of accurately placing them at one end of the scale or the other. The overarching goal for this project remains to provide valuable, timely information to academic advisors. Once the pilot completes, the utility will be evaluated and a decision will be made as to whether to implement the model into the production processes, making the results available to all academic advisors. At this point, it is unclear as to whether this will be in the form of a gradient categorization (e.g. red/yellow/green), a numeric score, or a percentage chance of withdrawal. Accordingly, the model will continue to be refined after initial implementation to best suit advisors’ needs. Future refinements will depend on the availability of additional detail data from both the learning management platform and the student information system. Concurrent work is proceeding to substantially improve access to that data, making integration into the model both technologically easier and substantially faster. 
 7. ACKNOWLEDGMENTS Our thanks to Andrew Tubley for help with the title of this paper.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Course Correction: Using Analytics to Predict Course Success</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>predictive analytics</dc:subject>
		<dc:subject>predictive modeling</dc:subject>
		<dc:subject>higher education</dc:subject>
		<dc:subject>retention</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-barber"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-barber"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mike-sharkey"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mike-sharkey"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/18/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-barber"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/mike-sharkey"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/19">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>The Learning Analytics Cycle: Closing the loop effectively</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/19/authorlist"/>
		<swrc:abstract>This paper develops Campbell and Oblinger’s [4] five-step model of learning analytics (Capture, Report, Predict, Act, Refine) and other theorisations of the field, and draws on broader educational theory (including Kolb and Schön) to articulate an incrementally more developed, explicit and theoretically-grounded Learning Analytics Cycle. This cycle conceptualises successful learning analytics work as four linked steps: learners (1) generating data (2) that is used to produce metrics, analytics or visualisations (3). The key step is ‘closing the loop’ by feeding back this product to learners through one or more interventions (4). This paper seeks to begin to place learning analytics practice on a base of established learning theory, and draws several implications from this theory for the improvement of learning analytics projects. These include speeding up or shortening the cycle so feedback happens more quickly, and widening the audience for feedback (in particular, considering learners and teachers as audiences for analytics) so that it can have a larger impact.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 A concern with improving learning is foundational within the field of learning analytics. It was there in Campbell and Oblinger’s early work [4] and is there in the definition of learning analytics from the First International Conference on Learning Analytics and Knowledge (LAK11) [22]: "the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs." The importance of interventions in learning analytics to close the feedback loop has been clear in the literature (if not always the practice) from the birth of the field. Analytics seeks to produce ‘actionable intelligence’ [5]; the key is that action is taken. Campbell and Oblinger [4] thus set out five steps in learning analytics: Capture, Report, Predict, Act, Refine. ‘Act’ explicitly includes making appropriate interventions, and this is echoed across the literature (e.g. [3, 9, 10, 13]). This paper builds on these ideas to articulate a Learning Analytics Cycle that makes the necessity of closing the feedback loop through appropriate interventions unmistakable. It also draws on the wider educational literature, seeking to place learning analytics on an established theoretical base, and develops a number of insights for learning analytics practice. 
 2. THE LEARNING ANALYTICS CYCLE.
 Figure 1, the Learning Analytics Cycle. 
 The cycle, shown in figure 1, starts with learners. They may be students studying a course at a university, or informal learners taking part in a MOOC (a Massive Open Online Course, where the learners and materials are distributed across the web), participants at a research conference, or casual learners browsing Open Educational Resources (OER). The next step is the generation and capture of data about or by the learners – for instance, demographic information about a potential student logged during a phone call enquiry about study at a university; login and clickstream data generated in a VLE/LMS; postings to a forum; assessment results; or even alumni status. Some can be generated automatically; some requires a large multidisciplinary team to expend significant effort. The third step is the processing of this data in to metrics or analytics, which provide some insight in to the learning process. These include visualisations, dashboards, lists of ‘at risk’ students, comparisons of outcome measures with benchmarks or previous cohorts, aggregations, and so on. Again, some can be generated automatically, but others may take significant effort. This stage is the heart of most learning analytics projects, and has been the focus of great innovation in tools, methods and methodologies – e.g. dashboards, predictive modelling, social network analysis, recommenders, and so on. However, the cycle is not complete until these metrics are used to drive one or more interventions that have some effect on learners. This might be a dashboard for learners that enables them to compare their activity with their peers or previous cohorts, or a tutor making personal contact with a student that a model has identified to be at very high risk of dropping out. The cycle can be complete even where the intervention does not reach the learners who originally generated the data. To take a very simple example, a teacher reviewing the final grades for a course and using that to inform how to teach it with the following cohort is an example of the cycle in action. Learning analytics does not necessarily include all fours steps. A project that created reports about learners, but without any mechanism to feed this back in to an improved learning experience, would still be a learning analytics project, but not a very effective one. 
 3. LEARNING THEORY.
 The Learning Analytics Cycle has so far been presented as a development of previous theorisations of learning analytics. However, it is also more fundamentally, a development of much older learning theory. 
 3.1 Kolb.
 One of the most prevalent learning theories is Kolb’s Experiential Learning Cycle [11], which builds on Dewey and Piaget, and adds Lewin’s conception of learning through feedback, which was inspired by electrical engineering (to which this paper returns). Kolb’s Learning Cycle takes concrete experience as its starting point; reflective observation on this experience in turn builds abstract conceptualisation, which feeds through in to active experimentation, the source of further concrete experience. There are two levels at which the Learning Analytics Cycle develops Kolb’s cycle. Firstly, taking the system as a whole, there is a direct correspondence: actions by or about learners (concrete experience) generate data (observation) from which metrics (abstract conceptualisation) are derived, which are used to guide an intervention (active experimentation). Secondly, at an individual level, learning analytics can greatly facilitate the learning process of individuals, by making reflective observation and abstract conceptualisation easier and more readily available. These stages correspond to the ‘interventions’ component of the Learning Analytics Cycle: the learning analytics system makes metrics available to an individual, who observes, conceptualises, and then experiments by making (or attempting to encourage) some change to learner behaviour. Kolb’s cycle and related ideas have been critiqued extensively (see e.g. [19]). One main line of critique is that they are reductive of a holistic, emotional process to a rational, cognitive phenomenon, which would apply equally to learning analytics. The other fundamental charge against Kolb’s model – that it lacks strong empirical evidence – is one that learning analytics is in an excellent position to refute, or should be. 
 3.2 Schön.
 Another prevalent theorisation of learning arises from the work of Donald Schön [1, 17, 18] on reflective practice: how professionals learn and adapt their behaviour. Schön emphasised the importance of reflection-in-action and reflection-on-action. In this view, reflection is a form of feedback process or loop, an iteration between espoused theories and theories-in-use. The Learning Analytics Cycle instantiates and enables reflective learning, at both an individual and organisational level. As with Kolb, the ‘intervention’ stage of the Learning Analytics Cycle is where reflective practitioners compare their espoused theories with theories-in-use. One significant conceptualisation developed and popularised by Schön and Argyris [1] is a distinction between single-loop learning and double-loop learning. Single-loop learning is aimed at achieving a set outcome by adjusting practice; double-loop learning includes the possibility of changing the set outcome. They use the example of a domestic thermostat: it turns the heat on or off to achieve its set temperature (single-loop learning); but a human can adjust the set temperature (double-loop learning). A learning analytics system may be used simply to attempt to achieve set goals (single-loop learning); greater value and insight will come if those goals themselves can be interrogated, challenged, and developed (double-loop learning). Learning analytics can thus be a powerful force for informing and validating learning theories. 
 3.3 Laurillard.
 In the UK, another widely-cited theory is Diana Laurillard’s Conversational Framework [12], which draws on Kolb’s cycle and Pask’s Conversation Theory [14]. In this theory, learning takes place through a series of ‘conversations’ between a teacher and a student (and with other students), underpinned by reflection and adaptation. These conversations happen on two levels: at the level of action, and at the level of conception or description. At an individual level, a Learning Analytics Cycle facilitates the conversation between the teacher and student: providing information on the students’ actions and conceptions, enabling richer adaptations and feedback in turn from the teacher’s constructed environment. The parallels at a whole-system level are less transparent but perhaps even richer. The Learning Analytics Cycle can be conceptualised as enabling conversations at multiple levels, between multiple actors, with iterative, adaptive feedback. 
 3.4 Other educational literature.
 The approaches to learning literature (e.g. [16, 21]) identifies qualitatively different approaches to study – a deep, surface or strategic approach. This literature has uncovered associations at the population level between approaches (of the learner and teacher) and the final outcome, including to widely-used evaluation questionnaires. Learning analytics offers the possibility of tracking and researching these associations in real time, and – most importantly – using them to enhance the learner’s experience before they come to the end of their study. 
 4. ENGINEERING THEORY.
 The educational theories mentioned above take inspiration from the cybernetic conception of control theory, and in particular, the closed-loop control system used widely in engineering of all sorts. In a closed-loop control system, the output(s) of the system is measured and then processed by a controller, which in turn makes an appropriate adjustment(s) to the input(s), creating a feedback loop. In an open-loop control system, the controller adjusts the input purely based on its own settings, without taking any account of the output. Open-loop control systems are typically quicker, simpler and easier to implement. However, closed-loop control systems are more robust at achieving the desired output, particularly when something within the system changes or the system is complex. The parallels for learning analytics are readily apparent. Organising learning without feedback from the outputs is akin to an open-loop control system: it may be quicker, but the final output may not be the desired one. The Learning Analytics Cycle works analogously to a closed-loop control system: the data generated by or about learners is the output, which is compared to some reference (e.g. previous learner data, or a desired outcome), which is then used to drive an intervention which alters the learning process (input). 
 5. IMPROVING EFFECTIVENESS: SPEED AND SCALE.
 A key consideration for the effectiveness of the feedback cycle is the speed and scale of the intervention. These are properties of the entire system: that is, they include the people, policies and practices connected to the learning. The people involved can be classified in to the following four stakeholder groups: - learner – anyone engaged in learning - teacher – anyone engaged directly in facilitating learning: includes teaching assistants, associate lecturers, adjunct faculty, faculty, academic staff, and peers in some contexts such as MOOCs - manager – anyone responsible for the organisation or administration of teachers: includes departmental-level and institutional-level management (e.g. managers, administrators, heads of department, Deans, executive officers (CxOs), presidents, provosts, vice-chancellors, rectors and their deputies) - policymaker – anyone responsible for the setting of policy, whether at a local, regional, state, national, or transnational/ intergovernmental level, and including funders. 
 As shown in figure 2, the learner is the closest to the learning activity. They can make very quick changes to their own learning, but limited changes to others’. The teacher is one step away from the learning activity, but is able to make interventions that may span several learners. At one further remove is the manager, who is slowed down by the need to receive second-hand reports from the teachers, but may well be able to make interventions that affect more learners than an individual teacher. Furthest from the learning activity is the policymaker, likely to be slowest of the four in speed of response, but with the widest responsibility. 
 Figure 2: The scale (horizontal axis) and speed (vertical axis) of intervention readily achieved by different stakeholder groups in a learning analytics system, with proximity to the learning activity (depth axis). 
 In a given learning analytics project, there are three strategies by which the effectiveness of the cycle can be improved. Firstly, the speed of response can be enhanced, e.g. by real-time feedback to stakeholders who can act more quickly, such as the teacher and the learner themselves. Secondly, the scale of response can be enhanced, e.g. by providing feedback to a larger number of stakeholders. Thirdly, the quality of the intervention itself can be improved, e.g. by testing the intervention to see whether it is effective, through feedback from the outputs of the learning (Schön’s double-loop learning discussed above), or by enabling more stakeholders to participate. (This has parallels in the Open Source Software dictum that “Given enough eyeballs, all bugs are shallow.” [15]) There may well also be an increase in quality if feedback is directed to those who have the best information about the learning, such as learners and teachers. The idea of improving learning through feedback via teachers and learners themselves is far from new (see e.g. [7, 8]), and notably, the Signals project at Purdue University [2], perhaps the bestknown successful example of learning analytics, has feedback to learners and teachers at its heart. 
 6. ASSESSMENT AND INAPPROPRIATE USE OF METRICS.
 Assessment can be considered to be a special case of the Learning Analytics Cycle. In traditional marking, a learner takes a test, which the teacher marks and returns to the learner (learner generates data which is processed in to a metric). All too often, the cycle is not completed at this point. Many learning analytics models treat assessment as the final outcome measure to be optimised, rather than an interim one. This is extremely valuable. But assessment data has far more potential than this: treating it as an input or intermediate variable can yield extremely valuable insights, and learning analytics systems can provide assessment-like feedback even in informal settings (e.g. [6]). It has been established for at least 40 years [20] that learners identify the ‘hidden curriculum’ revealed in the assessment. Teachers may say they want their students to pursue intellectual problems, apply their creativity and make mistakes from which they then learn, but if the assessment tasks predominantly reward rote learning, learners are likely to study that way. This is a specific instance of a general risk in learning analytics: of optimising to a metric that does not reflect what is more fundamentally desired as an outcome. All metrics carry a danger that the system will optimise for the metric, rather than what is actually valued. This danger is not new – Kolb argued emphatically that ‘learning is best conceived as a process, not in terms of outcomes’ ([11] p. 26) – but learning analytics makes it more pressing. Thus learning analytics should generate metrics that relate to what is valued in the learning process. If the final assessment rewards undesired behaviour, improving the control system to more effectively optimise the results will make the learning worse. 
 7. OPENNESS.
 Being open and transparent benefits learning analytics in (at least) three different ways. Firstly, it makes learning analytics more effective. As discussed above, if more people can see the metrics, there are more people to understand. Opening up metrics reduces potential barriers to effective working (e.g. teacher’s password expired, wrong permissions set, system complexity and performance). Secondly, transparency leads to greater social acceptability. Egregious misapplications of analytics are more likely to be identified and challenged by stakeholders – and correct, if the learning system of the organisation does not prevent it. Thirdly, data protection legislation may make it a legal requirement. This is not, of course, simple or straightforward. One cannot simply make all learners’ data and metrics available to the entire world on the web. However, significant potential is lost when restrictions are added needlessly. 
 8. CONCLUSION.
 The Learning Analytics Cycle, with its theoretical grounding, suggests ways in which learning analytics projects can be made more effective. Fundamentally, this requires closing the feedback loop through effective interventions that reach learners. These loops can be made more effective if they are faster, or larger in scale. Strategies to achieve this include considering learners and teachers as audiences for learning analytics as well as managers and policymakers.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>The Learning Analytics Cycle: Closing the loop effectively</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>academic analytics</dc:subject>
		<dc:subject>analytics</dc:subject>
		<dc:subject>policy</dc:subject>
		<dc:subject>feedback</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/doug-clow"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/doug-clow"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/19/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/doug-clow"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/20">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Sherpa: Increasing Student Success with a Recommendation Engine</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/20/authorlist"/>
		<swrc:abstract>Students flock to online services like Amazon, Pandora and Netflix that offer personalized recommendations, in stark contrast to the “one size fits all” services in higher education. In this session we demonstrate Sherpa, a recommendation engine for courses, information and services that utilizes both human and machine intelligence.</swrc:abstract>
		<led:body><![CDATA[ 1. INTRODUCTION.
 An unprecedented alignment of forces in the United States—from President Obama to state governors to private foundations—is calling for America to regain lost educational ground by once again having the highest proportion of students graduating from college by 2020. Concurrently, however, educational funding is shrinking. How can we make large gains in student success while spending less? By leveraging the sort of intelligent, automated computer “recommendation engines” proven successful by companies like Amazon, Netflix, Pandora and Apple. South Orange County Community College District (SOCCCD), a two-college district in Southern California with 43,000 students, has created Sherpa, an academic recommendation engine that combines human expertise and predictive analytics to provide students with the right information at the right time to facilitate better academic decisions. Sherpa uses time, event, or locationbased “triggers” to deliver multimodal (email, SMS, voice, textto-speech, or Facebook announcements) personalized communications such as: 
 Keywords: Sherpa, Recommendation Engines, Personalization, Student Success 
 - Helping students find acceptable alternatives when their preferred courses are full - Targeting at-risk students for academic interventions. - Tailoring information about campus events to individual interests At this session, we discuss the compelling nature of personalized online services, outline our software development process and provide a live demonstration of the Sherpa system. 
 2. DEVELOPMENT.
 2.1 Precursors.
 Previously, SOCCCD had developed MySite, an enterprise academic web portal, and My Academic Plan (MAP), an online academic planning tool that has been used by students to create over 107,000 academic plans since it went online in April 2007. Though MySite and MAP were successful, they, like nearly all systems in higher education, were passive in nature. We wanted a more proactive system capable of assisting students’ decisionmaking processes in a manner that would “nudge” them toward making better-informed academic decisions. Initially, Sherpa was envisioned as a proactive academic planning tool that would focus on course selection. However, the more we discussed such a system, the more expansive our vision became. We realized that if we built a platform rather than an isolated product--i.e., a recommendation architecture (see below) rather than a specialized system--it could provide guidance on a wide range of decisions including student services and specific instructional content. 
 2.2 Modalities.
 Though our two-college district has 43,000 students, the California Community College System, with 2.7 million students in 112 colleges, is the largest system of higher education in the United States. From the outset, we wanted to create a system capable of scaling up to serve millions of students. In addition, we wanted the system to be capable of delivering nudges using multiple communication modalities: our web portal, personalized RSS feeds, email messages, text messages, voice calls, text-tospeech audio, mobile device apps, or a custom Facebook application. 
 2.3 Nudges The term “nudge” was chosen deliberately to reflect the openaccess nature of community colleges, where students are rarely forbidden from taking any classes they desire. Sherpa includes three categories of nudges: Courses, Information, and Services. - The Courses module provides assistance in finding open course sections during class registration. Currently, its decision rules are codified by human subject matter experts; soon, other rules will be generated by data mining legacy data in order to base course recommendations on the academic performance of academically-successful students with similar college transcripts. - Information channels provide data feeds to students based on whether their personal attributes match attributes the author of the information felt would be relevant. - The Services module presents students with personalized links to online services such as course registration, book purchasing, or Matriculation. Nudges are created by subject matter experts using Boolean operators. First, a target population is created by concatenating rules (e.g., [At-Risk Athletes = [Student Athlete] + [GPA < 2.0]). Next, trigger conditions are set. Then, a message to the target population is crafted. Nudges can be print, audio, or video-based. Delivery of nudges can be triggered by dates, actions, or locations. - Date-based messages can be set to be delivered on absolute (xx/xx/xxxx) or relative dates (e.g., “three days before this individual student’s registration appointment”). - Actions are triggered by data changes, such as the appearance of a student’s grade in the Student Information System (SIS) or a class status change. - Location-based services await completion of Phase II of our Mobile App project, whereupon students who opt-in to sharing GPS information from their mobile devices will be able to have nudges triggered by geographic location (e.g., a student walking by the library receives a text informing his/her that the book they requested through interlibrary loan has arrived). 
 2.4 Design Team and Methodology.
 SOCCCD utilizes the Agile SCRUM software development methodology, which maximizes user involvement and flexibility. User involvement is maximized by using including administrators, staff, faculty, and students on the development team; flexibility is aided by creating software via an iterative series of three-week “Sprints”, the object of each Sprint being a functioning module of software. The Sherpa team includes a vice president, dean, public information officer, administrative assistant, a technology support staff member, student services manager, outreach specialist, instructors, and most importantly, students. 
 2.5 Feedback Mechanisms.
 In addition to the broad constitution of the design team, other feedback mechanisms include Quality Assurance Testing, User Acceptance Testing, usability studies conducted with TechSmith’s MORAE software, focus groups, and an annual survey. In addition, each nudge is accompanied by a 1-5 star rating system and a comment box; the former is used to automatically rank nudges according to students’ perception of their relative importance and the latter is reviewed regularly to fine-tune nudges for clarity. 
 2.6 Training.
 Since the Sherpa project is driven by a focus on students, we thought it appropriate to have students introduce the system to their peers. After consideration, we rejected print-based training in favor of short videos available on our MySite web portal: - In this video, students from the Sherpa design team describe the kinds of problems they face in college and how Sherpa can help solve those problems. www.youtube.com/watch?v=hIZIvgwsHM - In this video, students introduce the changes to the MySite web portal necessitated by the Sherpa project. They don’t mention Sherpa in the video because the MySite portal acts as the web “face” of Sherpa: www.youtube.com/watch?v=-oMlahqo4iQ 
 3. RESULTS.
 - Sherpa helped students who were closed out of a class find an acceptable alternative class 6,606 times since its deployment in the Fall 2011 semester. - Sherpa is now used as the messaging engine for our custom-created student information system (SIS), and is Instead of generic announcements, announcements are personalized for each student and integrated with our MySite web portal, either by the student’s inclusion in the target set for a given nudge or by allowing students to opt-in to various communication “channels” (e.g., Admissions, Athletics, Matriculation).]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Sherpa: Increasing Student Success with a Recommendation Engine</rdfs:label>
		<dc:subject>sherpa</dc:subject>
		<dc:subject>recommendation engines</dc:subject>
		<dc:subject>personalization</dc:subject>
		<dc:subject>student success</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-bramucci"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-bramucci"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-gaston"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-gaston"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/20/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-bramucci"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-gaston"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/21">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>GLASS: A Learning Analytics Visualization Tool</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/21/authorlist"/>
		<swrc:abstract>The use of technology in every day tasks enables the possibility to collect large amounts of observations of events taking place in diﬀerent environments. Most tools are capable of storing a detailed account of the operations executed by users in certain ﬁles commonly known as logs. These ﬁles can be further analyzed to infer information that is not directly visible such as the most popular applications, times of the day with highest activity, calories burnt after a running session, etc. Graphic visualizations of this data can be used to support this type of analysis as shown in [1]. Visualization can also be applied in the domain of learning experiences to track and analyse the data obtained from both learners and instructors. There are several tools that have been proposed in speciﬁc environments such as, for example, in personal learning environments [5], to foster self-reﬂection and awareness [2], and to support instructors in web-based distance learning [3]. These visualizations need to take into account aspects such as how to access and protect personal data, ﬁlter management, multi-user support and availability. In this paper, the web-based visualization platform GLASS (Gradient’s Learning Analytics System) is presented. The architecture of the tool has been conceived to support a large number of modular visualizations derived from a common dataset containing a large number of recorded events. The tool was developed following a bottom-up methodology to provide a set of basic operations required by any visualization. The design goal is to provide a highly versatile, modular platform that simpliﬁes the implementation of new visualizations. The main functionality elements considered in GLASS are database access, module management, visualization parameters, and the web interface. The platform uses datasets stored using the CAM schema (Contextualized Attention Metadata) [6]. This schema allows to capture events occurring during the use of various computer applications which, in our case, are the tools used by students when working in a learning environment. The process to obtain events from learning environments has been described in [4]. GLASS is able to connect to more than one CAM database, thus allowing access to events obtained in diﬀerent contexts. The tool is extensible through the installation of modules. A module is a structured set of scripts and resources that, given a dataset of events and a set of ﬁlters, generates at least one visualization. In order to simplify the development of new modules, the platform provides an API to manage common visualizations settings such as the date range and other typical ﬁlters. A visualization may include a simpler version suitable to be displayed in the user’s Dashboard, which is the entry page of the application. Figure 1 shows an example of dashboard in GLASS. Additionally, visualizations can be exported as HTML code to be embedded in another website. The GLASS architecture consists of four layers: data layer, code base, modules and visualizations, as depicted in Figure 2. The data layer is composed of a set of CAM databases and a database to store the platform parameters. The code base is in charge of the main functionalities of GLASS regarding module and user management and interfaces. Modules must comply with the platform speciﬁcations to generate visualizations and the settings that can aﬀect their appearance. Currently, the tool includes a default module that provides two visualizations as shown in Figure 1): a frequency time line of activity events and a bar-chart with grouped bars of events generated by diﬀerent user groups (e.g. events from students individually, or groups). The default module also serves as an example of how to develop a additional modules.</swrc:abstract>
		<led:body><![CDATA[ Acknowledgment.
 Work partially funded by the EEE project, “Plan Nacional de I+D+I TIN2011-28308-C03-01” and the “Emadrid: Investigacion y desarrollo de tecnologias para el e-learning en la Comunidad de Madrid” project (S2009/TIC-1650).]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>GLASS: A Learning Analytics Visualization Tool</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>visualization system</dc:subject>
		<dc:subject>visualization framework</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/derick-leony"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/derick-leony"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/abelardo-pardo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/abelardo-pardo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/luis-de-la-fuente-valentin"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/luis-de-la-fuente-valentin"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/david-sanchez-de-castro"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/david-sanchez-de-castro"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/21/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/derick-leony"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/abelardo-pardo"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/luis-de-la-fuente-valentin"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/david-sanchez-de-castro"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/22">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>What to do with actionable intelligence: E2Coach as an intervention engine</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/22/authorlist"/>
		<swrc:abstract>In this paper, we describe a new, analytics driven approach to supporting students in large introductory physics courses. For this project, we have assembled data for more than 49,000 physics students at the University of Michigan. For each, we combine an extensive portrait of background and preparation with details of progress through the course and final outcome. This information allows us to construct models predicting student performance with a dispersion of half a letter grade. We explore residuals to this model, conducting structured interviews with students who did better (and worse) than expected, identifying strategies which lead to student success (and failure) at all levels of preparation. This work was done in preparation for the launch of E2Coach: a computer tailored educational coaching project which provides a model for an intervention engine, capable of dealing with actionable information for thousands of students.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Nationally, more than half of students who arrive in college intending to complete degrees in STEM disciplines fail to do so [3]. The most disastrous drop-off is associated with gateway introductory courses in math, physics, chemistry, and biology [8]. These courses are usually large and always challenging, with average grades well below those in typical college courses. Many students emerge from these gateway courses having done worse than expected; their confidence is undermined and their desire to continue in a STEM discipline strongly diminished. This happens to students across the spectrum of performance: from solid A students receiving their first B+, to struggling C+ students slipping into the D range. If we wish to increase the number of students completing degrees in STEM disciplines, we must address the loss of potential STEM majors due to large, impersonal gateway courses.’ Students in gateway STEM courses are diverse by many measures, yet we ask them to learn using a single generic approach. They all read the same texts, hear the same lectures, do the same homework and class assignments, get the same advice, and are assessed using the same exams. We have worked hard at the University of Michigan to design physics classes that optimize learning for the typical student; these courses are excellent in the mean. But we have done little to adjust our teaching methods to meet the unique needs of individuals. We can do better. Technology exists which can give each student individualized coaching that will dynamically recognize their strengths, weaknesses, and performance trends, understand their motivations and goals, and guide them through the course, all the while encouraging them to continue toward a STEM degree. Intelligent tutoring systems focusing on domain specific knowledge have a long heritage [1][5] and are known to be effective. More recently, educational systems that focus on learner’s motivation and affect have received increased attention [2]. Tailored communication techniques are well established in the world of public health, where their efficacy has been extensively tested. Our goal is to gather relevant data about the students, collect the expert advice of both faculty and students, encode this in tailoring logic, and deliver personalized expert electronic coaching to the more than 1900 students who take introductory physics at Michigan each term. 
 2. CUSTOMIZING THE APPROACH.
 To better understand this approach, consider some of the details of the introductory physics course at Michigan. In such a course, we offer students a dozen tools for achieving their learning goals: commercial textbooks, custom coursepacks, pre-lecture reading quizzes, online homework systems with real-time feedback, interactive lectures with Peer Instruction, modeling of expert problem solving with additional problems to work in groups, multiple practice exams with solutions, notecards to use during exams, student led study groups organized by the UM Science Learning Center, all day tutoring facilities, an up-to-date online gradebook for feedback, and office hours with faculty. This wide array of learning tools is provided in the hope that it will meet all the needs of a diverse group of students. Unfortunately, most students receive no individualized advice about how to utilize these tools. They lack the flexibility to skip what will not help them or to access more of what they really need, and never receive feedback or encouragement that is aware of their personal goals, identity, or interests. A few percent do get personal advice and feedback; those who visit office hours regularly. Faculty experience with the lucky few suggests that specialized advice and encouragement can make a substantial difference in outcomes. Indeed personalized mentoring strategies of this kind are perhaps the only proven tools for STEM retention [10]. To provide customized, personal advice to each student, we must accomplish three separate tasks. First, we must generate some actionable intelligence – to gather data representing the state of each student adequate to decide what help and support they need. Second, we must know what action to take in each case – to gather expert advice. Finally, we need to have a mechanism for delivering the appropriate feedback to each and every student in the class – something which is impossible with the current system of face-to-face office hours. We need a technological way to provide each student with customized advice. 
 3. GENERATING ACTIONABLE INTELLIGENCE: THE BETTER-THANEXPECTED.
 The data which inspired this project have been collected and studied as part of the UM Physics Department’s “Better Than Expected” (BTE) project [6]. For this project we have gathered detailed information describing the progress of 48,579 students through our introductory physics courses over the last 14 years. From the University’s data warehouse, we combine a portrait of each student at the time they enter the class with internal gradebook information and final grades. These data allow us to quantify the impact of student preparation and background on course outcomes, and to construct predictive models of student performance. While a number of parameters correlate with final grade, prediction with a half letter grade dispersion can be accomplished using just one parameter: each student’s University of Michigan GPA at the start of the term. With hindsight this is not too surprising: A students are largely A students and C students are largely C students. Several examples of the correlation between incoming GPA and physics grade are shown in Figure 1. Our ability to track outcomes for many groups is especially important, as we have clear evidence that subsets of students underperform relative to others in these courses, even when controlling for a variety of parameters related to technical preparation. For example, first generation college students and those from low income families (<$50K/year) receive final grades about a quarter of a letter grade lower than their classmates when compared at the same entering ACT Math score. Female students are similarly disadvantaged, falling 0.2-0.3 letter grades below male students at all measures of incoming preparation: SAT or ACT math score, High School GPA, and even prior University of Michigan GPA (see Figure 1). Reducing these disparities with appropriate interventions is one of our primary goals. Research suggestsError! Bookmark not defined. that eliminating this underperformance will also have a substantial impact on the STEM retention of these students. 
 Figure 1: Example results from the UM “Better than Expected” project for Physics 140, the first semester course for engineering and physical science students. Left: Mean grades and dispersion as a function of UM GPA at the time the course begins. Right: Mean grades as a function of SAT math score. Results are shown for both male (diamonds and dotted lines) and female (triangles and dashed lines) students. Dotted and dashed lines show the 1 dispersion for male and female students repectively A strong gender disparity in physics grade is seen, with female students faring worse than male students at all levels of GPA and SAT math score. 
 3.1 Deciding what to do: gathering expert advice for success.
 From the BTE project, we know what performance to expect for each student, but significant dispersion in outcomes remains. It is here that we aim to act: we want to find out what interventions would help every student do better-than-expected. To this end, we gathered expertise from three sources: individual students, physics faculty members, and student study group leaders employed by the UM Science Learning Center. To begin, we have identified subsets of students who did better than expected (BTE) or worse than expected (WTE) in previous terms and have conducted structured interviews of them to help us understand what leads to these disparate outcomes. The initial round of interviews has revealed several important predictors of success previously invisible from our data. Response to the first exam is often the most important factor in ultimate student performance. Students who change their approach to the class are likely to improve their outcomes. We need to be able to encourage this behavior change, and to provide students with detailed information about how they should change. Prompt attention to setbacks is also essential. We have also conducted interviews of two groups of instructors; physics faculty members with many years of experience, and more advanced student study group leaders who have successfully completed these courses and often provide advice to current students. Advice from all three groups tells us what we should do with each student – what we should do with our actionable intelligence. Now we need only a method for delivering our intervention. 
 4. THE MICHIGAN TAILORING SYSTEM AND INDIVIDUALIZED EDUCATIONAL INTERVENTIONS 
 We are able to provide individual advice and coaching to every student by leveraging a powerful, proven tool: the Michigan Tailoring System. MTS is an open-source tailoring toolkit created by the UM Center for Health Communications Research (CHCR) [4]. The CHCR team has worked for decades to deliver expertly tailored health behavior interventions over a wide variety of topics, populations, settings, and communications channels. Computer tailored communications harness the power of personalized coaching from an expert, based on specific knowledge of the subject, while delivering services inexpensively to large, distributed populations. Systems of this kind outstrip the abilities of individual expert coaches in important ways; they access a wider range of information, intrinsically quantify the efficacy of their advice in the outcomes of a wide variety of subjects, and are not limited in time or space. MTS applications can be constantly refined; their efficacy always being improved, never forgetting a lesson learned, and never running out of time, patience, or enthusiasm. A great strength of computer tailoring is tireless scalability. Once the systems are constructed, they can coach ever larger groups of subjects with minimal additional investment. Tailoring approaches have been extensively tested within the public health community, where their efficacy is clearly established in peer reviewed journals. To give one example, a two group randomized trial of web-based tailoring in a smoking cessation project showed a 25% increase in continuous abstinence compared for those who received tailored as opposed to untailored communications [9]. Tailoring approaches in public health have also been commercialized widely. One example, HealthMedia Inc., was founded in 1998 by UM Professor, CHCR Founding Director Vic Strecher. HealthMedia now provides services to millions of participants each year across a broad range of interventions. During Fall 2010 the UM Physics Department joined forces with CHCR. Using support from the EDUCAUSE Next Generation Learning Challenge (NGLC), we began working to create an educational adaptation of the MTS system for use in our large introductory physics courses. This project has now been implemented, with a first intervention beginning in January 2012. 
 4.1 E2Coach.
 We call this educational adaptation of tailoring “E2Coach”, where we intend the E2 to evoke both electronic and expert. At the start of each class, E2Coach uses the results of a survey to absorb a complex array of information about each student. This voluntary initial survey will provide a rich portrait of each student who opts-in as they enter the course, including details about their background in physics and mathematics, their motivations for taking the course, desired and expected grades, attitudes toward physics, and confidence in being able to accomplish their goals. This initial portrait is augmented as the term goes on, with new information coming both from the gradebook of the course. The combination will provide a real-time portrait of each student’s progress. Combined with historical expectations for their final performance based on the BTE project, we have what we need to intervene. E2Coach provides the interface between students and the extensive and powerful resources available in each course, customizing recommendations for study habits, assignments for practice, feedback on progress, and encouragement they receive. At important points in the course, each student receives detailed feedback on their current status, along with normative information about how their work compares to their peers and predictions for what final grade they are most likely to receive if they continue to approach the course in the same way. A significant strength of this system is its ability to realistically predict how much better each student might hope to do if they improve their approach to the course. These predictions are based on the extensive historical information from the BTE project. E2Coach advice is delivered to each student as a personal web page filled with information and advice tailored to both their state and identity. Research in public health tailoring has clearly shown the power of personalized communication, with the efficacy of advice given in this way rising substantially with increased degrees of personalization. For example, testimonials provide a very effective way of delivering advice, and are much more effective when the identity of the testifier is closely matched to that of the recipient. For our purposes, testimonials are derived from UM students (the study group leaders). Each student receives advice from a former student who shares their background, goals, and concerns. So premedical students will not receive testimonials from particle physics faculty or engineering students who love physics, but from other premedical students less familiar with physics when they started and who, like the current student, felt their future as a physician was put at risk by this class. 
 4.2 E2Coach progress and evaluation.
 NGLC funding for the E2Coach system for Physics began in May 2011 and will continue for 15 months. The system has undergone a rapid development cycle, and was launched across all introductory physics courses in January 2012. Refinements will take place during Summer 2012, and a second semester of E2Coach courses will be offered in Fall 2012. This NGLC project will deliver tailored coaching messages to 3800 introductory physics students before the project ends in December 2012. By uniting student activity data with continual measures of performance, we also establish a powerful system for quantifying efficacy one that is intrinsically sensitive to the diverse nature of our student population. Because of the grade prediction schemes detailed above, we can separately assess the impact of preparation on students across the spectrum: those likely to struggle, certain to succeed, and headed for an average outcome. Since the system addresses each student individually, we have the opportunity to improve the performance of our students at all levels. Indeed this ability to have a positive impact on both at-risk and exceptionally successful students using the same system is one of the most attractive features of tailored coaching. To test the overall impact of E2Coach we will compare the performance of students using it to that of the 48,579 students in our BTE historical data set. We will look in particular for changes in some of the measures which motivated this work. At the most basic level, we will compare the performance of students in tailored classes to what we have found in untailored classes offered in the past. We have found that performance in these classes is affected not only by preparation but also by gender, socioeconomic status, and degree of family experience with higher education. These performance disparities may well be caused by the kind of psychosocial influences which tailoring is particularly well suited to address. The recent success of a values affirmation intervention in reducing the gender disparity at the University of Colorado supports this notion [10]. Eliminating these disparities is a central goal of this project: no group taking these classes should be disadvantaged by psychosocial factors unrelated to content knowledge. To test our success, we will compare the magnitude of each disparity in our new tailored courses to that seen in our historical data. The BTE project has also shown a disappointingly strong correlation between first exam performance and subsequent work for all students. Ideally struggle on a first exam would trigger a revised commitment to the course, rather than sealing a student’s fate. This provides a principle focus for our tailoring design: to encourage struggling students to change their study habits, use course resources more effectively, and address weaknesses in their preparation directly. To test the impact of E2Coach on the likelihood that students will recover from a rocky start, we will compare the correlation of first and later exams in tailored and untailored courses. It is possible that the three goals outlined above might be achieved by merely increasing the level of communication with students. It is important that we should separate the effect of individual tailoring of messages from that of merely increased communication. We will conduct this test during the Fall 2012 semester, randomly dividing students into two groups. The first group will receive the fully tailored E2Coach intervention. The second group will all receive identical communications tailored for the statistically average student. Neither instructors nor students will know who occupies each group until after the term. Once the term is complete, we will compare results for the individually tailored and uniform communication students. This will allow us to confidently separate the effects of tailoring from the effects of merely increased communication. A significant goal of this paper is to make the Michigan Tailoring System better known to the learning analytics community, and we will provide an overview of the work required to adopt this mature, open-source tool for educational applications.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>What to do with actionable intelligence: E2Coach as an intervention engine</rdfs:label>
		<dc:subject>tailored communication</dc:subject>
		<dc:subject>predictive models for student performance</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tim-mckay"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tim-mckay"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kate-miller"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kate-miller"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jared-tritz"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jared-tritz"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/22/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/tim-mckay"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/kate-miller"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/jared-tritz"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/23">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Exploring reflection in online communities</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/23/authorlist"/>
		<swrc:abstract>Commons-based Peer Production is the process by which internet communities create media and software artefacts. Learning is integral to the success of these communities as it encourages contribution on an individual level, helps to build and sustain commitment on a group level and provides a means for adaption at an organisational level. While some communities have established ways to support organisational learning – through a forum or thread reserved for community discussion – few have investigated how more in-depth visual and analytic interfaces could help formalise this process. In this paper, we explore how social network visualisation can be used to encourage reflection and thus support organisational learning in online communities. We make the following contributions: First, we describe Commons-Based Peer Production, in terms of a socio-technical learning system that includes individual, group and organisational learning. Second, we present a novel visualisation environment that embeds social network visualisation in an asynchronous collaborative architecture. Third, we present results from an evaluation and discuss the potential for visualisation to support the process of organisational reflection in online communities.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Commons-Based Peer Production (CBPP) is the process by which internet communities create and maintain digital public goods [2]. Generally, these systems work in what would appear to be a very chaotic manner. There is no fixed or coordinated schedule for contributors to a peer production project and users tend to contribute when and how they want. At the same time, there is no fixed management structure, and users are generally promoted in the community by virtue of their contribution. Often there is an area of the community’s space reserved for users to participate in discussions that relate to the community’s operation, such as the Village Pump in Wikipedia1 or Meta in the Stack Overflow2. This is, in essence, how the community learns as an organisation – users put forward their individual experiences that are then discussed with the boarder community. This can result in the implementation of new community policy as the community begins to evolve and adapt to how it once currently existed. While analytics are increasingly being explored as reflective mechanisms for leaning in a variety of contexts, there has been little research conducted on the use of analytic interfaces to support the organisational aspects of online communities. We believe that visual analytics can be used to support organisational learning in peer-based communities by not only providing a space for reflection but also promoting discussion and debate in the broader community. To asses this claim, we have developed a novel visual analytics tool, called Explore.su, which we have embedded in a collaborative architecture. This work builds on previous research into both social visualisation, which provoked reflection and story-telling in groups and communities [7], and social data analysis, which opened collaborative data analysis to a non-expert community [14]. 
 We divide this paper into two sections. In the first section we consider CBPP systems in terms of socio-technical learning systems, which supports three interrelated levels of learning: individual, group and organisational. We argue that each level of learning contributes to the system as a whole and then concentrate on how organisational learning is currently addressed in online communities. In concluding this section, we emphasise how the process of reflection is considered essential to organisational learning and argue that visualisation and the application of visual analytics can be used to support organisational learning in peer- based systems. In the second section, we introduce Explore.su, a collaborative visual analytics environment, which we developed to explore the process of reflection in a large Q&A community. We describe the design, implementation and evaluation of Explore.su and conclude with a discussion on reflection and the implementation of visual analytics in online communities. 
 2. LEARNING & PEER PRODUCTION.
 Commons-Based Peer Production (CBPP) is an approach to the creation and maintenance of digital public goods3 that harnesses the capability of communities organised, principally, without traditional management hierarchies. Communities engaged in CBPP have complex socio-technical infrastructures, which are predicated on cooperation, yet are subject to continuous change. We conceive of CBPP communities as socio-technical learning systems in which learning is undertaken at a number of interrelated levels: individual, group and organisational. 
 2.1 Individual Learning.
 Individual learning is one of the key motivating factors to the success of Open Source Software (OSS) [25]. Developers join OSS communities to learn, hone and improve their skills. In these communities, there is a much stronger emphasis placed on the social recognition of human-capital and on the autonomy of the individual. OSS developers are free to choose projects they consider cool, in vogue, or they feel could benefit from their contribution. The result is a more concerted effort on behalf of the individual both as a participant in the project and as an individual learner. The sort of learning can be generalised to other peer- production systems. In technical Q&A communities, for example, users can learn a great deal about a subject domain in a more interactive and participative manner. 
 2.2 Group Learning.
 When individuals come together the dynamics of learning is transformed as those individuals begin to work towards a shared consensus [3]. Peer production provides ample opportunity to work in groups, however, the nature of the collaboration will impact upon the approach to learning. Haythornthwaite, for instance, considers peer production as either lightweight or heavyweight, depending on the degree of freedom or ambiguity that exists around an individual’s role in the system [11]. Both lightweight and heavyweight models of peer production provide opportunities for learning; however, this occurs in the form of a learning collective or community of practice. 
 2.2.1 Learning Collectives.
 Thomas and Brown [6] argue that in communities, individuals learn to belong while in collectives individual learn to participate. While subtle in some regards, this shift recognises the importance of participation without over-emphasising the nature or indeed level of participation. Thus, learning can involve much less intensive forms of action such as rating or commenting on web content. 
 2.2.2 Communities of Practice.
 One of the most widely recognised (and highly cited) models of group learning is the community of practice model originally proposed by Wenger [26]. Communities of practice are founded on the recognition that learning is a continuous process and should not be separated out from the everyday activity of the learner. Rather, effective learning occurs when individuals are engaged in meaningful practices as part of the communities they value. Communities of practice require more intensive and often situated collaborations, than those found in learning collectives, and can often develop in many peer-based communities. 
 2.3 Organisational Learning.
 There is a broad academic and practitioner literature on organisational learning and a variety of perspectives have emerged in regards to the use of the term. In this section, we will briefly and rather broadly, address some of the core tents of organisational learning and then focus on how online communities, and in particular peer production communities, typically learn to adapt and change. Schon and Argyris published one of the first theoretical accounts of “organisational learning”, in which they described organisational learning as a method of “detecting and correcting errors” at an organisational level [1]. Their account introduced the notion of single loop and double loop learning and placed a particular emphasis on the role that reflection plays in effective organisational learning. Other authors, such as Kim, reinforced this view suggesting that organisational learning “increases an organisation’s capacity to take effective action” [18]. Levitt and March looked at the practice of organisational learning. They argued that experiential learning is prone to human interpretation and thus there is a need to address organisational learning with more objective analytical tools (such as statistical analysis for example) [19]. Crossan et al.’s 4I framework conceives of organisational learning as a process of dynamic renewal that occurs across three levels of an organisation: individual, group and organisational [5]. Their framework articulates how experience, intuited and interpreted at the individual level, is shared and integrated at the group level and then institutionalised, as new policies and procedures, at the organisational level. Organisational learning has been also addressed in the literature on online communities, however, rarely is the actual term organisational learning used. The majority of this research has been concerned with how online communities manage to adapt and change, given that power is distributed amongst the community members and that the technical and social elements of the community are tightly integrated. Butler et al., for example, looked at the policy mechanism in Wikipedia. They found that the Wikipedia had grown substantially - from a relatively open system to a much more sophisticated bureaucracy [4]. Jahnke found that a student online community evolved in a similar fashion from an undefined and amorphous state to a defined and rigid social structure [17]. Forte and Bruckman found that smaller autonomous projects had emerged within Wikipedia, each with their own set of operating norms and policy rules yet the basic norms of behaviour were inherited from the original site [8]. Schindler and Vrandecic describe how the introduction of new technical features in the German Wikipedia was the result of increasing pressure by the community [21]. 
 Our aim is to investigate ways to support organisational learning in online communities so that the policy mechanisms move beyond an experiential based model, which is prone to the problems of human interpretation, to one that is based on the application of more objective analytic interfaces. As a first step in this direction, we have developed a novel visual analytics tool that provides a community with an area for reflection in their social space. We believe that visual analytics cannot only be used to passively inform organisational learning in online communities but also provides a means for discussion and debate as regards to the ongoing evolution of community processes. 
 3. EXPLORE.SU.
 In the rest of this paper, we present the work done on Explore.su – a visual analytics tool that was developed to explore reflection in the Super User4 online community. 
 3.1 Related Work.
 Before continuing, we will briefly review the related work on social network visualisation and social data analysis. There is a wide range of software that supports social network visualisation. UCINET, Pajek and Gephi, for example, provide both statistical analysis and graph visualisation. While popular, many of these packages are developed for professional analysts and require substantial knowledge and expert interpretation during use. Researchers have, as a consequence, sought to develop social network visualisation tools that support the less- expert end-user. 
 Figure 1 shows a node-link (graph-based) visualisation from Vizster (Heer and boyd 2005). 
 Vizster, by Heer and boyd, is an example of a node-link visualisation that enables end-users to explore their own social network [13]. While provoking cycles of analysis, reflection and story-telling, Vizster (as illustrated in Figure 1) was designed for ego-centric networks, which are focused around a single user, and the approach will not scale elegantly with larger networks, such as the Super User online community. Indeed, this is a broader reflection of node-link (graph-based) visualisations in general [9]. Networks of over a 150 nodes are generally incomprehensible when visualised using node-link diagrams. While algorithms such as spring or force-directed layout help to organise the clutter of nodes and links into more meaningful representations, when conducted on large networks, the execution time is prohibitive. Perer and Shneiderman argue that many professional network analysts render the visualisation for publication only after having conducted the analysis [20]. When wishing to visualise change in a network, this problem is significantly exacerbated, as node-link layout algorithms remain sluggish or inelegant when dealing with the evolution of a network over time. 
 These difficulties have led researchers to investigate the use adjacency matrices as an alternative approach to node-link diagrams. Henry developed Matrix Explorer, a social network analysis tool that visualises large-scale social networks at both global and local levels [16]. Matrices are used for global representations while the more familiar node-link visualisations are used to show local relationships. The visualisation aims to shift towards the end-user (historian or sociologist), moving away from a complex interface to a more user friendly visualisation environment.      To handle scale, Henry implements block modelling, a common clustering technique used in network analysis, to order and represent the social network. While certainly a more compact approach, the result of block-modelling can often appear arbitrary and difficult to interpret. In an attempt to deal with such constraints, Van Ham et al. developed Honeycomb, a visualisation for large scale social networks [10]. Their approach, as illustrated in Figure 2, involves using “concrete organisational hierarchies to drive the analysis”. This is a more intuitive clustering approach that, unlike block-modelling, reflects a user’s prior understanding of the social network. Adjacency matrices can also handle the dynamics of a network in a much more elegant fashion than typical node-link diagrams. A matrices representation remains static as each node is already assigned a position in space. Thus, reflecting change is more accurate, and the result is generally more intuitive. This can be beneficial in two ways. First, change can be illustrated as a heat map in which significant change is represented by the intensity of colour (Figure 2). Second, the visualisation can accurately represent network spread, changes in the network that result from emergent patterns, such as population growth. Matrices are also able to portray areas of inactivity (or the absence of connections) as well as those of intense activity [10]. Again, this is because each node already holds a position in space that, unlike node-link diagrams, can show the absence of information over time. 
 Figure 2 shows a snapshot of the Honeycomb visualisation developed by Van Ham et al 2009. Connections are organised by country and then continent. Blue indicates negative change, red indicates positive change and grey indicates a connection. Colour intensity is used to illustrate the quantity of change. 
 Other approaches, which are not strictly network visualisations but have been used to visualise online communities, include History flow [24], AuthorLines [23] and Communication-garden [27]. While each helps to illustrate user activity in online communities, and portray the characteristics of a community, none of these methods convey community dynamics (or the interaction between users) as successfully as node-link or matrix visualisations. 
 3.1.2 Social Data Analysis.
 Social data analysis seeks to take advantage of social as well as cognitive and perceptual process, during the visual analysis of data. Much of the work to date has sought to render visualisation and collaboration tools more usable to communities of non- professional end-users. By far the most popular example is Many Eyes, a collaborative visualisation site developed by IBM [22]. Many Eyes has a complete suite of visualisations, from wordles to graphs, and users are supported in uploading their own datasets or exploring those uploaded by others. However, the collaborative functionality of Many Eyes remains relatively un-advanced, as users are unable to embed annotations in the visualisations. Heer’s Sense.us implements more advanced annotation mechanisms, such as view sharing, doubled-linked discussions and embedded annotations [15].            While illustrating that visualisation, and analytic reasoning, can include social as well as perceptual and cognitive processes, asynchronous collaboration has yet to be applied to social network visualisation. 
 3.1.3 Summary.
 While, at present, node-link visualisations experience difficulty with large and dynamic networks, other social visualisations do not express the dynamics (or interactions between members) of an online community effectively. Matrices visualisations, on the other hand, handle large and dynamic networks with reasonable elegance, and can illustrate areas of little as well as much network activity. Although social data-analysis has been implemented with varying degrees of success, there has been relatively no work on implementing asynchronous collaboration mechanisms with social network visualisation. 
 3.2 The design of Explore.su.
 We outlined several design goals that helped inform the development of the system. 1. Visualise the social and temporal patterns of the entire Super User community. The social patterns describe interactivity between members of the community and the temporal patterns describe how these interactive patterns change over time [7]. 2. Visualise different user actions. Members of Super User can post questions and answers, vote, comment as well as revise their own and other member’s posts. Each of these actions is considered as a separate communication-act and thus each is addressed with a separate visualisation. 3. Provide annotation and collaboration tools to encourage discussion and reflection 
 3.3 The Super User Online Community.
 The focus of the evaluation is the Super User online community, an implementation of Commons-Based Peer Production, which was introduced to address the domain of computer hardware and software. Super User is one of a series of community-driven question and answer sites called the Stack Exchange Network, which, with Stack Overflow, has risen to a position of prominence over the last number of years. The socio-technical implementation of Super User is of particular interest as it seeks to address many of the perceived deficiencies with traditional question and answer sites. First, Super User has both comments and posts. Posts (as questions and answers) are considered first class entities, while comments provide a way for users to seek clarification in regards to a post or suggest an amendment to a post. Posts can be considered as analogous to Wikipedia article pages while comments can be considered similar to an article’s talk pages. 
 Second, each community is specific to a domain (such as programming, gaming or cooking) and questions that are considered off topic can be migrated from one community to another. Again, this approach is analogous to Wikipedia’s name spaces. Third, Super User has an explicit reputation reward, of badges and reputation, which systematically encourages and rewards behaviour. Finally, Super User is collaboratively edited, a feature again drawn from Wikipedia, and has implemented a similar approach to the Wikipedia's Village Pump called a meta site. 
 3.4 Visualisation.
 We choose to use adjacency matrices as a representation of community’s network. This was largely because matrices can handle scale and evolution in a more elegant manner than node- link diagrams. Furthermore, matrices illustrate the absence, as well as the presence, of network activity. Figure 3 illustrates the user interface for Explore.su, Figure 4 shows revision patterns for a 24 hours period and Figure 5 shows a popup, which lists all the individual communication-acts for a glyph in the matrix, and is displayed when a user clicks any glyph in the matrix. This provides access to both a global view (illustrated by the adjacency matrix) and a local view of individual interactions. 
 Figure 3: The Explore.su interface comprises of the following components: 
 A) Adjacency Matrix – visualises the communication-acts (questions and answer patterns or commenting patterns or revision patterns or patterns of accepted answers) for the entire community over a given 24 hour period. B) Adjustable Timeline (graph and slider) – allows the user to traverse time, which in turn updates the matrix to a given 24 hour period. Dragging the slider animates the adjacency matrix, thus visualising temporal patterns as shifts in the community’s social patterns. The green line graph illustrates variations in community activity. In the above image, each dip in green line graph illustrates a dip in activity at the weekend. 
 C) User Actions, Graphical Annotation Tools and bookmark. - User Actions – enables the user to visualise different communication-acts. For example, a user can visualise questions and answer patterns or commenting patterns or revision patterns and or patterns of accepted answers. 
 - Graphical annotation tools – allows users to graphically annotate the visualisation. E.g. point, draw or highlight. 
 - Bookmark – allows the user to attach an annotated visualisation to a comment. This bookmark will retain the state of the visualisation (date and annotations). D) Threaded discussion – a set of threaded comments with visualisation bookmarks that allow users to load a visualisation’s state (including any contributed annotations) into the adjacency matrix. 
 3.4.1 Social Patterns.
 The matrix visualises community activity for a given 24 hour period. The visualisation is updated nightly (12.01 GMT), giving a relatively up-to-date representation of the community’s communication-acts. As is generally the case with social network visualisations, the Explore.su visualisation is created from the community’s reply-to graph. So if user A replies to user B, a connection is created between those two users. However, communication behaviour in online communities is often non- reciprocal. So, user A replying to user B creates an in-link between those two users without the need for user B to reply-back to user A.       Plotting the in-links between users creates an asymmetrical visualisation, which not only highlights areas of activity but also illustrate areas of inactivity (via the absence of connections). 
 Figure 4 illustrates revision activity in the Super User online community. Colour intensity is used to illustrate the number of connections between users of different reputation categories. There are two noticeable patterns. The first dark diagonal line illustrates users revising their own posts, an activity encouraged by the community. The second clump of activity on the left, lower section of the diagonal illustrates users with a high reputation revising the posts of users with a low reputation. This is a convention of the community, as rarely do new users revise other user’s posts. Given the scale of the community (~50,000 users at the time of writing), rendering a single one-to-one matrices was unfeasible. We therefore adopted the approach proposed by van Ham et al. and used a hierarchical overview to drive analysis. We choose reputation as a way to structure the visualisation as it is both intuitive and meaningful to the community. As discussed under the heading Common-Based Peer Production, reputation is a systematic mechanism for rewarding user contributions to the community. So users with a high reputation are generally trusted by the community, while users with a lower reputation are considered more recent members or are members who have yet to contribute significantly (in terms of reward-able activities) to the community. The distribution of reputation can be represented with a log scale. There are a large number of users (~26,655), for example, with a reputation of below ten, while there are only a few users with a reputation of over 50,000. To reflect this distribution, we divided reputation into 37 categories, starting with 0-10 and finishing with 90,000-100,000. The first 10 categories are increased by 10, so 0- 10, 10-20, 20-30 etc. The next ten categories are increased by 100, so 100-200, 200-300, 300-400 etc. The next ten categories are increased by 1000, so 10000-2000, 2000-3000, 3000-4000 and finally the remaining categories are increased by 10,000, so 10,000-20,000, 30,000-40,000 etc. The aim of this representation is to visualise a large community yet, at the same time, reflect the scale-free network topology familiar in large, open web-based systems and provide an intelligible visualisation for end-users. 
 3.4.2 Temporal Patterns.
 We also aimed to visualise, and hence provide access to, the community’s unfolding temporal patterns. Temporal patterns describe the naturally occurring rhythms of collaboration, which, when visualised, can help to illustrate aspects of a community’s evolution, such as sudden growth or network spread. At the same time, temporality is proven to play an important role in collaboration activities, enabling groups to coordinate their conception of time and improve efficiency and effectiveness. While there are several approaches to visualising temporal patterns in adjacency matrices, including thumbnail views for example, we implemented an adjustable timeline (as a line graph and slider in Figure 3), which enables users to adjust the date, which in turn updates the adjacency matrix. Dragging the slider continually updates and thus animates the matrix, enabling the user to quickly observe temporal shifts in the community’s social patterns. 
 Figure 5: Clicking on any square on the adjacency matrix will show a popup, which displays the communication-acts represented by that square. In this example, the popup displays questions and answers from users with a reputation of 100 to 200. This approach provides both a global and local representation of the community’s activity. 
 3.5 Collaboration Support.
 Collaboration support draws from Sense.us and from Heer’s recommendations for social data analysis [12], providing simple yet intuitive ways for users to gesture towards items of interest and to share observations with other users of the system. 
 3.5.1 Graphical annotation tools.
 We implemented several graphical annotation tools to help users communicate intent and contextualise an observation. Pointing is the most important tool in this context, providing users with a way to gesture towards a feature of interest or simply say “look here”. Other forms of annotation, such as highlighting and drawing, are also provided. 
 3.5.2 Bookmarking and view sharing.
 To analyse the visualisation, users must be able to see the same view, providing context for observations and user actions. In Explore.su, this is achieved with simple collaboration functionality in the form of visualisation bookmarking, commenting and view sharing. 
 3.5.3 Threaded discussions.
 Comments enable users to elaborate on their annotations, to describe the observations in more detail and to share their observations with other users. Comments provide a platform for discussion. 
 4. EVALUATION.
 We conducted a two staged evaluation, which involved a pilot lab study followed by an exploratory user study with the Super User community. The pilot study was conducted to ensure that the visualisation was comprehensible and that the overall system design was coherent. The exploratory study use was run as a live deployment over a two week period. 
 4.1 Pilot study.
 The pilot study involved 7 subjects (6 males and 1 female). All subjects are computer science PhD researchers from our faculty. None of the subjects are online community researchers and their specialities range from semantic technologies to ubicomp systems. In addition, none of the subjects were familiar with Super User, however, 4 of the subjects had used Stack Overflow, and all of the subjects were familiar with Yahoo Answers (an alternative non-domain specific question/answer site). The first 5 subjects were given a brief tutorial of the system, asked to play around or familiarise themselves with the different interactive elements and then asked to comment on their findings (submit a minimum of 2 comments). The second 2 subjects were given a tutorial video, outlining the system and the process of the study, asked to play around and familiarise themselves with the tools and then comment on their findings (min of 3 comments). While all the subjects were familiar with the core tenets of information visualisation, none are visualisation researchers and none were familiar with collaborative visualisation. No tasks were given, instead users were asked to browse around the visualisation, identify patterns of interest and comment on those patterns using the graphical annotation and collaboration tools. Each session lasted between 50 and 60 minutes. Each session was recorded, notes were taken by an observer and the subjects were asked to think aloud. Having submitted 3 comments to the system, subjects were asked to complete two short questionnaires - a participant questionnaire and a SUS (Simple Usability Score) test. 
 4.1.1 Evaluation Feedback.
 Some users expressed frustration at not being able to apply more fine grained filtering to the visualisation, to drill down a little deeper, or to compare and contrast the different user actions – such as following a specific thread over time or identifying the actions taken during the lifecycle of a thread. Other users expressed frustration at not being able to see what people were talking about. For example, one participant asked “what is the relation between the different topics and the activity represented by the visualisation?“ Another participant inquired “what are they talking about?” Some subjects also found it difficult to initially understand the matrices – as a visual metaphor – especially considering the use of reputation as an abstract representation. Few users were familiar with use adjacency matrices as most indicated more familiarity with node-link visualisations. However, once they grasped the general idea, and understood the visualisation as an overview tool, most users set about identifying patterns of interest. 
 4.2 Exploratory user study.
 The pilot study allowed us to refine our initial implementation and to evaluate how users perceived the matrices over the more familiar node-link diagrams, the second evaluation provided us with a platform to study reflection in the Super User online community. We recruited members of the Super User online community. A proposal for the study was posted on the Super User meta-site5. This was initially endorsed by a community moderator but subsequently removed within twelve hours by a Stack Exchange moderator who argued that this was an advertisement and against Stack Exchange policy.             As a consequence, each day we advertised in the Super User chat rooms, which tend to be used by the more committed user-base. 28 people signed into the system, and 17 completed the evaluation. As with the lab study, participants were asked to sign in, watch a short tutorial video, submit a minimum of five comments and then complete two short questionnaires. The first questionnaire related to the process of the study, inquiring about their impressions of the study and if they would be amenable to further contact, while the second questionnaire was, again, a SUS (Simple Usability Score) test. The process, they were informed, would take a minimum of 30 minutes. Six Amazon vouchers were raffled to encourage participation. 
 4.2.1 Findings.
 We received positive initial feedback about the prototype, especially from the more senior members of the site. Initially, some interesting comments were submitted outside of the Explore.su system – in the Super User chat room and on the Super User meta site. For example, on the Super User meta-site, one user commented “considering how insightful this is, maybe SE (Stack Exchange) should think of implementing?” In the Super User chat room, another commented on an insight they discovered using the tool “fascinating, 100-rep people are the ones who're asking the most, 1000-3000 rep ones are the ones to answer the most”. 
 Figure 6: Results of the content analysis on Explore.su.
 commentary. Each category is considered mutually exclusive. Observations were most prevalent.       However, several participants submitted hypotheses or questioned a previous user’s observation. Later, the same user contributed “comments and revisions have the same pattern of distribution, obviously comments distribution is more dense [sic]”. Participants also submitted comments as part of the Explore.su system. For example, one participant commented “these two clusters, based on the Questioner's rep, make me wonder how users cross the gap from 50 to 100 rep (reputation)”. Also, several users identified clusters around reputation of a 100. For instance, “here there's a concentration around 100 rep (reputation)” and “there's a lot of activity concentrated around the 100 rep (reputation)”. Finally, one participant remarked that “while there is a large variation of the reputation of commenters [sic], there is good correlation of users presumably actively responding to comments (diagonal line), especially in the 100-1000 range”. 
 4.2.2 Content analysis of comments.
 We conducted a content analysis on the commentary from the exploratory user study. We wanted to learn to what extent the visualisation provoked instances of reflection. We also wanted to assess the depth of reflection (whether an observation or a hypothesis for example), whether any instance of reflection involved discussion amongst the participants in the study or led to a broader discussion about the general operation of the community (in regards to conventions of norms for instance). We adopted the coding scheme that Heer et al. developed previously for their work on social data analysis [15]. However, five out of the eleven categories in Heer at al.’s coding scheme proved redundant during the coding procedure. The redundant categories were linking, socialising, testing, tips and to-do. The seven categories that were used during coding were data integrity, question, affirmation, hypothesis, system design and observation. Two coders coded the comments and we computed the Cohen kappa statistic for inter-rater reliability at 0.77. The results of the content analysis are illustrated in Figure 6. 
 5. DISCUSSION.
 We found that in both studies participants were able to comprehend and interpret the visualisation. From the exploratory study content analysis (Figure 6), it is clear that the majority of comments were observational (48%), considered as descriptions of the activity portrayed in the visualisation, which is in contrast to more reflective or hypothesis-driven comments (15%). In some cases participants asked questions of the data (9%) or affirmed an observation contributed previously by another participant (9%). System design was also commented upon, which included feature suggestions and requests (15%). However, instances of reflection did occur in which participants attributed additional meaning to the underlying patterns or suggested a reason for their observations beyond that of what is evident in the visualisation. In some cases, this involved participants replying, and affirming, a previous comment submitted by another participant, or indeed suggesting an alternative hypothesis for the original observation. While participants used the threaded discussion, under half the comments had annotations (45%), and 15% of comments were submitted as private. In the rest of this section, we discuss our findings. We concentrate on reflection and the implementation of visual analytics in online communities. 
 5.1 Reflection in online communities.
 We have found supporting evidence to suggest social network visualisation can provoke discussion and provide a context for reflection in online communities. During the exploratory user study, several participants reflected upon the community’s revision process (the most clearly identifiable pattern in the visualisation). For example, one participant noted: “Posters with a reputation below 500 are often edited, mostly by users with rep (reputation) from 700 up to 10k. People with lower rep (reputation) don't really seem to edit that much, although they theoretically can...” [sic] While a second participant replied: 
 “The users who appear to be editing posts at lower rep levels appear to be the original question asker. I would guess that at lower rep levels people are much more adverse to ‘messing up’ other peoples questions and instead focus on 
 their own...” [sic].
 As with Wikipedia, Super User is collaboratively edited; thus users are encouraged to revise both their own and other user’s submissions. While the revision process is not officially related to user reputation, there is a clear correlation between the amount of revision work a user undertakes and their standing (as measured by reputation) in the Super User community (as illustrated in the above examples). Another example of reflection occurs between two participants who discuss the relationship between reputation and post activity. 
 “Although it shifts from day to day, the highest correlation can be found here, either showing that there are many users with rep from 100 to 1k, or they are the ones who answer most, 
 and most questions are posted by users below 1k”.
 However, a second participant replied: “I noticed that too, and it might be interesting to note that the 100 rep (reputation) range can be users that have migrated from other (Stack Exchange) sites.” 
 In this example, the second participant suggests an alternative hypothesis for why the majority of activity occurs between users with a reputation of around 100. He suggests that these users may have migrated from another Stack Exchange site in which they would have amassed enough reputation to automatically receive a reputation of a 101 when arriving on Super User. Hence, the concentration of activity could be explained by migrations to Super User as opposed to a glut of users within that specific reputation category. Many questions are migrated from other Stack Exchange sites to Super User, as the domain of hardware and software is often confused with other domains such as programming or Web Applications (two other Stack Exchange sites), and users may find a question they submitted to Stack Overflow appearing in the Super User online community. 
 This view was affirmed in a further comment submitted by another participant. Here the participant suggests that the migrated users are well accustomed to the conventions of the Stack Exchange network: “I find it interesting that the (100,100) box has had a high number consistently. This furthers xxxx's idea of a poster commenting on his/her own post as the typical 'migrant' from another SE site already knows how to use the Questions and Comments correctly...” In the context of this study, the depth of reflection was limited by the approach to visualisation, which, fundamentally, sought to provide an overview of community activity. However, the visualisation provoked some interesting observations, some of which led to a deeper reflection based on conversations between participants. While we used live data for the study, and thus could not guarantee visible patterns of interest, users were still able to recognise the conventions of their community and to attribute additional meaning to their observations. Beyond developing a deeper understanding of community processes, it is difficult to argue whether visualisation could support a decision- making episode in an online community or indeed lead to a change in community policy, especially when considering the collaborative nature of policy-making. However, reflection in a shared context, as illustrated in the above examples, can help provoke discussion in regards to the conventions and norms of the community. 
 5.2 Visualisation communicates norms.
 From our work on Explore.su, we also found that visualisation is an effective way to communicate norms. Figure 7 shows the distributions for both revisions and comments in the Super User online community. Clearly, there is a strong correlation between reputation and revision, as illustrated in the first picture; while, comments, on the other hand, are more evenly distributed across user reputations. Users, regardless of reputation, make use of comments as a way to seek and provide clarification and thus refine and improve posts. This can have two applications: First, visualisation can provide feedback that can help to encourage productive and discouraging unproductive forms of behaviour in online communities. Second, visualisation can be used as a way to socialise new users by illustrating the norms and conventions of the community. 
 Figure 7: Communicating norms. 
 The image on the left illustrates the distribution of revisions in Super User. Clearly, users revise their own posts. Also, users with a high reputation revise other user’s posts. The image on the right illustrates the distribution of comments in Super User. In this image there is no clear correlation between comments and reputation as the distribution is much more evenly spread across reputation categories. 
 5.3 Networks for end-users?.
 Advances in visualisation JavaScript libraries, coupled with the increasing prevalence of public APIs, provides a great opportunity for visualisation and community researchers to develop web- based visualisations that can improve transparency, increase awareness and support more concerted processes such as reflection. However, the question still remains how useful are social network visualisations for communities of end-users? While we found that participants expressed a genuine interest in exploring their community through Explore.su, the majority of those who participated in the study were community elders or community moderators. For instance, one user commented that “from what I can tell most of the people who I know would find it useful have already been using it” (i.e. community moderators). 
 5.4 Viz as a conversational component.
 While the exploratory user study provided a platform for discussion, introducing new collaboration spaces is difficult and can be perceived as redundant by online communities that have existing and well established collaboration architectures. From the outset this was evident in the exploratory user study, as participants repeatedly returned to the Super User chat room to share observations drawn from Explore.su. As a consequence, we suggest that a more successful approach could support the generation of small, addressable and interactive visualisations that community members embed in their existing social software – to monitor a process, highlight a particular activity or provide a context for discussion. This approach harnesses the community’s existing collaborative infrastructure, which requires no additional appropriation by the community. 
 5.5 Limitations.
 Finally, there are a number of limitations to this study. Participants tended to analyse the visualisation as opposed to the activity represented by the visualisation. This could be improved by either providing a more interactive visualisation or focusing on a sub-group of users, such as the moderators, who are more actively engaged in the community. New users may have less of an interest in the activity of a community over those who are already more committed. Also, the sample size is relatively small, when compared with the population of the community. We ran the evaluation over a two week period and had some difficulties attracting participants (due to Super User’s policy in regards to advertising on their site or meta-site). While evaluating visualisation remains difficult - and there are a range of other methodologies available to the researcher - the importance placed on engaging the community underscored our approach. Nevertheless, explicitly differentiating between the visualisation and the act of reflection may prove as a more beneficial approach to evaluation when conducted in similar contexts. 
 6. CONCLUSIONS AND FUTURE WORK.
 Commons-Based Peer Production has enabled large, online communities of diverse demographics to produce media and software artefacts. These communities have distributed management structures and are subject to continuous change. We propose visual analytics as a way to encourage reflection and support organisational learning in peer-based communities. To evaluate these ideas, we developed a social network visualisation, called Explore.su, which we embedded in a collaborative architecture. We evaluated Explore.su with members of the Super User online community and found that the visualisation provoked both observational and hypothesis-driven commentary. While providing some evidence for the merits of reflection in online communities, the approach to visualisation requires further attention. Embedding visualisations in existing social spaces, for example, will increase the likelihood of adoption by community members.       At the same time, how instructive network visualisations are for end-users remains an open question and an avenue for further research. 
 There are three directions for future work. First, the depth of reflection was constrained by the approach to visualisation. The visualisation did not provide the ability to explore the data in great detail. Thus, there remains scope to refine and then to re- evaluate the approach to visualisation. Second, as successful online communities have existing collaboration spaces, there is room to investigate the application of small addressable visualisations that users embed in their existing social spaces. Third, providing access to social and temporal patterns may not necessarily inform established users to any great degree. There is scope to investigate the degree to which network visualisations, and the process of reflection, can educate users about their community. This would involve creating a measure and applying pre-and-post questionnaires to assess the depth of learning. 
 7. ACKNOWLEDGMENTS.
 This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Trinity College Dublin. Thanks to Super User for participating and providing the data used in this research.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Exploring reflection in online communities</rdfs:label>
		<dc:subject>commons-based peer production</dc:subject>
		<dc:subject>visualisation</dc:subject>
		<dc:subject>online communities</dc:subject>
		<dc:subject>social network visualisation</dc:subject>
		<dc:subject>analytics</dc:subject>
		<dc:subject>collaboration</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/john-mcauley"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/john-mcauley"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/alexander-o-connor"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/alexander-o-connor"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dr-dave-lewis"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dr-dave-lewis"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/23/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/john-mcauley"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/alexander-o-connor"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/dr-dave-lewis"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/24">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Using an Instructional Expert to Mediate the Locus of Control in Adaptive E-Learning Systems</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/24/authorlist"/>
		<swrc:abstract>This paper considers the issue of the locus of control in adaptive e-learning environments from the perspective of a new stakeholder; the instructional expert. With an ever increasing ability to gain insight into learners based on their online activities, instructors and instructional designers are poised to add value to the process of adaptation, a process normally reserved for either systems designers or the end user. This work describes the design of an e-learning system which provides automated analytics information to these experts for consideration, and then leverages the insights these experts have made as the basis for content and feature adaptation.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 An classic tension between the ﬁelds of adaptive systems and human computer interaction centres on the question of the locus of control : should the system adapt to perceived user needs, or should the system be adaptable by the end user at their demand? There is no clear right answer, nor is it a binary choice that must be made. Instead, a variety of successful systems have made choices between the poles of the adapt/adaptable continuum, taking into account the users, tasks, and domains at play. The goal of learning analytics is to provide insight into learners based upon their activity in e-learning systems. How this insight is used is up to the administrators, instructors, and instructional designers that have access to it. Remediation of known suboptimal behaviours is perhaps the principle interest of the area, and a number of diﬀerent techniques ranging from automated (as in intelligent tutoring systems), semi-automated (as in nudge analytics [3], perhaps exempliﬁed in the signals project [5]), and non-automated (through instructor or peer help interaction) remediation have been explored. Similar to the issue of the locus of control, there is no clear correct choice between these three levels of automation; all of the previously described techniques have shown that they can increase learner knowledge and satisfaction, and choosing one over others depends largely on the capabilities and quality of data you have available. In this work we address the question of the locus of control in adaptive e-learning systems in light of having instructional experts empowered with analytics knowledge. Instead of either automatically adapting to or being adaptable by learners, we aim to consider how instructors or instructional designers can gain insights from usage data which they then use to parametrize the way in which adaptation in the system is to take place. Thus the tension of the locus of control is mediated by a pedagogue who acts as a guiding hand, quietly informing how the system both adapts and presents cues to the learner for adaptation. 
 2. MOTIVATION & CONTEXT.
 In previous work we have shown how lecture video content can be automatically segmented into meaningful pieces using a combination of expert data and image recognition [1]. Through interviews with a group of learners, it became clear that diﬀerent learners approached the issue of how to segmenting video diﬀerently, and this diﬀerence was largely the result of the perceived usefulness of segments for a given learning task. Using the same system we have shown that learners, based on their usage in the system, can be automatically clustered into diﬀerent groups [2]. These groups appear to be indicative of end-user task; some learners would watch lecture videos every week, some only during the early or late portions of the course, while still others would watch video only when assessment drew near. Since clustering is an unsupervised technique, the groupings of students found in this second investigation aren’t meaningful until an instructional expert has labelled them. Without knowing this label, the system is unable to provide diﬀerent segmentations in a principled manner. It is possible to provide this labelling once for the application as a whole, which can then use these labels in choosing an appropriate segmentation algorithm for a given learner. But we would be remiss to do so without ﬁrst verifying that the clusters discovered are true for all domains, instructors, and circumstances that the system might be used in – a signiﬁcant endeavour indeed! Further, even if it were shown that clusters are stable across domains, and clusters were validated with respect to educational tasks, video segmentation is but one piece of an adaptive e-learning environment; this process would need to be repeated for each element in the system that is to be made adaptive. 
 Figure 1: A mockup of a lecture video playback system. 
 3. DESIGNING INSTRUCTIONAL EXPERTS INTO THE PROCESS.
 Our solution to this issue is to not design the system as an adaptive system per se, but to design it as an adaptable system where an instructional expert chooses how and when the system should present itself to the end user. In short, the system monitors learner usage, presents analytics information to an instructor or instructional designer who then labels meaningful patterns and parametrizes how adaption within the system should occur when these patterns are found. Consider the case of the video lecture system described in [2] a mockup1 of which is shown in ﬁgure 1. In the system there are multiple videos show to users depending on the capabilities of the classroom. Data projector video is segmented, and a list of segments is shown to the user on the left hand side. Clicking on a segment navigates the user to the corresponding portion of the video., and traditional video scrubber tools as well as a note taking widget are available. In this system the note taking widget contains both a private note-taking space, as well as the combined outputs of all students who have taken notes (a shared space). As students use the system they leave behind traces of what they have done; segments they have clicked on; pieces of video they have watched, paused, and rewatched; notes that they have made; etc. An ongoing challenge is how to present this information to instructional experts who may not understand statistical clustering techniques. We are considering a ‘learner-ﬁrst’ approach, in which visualizations of the results of clustering are shown using treemaps, where the top level treemap describes all learners who are registered in the course2 (ﬁgure 2). The expert can then modify the criteria by which learners are clustered using attributes available to them on the left hand side, and explore the results of the clustering process on the right hand side. Key to this method is that the clusters have no meaning to the system until they are labelled. The instructional expert does this by selecting a cluster (a rectangle in the treemap), inspecting the data using traditional charting tools (shown at the bottom), and editing the label ﬁeld. Each cluster is hierarchical, allowing the expert to recursively inspect and label sub-clusters of the data by double clicking. Clustering is static process based upon the attributes which the pedagogue has identiﬁed (in the left hand window). Membership of learners in clusters will change over time as more user data is collected, but the deﬁnitions of each cluster (the centroid ) will not change until the expert chooses to delete labels. A learner may be in multiple clusters at once. The instructional expert may choose to cluster data around some attribute set and provide labels for those clusters, then cluster around another attribute set for other purpose and come up with diﬀerent labels. The eﬀect of being in multiple clusters is that the system may be able to adapt the user interface in multiple ways. For example, a learner who is reviewing content for an exam and is a social constructivist learner may be recognized as such, and the system may adapt lecture video segmenting to provide overviews of relevant material while at the same time making available social tools such as chatrooms or shared notetaking features. Or, a learner who regularly returns to content and is a non-native language speaker may be shown closed captioning tools and more detailed segments to aid in navigation, while learners who had been shown to navigate quickly between segments may be provided video in high speed playback. Once clustering data has been labelled, the instructional expert can make these kinds of parametrizations to describe how adaptation takes place. We envision this using an interface similar to that which the student sees, where the pedagogue can add, remove, or characterize widgets based on the clusters a learner may belong to (ﬁgure 3). Parameters are widget-speciﬁc, and a default application view exists for those learners who are not in a labelled cluster. 
 Figure 2: Data exploration page; a list of the possible attributes to cluster by are shown on the left hand side. The treemap at the top right shows the clusters found, as well as the number of learners in each cluster and the expert-provided label. In this example, the expert has labelled the smallest cluster ‘reviewers’, and is exploring the data through traditional charts and graphs at the bottom right. 
 Figure 3: Parametrization of the segmentation widget. Note that each widget (in background) has a drop down allowing the pedagogue to delete the widget, add a new widget, or parameterize the widget that already exists. The parameters are supplied for each cluster label in the system; in this case there are three labels (Reviewing, Social, and First Time). Widget parameters, such as ‘1 every 30s’, are speciﬁc to the widget being customized, and we envision the use of controlled vocabularies and interface mechanisms to make this natural. 
 4. CONCLUSIONS.
 This work is proposing that instructors and instructional designers be included as mediating agents with respect to the locus of control for adaptable systems where learning analytics data is available. By having instructional experts parametrize how adaptation happens, the burden of validating the educational eﬀectiveness of a given adaptation by system developers is lessened. Further, this approach provides an inclusive method of customizing an adaptive elearning system for diﬀerent educational domains, tasks, and contexts. As a design, this work leaves us with unanswered questions of end-user perceptions of such a system, some of which we elaborate on here: - Will instructors, content experts, and instructional designers see value in attaining the insights and providing methods of adaptation? - Can the system be written such that it is accessible to these experts, and uses language and terms they understand? - Does this approach force on already burdened educators the need (either explicitly or implicitly) to micromanage the adaptive systems that support their courses? - Will adaptations be natural for learners, or does more of the adaptation process need to be opened up to them (for instance, through scrutable modelling [4])? - Are adaptations reusable enough to be shared such that they can serve as a starting point for new instructors and instructional designers who want to partake in this kind of endeavour? The areas of educational data mining, adaptive hypermedia, artiﬁcial intelligence in education, and intelligent tutoring systems are largely void of researchers situated in traditional education departments. With this work, we’re hoping broadening the dialogue around adaptive e-learning systems to include these experts of instruction directly. We do so by proposing that the starting point for adaptation sit in the hands of instructors and instructional designers, and that they determine, based on learning analytics, what actions should be taken. In designing the parameters for these environments, we believe instructional experts will reason more deeply about the patterns found in their classroom data. We aim to capitalize on this insight, and hope that not only will those experts see pedagogical gains in their daily activities, but that education researchers will use these methods to contribute to the growth of the ﬁeld of e-learning.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Using an Instructional Expert to Mediate the Locus of Control in Adaptive E-Learning Systems</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>adaptive systems</dc:subject>
		<dc:subject>locus of control</dc:subject>
		<dc:subject>instructional design</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-greer"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-greer"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carl-gutwin"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carl-gutwin"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/24/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-greer"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/carl-gutwin"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/25">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Investigating the Core Group Effect in Usage of Resources with Analytics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/25/authorlist"/>
		<swrc:abstract>In many educational institutions, face to face as well as on-line teaching is supported by the use of a Learning Management System (LMS). To be able to analyze better data stored by LMS, we have started developing a dedicated tool for this purpose. While analyzing usage data with teachers, we have noticed that the number of students attempting non self-tests decreases during the semester. Teachers were interested in investigating this pattern further to uncover the strategy adopted by students. In this paper, we explain our approach to investigate the core group effect in resources usage: given a set of resources, is a group of students emerging that continuously uses the resources or, on the contrary, are the resources used on an irregular basis by different students? We answer this question checking the confidence of what we call local rules and global rules. We show a case study conducted with our analysis tool as a first step to validate our approach.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 In the Beuth University of Applied Sciences as in many educational institutions worldwide, face to face and on-line teaching are supported by the use of a Learning Management System (LMS), Moodle [1] . A number of our industrial partners use also an LMS for continuing education. The development and maintenance of learning resources for an LMS requires some effort. Hence it is important to be aware of how learners learn with the learning resources put for them on-line. Though they store users' data, LMS have limited reporting and statistics facilities, which is natural. Their purpose is teaching and learning, not analyzing users' behaviors. To be able to analyze better data stored by LMS, we have started developing a dedicated tool for this purpose [2, 3]. This software should be a Web Application that could be personalized to serve different kinds of end-users such as education providers, teachers, course designers, students and so on. In that respect our tool is similar to AAT [4] or eLAT [5]. End-users are likely to be non proficient in Information Technology, therefore it is essential for an analysis tool (i) to be intuitive in its use, (ii) to present results that end-users can easily interpret and (iii) to use analysis techniques that are well understood, as stressed in [6], and that have been validated in order to inform properly stakeholders and in order to prevent wrong decision making whose consequences are difficult to predict. The present contribution is concerned with this last aspect. While analyzing usage data with teachers, we have come across an interesting pattern like the one depicted in Figure 1 concerning non-compulsory self-tests, here ex1 to ex7, that teachers make available during the semester. This pattern indicates that the number of students attempting these self-tests decreases during the semester. Teachers were interested in investigating this pattern further to uncover the strategy adopted by students: Are they gradually giving up completely, which means that the students who attempt self-test i is roughly a sub-group of the students who attempted the preceding self-test? Or are they eclectic in their choice, which means students attempt randomly some self-tests during the semester though they attempt them less as the semester progresses? In this contribution we explain how we handle these questions. The next section briefly introduces the tool we use for our analysis. Then we expose how to analyze the core group effect when usage of resources declines and, in the follower section, present a case study as a first step to empirically validate our approach. The conclusion ends this paper. 
 2. THE TOOL.
 As mentioned in the introduction, we design a dedicated tool for analysis that is completely separated from the LMS. This means that its data source is also independent of any LMS. In that respect we follow the principles encountered in business applications: the data warehouse used for data analysis is independent of the transactional database used for daily business. Therefore our tool is separated into two main parts: building the data source and querying/analyzing it. The ExtractAndMap module, [2, 3] extracts all usage data stored by an LMS and maps it to Mining-DB, a specially designed database. Mining-DB has a form better suited for querying and mining than the database of an LMS. Further analysis is independent of any LMS. Indeed, the main class of ExtractAndMap can be implemented to extract data from any LMS. For the time being it has been implemented for Moodle. If an institution uses different LMS, all usage data for analysis is in one place, in Mining-DB. This module runs in the background and regularly updates Mining-DB, which then contains current and historical data. This feature makes our tool different from [4], that can not handle data originating from different LMS at once or from tools that are specific to a LMS like [7] or [8]. Another feature of ExtractAndMap is that it renders users anonymous complying with the principles on data privacy in our university. Simple queries on Mining-DB allow to answer a number of very informative questions like: “How many new courses have been created in summer semester 2011?”, “How many students have subscribed the course Introductory Programming with Java and the course formal basics of computer science?”, “How many students have completed self-test 1 in the course Introductory Programming with Java ?”, “What is the average mark in the exam formal basics of computer science of students who have attempted self-test 6 and self-test 7?”, “How many students have accessed Resource X on April, 14 2011?”. A single query can involve an arbitrary number of courses or resources. Currently queries are answered using the MySQL language [9] or the Pentaho reporting tool [10]. The Pentaho reporting tool offers the facility of predefining reports for different kinds of end-users. 
 3. ANALYSING THE CORE GROUP EFFECT.
 We investigate the learning behaviour of students with respect to the resources put in the LMS: how do they use them during the semester? First we explore how each resource is used, which is done with simple queries. In the case we observe a decrease as shown in Figure 1, we want to explore the following behaviour: is a group of students emerging that continuously use the resources or, on the contrary, are the resources used on an irregular basis by different students? In the sequel we denote by |X| the number of students that use resource X and by |X, Y| the number of students who use both resources X and Y. We suppose first that a perfect core group emerges that consequently keeps accessing the resources: students who use X form a subset of those who use some previous resource Y. In that case the number of students who used resources X and Y is the same as the number of students who used X, what is written |X, Y| = |X|, or equivalently, |X, Y| / |X| = 1. In terms of association rules [11], the quantity |X, Y| / |X| is the confidence of the rule X → Y . This rule in our context is interpreted as “if students consulted X, they also consulted Y”. On the contrary, when resources tend to be used on an irregular basis, a number of students consulted resource X without consulting some previous resource Y. In that case |X, Y| ≠ |X|, or equivalently, |X, Y| / |X| < 1. In the worst case, the group who used X is completely distinct from the group who consulted Y and |X, Y| = 0, thus |X, Y| / |X| = 0. The quantity |X, Y| / |X| measures the proportion of the students who used Y among the students who used X. Summing up confidence of the rule X → Y allows to measure to what extend the group of students who consulted some previous resource Y forms a subgroup of those who consulted resource X. In case of a real subgroup confidence is 1. From experience a value of 0.8 or bigger seems adequate to denote a trend towards subgroup. In our context we have a set of n ordered resources Xi where 0 < i < k ≤ n means that Xi has been put online earlier than Xk or Xi is previous to Xk. In the case of a perfect core group, there are many rules with a confidence of 1. First all rules with some previous resource in the consequent: Xk,→ Xi for any i and k such that 0 < i < k ≤ n.. Then it is easy to see that rules with any set of previous resources in the consequent will have also a confidence of 1: Xk,→ S, where S is a set of resources and any resource in S has an index smaller than k. In other words, if a perfect core group emerges, there is a rules “deluge” with a maximum confidence: all rules with on the left side a resource that comes at a later date than the resources on the right side. We make the hypothesis that it is enough to check the confidence of two kinds of association rules, what we call local rules and global rules, for teachers to be aware and to follow the emergence of a core group. Local rules have the form Xi+1 → Xi. These rules are concerned with what happens locally in time: use of a resource and the preceding one. If a group of students starts emerging that continuously uses the resources, this kind of rules should have a high confidence, 0.8 or above. On the contrary confidence should be low if different students irregularly use these resources. Global rules have the form Xi+1 →X1, X2 , … ,Xi and check globally the first i+1 resources: if students use resource Xi+1 then they also use all earlier resources. Again, if a core group starts emerging, confidence of these rules should be fairly high. This hypothesis stipulates that in practice confidence of all possible other rules as mentioned above with the rules deluge will be very similar to the confidence of local and global rules and it is not necessary to check them. 
 4. CASE STUDY.
 We have investigated how students use self-tests in two courses Formal Basics of Computer Science and Introductory Programming with Java both taught in face to face teaching to first semester students enrolled in the degree “Computer Science and Media” at the Beuth University of Applied Sciences, Berlin, during Winter Semester 09/10. 57 students were enrolled in Formal Basics of Computer Science and 65 students were enrolled in Introductory Programming with Java, 46 being enrolled in both courses. Both courses proposed 7 self-tests denoted ex1, ex2 ... ex7 put online gradually during the semester. These tests are not compulsory and students do not earn any mark when they complete them. They are given as complementary resources to support them further in their studies. However, the Java course is quite difficult, especially for students with no experience in programming. The failure rate of the Java course is higher that the one of the Formal Basics course. The teacher in charge of the Java course stresses the usefulness of the self-tests more than the teacher of the Formal Basics course does. This case study focuses on the 46 students enrolled in both courses and investigates how they have attempted these self-tests. 
 Figure 1. Number of students attempting self-tests. Left: Formal Basics course, right: Java course. 
 Figure 1 shows how many students have attempted those tests. One notices a similar pattern though more consistent for the Java course: as the semester progresses less students attempt the selftests. The last test takes place shortly before the exam. Figure 2 shows the confidence of local association rules, 2→1 means if students attempt the second self-test, they attempt the first one. One notices that confidence is quite high for the Java course, except for the last rule, the smallest value being 0.77. About 80% or more of the students who attempt an exercise have attempted the preceding one. It is interesting to look at Figure 1 and 2 together. 
 Figure 2. Confidence of local rules. Bottom: Formal Basics course, Top: Java course. 
 Tests 2 and 3 of the Formal Basics course have been attempted by 23 students as Figure 1 shows. Figure 2 tells us that about 40% students who attempted test 2 did not attempt test 3. Altogether 32 students of the Formal Basics course were active with test 2 or test 3 while it is 29 for the Java course. In the same way, 18 students from the Formal Basics course and 19 students of the Java course were active at attempting exercise 6 or 7. Global rules are shown in Figure 3. Here again for the Java course, omitting the last test, about 70% or more of the students who have attempted an exercise have attempted all preceding ones. 
 Figure 3. Confidence of global rules. Bottom: formal basics course, Top: Java course. 
 Students in these two courses have followed a slightly different strategy. Figure 2 and 3 show a trend for the emergence of a core group till test 6 in the Java course, while from test 3 onwards student tend to pick and choose in the Formal Basics course. Then we have checked the confidence of all rules of the form j→S with j≤ 6 and S a set of self-tests occurring before j different from the sets checked with local or global rules. With one exception (0.71 for the rule 4 → 1, 2, 3), confidence is never below 0.77 for the Java course and always well above 0.8 if S contains only 1 resource. For the Formal Basics course confidence of these rules varies between 0.18 and 0.65, many values being around 0.30. These results are in the range given by the local and global rules and thus confirm our hypothesis. We have obtained very similar results when considering all students, not only those registered in both courses. In that setting we have investigated local and global rules from an association rules point of view [11] and finish this section summarising the findings First notice that we are not interested in support, a measure often needed to extract association rules that occur often enough in the data. Support of global and local rules varies greatly, 0.06 (smallest value in the Formal Basics course) to more than 0.54 (greatest value in the Java course). Global rules particularly can be rare association rules [12]. Then we have checked with three further measures the interestingness of our rules: lift, cosine and correlation [13]. The values obtained rate the local and global rules of the Java course as interesting, whereas cosine and correlation rated most of the local and global rules of the Formal Basics course as borderline. Further we have checked the marks in the final exam in different settings: average in general, average of students who attempted at least one exercise, average of students who did not attempt any exercise, average of students who attempted exercise 1, average of students who attempted exercise 2, and so on till average of students who attempted exercise 7. In both courses the average of students who attempted at least one exercise was higher than the general average. A striking difference was that the highest average was for students who attempted exercise 7 in the Java course [2]. Such an effect was not visible for the Formal Basics course. We have extracted local and global rules using queries only, not using any association rule mining algorithm. There are two reasons for that. First we know exactly which associations we are looking for. There is no need for an algorithm that has to discover possible associations. Second support is not important in our case and can vary greatly. It would be difficult to adjust support to find the desired local and global rules with some association mining tool. 
 5. CONCLUSION.
 In this paper we explain our approach to investigate the core group effect in resources usage. With core group effect we mean the following: Given a set of resources whose usage decreases over time, is a group of students emerging that continuously use the resources or, on the contrary, are the resources used on an irregular basis by different students? We do not check all possible dependencies between usage of resources but only those given by what we call local rules and global rules. When confidence of these rules is around or above 0.8, we interpret these rules as a trend towards cor group effect. The idea can be generalized when usage stays constant over time and one wants to investigate whether resources are used by a stable group of students. When usage of resources increases, reversing the rules should show whether a core group is growing. For the teacher to intervene our experience shows that the information given by local and global rules has to be completed with more analysis, like past values about marks in the final exam: Do students who attempt all exercises get better marks? Do students who attempt at least one exercise get better marks? This information has to be combined with the follow-up of use of resources to intervene properly. 
 6. ACKNOWLEDGMENTS.
 This work is partly supported by the Institute für Angewandte Forschung Berlin and the European Social Fund for the Berlin state project “LeMo” .]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Investigating the Core Group Effect in Usage of Resources with Analytics</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/25/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/26">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>It's Just About Learning the Multiplication Table</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/26/authorlist"/>
		<swrc:abstract>One of the first and basic mathematical knowledge of school children is the multiplication table. At the age of 8 to 10 each child has to learn by training step by step, or more scientifically, by using a behavioristic learning concept. Due to this fact it can be mentioned that we know very well about the pedagogical approach, but on the other side there is rather less knowledge about the increase of step-by-step knowledge of the school children. In this publication we present some data documenting the fluctuation in the process of acquiring the multiplication tables. We report the development of an algorithm which is able to adapt the given tasks out of a given pool to unknown pupils. For this purpose a web-based application for learning the multiplication table was developed and then tested by children. Afterwards socalled learning curves of each child were drawn and analyzed by the research team as well as teachers carrying out interesting outcomes. Learning itself is maybe not as predictable as we know from pedagogical experiences, it is a very individualized process of the learners themselves. It can be summarized that the algorithm itself as well as the learning curves are very useful for studying the learning success. Therefore it can be concluded that learning analytics will become an important step for teachers and learners of tomorrow.</swrc:abstract>
		<led:body><![CDATA[

 1. Introduction.
 Phil Long and George Siemens stated [11] that “the most dramatic factor shaping the future of higher education is something that we can’t actually touch or see: big data and analytics.” In other words if a lot of learning data is measured and analyzed, new insights into the learning process will be given and therefore new didactical approaches will help to improve our learning behaviors – not only in higher education but in the whole educational system. Of course the main question addressed by Erik Duval [2] is about what exactly should be measured to get a deeper understanding of how learning takes place. In this publication we like to answer the following research question: “How can we improve learning the multiplication table with the help of an individualized learning program and a followup data analyzing?” First of all, it must be mentioned that learning the multiplication table is a central subject of mathematics in primary school. Young children gradually develop an intuitive and practical arithmetic that they use to solve problems successfully and confidently in their everyday life. But each child is different and some of the basic knowledge about mathematics is acquired even before children start school [8] [14]. Considering current didactical knowledge about learning math tells us, that there are cultural differences, some obviously based on language implications [3]. Each child needs a general linguistic competence before starting to operate with numbers. Some publications point out that mathematic is the first non-native language [9]. For example, understanding the simple expression “multiply” is implicitly not only a mathematical problem but also a linguistic one. Many children who carry out the algorithms correctly do not really understand reasons for crucial aspects of the procedure [6]. The most common and traditional way to learn the multiplication table is drill and practice without taking care of any linguistic preconditions. “Multiplication Tables in 21 days” [12] is one of many offers of teach the kids online or to use traditional materials. From the perspective of special education it is well-known that this way of teaching and learning math is problematic and leads to disorders and blockades on the learner’s side. The “defective number module hypothesis” indicates a genetic defect, a more physiological cause of basic brain functionality [1]. Teachers gladly accepted such a biological explanation as a relief for their problems with a child. According to current neurological oriented research results, but also based on a very old educational tradition, teaching should use tactile, optical and acoustic processing methods in every single case – intensively when they observe deviations from the mainstream. It is important to allow multisensory math to be taught to children with special educational needs. Kendall [7] offers some practical suggestions of how to achieve this. Furthermore face-to-face teaching has to bear in mind also interlinking between different assignments. But the daily practice is dominated by pure “row learning” of the multiplication table [5]. 
 It has to be borne in mind that learning per se is an active individual process on the learners themselves. The competence “perfect handling the multiplication table” differs enormously in the needed learning time and resulting learning effort – from weeks to years. 
 It is obvious that the simple problem “learning the multiplication table” is not as trivial as it seems to be at first glance. The goal of our research is to develop a web-based system that assists learners as well as teachers. Furthermore it should provide teachers with an overview about the current learning process of their pupils. The research group (consisting of educators, e-learning experts, an educational scientist and IT-developers) specified their requirements as follows: 
 - The system should estimate the competence grade of the learner. - The system should provide appropriate exercises according to the competence grade of the learner. Nevertheless the exercises should tend to be challenging. - The system should ensure that already well-done exercises are repeated and practiced. After succeeding a problem the probability for a repeated display decreases in two levels (analogous to a Leitner System [10]). - In general the system should be motivating and show that learning can be fun. - The system should record and safe fine-grained data of all done exercises, test results and the current competence grade of the learner in order to prepare the next sessions in an adequate way. - Thereby it should unburden the teachers from that task. 
 In contrast to many other approaches to adopt learners in this research work there are no probabilistic strategies for estimating competencies used. Even more the goal is to generate a complete table to inform learners as well as teachers about their problems in every single task. And for sure teachers want to know something about the process, the used time and the up and down of the learning curves. 
 Similar research work in the area of Intelligent Tutoring Systems (ITS) [4] [13] pointed out that such an approach can raise many problems for daily-life-situations of an actual classroom. In other words, our research work concentrates on a particular and small learning problem, but likes to ensure the learning success. Finally, the implemented intelligent algorithm is not only useable for the specific problem of learning the multiplication table. Any problem that is presented in a set of flashcards could be easily transferred to a system with this functionality. 
 2. Technical details about the application.
 Beforehand some technical details need to be mentioned that were defined by the project team: The application is written in PHP1 5. The interface is based on HTML2 and CSS3. Because of the different render engines of the browsers the CSS-Framework YAML4 is used. Carrying out a solid software development the application is programmed with the help of Zend Framework5. The main approach is to store the data of each learner to analyze their learning process. 
 3. Algorithm.
 The approach of this research study is to develop an algorithm that allows an individual learning by providing exercises in dependence on the learner’s knowledge. Therefore the main task is to consider which exercise is provided next. On the one side, the exercise should be challenging on the other side it must be adequate; not too difficult but not too easy at all. Therefore the core task of the program is the new designed algorithm. This algorithm decides which question is chosen to be next. The general idea is to always find the “best learnable” question in reference to Vygotskys [15] idea of "the zone of proximal development" for the user. Therefore two parameters are important: - Difficulty of the exercise: First of all each learning exercise has to get a difficulty rank. If the task difficulties are already known from a preliminary investigation (for example a pedagogical study), they should be used in that way. Otherwise a hierarchy of estimated difficulties must be assumed according to specific pedagogical experiences. The difficulty ranks should then be converted to a scale with values between 0 and 1. This means an easy question gets a high success probability value (close to 1) and a more difficult one gets a low success probability value (close to 0). - Degree of competence: To keep track of the learning progress of a user the degree of competence is calculated (0 ... 1), called learning rate. The calculation of this value depends on the performance of each learner and is calculated in real time. - When we started developing this program we used “degree of competence” in the sense of ratio of items, which have been solved to the sum of items which has been offered. During the further process we found out that it is more useful to have a less volatile variable for determining the level of the next question and two others (learning rate 1 & 2) for describing the success and analyses. 
 3.1 Degree of competence.
 The basic idea of a degree of competence (DOC) is to indicate which question is difficult and which is easy in relation to the current user´s knowledge. It must be considered, that for the first session we don’t know anything about the competence level of the student so far. With every answered question we get a more and more reliable estimation of the real competence. The main idea is that each user has their specific degree of competence equaling a value between 0 and 1. 1 means the user is advanced and every item could be selected as a suitable task or question, 0.5 indicates an intermediate user and 0 is a beginner. In general the degree of competence reflects the current knowledge. Questions graded “below” the individual degree of competence are solved and known by the learner; questions “above” are questions to be learned. When the next learning item is chosen it must be avoided that the task is neither too difficult (learners will get frustrated) nor too easy (learners will become bored). To fulfill that demand we have to assess the questions given to be homogenous compared to traditional tasks. The amount of data collected by the application helps us to discuss for example whether this homogeneity is really given, and whether the students achieve the learning goals in typical, well known and logical or different and more individualized learning paths. We use the idea of a so-called “extended learning area” to describe the pool of which learners get the next tasks from. By definition the whole learning area is a combination of the actual learning area (known by the learner) and the extended learning area (unknown by the learner). In our pilot study we defined the extended learning area to be 25% above the learner’s degree of competence. Of course this parameter can be adjusted by teachers. In a situation where the teacher anticipates a low level of competence in this domain, at the beginning of learning the tables, this value must be kept down. Over the time, for example when learners are starting to learn the multi-digit multiplication, it could be become higher. Figure 1 visualizes the whole learning area sectioned by the degree of competence into an area of known questions (actual learning area) and one of future questions (extended learning area) limited by the extended degree of competence. 
 Figure 1 Degree of competences. 
 In the initial test phase, we found out that these settings are complicated, especially during the first session when the students have only solved some few problems. A computer program is restricted in perceiving students’ interaction only towards solved or unsolved learning questions. A teacher perceives of course, many other and different signals (nervousness, facial expression, comments on the task …) that may affect the decision which task is to be given next. With other words especially an improper prea-ssessment of learners’ knowledge can be frustrating by keeping learners too long overstrained or bored. Furthermore it must be avoided, that the tasks’ difficulty escalates too quickly and questions therefore become inadequate. In the following chapter the design of the pretest is pointed out. 
 3.2 Pretest.
 At the beginning, before the learning session, each learner will be asked two questions, starting with a moderate one; if the answer is correct; a more difficult one will be presented - the estimated degree of competence is set around 0,75. If the answer is incorrect an easier one will be provided (around 0,25). Figure 2 points out the flow diagram to get the estimated degree of competence in the beginning. For example if the first answer is correct and the second one false, the estimated learning rate results 0,50. Finally to get adequate questions (easy – medium – difficult) the following categories are defined - Easy: A question with a probability for a correct answer of 0,78 – 0,69 - Medium: A question with a probability success value of 0,54 – 0,47 - Difficult: A question with a probability success value of 0,20 – 0,13 
 Figure 2 Pretest – estimated degree of competence.
 3.3 Answer classification.
 Another issue that must be addressed in our context is the classification of well-known learning problems. From a pedagogical point of view it must be stated that presenting a right solution is not a satisfactory indicator. Especially learning the multiplication table is learning by drill & practice which means if a learner practices one problem more often they will get more confident that they can manage this item. Therefore also the developed algorithm must ensure that a task is “well-known”. This issue is addressed by establishing a “well-known” parameter. 
 The answers of the learners are marked with 0, 1 or 2: - 0 means a wrong answer - 1 shows that the user knew the correct answer once - 2 indicates that the student had two consecutive correct answers (this means a question is “well known”) 
 Questions that have been signed with 2 were set back to 0 immediately in case the student failed. After the pilot stage it was decided to set the “well-known” parameter always to 2 when the degree of competence of the learner is 0,3 higher than the difficulty of the task. We wanted to reduce the frequency of getting too easy tasks for good students. Each time a student solves a problem, the result will be stored by the application. We calculate a “learning rate 2” to be the ratio of the sum of solved items signed with 2 and the number of items. We also compute a “learning rate 1”. This ratio is based on the sum of all solved items, e.g. signed with 1 or 2. 
 It is to remark, that these two levels of knowledge and the set procedures have the functionality of a Leitner System [10]. The results of psychology of memory give the recommendation that learned tasks should be used again and again, but there is only a decreasing frequency necessary to avoid forgetting the right solutions. 
 3.4 Adjusting competence.
 In the first session we only have the results of the pretest to estimate the competence of the respective student and to allocate a suitable question. With every new test we get more and better information for specifying the student’s current competence. At the beginning of our tests we simply calculated the ratio of the number of correct answers to all given answers. This leads to the following formula: 
 (FORMULA_1).
 This formula resulted in a too high volatility at the beginning of the learning progress. For example if a learner answers the first 2 questions in the pretest correctly the program will assume a degree of competence of 0,75. After answering the next 2 questions wrong the degree of competence decreases to 0,5. The fast increase/decrease of the factor of course will cause that learners become quickly frustrated. A second approach with the intention of getting a more stable value was to calculate the degree of competence by counting answers only with the classification “well-known” (2). The resulting formula is: 
 (FORMULA_2).
 number of items = number of learning problems/flashcards provided by the system. 
 This formula has some disadvantages as well, especially for learners with a too low estimated degree of competence resulting from the pretest. For these learners it might be hard to increase their degree of competence, i.e. to correct the erroneous data. Finally the degree of competence is defined by the procedure as follows: We compute the ratio of both - number of correct answers as well as well-known answers - and the number of items. 
 (FORMULA_3).
 The results are saved in the database. In general learning progress is considered as completed, if every question is signed with 2 (well-known). When a learner starts their first session the following procedure combines the result of the pretest and the actual degree of competence: This special procedure is only used at the beginning if 
 (FORMULA_4).
 Then we compute a “weight”:.
 (FORMULA_5).
 With this weight, the degree of competence is calculated as a combination of the estimated degree of competence of the pretest and the previous performance: 
 (FORMULA_6).
 This degree of competence is important to find the next task in the learning area. Therefore it must be guaranteed that the DOC is not in-/decreasing too fast in order to provide adequate questions. 
 3.5 Selecting items.
 After the degree of competence is calculated the next item presented to the learner must be chosen. For this purpose, the algorithm chooses items out of three categories. 
 - Extended and Actual Learning Area (items signed with 0) - Actual Learning Area (items signed with 1) - Actual Learning Area (items signed with 2) 
 A random number out of the interval 0 and 1 is used to decide which category is activated. Therefore three cases are defined: 
 - Case 1: If the random number (x) is smaller or equal 0,05 a well-known question marked with 2 is chosen. - Case 2: If the random number is 0,05 > x >= 0,15 a known question marked with 1 is chosen. - Case 3: If the random number is x > 0,15 a (unknown) question out of the extended and actual learning area is chosen. 
 In general all items are preordered according to their difficulties and have corresponding rank numbers. In case 1 another random number is drawn to get a position in the ranking of the known items. Beginning with this position the algorithm is looking for the next less difficult item, which is signed with 2. In case 2 the algorithm is starting with the rank of the easiest item and is looking for the next item, which is signed with 1. In case 3 our selection is adjusted with the degree of competence. We compute an upper rank for the selection: 
 (FORMULA_7).
 Afterwards the algorithm is starting to search beginning with rank position rank or less difficult items. The next item, which is signed with 0, is selected. In the case the algorithm is running out of the ranking, because no suitable item could be found, the procedure is repeated again and again until it finds an item. We are sure that an item is found, because the program selects already known problems for repetition with a p=15%. If all items are signed with 2 the student will receive this information together with an option to finish. The student may proceed with the training if preferred so. 
 4. Algorithm overview.
 Finally a short overview of the workflow (algorithm) is given:.
 1. Pretest (see section 3.2, figure 2) 2. Calculate the degree of competence (see 3.4) 3. Choose next question according to the degree of competence (See section 3.5). For this, the probability to choose a question out of the three classes is (could be adjusted): o 75% learning area o 10% known questions o 5% well-known questions 4. Process the given answer (See section 3.2) 5. Calculate new degree of competence (See section 3.4) 6. Check if learning progress is finished. If yes output a message to the user otherwise go to step 3. Repeat the algorithm until the user stops playing. 
 5. Prototype.
 The program is implemented as a game. Learners can earn points for each correct answer given and reach new stages (called rank). Points are gathered or can be lost if the answer given was wrong. Furthermore the answering speed is also important. The quicker a question is answered correctly the more points are gained. Finally the game requites also consecutive correct answers by giving more points. The prototype is a web application currently available following the URL (last visited October 2011): http://vlpc01.tugraz.ac.at/~georg/index.php/user/login. After registering an account learners are able to login. Depending on the learner’s user type they are redirected to the according page. The learners will get the main game interface (see figure 3). Administrators will see an additional navigation bar, which allows choosing between getting statistical information about all existing learners and simply playing the game. Due to the fact that it is a first prototype the interface is kept simple with a first design suggestion. 
 Figure 3 Main screen of the application.. 
 Figure 3 displays the main game interface (currently only a German version is running). The markers 1 to 6 refer to the most important areas of the interface: 
 1. The questions which have to be answered (item of the multiplication table) 2. Free space for feedback to the user, for example “Correct answer”, “Wrong answer” or “Not entered a number” 3. Input field for providing the answer. It can be submitted by clicking on the green button “Antworten” or pressing “Enter” on the keyboard. 4. The timeline shows the remaining time for giving an answer. Actually the learner has 60 seconds to answer (predefined by the teacher). Depending on the answering speed points are calculated. 5. Shows the actual rank of the user. 6. Displays the actual points of the learner. Of course the more points a learner gets, the higher is their rank. 
 6. Study.
 After finishing the web application the first research study was carried out at a primary school in Austria. In summer semester 2011 the program was handed out to 42 pupils of the primary school Laubegg6 (age: 9-10). After a short introduction of the main principles of the program and setting personal accounts for each learner the study was started. It lasted at least 4 weeks. Some of the learners ignored this time restriction and played the game again and again over months. Learners learned on computers at the school as well as on their personal computers at home. It can be summarized that in the time frame of the study 12.926 answers where given which means that on average each learner answered 308 questions or in summary they did the whole multiplication table 3,4 times. Bearing in mind that there was no real pressure from teacher’s side using the program it is a considerable pleasant high number. Furthermore it can be stated that pupils seemed to enjoy using the application or at least got not bored. 
 7. Discussion.
 7.1 General Overview.
 First of all an overview of the general result is given (figure 4) by displaying the number of trials versus the final number of wellknown items. In detail figure 4 shows that in summary 18 learners (43%) mastered every 90 single multiplications at least twice (to get marked 2) of the multiplication tables. Many of them obviously enjoyed the game: One of them solved even 1.486 assignments on voluntary basis. If learners make no mistakes, they need approximately 190 trials to show their competence for each item and get marked “well-known” (2). According to our defined algorithm and due to the fact that the probability of getting a correctly solved item twice in the very first beginning of the game is only about 10% the number of 190 trials seems to be a quite solid one. More than half of the learners are below the aspired 90 correct single multiplications of the multiplication tables. There might be several reasons for this: The learner - did not get used to / has problems with the interface - does not know the necessary operations; is not able to solve the learning problem correctly - misinterprets an assignment - is distracted by their environment and makes wrong clicks - is badly concentrated for several reasons. As mentioned before, six learners proceed to work with the program, even after the program offered to stop because every assignment had been answered correctly twice. In such cases, the program serves as a diagnostic instrument: It is more reliable than a paper-pencil test when a learner masters the multiplication table with this application. 
 Figure 4 Well-known items versus trials. 
 As mentioned before, more than half of the learners did not reach the 100 percent level. We already listed possible reasons why a learner does not solve the assignments correctly. Besides these arguments, it is necessary to reflect whether weaker learners could have fewer possibilities using the program, especially at home. The above-mentioned “high-trial” learner for example is known as someone who was able to train and use the program also at home. Eventually, the better students had more chances to work at the computers in classes, so that the teachers were able to work with weaker learners. Perhaps, learners with worse performances had general problems, like to open and to handle the program. Nevertheless, these are only further assumptions, which must be considered in future studies. 
 7.2 Demotivated learner (ID 21).
 One learner with a weak performance attracted our attention because of a very high number of trials (513) (figure 5). A detailed inspection showed that she/he did not work very intensively. In the first two tasks he/she failed, then eleven tasks were ok, her/his performance rose abruptly. However, afterwards, she/he continued approximately 400 times to wait the whole answering time without doing anything but asking for a new assignment. 
 Figure 5 Unmotivated performance. 
 7.3 Motivated captain.
 Figure 6 shows the learning history of the most diligent learner. In the beginning, the assignments were solved correctly, then some mistakes occurred, afterwards a learning process can be recognized and finally with some occasional mistakes the learner works on a high performance level. Obviously, the learner was highly motivated to deal with the assignments given by the program: She/he absolutely wanted to be and stay the first in the high score. Furthermore there are an amazing high number of trials, which leads to the assumption that the learner likes to do the exercises. It seems that there is a dependence between the high number of trials and the almost perfect knowledge of the learning field. 
 Figure 6 Motivated captain. 
 Teachers have to face such situations. Why did the learner become demotivated? Why did he/she did not proceed going on with learning and gave up? This case is quite dramatic, because the learner her/himself disrupted the ongoing learning process. 
 7.4 The medium learner (ID 115). 
 One of the most interesting learning histories can be seen in figure 7. It points out the way the application works if a learner is not performing very well. In the beginning, the learner made mistakes in every second assignment (0,5), followed by 7 mistakes consecutively. This is the reason for the big decrease (0,15). Afterwards, the learner gave a number of right answers and the rate of correct answers increased back to 0,5. In the following phase an up and down can be seen till a number of right consecutive answers helps to reach a level of 0,7. But then the number of mistakes rose again and the rate went down to about 0.5. This characteristic curve illustrates how a learner is learning from mistakes and is getting better by failing an assignment and slowly solving it next time - “learning by failing”. 
 Figure 7 The medium learner. 
 This effect can be seen more detailed in figure 8; it shows the learning rate 2 of the same student. As mentioned in previous chapters an item is marked as 2 if it was solved at least twice consecutively. In the initial phase the same items are not presented very often due to the fact that the probability is defined with 15% (see chapter 4) and because the program prefers assignments that were not correctly answered beforehand. Afterwards, the performance increases fast to a certain stage (20 items are marked as 2) and falls a little bit (“learning by failing”). The next 140 questions caused a so to say sideward trend; the student answered some questions right some wrong. Then the curve jumped up. The student has learned the solutions of a more difficult group of items, we do not know whether they got training beside the program or not. It needed about 120 questions more until the curve was growing again to foster the learning efforts. Finally we see that the last 15 assignments obviously lastingly cause some problems. It can be concluded that the learner needs just some additional time or other exercises and materials to perform the whole multiplication table perfectly. 
 Figure 8 Unsteady and slow growing learning curve. 
 Figure 9 – the learning rate 1 of the student – underlines the “learning by failing” effect. There were about 300 trials almost every problem is signed as known (one time) - but only 20 items are signed as well-known (2 times). Furthermore in about 20 percent of the items marked as “well-known” mistakes occurred (figure 8). Finally, the learner seems to be concentrated better or they learned the tables and solved 70 of the 90 items twice. 
 Figure 9 Learning rate – the DOC. 
 7.5 The weak learner.
 Figure 10 illustrates the exercise list of one learner, who had performed easy questions with simple assignments, but got worse afterwards, when the difficulty of the questions jumped over their degree of competence. We call the effect the “fast increase effect”, which means the increase is too fast and not appropriate to the learner’s knowledge. 
 Figure 10 "Fast increase effect". 
 The learning rate curve of the same learner in figure 11 describes the same behavior: a fast increase followed by a very unstable phase and decreasing output. In comparison to the case before, the learner is only able to manage about 25 percent of all items correctly. It is assumable that the learner is not mastering the whole multiplication table. By all means, they work with the program, make mistakes, are corrected, learn and for sure need much more time to master all items finally. 
 Figure 11 Learning rate of a weak learner. 
 7.6 Lazy bones.
 Figure 12 illustrates the learning curve of lazy learners because of the very little activity in relation to the competence. Of about 100 problems 80% were solved. We need more precise observation, what is going on there. Is it a problem, that these students need an additional (social) motivation? The process is very slow and the performance is not good in this test situation. Could some other problems be the cause for this lack? 
 Figure 10 A typical learning rate for learners with low trials. 
 7.7 General remarks.
 We think it is useful to test and to train the students with such programs. They have possibilities to get “out of the stream” with provocations to learn, which is the same in a normal class. But here we have a protocol of learning progress. In every case the delivered tasks are more frequent and more precisely orientated on the individual level and next reachable tasks than in a standard situation, with a teacher without such an information-processing capacity tool. 
 8. Conclusion and future work.
 In this publication we discuss the implementation of a new and intelligent algorithm to assist school children training one of the basic learning goals in primary schools – the multiplication table. Furthermore a field study with 42 learners was carried out and analyzed. The power of learning analytics allows the research team to think about the outcomes and carries out different types of learning curves. In general we can state that there are some major types of curves – learners who are very knowledgeable, those who are in a stage close to being knowledgeable and those who still need a lot of learning effort to reach the learning goals. However it is recognized that analyzing just one curve (correct answers/total number of process items) is not sufficient enough to cover the state of the learning process in most of the provided cases. There is a need to have a look at learning rate 1 and 2 too. Moreover sometimes the curves gave the researchers and teachers only a hint that a pedagogical intervention is absolutely necessary to enhance the learning results. For our future work we consider a couple of ideas how the current application can be improved, but mainly it must be stated that there should be a much closer look at the learners. This can perhaps be done in some more intelligent analyses of the data or in more cooperation with the teacher. We will have to analyze the reasons why some learners are demotivated or why their learning rate decreases during longer intervals. We also discussed whether it could be more appropriate to change the design from gaming to a more informative display of the actual knowledge. This could be a matrix, which displays the results with the 0,1 or 2 signs. In the future we also need a better aggregation, compression and visualization of the learning outcomes. This could indicate those people, who need more attention from the teacher and probably immediate interventions. Another point is that we measured the time to solve the questions only to limit the time but not to construct additional aspects of the learning behavior. Overall we are convinced that the application is a further step towards an interesting learning future. The teacher saves time for management. Analyzing learners’ results and performance over a longer time period brings more reliable and systematical insights to teachers for their daily work in classrooms and improve the learning success of each learner. Nevertheless learning is a highly social process and is an active process on the part of the learner, where knowledge and understanding is constructed by the learner. With other words the implemented tools will help teachers to get a better feeling about the individual learning process and allow a just-in-time reaction. 
 9. Acknowledgements.
 We express our gratitude to the teachers of the primary school in Laubegg (Styria, Austria) as well as all participating school children. We are equally indebted to our funding agency “Internet Foundation Austria (IPA)” for supporting our ideas and helping us to work on the future of education.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>It's Just About Learning the Multiplication Table</rdfs:label>
		<dc:subject>multiplication table</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>learning curve</dc:subject>
		<dc:subject>degree of competence</dc:subject>
		<dc:subject>algorithm</dc:subject>
		<dc:subject>school children</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-schon"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-schon"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-ebner"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-ebner"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/georg-kothmeier"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/georg-kothmeier"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/26/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-schon"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-ebner"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/georg-kothmeier"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/27">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Exploring Qualitative Analytics for E-Mentoring Relationships Building in an Online Social Learning Environment</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/27/authorlist"/>
		<swrc:abstract>The language of mentoring has become established within the workplace and has gained ground within education. As work based education moves online so we see an increased use of what is termed e-mentoring. In this paper we explore some of the challenges of forming and supporting mentoring relationships virtually, and we explore the solutions afforded by online social learning and Web 2.0. Based on a conceptualization of learning network theory derived from the literature and the qualitative learning analytics, we propose that an e-mentoring relationships is mediated by a connection with or through a person or learning objects. We provide an example to illustrate how this might work.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 The Open University (OU) is a UK based open and distance learning provider. It has about 12,000 staff and around 200,000 students distributed across the world at any one time. Like any large organization mentors play a vital role in the professional development of individuals [11]. Mentoring is a social and psychological relationship and typically takes place face to face, where the value is seen to come from those personal interactions. Those type of relationships present a key challenge to a distributed organization like the OU, where students and staff are not necessarily co-located. In an effort to develop a good online or e-mentoring service, we started to investigate how to build up and better support mentoring relationships using Web2.0 technology by creating a platform called SocialLearn (SL). This paper proposes a framework and features that focus on how we might employ transient connections (weak ties) within social media to develop more “meaningful” (strong ties). 
 2. ONLINE SOCIAL LEARNING.
 Mentoring is a social relationship. Conole (2008) [5] noted that the real opportunities that Web 2.0 affords is within online social and situated learning. Online social and situated learning focus on learning as social participation and shifting from an individual and information focused learning to an online social learning and communication/collaboration. To foster these relationships online social learning platforms should focus on social interactions through activity streams, following and making connections, to draw users towards content or learning objects. This paper explores these social relations with reference to mentoring. 
 3. MENTORING.
 Haggard, et al. (2011) [11] systematically reviewed the mentoring literature between 1980 and 2009. They found over 40 different definitions for mentoring. In this section we explore some of the ways that mentoring has been defined. 
 3.1 What is Mentoring.
 “Classic mentoring” features one to one relationships between a more senior or experienced individual and a less senior less experienced individual. Attempts to create a mentoring typology often focus on formality (formal to informal) and structure (professional competency to unstructured). However, while the degree of formality is a factor, the relationships can be far more complex than this, as it is a personal relationship [17]. Wong and Premkumar (2007) [19] present three mentoring models: The apprentice model, the competency model, and the reflective model. In the apprentice model, akin to traditional apprenticeships, the mentee learns through observing and copying the mentor; in the competency model, the mentors provide feedback on the mentee's performance; in the reflective model, the mentors focus on developing self-reflection of the mentees. The different mentoring definitions and models invite us to think about the different mentoring relationships and how the mentoring theories relate to Web 2.0 pedagogies. Within online social learning the focus is on learning by interactions and connections with and through a person or a learning object which is likely to be informal and unstructured. In addition to these mentoring models that focus on the learning process, Haggard, et al. (2011) [11] suggested that we must also consider a range of other personal factors, such as gender, career stage, age difference between mentor and mentee, etc. This suggests that within any online mentoring1 relationship the ability to be able to “know” something about another person is as important as what they do. A person’s profile on a social media sites help us “know” the person. This suggests that we need to create mechanisms that allow people to see others activity and view aspects of their profile. 
 3.2 Mentoring Motivation.
 Wong and Premkumar (2007) [19] provided a mentor motivation checklist to illustrate some of the reasons why people engage in mentoring relationship, such as people like the feeling of advising others, they find it satisfying, etc. It is interesting to see that the motivations are associated with engaging in Web 2.0 communities and earning credit and reputation. For example, “generalized reciprocity” [5] in online spaces is widely reported; there is a suggestion that newcomers develop an obligation to help others in the future through the valuable help and advice they receive as newcomers [15]. Allen (2007) [2] found that in addition to the simple motivations cited earlier more complex psychosocial themes emerge. Perceived similarity is a factor – so called similarity-attraction paradigm- where selection is based on the overlaps in interests, Another important factor to consider is mentor and mentee's performance where social exchange theory drives mentor-mentee selection, with mentors choosing mentees with strong performance, high ability and ample willingness with links to “quality” within social media and again the role of the profile becomes important. Online social learning spaces cannot account for all of these factors, but they inform our thinking. For example, we can account for similarity, allowing users to see a profile and judge similarity, and by using analytics we can make recommendations, we can account for performance, by creating criteria that allow users to display, view, rate and evaluate other users through their profiles. 
 3.3 Shifting Sense of Mentoring.
 Our discussions above has shown that “experience” or “seniority” is read in slightly different way between “classic mentoring” and the mentoring in the online social learning context. There is a flatter hierarchy in online mentoring than we see in “classic mentoring” and this is considered to have benefits in terms of student engagement retention and progression. The importance of peer support in learning is also recognized in the workplace. The informal social interaction around shared tasks and challenges is now seen as a vital part of learning at work [9]. Discourses on e-mentoring also destabilize notions of “classic mentoring”. The ability for mentoring to be relatively anonymous and for mentors to be involved in multiple overlapping relationships changes the relationship psychologically and practically. Alevizou (2010)’s work [1] on Web 2.0 argued that the peer interaction and collaboration learning fostered by Web 2.0 is a kind of distributed online mentoring. These opportunities are being realized by several companies that offer secure mentoring services (e.g. Mentor Pro2), and open sites like Horsemouth3. These sites offer a complex range of online mentoring, from career, education, business, to secure services for vulnerable adults and young people, and what might be more accurately termed life coaching. We need to account for “classic mentoring”. However, in Web 2.0 discussions on online mentoring is a diffuse relationship. It is seen as part of the democratization of education where “communities” support each other to create, understand and share resources [6]. This means classic mentoring theories are not enough to explain different types of e-mentoring relationships that might evolve. We wanted to explore how the form of online social networks informed the development closer relationships. 
 4. SOCIAL NETWORKING.
 Granovetter (1973) [10]’s work on social ties explores the role that weaker ties have within networks. Strong ties are those things that bind groups (strong ties have overlapping network and interests), while weak ties (casual contacts) allows us to connect with other networks and open up new areas to explore. Sites like LinkedIn4 and Academia.edu5 operate on this principal. Haythornthwaite (2002) [12] investigated the relationship between latent (he inactivated weak ties), weak and strong ties and different communication media. She found that new mediums of communication layered over existing ones can help strengthen weak ties. In addition, a new communication medium can turn latent ties into weak ties. This suggests that online social media can play a role in activating and strengthening ties. Our reading of networks and ties is not one where networks and ties only exist between individual, we recognize that they also exist between what we call mediating objects (people, images, groups, event posting, etc.). While people can and do connect “with” each other, that connection is often mediated through a mediating object [8]. Without these mediating objects it is difficult for people to form connections [14]. Our imprint on those objects helps us connect “through” the mediation objects “with” other people and learning objects. These social traces help us make sense of the online world, and as we make sense of it our tracks help others. Returning to mentoring, the suggestion here is that an e-mentoring relationship can be built up from a general connection to a mediating object. The ability to connect with and through rich content that Web 2.0 affords can play a role in developing and maintain communities that support online mentoring [16]. We can activate those latent ties. If we then harness some of the motivational cues identified earlier in mentoring, for example, similarity or performance, then we may be able to turn some of those weak ties into “strong ties”. 
 5. PILOT STUDY AND QUALITATIVE ANALYSIS RESULTS.
 The data we draw on in this paper is from a six months pilot study that looks at how, and what kinds of work based learning SL can support. SL is a web2.0 online social learning platform and tool kit developed by the OU. Twelve members of staff occupying a range of positions participated in the pilot. Each participant attended a two-hour initial workshop, which provided an introduction to the pilot study and to SL and offered hands-on experience. Participants’ activities on SL were screen captured using Camtasia software6. During the sessions they were asked to narrate their journey. Thematic analysis has been applied to the screen captures and audio scripts. We used semi-structured interviews to explore the theme analysis results in more details. Nine out of twelve original participants were interviewed. The interviews were transcribed and coded to identify dominant themes. The qualitative thematic analysis was also employed for analyzing the interview script. The qualitative analysis results show that whilst the participants see SL’s potential as: a complement to existing work-based learning tools; a way of supporting flexible workbased learning; a way of building learning networks; a way of bringing resources together; a way of providing training and support for staff and employees based in the regions and nations, they also look for particular kind of mentoring functions to support their online social learning. Our wider reading and observations of e-mentoring systems indicated that while our system could readily support “traditional” mentoring, it also has the potential to support more diffuse relationships that would support a sense of community and “generalized reciprocity”. We found that confident social media users were already doing this. During pilot they filled in their profile and quickly began to establish connections, they then used those connections to locate and make other connections. What also became clear in the interviews (even before the launch of Google+7) is that people wanted to be able to differentiate between different types of connection. This has been found in informal work related online networks [18], and it is our sense that this will be important in the workplace. Our solution is to allow users to connect in different ways, for example follow, and to be able to add tags that specify the type of connection, for example adviser or even mentor. The use of the profile and activities to connect and make sense of SL, along with issues raised in interviews highlighted the importance of users’ profiles and making connections visible. This links with the similarity attraction paradigm in mentoring and also touches on aspects of trust. Trust is important online forums, as we often lack the normal cues that allow us to assess whether to trust another person, or source of information. Research on large online forums has found trust, or the cognitive decisions we make around how credible a source is based on our perception of how honest and reliable the source is, our sense of what the intention is, and competence [3]. Online we use social factors (rating and voting) to assess reliability, the users profile and badges to demonstrate competence. One of the key functions of any social site is the ability to connect with others. We noted earlier the role that instant messages sites like Twitter play in connections and establishing ties. While “following” is essentially a weak tie [4], coupled with other channels of communication these weak ties can become strong ties. While our early build did support connections, the sense of sharing the space with others appeared to be absent. It appears that users need to be able to see others (through their activities), and be able to understand and interpret what others were doing. While this highlights the importance of the profile and activity streams, it also asks us to consider how we “push” content to users. In our framework (Figure 1) we suggest that users connect with learning objects and people, and through those they can in turn connect to other learning objects and people – this is what they “pull” towards themselves. However, it became apparent that users also wanted us to “push” content and people to them. 
 6. BUILDING ONLINE MENTORING THROUGH CONNECTION AND RECOMMENDATION.
 In this section, we explore how connections are made with and through mediating objects and how that informs the content we “push to” (recommend to) users and the way users can “pull” (search) content to themselves within the framework (Figure 1). Following work on the role of learning objects in mediating social interactions online [8], we extended the notion of networks and connections from people to what we call mediating objects. While we recognize that users will connect and interact with learning objects and people in different ways, we consider these ties are important, and just like our connections with other people the strength of our ties will vary. This is important to our understanding how people connect with and through people and learning objects, and thus an important factor in understanding how the people use connections in online social learning. The framework indicates that users connect “with” other people and content (key 1 and 2), as they build up those connections, they leave traces that allow them and other users to connect “through” to more people and content (key 3). Further, the users’ connection behaviors through actions (key 4) will contribute to develop recommendations for the users (key 5). 
 Figure 1: Connection and recommendation framework for building online mentoring relationships in a social learning environment. 
 7. CONCLUSION.
 What we describe here is not “classic mentoring”, and we did not aim to merely illustrate “classic mentoring” online. Instead we took some of the key elements of mentoring and reflected through social learning and Web 2.0. We found that many of the tension and difficulties that arise in online mentoring relate to attempts to see it only in relation to “classic mentoring. Mentoring online is ambiguous and opens to multiple interpretations [11]. Our proposed online mentoring tools do account for “classic mentoring”. However, our main focus is the underlying psycho-social factors and Web 2.0 connections. We have found a great deal of common ground with research on social learning and Web 2.0. For example, the importance of performance criteria and being able to select people based in similarity within mentoring [2], is mirrored by the need to see and know about others online [13] before developing connections and trust [3]. In this model of mentoring (closer to peer mentoring or peer support in workbased learning) the relationships are likely to be more diffuse and feature connections that vary in frequency and intensity. Here we drew on and developed the work of [10] on the strength of weak ties (see also [12]). We know that these weak ties are important in accessing new knowledge and information [10], and that Web 2.0 tools (e.g. following in Twitter/Facebook) are effective at creating networks of weak ties [4]. We explored this in relation to people and learning objects [8], and the role that ties played in developing a sense of place (for example [7] on Twitter), and how that sense of commonality fostered “generalized reciprocity”. Clearly this is only one of the ways that users may develop online mentoring relationships. One to one relationships or the allocation of mentors is far more common in the workplace, and over the next few months we will be developing those types of tools and pilot and evaluate the updated specifications.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Exploring Qualitative Analytics for E-Mentoring Relationships Building in an Online Social Learning Environment</rdfs:label>
		<dc:subject>online social learning</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>mentoring</dc:subject>
		<dc:subject>relationships</dc:subject>
		<dc:subject>learning network and ties</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/haiming-liu"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/haiming-liu"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ronald-macintyre"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ronald-macintyre"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/27/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/haiming-liu"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/ronald-macintyre"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/28">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Goal-oriented visualizations of activity tracking: a case study with engineering students</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/28/authorlist"/>
		<swrc:abstract>Increasing motivation of students and helping them to reﬂect on their learning processes is an important driver for learning analytics research. This paper presents our research on the development of a dashboard that enables self-reﬂection on activities and comparison with peers. We describe evaluation results of four iterations of a design based research methodology that assess the usability, use and usefulness of diﬀerent visualizations. Lessons learned from the diﬀerent evaluations performed during each iteration are described. In addition, these evaluations illustrate that the dashboard is a useful tool for students. However, further research is needed to assess the impact on the learning process.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Increasing student motivation and assisting students with self-reﬂection on their learning processes is an important driver for learning analytics research. Student motivation can improve when students can deﬁne their own goals [30]. Visualizations of time spent and resource use can improve awareness and self-reﬂection [15]. Learning management systems (LMS) track most of the user interaction that can be used for learning analytics. However, many of the activities take place outside of the LMS, such as brainstorming or programming activities. This paper presents the ﬁrst results of a case study in a “problem solving and design” course for second year engineering students at the Katholieke Universiteit Leuven. In this course, the students have to develop software and go through the diﬀerent phases of software development process, such as design, programming and reporting. To this end, they use tools such as LibreOﬃce1 , the Eclipse IDE2 and Mozilla Firefox3 . They have to share tasks and responsibilities between group members. Controlling the risks and evolution of such tasks is part of the assignment. We developed a dashboard with visualizations of activity data. The overall goal of this dashboard is to enable students to reﬂect on their own activity and compare it with their peers. The time spent with diﬀerent tools, websites and Eclipse IDE documents are tracked by RescueTime4 and the Rabbit Eclipse plug-in5 . The collected information is displayed in a dashboard containing goal-oriented visualizations. In the visualizations, the students can ﬁlter by diﬀerent criteria, such as course goals and dates. Such ﬁlters allow contextualization of the visualized data for the user. Linking the visualizations with the learning goals can help students and teachers to assess whether the goal has been achieved [12]. The dashboard is developed using the design-based research methodology, which relies on rapid prototyping, deployment of artifacts and observation in iterative cycles. The paper is organized as follows: in the next section, we present related work. Section 3 presents the research methodology. The four iterations of the design process are discussed in sections 4, 5, 6 and 7. Future work and conclusions are presented in Section 8 and 9. 
 2. RELATED WORK.
 Learning analytics considers the analysis of communication logs [33, 6], learning resources [25], learning management system logs and existing learning designs [21, 32], and the activity outside of the learning management systems [29, 9]. The result of this analysis can be used to improve the creation of predictive models [37, 13], recommendations [42] and reﬂection [15]. This paper focuses on activity outside learning management systems using existing tracking tools. Self-tracking tools can be used for capturing activities of students with diﬀerent tools. The goal is to help students learn how they and other students are using their applications to achieve concrete goals. Self-tracking is becoming popular in many domains, including Personal Informatics [20]. Applications in these domains help people understand their habits and behavior, through tracking and visualization, e.g. for sleep tracking and email communication patterns. Tracking of health data can motivate users with ﬁtness exercises [31, 4] and enable early diagnosis of illness [39, 2, 1, 16]. Within companies, tracking and visualizations are used to analyze business and manufacturing processes [35], as well as productivity [3]. Behind these tools are communities where users can share experiences, publish their tracking data in social networks or compare the data with others. In a learning context, students and teachers are part of a community. These tools can play an important role to share and learn from their behavior with applications to achieve the goals of the course. Khan Academy enables tutors to check progress of students [8]. A dashboard is used where a table provides a goal status overview per student. For every student, a timeline shows the distribution of achieved goals and a bar chart visualizes the time spent with diﬀerent kinds of resources. Other learning dashboards use pie charts to describe the access distribution of diﬀerent user roles, simple plots to express time spent and tables to indicate the success rate for assignments [23]. In adaptive learning environments, dashboards contain box plots to compare grades and averages of users who have followed diﬀerent paths [5]. In mashup learning environments, pie charts have been used to represent knowledge in diﬀerent areas [24]. Tree visualizations are useful to express learning paths and to describe prerequisites. Each path can represent a knowledge area or subarea in a domain [26, 27]. In addition, there are models exploring ways to analyze electronic traces to create group models that can operate as mirrors which enable the individuals and teams to reﬂect on their progress through visualizations [41, 18]. The work presented in this paper focuses on tracking activity from diﬀerent applications. Our dashboard uses different trackers that generate diﬀerent kinds of data and applies diﬀerent visualization techniques. The overall goal of this mashup of visualizations is to enable students to learn how they are using the tools and how much progress they make towards goals in comparison with peers. 
 3.RESEARCH METHODOLOGY.
 The design-based research methodology has been applied to conduct this research. This methodology relies on rapid prototyping to evaluate ideas in frequent short iteration cycles [43]. The approach enables to collect both qualitative and quantitative evaluation data during the whole software design process [28]. In the two ﬁrst iterations, we developed a paper-based and a digital prototype. The evaluation of those iterations collected qualitative data from interviews and user observations of 15-30 minutes using the think-aloud protocol [19]. Six teachers and teaching assistants participated in the ﬁrst iteration and 5 in the second iteration. Evaluations with these participants are useful to collect requirements and to identify potential usability issues with the interaction techniques. 
 The third and fourth iteration are conducted by a mixed research evaluation methodology with questionnaires and open ended questions. In these iterations, we conducted the evaluations with 36 and 10 students, respectively. These questionnaires focused on concrete aspects of the application and allowed statistical analysis of the evaluation data. 
 4. PAPER PROTOTYPE.
 Paper prototyping is an important ﬁrst step in user interface design to get quick feedback [28] and minimize costs in the software design process [11]. 
 4.1 Design and implementation.
 User activities and their visualization need to be related to learning goals in order for teachers or students to be able to reﬂect and make decisions. Linking these visualizations to the intended goals allows to assess whether these goals have been achieved [12]. The design of the paper prototype focuses on the abovementioned use cases: (a) Students can reﬂect on diﬀerent visualizations contextualized by the learning goals of the course and (b) enable social support through communication between students and teachers. The ﬁrst use case is addressed by using visualizations of user activities. More speciﬁcally, we visualize the behavior of students with diﬀerent tools they are using for course activities (e.g. Eclipse IDE for programming and Microsoft Word for writing) to gain insight into what students have done to achieve a goal. To this end, the students and status of the goals are visualized as a table (visualization 1 Figure 1), as such visualization is one of the simplest ways to get an overview of the course [36]. Students are listed in the ﬁrst column and the rest of columns represent the goals of the course and their status. A timeline [17] with bubbles (visualization 3) represents the number goals over time. Visualization 5 is a timeline that shows the number of events or time (depending of the tracked source of information) per tool that the user has used along the time to achieve the goal. Finally, visualization 5, 7 and 8 display time spent or number of events per weekday, actions and diﬀerent items. All visualizations together in the dashboard enable ﬁltering information from a generic perspective (table with goals and students) to a more detailed description (type of documents and activity used), following the visual information seeking mantra [38]. This prototype enables users to interact with the visualizations, i.e. if a user clicks on the Monday bar (visualization 6), then visualization 7 and 8 displays related information to the clicked day. The second use case is addressed by providing chat functionalities for communication between teachers and students (number 2 and 4 in Figure 1). To enable social support, communication between users and sharing experiences can help users to achieve their goals. We provide two widgets intended for this purpose. One widget shows publicly the message and the other is reserved for private communication. The communication is always related to a speciﬁc goal contextualizing the scope of the conversation. 
 4.2 Evaluation.
 4.2.1 Demographics and Evaluation Setup.
 Users were interviewed and observed during 15-30 minutes. They had to perform diﬀerent predeﬁned tasks such as ﬁltering goals of this week. The think-aloud protocol was applied. 
 Figure 1: Paper prototype. 
 The paper prototype was evaluated with six people (1 female and 5 males computer science teachers and assistants). Three participants were between 25-30 years old and two participants between 40-50. 
 4.2.2 Evaluation results and discussion.
 In this subsection, we introduce ﬁrst the more remarkable problems and suggestions of the users, and ﬁnally the proposed solutions. Three issues were highlighted with visualization 1. First, the headers in the table are the titles of the goals. The size of the table increases proportionally to the number of goals. If the number of goals is high, the user will not be able to get easily an overview due to the size of the table. Although the user can ﬁlter the goals on the table restricting the period of time, it requires additional steps for the user and aﬀects the usability of the application. Second, the ﬁltering feature is deﬁned by drop-down lists for the day, month and year and requires too much clicks. Third, the table is showing a pop-up with static information when the mouse hovers over a cell. Pop-ups showing always the same information were identiﬁed as redundant by the users. In addition, users requested more sorting options for the table. There were several problems to understand visualization 3. The visualization shows redundant information compared with the table. The table also includes goals and users can ﬁlter by time, so they can obtain the same information with this visualization. Users expected some additional information that they did ﬁnd in the table. Although the problem of this visualization is the redundant information, from previous evaluations [34] we also know that visualizations can be diﬃcult to understand depending on the user background. Users proposed to replace chat functionality with activity streams such as Facebook or Twitter. In addition, there were disagreements about merging communication and visualization in the same use case. In general, there is a lack of information about what the visualizations are showing. We propose several solutions to address these problems. The headers of the table will be replaced by goal identiﬁers (used approach in Khan academy to represent goals). Additional information such as goal title, description and ﬁlters can be displayed in a diﬀerent place. The drop-down lists can be replaced by a calendar feature that is more intuitive. The pop-ups can be replaced by legends. Regarding visualization 3, we propose to replace it by a motion chart visualization. Such a visualization allows to show the evolution of the user activity and to compare it with the average of a group over time. In this way, we provide additional information as requested by the users. Personalization of dashboards can be a solution for diﬀerent user backgrounds, as users can choose the visualization that they want to see. However, we have to keep in mind that personalization is an additional option. Users need to have a starting point to work with the application at the beginning. We can not oﬀer the users a white screen and rely on the user for the whole conﬁguration. Chat functionality is discarded because the focus of this research is visualization of learning analytics. Finally, we centralize the ﬁlter information in one place to ﬁx the lack of information. Similar to chart legends do with charts, we try to provide a place that helps to understand the visualizations. In addition, we include extra information in every visualization to explain what it shows. 
 5. DIGITAL PROTOTYPE.
 This iteration focuses on addressing usability issues detected in the previous iteration. 
 Figure 2: Distribution of technologies. 
 5.1 Design and implementation.
 Dashboard personalization is provided by using widget technology. Such technology enables easy addition and deletion of widgets with diﬀerent visualizations (see ﬁgure 2 to see the diﬀerent technologies). We use the OpenSocial speciﬁcation6 to enable deployment in OpenSocial compliant widget containers, e.g. iGoogle7 or Apache Shindig8 . OpenSocial enables inter-widget communication via OpenApp [14] that allows to send information from one widget to another. User interactions with one widget are broadcasted to other widgets. These widgets can then also act upon these events, i.e. to ﬁlter data. Furthermore, iGoogle supports the concept of spaces. These spaces can be used to support diﬀerent organizations of widgets. Regarding visualization libraries, we chose the Google Chart library9 , as it provides a convenient event system and it has a large support community. New visualizations are continuously being added. In this iteration, we deploy seven widgets based on the visualizations from the previous iteration in iGoogle (see visualizations 1,3,5,6,7 and 8 in ﬁgure 1) . Second, we changed the timeline (visualization 3 in Figure1) by a motion chart (widget 2 in Figure 3). In the motion chart, x-axis is the activity of the user and y-axis is the peers average activity. A timeline chart can also be used to represent this data. However, when several goals overlap and are represented over the same time period, the user could be confused with too much lines and colors. A motion chart simpliﬁes the representation. Third, widget 3 in Figure 3 is added to the dashboard and centralizes ﬁlter information. 
 5.2 Evaluation.
 5.2.1 Evaluation data.
 Users were interviewed and observed during 15-30 minutes. They had to perform diﬀerent predeﬁned tasks such as ﬁltering goals of this week following the think-aloud protocol. In this iteration, we use hardcoded dummy data for the goal table and motion chart and data from a previous evaluation [34]. This allows us to emulate more realistic dashboard behavior. 
 5.2.2 Demographics and Evaluation setup.
 The digital prototype was evaluated with ﬁve male computer science teachers and assistants. Four of them are between 25 and 30 years old and one between 40 and 50. All of them know what a widget is. Three of them participated in the previous evaluation. 
 5.2.3 Evaluation results and discussion.
 One remark on this iteration is about our rationale (see Subsection 4.2.2) to create widget 3. The idea is to centralize all information in one place. However, the user perception is that every widget is independent from others, so they do not want to look up this information in another widget. In addition, depending on the screen resolution they have to scroll up to see the widget information and scroll down for the widget with the visualization. This dependency aﬀects to the usability of the application. In this iteration, the selection of the table visualization receives good feedback due to the sorting functionality. However, regarding the calendar feature, users suggested to have diﬀerent possibilities, such as a slider. In addition, functionalities to organize the goals by weeks and buttons for next and previous weeks were requested. The motion chart was more complex than expected. Users spent quite some time using the diﬀerent conﬁguration options such as color and size. Although users consider it diﬃcult to understand at the beginning, they indicated that the motion chart can provide useful information. However, all users remarked that they would like to see the dashboard in a real use case in order to assess its usefulness. There are also minor remarks such as letter font and data inconsistency, small size of the text boxes and table ﬁltering style. We focus on solving the ﬁrst issue. We propose to eliminate widget 3 and adding titles to the visualizations that can be updated based on ﬁlters. Removing this widgets also provides more space for bigger visualizations. For the next iteration, we eliminate calendar features because we do not need this kind of functionality in the use case study. 
 6. FIRST WORKING RELEASE.
 This iteration focuses on the real deployment of the dashboard. We selected an existing tracking system and adapted the existing widgets for this new scenario. 
 6.1 Design and implementation.
 We considered two tracking systems: RescueTime and Wakoopa10 . They categorize tools and websites based on a functionality taxonomy such as Development, Browsers and Design. We selected RescueTime because it oﬀers better security to access user data. As the next iteration with students involves real student data, such security and privacy considerations are very important. We use the Rabbit Eclipse plug-in to track IDE Eclipse interaction. Students are developing software in Java and the Rabbit Eclipse plug-in allows tracking who is working on which part of the project. The plug-in is open source and also tracks the time spent on documents (see ﬁgure 4). The tracked information is collected via web services and exposed to the widgets via JSON. The dataset describes the time spent per application, document and website. This information is displayed in 8 diﬀerent widgets as described in previous iterations. In this iteration, we modify some widgets, because the RescueTime taxonomy enables us to categorize the tools by intended activity. This information can be useful for the students. Widget 1 and 2 in Figure 5 are the same as described in Subection 5.1. Widget 3 shows the time per day spent by activity based on the taxonomy classiﬁcation of RescueTime. The information is visualized using an annotated time line. Widget 4 is a bar chart that compares the global time spent per activity compared with the average time. Widget 5 shows the time spent per application. Widget 6 compares the time spent per application with other members of the group. Widget 7 shows the time spent on Eclipse projects ﬁles and, ﬁnally, widget 8 shows the time spent on websites compared with the average of the group. The widgets use inter-widget communication for dataset ﬁltering. Table 1 presents the connection details. This table explains which information is sent by every widget, and which widgets listen to events to ﬁlter their visualizations. For instance, when users click on a user in widget 1, this widget broadcasts the identiﬁer of the user. Other widgets listen to this event and can show the information related to the user identiﬁer. 
 Figure 3: Digital prototype. 
 Figure 4: Source data aggregation. 
 Table 1: Overview of Event. 
 Figure 5: First release implementation. 
 6.2 Evaluation.
 6.2.1 Evaluation data.
 In this iteration, we evaluated the dashboard with students. The data is tracked with RescueTime and the Rabbit Eclipse plug-in. As this evaluation took place at the start of the course, we did not have data from students, so two users (a developer and a project manger of our team) oﬀered their RescueTime and Eclipse data for the experiment. The approach might inﬂuence the perceived usefulness, because students can not relate to their real data yet. However, the evaluation enabled us to obtain ﬁrst feedback from students before the data collection started. 
 6.2.2 Demographics and Evaluation setup.
 This experiment ran with 36 students between 18 and 20 years old (30 males and 6 females) in an engineering bachelor course. We presented the dashboard the ﬁrst day of the course. The privacy constraints and the data tracking characteristics of the experiment were explained. Students were also informed that they can stop RescueTime when they think it can aﬀect their privacy. A questionnaire was used to collect quantitative data regarding ﬁrst perceived usefulness, eﬀectiveness, usability, satisfaction and privacy concerns. The questionnaire also has two open-ended questions about privacy considerations and general positive and negative aspects. We wanted to evaluate whether students consider the dashboard useful and whether speciﬁc changes were needed to deploy the dashboard in this course. In the ﬁrst question of the evaluation, the students get 80 points that they have to divide over the widgets to rank them. This question was intended to get insight into which visualizations are considered more valuable by the students. The next seven questions are extracted from the USE questionnaire [22]. The full questionnaire was not used due to time restrictions. Three questions are related to usefulness and eﬀectiveness and the next four questions to usability and user satisfaction. Finally, the three last questions are related to privacy concerns. Question number 10 inquires whether students would be receptive to include tracking activity out of the lab. 
 6.2.3 Evaluation results and discussion.
 Results of the widget scoring question indicate that there is no clear winner (Figure 6), as we expected. Widget 3 and 4 have slightly higher scores. Both are related to activity type. Widgets 5 and 6 score the lowest. These widgets show the tools instead of activity type and can be found redundant. Widget 8 provides information about what web sites have been visited and also scores high. In the open questions, 12 users ﬁnd it useful to see what websites other students are visiting. Widget 2, the motion chat, scores the lowest. Our perception is that the motion chart is more diﬃcult to understand. In the next iterations, we pay special attention to the learnability of this visualization. The questionnaire results are summarized in ﬁgure 7. The results indicate that students consider the dashboard useful (question 1 and 2) and that they think that the dashboard can help them to achieve goals (question 3). However, usability (question 4 and 5) and satisfaction (question 6 and 7) are scored neutral. As the students could not play yet with the dashboard, the scoring of these two factors was diﬃcult. 
 Figure 6: Widget scores box plot. 
 Figure 7: Questionnaire results. 
 From question 8 (see Figure 7), ‘I like to see what other members do during the course’, we learn that the students like to be aware of what their peers are doing. We can conclude from ‘I feel conﬁdent using the tracking system in the lab during the course’ (question 9), that the lab is a suitable context to track their activity. Question 10 (‘I would feel conﬁdent using the tracking system outside of the lab’) is rated the lowest. This outcome suggests that the students would feel uncomfortable if they were tracked outside the lab. The open questions provide us with useful details. One of the most common remarks is that they like to see how students and their peers behave. 12 students like to compare their activity with others. However, 3 students indicate that they are disappointed that others can see their activity. One student suggests that tracking can cause stress and consequently decrease productivity [40]. Most students mention that the feeling of being observed is a negative aspect. 
 Another student argues that our visualizations can possibly modify their working style because they may want to behave similar to other students. We have to consider all these factors in future evaluations. One student suggested to add support for detecting user distraction. Three users suggested to also enable access to the raw tracked data. These students were interested to see how RescueTime tracks data. Another proposal is to store the tracking information locally and ask for user permission before sending the information. This is an important suggestion to deal with potential privacy concerns. 
 7. SECOND WORKING RELEASE.
 The evaluation in the previous iteration is based on noncourse data and a demo of the application. In this iteration, we focus on ﬁrst results of the dashboard evaluation with real student data in a real course setting. As we describe in Section 8, more evaluations will be performed during the course in the following months. 
 7.1 Design and implementation.
 In this iteration, we analyzed the generated data to see how the students behave during the lab sessions. The dashboard was made available to 36 students. We created anonymous email and RescueTime accounts for each student. Students had to conﬁgure RescueTime and the Rabbit Eclipse plugin with their credentials. 
 7.2 Evaluation.
 7.2.1 Evaluation data.
 In this iteration, we evaluate the dashboard with student data. Students carried out diﬀerent tasks, such as elaboration of scenarios, use cases and an implementation of a small web application during four lab sessions. As these tasks are partially performed without the computer, the tracked data is still limited in this phase. 
 7.2.2 Demographics and Evaluation setup.
 This experiment ran with 10 students between 18 and 20 years old (8 males and 1 females), a subgroup from the previous iteration. A subgroup was used to be able to better assist the students if problems would show up. Students were encouraged to reﬂect on the dashboard visualizations during 10 minutes. They ﬁlled in a SUS questionnaire [10] afterwards. Such a questionnaire allows us to compare our application with more than 200 studies [7]. We added questions to score widgets (as used in the previous iteration), evaluate usefulness and satisfaction, and open ended questions. During the evaluation, we removed widget 7 (see ﬁgure 5) because the only activity in Eclipse was the development during a tutorial, which would not provide useful information. 
 Figure 8: Widget scores box plot.
 7.2.3 Evaluation results and discussion.
 The ﬁnal SUS score is 72 points out of 100. Based on [7], this score rates the dashboard as good regarding usability. The widget scoring question results are summarized in Figure 8. Widgets 4, 6 and 8 are rated highest. We think that this is due to the limited data because we are in the initial period of the course. While bar charts display absolute information and are valuable even with limited data, timelines loose meaning because the user cannot see much evolution over the diﬀerent sessions. The 5-item likert scale, ‘I would feel conﬁdent using it in another course’, inquires about the usefulness and was rated on average 2.9. Users are not used to reﬂection on their own work using these tools. If the reﬂection task is mandatory during the course, they perform the task. However, they seem to prefer avoiding such tasks. The users do not ﬁnd the dashboard beneﬁcial enough to use it regularly. Part of this research is intended to increase the user’s interest for these kind of tools. We asked the students about what they learnt. Three students highlighted the fact that there is not much data because they have not been working with the computers all the time. For instance, the dashboard does not represent the time students spent on the scenarios. Three other students found patterns in their Internet use. For instance, one student pointed out that his peers did not visit the course wiki as often as he did and realized that he was the person in charge to check this information. Five students indicated that visualizations are nice or even fun to use. 
 8. FUTURE WORK.
 The experiment runs during the whole semester and two more evaluations are scheduled. The essence of our future work is to actually evaluate the dashboard with the students as the course evolves and collects more real data. Such more elaborate data is required to assess in more detail the added value of these tools. The current version of the dashboard enables students to compare their progress with peers on tasks that are deﬁned by the teacher. In the next phase, we will add support to enable students to deﬁne their own goals. For instance, they could deﬁne how much time they want to spend every day on concrete tasks. Self-deﬁnition of goals is an important part of Personal Informatics. In addition, the dashboard technology allows easy customization. We can easily develop more visualization widgets that users can set up based on the context and their visualization background. We need to evaluate the inﬂuence of such customization factors in additional experiments. 
 9. CONCLUSION.
 In this paper, we presented the ﬁrst results of a case study with second year engineering students. We conclude that students consider the dashboard useful to learn how they are using the tools. However, users are not motivated to use the dashboard. Visualization enables exploration of large datasets, but diﬀerent visualization backgrounds can inﬂuence on the understanding of the data. Implementing the dashboard as a mash-up of widgets is our proposal to address this issue. The aproach allows us to oﬀer the users diﬀerent visualization conﬁgurations. The dashboard can be useful to support self-reﬂection and progress in comparison with peers. Students are interested to be aware of what their peers are doing. However, privacy concerns are involved in this process. The students are receptive to be tracked during their lab sessions. However, they do not like to be tracked outside a course environment due to privacy concerns. As additional work out of the lab sessions is not required for the course, this does not have implications on the current evaluation setup. However, the issue needs to be researched in order to generalize these kinds of experiments beyond the current course setting. 
 10. ACKNOWLEDGMENTS.
 The research leading to these results has received funding from the European Community Seventh Framework Programme (FP7/2007-2013) under grant agreement no 231396 (ROLE). Katrien Verbert is a Postdoctoral Fellow of the Research Foundation – Flanders (FWO).]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Goal-oriented visualizations of activity tracking: a case study with engineering students</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>visualization</dc:subject>
		<dc:subject>reflection</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-luis-santos"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-luis-santos"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sten-govaerts"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sten-govaerts"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/28/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-luis-santos"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/sten-govaerts"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/29">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Applying Quantiﬁed Self Approaches to Support Reﬂective Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/29/authorlist"/>
		<swrc:abstract>This paper presents a framework for technical support of reﬂective learning, derived from a uniﬁcation of reﬂective learning theory with a conceptual framework of Quantiﬁed Self tools – tools for collecting personally relevant information for gaining self-knowledge. Reﬂective learning means returning to and evaluating past experiences in order to promote continuous learning and improve future experiences. Whilst the reﬂective learning theories do not suﬃciently consider technical support, Quantiﬁed Self (QS) approaches are rather experimental and the many emergent tools are disconnected from the goals and beneﬁts of their use. This paper brings these two strands into one uniﬁed framework that shows how QS approaches can support reﬂective learning processes on the one hand and how reﬂective learning can inform the design of new QS tools for informal learning purposes on the other hand.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Reﬂection is becoming of relevance in the learning community and therefore reﬂective learning is being investigated in both educational and work settings. According to Boud et al. [1], learning by reﬂection (or reﬂective learning) oﬀers the chance of learning by returning to and evaluating past work and personal experiences in order to improve future experiences and promote continuous learning. Several approaches show initiatives to support reﬂective learning through technology in diﬀerent settings [13, 8, 6], but we lack an unifying framework that describes the role of technology in the reﬂective process. On the pragmatic side, a new kind of lifelogging approaches pursued by a community known as Quantiﬁed Self (QS)1 are becoming increasingly popular. QS is a collaboration of users and tool makers who share an interest in “selfknowledge through numbers”, i.e. self-knowledge through self-tracking. This interest results in a variety of tools to collect personally relevant information for self-reﬂection and self-monitoring, with the purpose of gaining knowledge about one’s own behaviors, habits and thoughts. Hence, QS approaches oﬀer a rich source of data for learning analytics that has not been available for learning processes before. This way, whereas QS approaches are pragmatic, having as main driver the experimentation; reﬂective learning is driven by theories that are evolving since the beginning of the 19th century. In an approach to join these two streams, this paper presents a framework that shows how QS approaches can support the process of learning by reﬂection and informs the design of new QS tools for informal learning purposes. The starting point for the design of the framework was the survey of several QS tools, which allowed to analyze the characteristics these tools may have in common. Moreover, the continuous advances in technology can facilitate the data gathering and therefore the quality and features of the tools. Sensor technologies are being improved, mobile technologies and devices are more widespread and Internet provides ubiquitous access to information. In the following, we describe the theoretical and pragmatic background of reﬂective learning and Quantiﬁed Self in Sec. 2, before we present our framework to apply QS approaches to support reﬂective learning (Sec. 3). Finally, we conclude this paper with its discussion in Sec. 4. 
 2. BACKGROUND OF THE FRAMEWORK.
 2.1 Theoretical Background.
 Decades of research in reﬂective learning have highlighted diﬀerent aspects of reﬂective learning, leading to multiple theories [3, 7, 1, 12]. Hence, it is diﬃcult to deﬁne a shared understanding about reﬂection. We were looking for a theory that provides insights into the cognitive processes and can be a basis for the integration of technology into the reﬂection process. We chose the model introduced by Boud et al. [1] because it considers the complete cognitive process, including aﬀective aspects, but does not deﬁne the concrete activities around this process or a speciﬁc domain. In the model by Boud et al., reﬂective learning refers to “those intellectual and aﬀective activities in which individuals engage to explore their experiences in order to lead to new understandings and appreciations” [1]. Therefore, the reﬂective process is based on the experiences of the learner, i.e. ”the total response of a person to a situation, including behavior, ideas and feelings”. The reﬂection process consists of three stages, in which the learner re-evaluates past experiences by attending to its various aspects, and thereby producing outcomes, which can be cognitive, aﬀective or behavioral. The reﬂection process and its context, experiences and outcomes, are depicted in Figure 1. A critical point is the start of the reﬂection process that leads to the initial return to an experience, which is not explicitly deﬁned by [1] because “most events which precipitate reﬂection arise out of normal occurrences of one’s life.” However, the provided examples can be linked to cognitive dissonance theory [5]. 
 Figure 1: The reﬂection process in context [1]. 
 2.2 Pragmatical Background.
 On the pragmatic side, we have a new kind of lifelogging approaches with the recently emerged QS community, that promotes “self-knowledge through numbers”. Their lifelogging experiments and their tools have the intention of gaining knowledge about their own behaviors, habits and thoughts by collecting relevant information related to them. The starting point of the QS initiative are not scientiﬁc theories, but based on empirical self-experimentation. Apart from QS, all these approaches and tools are also known as personal informatics, living by numbers, self-surveillance, self-tracking and personal analytics [9]. Since the QS community was founded, we have seen a wide variety of approaches where people track, e.g., more than 40 diﬀerent categories of information about the own health, the power usage of a thatched cottage or Vitamin D consumption [2]. Besides, plenty of tools are already available, which facilitate the tracking of diﬀerent aspects of our lives. Some of these tools are web-based applications (e.g. Dopplr2 , daytum3 , moodscope4 ) others are devices provided with physiological or environmental sensors (e.g. MIO5 , SenseCam6 , DirectLife7 ) and yet others consist of mobile applications (e.g. Sleep Cycle8 , oneLog9 , My Tracks10 ). 
 Figure 2: Role of the three QS potentials in the process of reﬂective learning. c FZI. 
 3. A FRAMEWORK TO APPLY QS APPROACHES TO SUPPORT REFLECTIVE LEARNING 
 In the previous section, reﬂective learning and QS were introduced and deﬁned for the purpose of this paper. We now present a framework that combines these research strands into a model for the technical support for reﬂective learning; centered around the model of Boud et al [1]. In our framework, three main support dimensions are identiﬁed, namely: tracking cues, triggering and, recalling and revisiting experiences (see Fig. 2): (a) Tracking cues: capturing and keeping track of certain data as basis for the whole reﬂective learning process. (b) Triggering: fostering the initiation of reﬂective processes in the learner, based on the gathered data and the analysis performed on it. (c) Recalling and revisiting experiences: supporting learners in recalling and revisiting through the enrichment and presentation of data in order to make sense of past experiences. 
 Figure 2 shows these three dimensions in relation to the reﬂective learning model of Boud et al. presented in the previous section. Firstly, tracking cues is directly related to tracking of behavior, ideas and feelings, which are the source of the reﬂective process on the one hand, and on the other hand related to the measurement of outcomes (e.g. new perspectives or change in behavior), which are continuously integrated with the original cues in order to feed future iterative reﬂection processes. Secondly, triggering is related to the start of the reﬂective process. Finally, the recalling and revisiting experiences enrich the process of returning to and evaluating experiences, as well as that of attending to feelings. In the following we further diﬀerentiate the support dimensions based on how these can be instantiated by QS tools. 
 3.1 Tracking Cues.
 Tracking means the observation of a person and his/her context in order to aid the reﬂective process. Tracking strives to quantify (aspects of) a person’s life in order to enable some objectivity in understanding it. Tracking facilitates reﬂective learning by collecting data on experiences and outcomes that can then be used as objective basis in reﬂection and triggering. We further characterize tracking by the means that are used, the object that is tracked, and the goal that is being strived for. 
 3.1.1 Tracking means.
 Two main ways for tracking exist: self reporting through often specialized software and hardware sensors that directly track behavior. Software Sensors: Software sensors are applications that aid the user in capturing experiences and may be desktopbased, web-based or mobile-based. Software sensors are particularly important for experiences that cannot (currently) be directly measured (such as feelings, ideas) and are often much simpler, more ﬂexible and cheaper to realize than hardware sensors. Software sensors are currently used in a broad variety of QS applications. Hardware Sensors: Hardware sensors are devices that automatically capture data that can be used to deduce experiences or collect contextual information. Common categories of sensors are: environmental sensors (e.g., light sensors, thermometers or microphone) and physiological sensors (accelerometers, heart rate sensors, sphygmomanometers, etc. ). 
 3.1.2 Tracked Aspects.
 Of crucial importance to QS applications is the selection of data about experiences and outcomes that is being tracked; what is tracked is likely to have a large eﬀect on user acceptance and eﬃciency for reﬂective learning. The tracked aspects found in QS applications can be classiﬁed in the following way: Emotional Aspects: Emotional aspects such as mood, stress, interest, anxiety, etc. Private and Work Data: Data from work processes and our lives such as photographies, the browser’s history, digital documents, music, or use of a particular software etc. Physiological Data: These are physical indicators and biological signals that describe a person’s state of health. The main approaches comprise the measurement of physical activity (for applications focusing on sport) and factors indicating health and sickness (e.g. glucose level). General Activity: Data about a users’ general activity such as the number of cigarettes, cups of coﬀee or hours spent in a certain activity. 
 3.1.3 Purposes.
 Another important classiﬁcation dimension is that of the purpose of a QS application; the goal which the user tries to achieve by using this application. This purpose drives and guides which measures are tracked and which means are appropriate. 
 3.2 Triggering.
 Within the reﬂective learning process, triggers are responsible for starting the actual reﬂection process. The role of triggers is to raise awareness and detect discrepancy. We diﬀerentiate between active and passive triggering. 
 3.2.1 Active Triggering.
 Active triggering consists of the tool sending a notiﬁcation or catching the attention of the user explicitly. In order to support active triggering, an application must perform data analysis to detect experiences that are suitable for initiating reﬂection. Such a situation may be a mismatch between a user’s goals and current level, comparison to a global threshold or other persons or a deviation from personal patterns. 
 3.2.2 Passive Triggering.
 A system supporting only passive triggering does not identify experiences suitable for fostering reﬂection or it would not actively contact the user. This kind of system only displays the collected data in a suitable way. It relies on the user to be triggered by somethings outside of the system or on the user regularly visiting the site and then detecting something that starts a reﬂection process. 
 3.3 Recalling and Revisiting Experiences.
 Diﬀerent aspects aﬀect the recalling and revisiting of past experiences, when analyzing the beneﬁts that QS approaches could oﬀer. Enrichment and presentation of the data may facilitate the revisiting of the data to analyze past experiences and reﬂect about them, and therefore enhance the learning process of the user. So support of QS applications can exist along multiple dimensions: Contextualization, Data Fusion, Data Analysis, and Visualization. 
 3.3.1 Contextualizing.
 The data being tracked can be enriched with other context data. This contextualization of the data with other sources of information may be performed by the same tool or result from the interaction between tools (e.g. two mobile applications or a sensor with a desktop application). Adapting the context deﬁnition from [4] we deﬁne context within this framework as: Context is any information that can be used to characterize the situation of a tracked entity and that can aid the reﬂection process. Social Context: Data can be augmented with information about the social context of the user. This can be a comparison to Facebook friends or a comparison to all users. This helps to compare own performance/measures with the others and provides additional data to others in expectation to retrieve more data in exchange and ultimately see one’s own experiences in relation to other’s. An aggregation of data over multiple users may provide new perspectives on experiences and oﬀer new abstraction levels. Such an aggregation can be useful for individual reﬂection but also at a collaborative level, e.g. reviewing team performance over one month [10]. Spacial Context: The location in terms of city, street or even the room. As context this data can aid reﬂection by helping the user to understand the relation between place and her behavior - such as understanding the eﬀects of high altitude on his or her heart rate or the calming eﬀect of visits to speciﬁc places. Historical Context: Comparing current values to historic ones allows to see upward or downward trends or to identify deviations from a historic norm that may indicate a problem. It may also help to identify the diﬀerence between periodic ﬂuctuations (such as variations in weight or ﬁtness according to the seasons) and other deviations from the norm that may indicate progress or a problem. Item Metadata: Any metadata available about the things a user interacts with – such as the information that a particular website is not work related but rather distracting, or that a food contains a large amount of sugar. Context From Other Datasets: There are also numerous datasets (e.g. weather or work schedule) that might can also be used in contextualizing. 
 3.3.2 Data Fusion: Objective, Self, Peer and Group Assessment.
 One important aid to the reﬂection process can be the fusion and comparison of objective (i.e. measured by sensors), self (i.e. self reported data from the user), peer and group assessment (reported data from others about a user). There may be diﬀerences and discrepancies between these views that can foster reﬂection, can help to bridge the gap from subjective to objective experiences and in this way yield new insights and lead to learning. This relates to stage two of the reﬂection process – attending to feelings. Negative impressions can be discharged by comparing the individual perspective to objective measurements. Aggregation of subjective articulations over time or over diﬀerent users can result in a more objective view (see also [11]). 
 3.3.3 Data Analysis: Aggregation, Averages, etc. 
 Diﬀerent forms of data processing help to present the user useful measurements (e.g. number of cups of tea per day/week, average mood of my colleagues, etc.). In [10] we suggested formal, graphic and mathematic aggregation, depending on the data and purpose of the aggregation. For instance, aggregation in tag clouds as example of formal aggregation may need large amounts of data to become valuable but can be applied to semi or unstructured data like texts. Further, it might be desirable to hide the source of the underlying data through aggregation and in this way to create anonymity and privacy. 
 3.3.4 Visualization.
 Attractive and intuitive presentation and visualization forms for the users should be chosen which, at the same time, foster the analysis of the data for reﬂective learning purposes and being otherwise one of the major barriers (see [9]). 
 4. DISCUSSION AND CONCLUSION.
 This paper presented a framework for the application of QS applications to support reﬂective learning. In addition to ordering this strand of research, this framework is geared towards being used to understand the design space this kind of applications as well as understanding which parts haven’t been addressed by research. In the following we want to introduce some of the issues that were identiﬁed when reviewing existing research into QS applications within this framework. Assuming that QS tools can be shown to help people achieve their desired outcomes, there is also a luck of understanding on how to identify the situations where they are likely to work, which are the right measures to track and ﬁnally how to spread the user beyond the current relatively narrow user base. Currently there is also relatively little work on contextualizing the data to improve the reﬂective process. Diﬀerent QS applications are islands where data from one application and sensor cannot be used to understand that of another people. The use of external data sets (such as historic weather data in ﬁtness applications) is even less common. Overall the proposed combination of reﬂective learning and QS applications in the proposed framework concretizes the vision of learning analytics for a particular model of learning and class of support tools. In doing so it allows to identify promising venues for future research. It also shows the way how the notion of learning analytics can be applied beyond classroom settings in daily life to support all kinds of learning and self improvement. 
 Acknowledgements.
 Work presented in this paper was partly conducted within the project “MIRROR - Reﬂective learning at work” funded under the FP7 of the European Commission (project number 257617).]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Applying Quantiﬁed Self Approaches to Support Reﬂective Learning</rdfs:label>
		<dc:subject>reflective learning</dc:subject>
		<dc:subject>quantified self</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>framework</dc:subject>
		<dc:subject>mobile applications</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/veronica-rivera-pelayo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/veronica-rivera-pelayo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/valentin-zacharias"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/valentin-zacharias"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/lars-muller"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/lars-muller"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/simone-braun"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/simone-braun"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/29/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/veronica-rivera-pelayo"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/valentin-zacharias"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/lars-muller"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/simone-braun"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/30">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Multi-mediated Community Structure in a Socio-Technical Network</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/30/authorlist"/>
		<swrc:abstract>Digital environments for networked learning and professional networks may not comprise one “community:” identification of clusters of affiliated groups of participants that potentially constitute embedded communities is an empirical matter, and one of interest to managers of large learning and professional networks. Also, these socio-technical networks are typically multi-mediated, in that they offer multiple means of participation, each with their own interactional affordances. Different communities may be using the multiple media in different ways. We have developed an analytic framework for extracting events from log files and representing interaction and affiliations at different granularities as needed for analysis. In this paper we show how bimodal networks of actors and media artifacts can be constructed in which directed arcs relate actors to the artifacts they read, write or edit, and how the resulting graphs can be used to detect community structures that extend across different media. We illustrate these ideas with a study that characterizes community structure within the Tapped In network of educational professionals, and how the associations between members of this network are distributed across media (chat rooms, discussion forums and file sharing).</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Learning in university settings, professional communities, and virtual organizations [3, 5, 12] is increasingly technologically embedded, with the rapid adoption of information and communication technologies in support of “online,” “distributed,” and “networked” learning and knowledge creation activities [2, 10, 37], and their blending with face-to-face venues [15]. A related trend is towards open learning communities. In corporate or other work settings, professional learning communities may cross team contexts rather than being isolated in work teams [38]. The sharing of resources in these networks benefits both the individual users within these networks and their collectives [18, 37], and the network and socio-technical infrastructures in of themselves constitute a form of socio-technical capital [16, 28]. We will refer to these various technologically embedded social networks as socio-technical networks [21]. A fundamental question in all of these settings is how learning and other enhancements of knowledge, skill and capital take place through the interplay between individual and collective agency. Such a question demands analyses that connect learning activity in specific times and places with the larger socio-technical network contexts in which they take place. A related analytic challenge is that the granularity at which events are recorded may not match analytic needs. Addressing these analytic challenges by connecting levels of analysis is one objective of our developing analytic approach [34]. Many digital environments for networked learning and professional networks are multi-mediated, in that they offer multiple means of participation, each with their own interactional and social affordances (e.g., asynchronous discussion forums, quasi-synchronous chats, and file sharing). Thus, there are different mediational means through which members may be affiliated. Licoppe and Smoreda [25] found that the choice of technologies by which people share personal news or keep in touch with each other both reflects and reaffirms the nature of the relationship between the interlocutors. The present line of research applies this idea at a collective rather than dyadic level: communities are embedded within and make use of technological media for interaction in ways that reflect and reaffirm their community nature. We address the question of community identification as an empirical matter, discovering clusters of affiliated groups of participants in socio-technical networks rather than assuming that the network constitutes one community or that that prior or external communities are replicated within the sociotechnical system. We then examine how the discovered clusters correspond to participants’ organizational affiliations and projects. The fact that learning and knowledge creation activities in these networked environments are often distributed across multiple media and sites leads to a second analytic challenge. The networked learning environments we study offer mixtures of threaded discussion, synchronous chats, wikis, whiteboards, profiles, and resource sharing. Events in these media may be logged in different formats and databases, disassociating actions that for participants were part of a single unified activity. This disassociation is exacerbated when activity is distributed across multiple virtual sites or spread over time, and by the need to work at higher levels of description alluded to earlier in the first analytic challenge. To address both of these analytic challenges, we developed an analytic hierarchy that support bridging between local analysis of sequential activity and global analysis of mediated affiliations and ties, and associated representations that abstracts from media-specific transcript representations [34]. This hierarchy includes an intermediate representation, the associogram, that supports structural network analysis while preserving information about mediation and direction of interaction. The concerns discussed above are particularly salient in the professional learning network we are studying, SRI's Tapped In professional network of educators [14, 30]. The network has consisted of both organizational “tenants” and individuals who come for their own enrichment. There are multiple forms of participation and mediational means by which participant associate with each other. This paper reports an analysis of community structure within the Tapped In network, and how this structure is distributed across media (chat rooms, discussion forums and file sharing). The paper also illustrates an application of our analytic framework. Affiliation networks of actors and media artifacts were constructed in which directed arcs relate actors to the artifacts they read, write or edit. Analysis of these networks exposes community clusters within the network and how they are distributed differently across media types. The remainder of this paper begins with a description of Tapped In. We then summarize the analytic approach mentioned above. After describing how the Tapped In data was prepared for analysis in the associogram representation, we provide empirical results. We characterize the overall network studied, and then focus on the top six largest sub-networks detected by a “community detection” (modularity partitioning) algorithm. These sub-networks are described in terms of what we know about the most active participants in each network, showing that sub-networks derived on a purely structural basis correspond to clusters of institutional affiliations and purposeful collective activity in Tapped In. The paper ends with discussion of what this tells us about Tapped In, and what this tells us about analyzing distributed activity in socio-technical networks. 
 2. TAPPED IN.
 The study examines participant interaction in SRI International’s Tapped In (tappedin.org), an international (albeit mostly US) network of educators engaged in diverse forms of informal and formal professional development and peer support [14, 30]. Cumulatively, Tapped In has hosted the content and activities of more than 150,000 education professionals (over 20,000 per year in our study period) in thousands of user-created spaces that contain threaded discussions, shared files and URLs, text chats, an event calendar, and other tools to support collaborative work. More than 50 “tenant” organizations, including education agencies and institutions of higher education, have used Tapped In to meet the needs of students and faculty with online courses, workshops, seminars, mentoring programs, and other collaborative activities. Also, approximately 40-60 communitywide activities per month were explicitly designed by Tapped In volunteer members to help connect members. (Volunteers drive the majority of Tapped In activity.) Extensive data collection capabilities underlying the system captured the activity of all members and groups. SRI colleagues provided eight years of anonymized data to us. Out of this data, we selected a period of peak usage that occurred from September 2005 through May 2007 for analysis in this study. Because Tapped In is populated with members of multiple tenant organizations as well as unaffiliated members, it is best seen as a network of education professionals rather than a single “community.” Members may move freely between most forms of participation. The question of what communities (or clusters) exist in this network is a matter for empirical investigation. We approach this question in terms of the artifact-mediated associations found between members. 
 3. ANALYTIC APPROACH.
 Our prior research is methodologically eclectic, taking insights from multiple traditions. At a fine granularity, rich descriptions and the unpacking of the sequential structure of situated activity leads to insights into the experiences of participants and the methods by which they accomplish their objectives. Yet microanalytic approaches do not capture emergent social structures that are constructed by yet influence local activity. At the other extreme, social network data and analytic techniques can uncover social regularities, but binary “ties” obscure the situated interactions that constitute and sustain these ties. Both levels of analysis are needed, and we also see the need for bridges between the two levels. There is a dialectic in socio-technical networks in which the local and the global influence and indeed constitute each other. These influences are mediated by cultural artifacts as well as by human actors [20]. Latour’s [23] version of ActorNetwork Theory inspires us to trace out the artifact-mediated pathways of influence between actors, acknowledging that nonhuman entities or “actants” are influential in the network as well as human actors. We use the term “association” for these mediated pathways, preferring it to “relationship” or “tie” because the latter have socio-emotive connotations, while we include in our scope of consideration the diversity of other ways in which people interact with or influence each other [10, 18], or even potentially may do so in the future [31]. Motivated in this manner, we have developed an abstract transcript representation called the Entity-Event-Contingency (EEC) graph that provides a unified analytic artifact [33]; and an analytic hierarchy derived from the EEC that supports multiple levels of analysis [34]. To construct the EEC, log files are abstracted to collections of events (actors taking actions on media objects), supported by a domain model describing the participating entities (actors, actions and digital objects or “artifacts”). Other information is also recorded, such as time and virtual location. To support sequential analysis of interaction, directed graphs that record observed relationships between events called contingencies are constructed [33]. The present study does not use contingency graphs, but rather abstracts further to associograms, two-mode directed graphs that record how associations between actors are mediated by their creation and modification of and access to digital artifacts. 
 3.1 Associograms.
 Associograms are like affiliation networks, but they relate actors to mediating artifacts and can be directed: arcs are directed from actors to the artifacts they read, and from artifacts to the actors who wrote or edited the artifact (Figure 1). The arcs represent a form of dependency: they may be reversed to indicate direction of information flow. Existing social network analytic techniques for affiliation network analysis may be applied to associograms, or transitive closure can be used to convert associograms to sociograms to which other existing techniques can apply [36]. The results of network analysis can then be interpreted by reference to the other levels of analysis. Thus associograms bridge between interaction data and network analysis. Associograms offer a different viewpoint on a network than sociograms. An associogram tie links an actor to an artifact; therefore, an artifact always mediates an association between two actors. Another way to visualize this is to expand a sociogram tie, identify the means by two actors have interacted, and then explicitly display the mediating factors. Because digital artifacts afford different types of interactions, associations between actors need not be the result of “direct” exchange or transmission of information. While the tie in a sociogram might represent a relationship (e.g. family member) or metric that defines the relationship between the actors (e.g. frequency of contact), an association defines the relationship in terms of the distribution of affiliations across a set of mediating artifacts. This enables study of how users connect with others within a socio-technical system, and whether the different media types affect the relationships and structures that are formed. 
 3.2 Data Preparation.
 We now transition to discussion of aspects of method specific to our analysis of Tapped In. Preparation of the data required many months of work, and was greatly facilitated by SRI colleagues. We parsed and filtered logs of user activity involving three different types of digital artifacts: files, asynchronous threaded discussion forums, and quasi-synchronous chat rooms. The relevant interactions consist of users accessing (reading and downloading) or contributing (posting and uploading) to one of these three artifact types. Private chats were excluded from our analysis, as we are focusing on observable public behavior. All activity in the K-12 (student) campus was excluded, as our research (and human subjects permission) focuses on the professional community. Guest accounts were also filtered, as different rotating individuals use these. File access was filtered to include only intentional access such as clicking on a link to a file such as a Word document, Powerpoint presentation, etc. (excluding downloads incidental to entering a room). Data from several different EEC and Tapped In database tables, including old system backups that provided missing data, were consolidated to identify all interactions for each artifact type. The resulting associograms were exported to VNA format so various network analysis software tools could load them. Attributes for the different entities were included with vertices to help with later analyses, and the arcs (directed edges) were weighted according to the number of times the given actor-artifact affiliation was observed, as discussed below. We constructed an associogram in which vertices represented actors, files, discussions, and chat rooms. A file vertex represents a single file, a discussion vertex represents an entire threaded discussion, and a chat vertex represents a chat room. 
 Figure 1. Constructing an associogram from events. For example, the event of P2 writing message m1 is represented as a dependency of m1 on P2, and the event of P1 reading m1 is represented as a dependency of P1 on m1. The cycle between P1 and P2 indicates an interactional relationship, while P3 is a consumer of artifacts created by P1 and P2. 
 The direction of the arcs (directed edges) is a form of state dependency: the source of an arc is dependent on the target of the arc in that information or content has potentially moved from the latter to the former. A file vertex points to an actor if the actor created (uploaded) that file; an actor vertex points to a file vertex if the actor has downloaded the file. A discussion vertex points to an actor vertex if the actor has posted a message in the corresponding discussion forum; an actor vertex points to a discussion vertex if the actor has accessed messages in the discussion forum (having loaded the discussion page). A chat room points to an actor if the actor posted a chat contribution in that chat room while someone else was present; an actor vertex points to a chat vertex if the actor was present in the room when another actor posted a chat contribution. Finally, each of these arcs is weighted according to the number of time the events just described were seen. For example, if an actor's arc to a chat room has a weight of 375 then the actor was present for 375 contributions made by other actors. 
 3.3 Data Analysis.
 We used the Gephi [6] software package to analyze the associogram for our data. Gephi is an open-source tool that provides a suite of network metrics and interactive visualizations. At this writing, Gephi is still in the beta stage of development (version 0.8), and it is not as mature as Pajek or UCINet in terms of available sociometrics. However, unlike these older software tools (which use matrix representations of graphs), Gephi uses an adjacency list representation of graphs, so is able to handle very large graphs. Gephi provides several of the more recent layout algorithms for large graphs and a recent community detection algorithm with good properties (discussed below). Once our graph was partitioned into candidate communities, we used Gephi to visualize the partitioning in various ways. We also used Gephi's “Data Laboratory” to inspect members of the partitions. The suite of metrics available in Gephi was run to generate basic descriptive statistics. We also exported spreadsheets of various partitions to compute sums of unweighted and weighted in-degree and out-degree in order to interpret the distribution of communities across artifact types. When needed, we accessed our original database to look up additional information on the entities involved or inspected the live Tapped In environment. 
 3.3.1 Data Visualization.
 We applied Gephi’s implementation of the OpenOrd [26] layout algorithm to visualize the network. OpenOrd is a force-directed algorithm that scales well to support very large networks. OpenOrd is based on Frutcherman-Reingold, an O(n2) forcedirected layout algorithm, but resolves two of the latter’s defects: Frutcherman-Reingold is too slow for large graphs, and may obscure global structure. OpenOrd uses a multilevel approach, where a sequence of successively coarser graphs is constructed by clustering vertices according to edge weights and distances and representing each cluster with a single vertex in the coarser graph. Layout is computed with the coarsest graph obtained and then the constituent vertices are placed in the location of their cluster to initiate the next finer iteration of layout. Additionally, edge cutting is used to prevent long edges from unduly pulling together vertices that are best displayed in distinct global structures. Several phases are used in which parameters are varied to resolve tradeoffs between initial clustering, expansion, tightening of clusters, and fine-tuning, The OpenOrd algorithm is very fast and exposes structure in large graphs. 
 3.3.2 Community Detection.
 In the network analysis literature, “community” refers to clusters of mutually associated vertices under graph-theoretic definitions rather than to the sociological concept, and “community detection” refers to finding sub-graphs that constitute such structural clusters. However, a good graph-theoretic definition should capture the intuition that individuals in a sociological community are more closely associated with each other than they are with individuals outside of their community. In this paper, we understand “community” as empirically associated actants for whom it is also possible to identify some focus of their shared activity. This sense of “community” is much looser than the traditional gemeinschaft [35], and does not make claims about participant’s own identities [11]. We believe that a looser definition is appropriate for the networked age [10, 37]. We use graph theoretic terms (e.g., “partition”) when discussing algorithmic results that are candidates for interpretation as a community, and usually reserve “community” for when we are entering into the realm of such interpretation by trying to identify what a cluster has in common, except when referring to the larger endeavor of “community detection”. A variety of graph-theoretic definitions of communities are available, and there are multiple algorithms for each. Algorithms based on the modularity metric are widely used. The modularity metric compares the density of weighted links inside partitions to weighted links crossing between partitions, ranging from 1 (high modularity) to -1 (no modularity). A partitioning of a graph into highly modular partitions (a.k.a. “modularity classes” in Gephi) defines nonoverlapping communities. Finding the best possible partition under a modularity metric is computationally hard (impractical to compute on large networks) [9]. Blondel et al. [7] offer a fast approximation algorithm that gives good results. On each pass, each vertex is initially placed into its own separate partition, and then the algorithm examines the neighbors of each vertex to see whether moving the vertex to a neighbor’s partition can increase modularity. This process iterates over vertices until no vertices move between partitions. Then each partition so constructed is collapsed into a single vertex, with the edges to other partitions merged, summing their weights. The algorithm then repeats on the collapsed vertices until there is no further change in partition membership. At this point, a local maximum has been reached that is not guaranteed to be the most optimal partition under the modularity metric, but has been shown to be a good approximation. 
 4. EMPIRICAL RESULTS.
 In this section we summarize metrics for the overall sociotechnical network studied: all activity within Tapped In surrounding chats, discussions and files (filtered as discussed previously) for the period from 9/2005 to 5/2007. We also summarize the metrics for media-specific networks. Then we present the modular partitions found by the Blondel et al. algorithm and interpret the several largest partitions as communities. We do so by examining who the top actants (actors and artifacts) are, what their affiliations and/or intended purpose seems to be, and how their activity distributes across media. These results are used to illustrate the utility of this kind of analysis. 
 4.1 Overall Network Metrics.
 In the combined network, there are 40,490 vertices (a.k.a entities in the EEC, or actants in the world). These comprise 19,842 Actors (49.00%), 12,037 Discussions (29.73%), 5,862 Files (14.48%), and 2,749 Chat Rooms (6.79%). The combined network has 229,072 edges. The presence of an edge represents the existence of a person-artifact association. Each edge may represent one or more events involving that person and artifact. Edges are weighted according to the number of events. The sum of weights on all edges gives us the number of events encompassed by the total analysis. The sum of weighted degree in the entire network is 20,431,944, which constitutes the number of events (as we have defined them) analyzed. The weighted out-degree gives the number of “write” events for the given artifact (the arc indicates that the state of the artifact depends on the target actor), so the sum of weighted out-degree across all artifacts of a given type gives the number of 'write' events for that artifact type. Similarly, the weighted in-degree gives the number of “read” events for an artifact (the arc indicates that the actor has accessed the artifact), so the sum gives the total number of such events for an artifact type. 
 Table 1. Weighted degrees (events). 
 The sums shown in Table 1 indicate how activity distributes across artifact types, with the caveat that the units are different activities. There are over twice as many events in the chat rooms as in discussions, and few file events, which reflects typical frequencies with which one might interact with each of these artifact types. These results illustrate how the directed, weighted, and multimodal properties of associograms provide information about mediated activity not available in simple sociograms. 
 Some further graph metrics help characterize this network in comparison to other networks. The density is less than .001 (does not display any significant digits in Gephi), so this is a sparse graph. The network diameter is 17, and the average path length is 4.398, smaller than the “6 degrees of separation” found in other networks [e.g., 24]. This result is especially remarkable given that the associogram is a bimodal graph in which an artifact must be present between every person connected. If the graph were collapsed to direct actor-actor ties the average path length would be about half, so this network is more closely knit than typical networks, although others have found similar results [4]. The close connectivity is partly due to associations via a single artifact, the Tapped In Reception (R1), which most users pass through after logging in. This room has a normalized betweeness centrality [8] of 0.665, and has by far the highest degree (2,511,057 weighted or 18,810 unweighted) of any actant. If this mediating artifact is deleted from the graph, average path length goes up to 6.02, or about 3 if intermediate artifacts are removed. We return to the relevance of R1 shortly. 
 4.2 Modular Partitions.
 The Blondel et al. algorithm [7] constructs 171 partitions with an modularity score of 0.817. The combined network is visualized in Figure 2. Color represents the modular partitions generated by the algorithm. Figure 3 shows the six largest partitions by number of vertices. Henceforth we refer to these six partitions as partitions 1-6. The overall visualization in Figure 2 shows a strong central cluster, as do several of visualizations of the six largest partitions in Figure 3. The fact that these central clusters are co-located in the OpenOrd layout (which places more strongly related vertices near each other) indicates that there is overlap between the cores of these potential communities. 
 Figure 2. Combined Associogram for Actors, Chat Rooms, Discussion Forums and Files. Colors indicate highmodularity partitions (candidate communities) found. Vertex side indicates weighted degree. 
 The visualization also juxtaposes vertices for several important actants on top of each other. In order to separate the important actants and see the relationships between them, Figure 4 shows only those actants of unweighted degree greater than 282, with vertex size scaled by weighted degree, and using a radial layout with a non-overlapping filter. Several of the major actants are labeled anonymously, and will be discussed below. A, B, C, and D are actors, D1 is a discussion forum, and R# indicate chat rooms. The colors indicate partitions, as in Figures 2-3. (If color is not visible, of the numbered vertices, R1, R3, R4, and R5 are in partition 1; A, B, C, D, and R6 are in partition 2; R8 is in partition 3; D1 in partition 4; and R2 in partition 6.) Figure 4 shows that there are strong relationships between partition 1 and the other networks, particularly partition 2. We were concerned that the large degree of R1, the Tapped In Reception chat room around which partition 1 forms, might skew the community analysis. Its degree and weighted degrees are a factor of 10 larger than the next largest actant. This may be due to the fact that R1 is the default room into which anyone without a specific tenant affiliation is placed when logging in, and an association will form if anyone chats while they are there. The “help desk” is located in R1, and help desk volunteers often greet others who enter this room. R1 plays an important role in the functioning of Tapped In, but from the standpoint of community analysis R1 presents a dilemma. On the one hand, associations via the reception are a valid community phenomenon: conversations between friends may take place there, and R1 may provide the setting for serendipitous meetings between persons who might not otherwise have met. 
 Figure 3. The six largest partitions. Top left: partition 1; top right: partition 2; bottom left: partitions 3 (brown, left side) and 4 (blue, right side); bottom right: partitions 5 (lighter tan, upper half) and 6 (reddish brown, lower half). 
 On the other hand, many of these associations form without the actor having made a deliberate choice to be there. Anyone in the public campus who has not changed their home room setting will enter this room when logging in and will automatically be associated via R1 with anyone else who has chatted there. The dilemma is exacerbated by a limitation of the Blondel et al. community detection algorithm (and many other community detection algorithms): it does not allow vertices to lie in multiple communities. Thus, any actant placed into the modularity class surrounding R1 will not be available for membership in other modularity class, potentially obscuring their intentional participation elsewhere. In order to assess the risk of superficial associations via R1 obscuring more purposeful communities, we conducted an analysis with R1 (and all of its associations) deleted. As expected, the large partition centered on R1 (partition 1) disappeared and partition 2 became the largest partition, absorbing many of the important actants formerly in partition 1. The interpretation of this new collapsed partition was very clear, and the other largest partitions still produced similar interpretations as communities. However, we decided to leave R1 in for this final analysis, for two reasons: First, many actors were orphaned by the removal of R1: 2178 isolates appear. Second, we felt that we should grapple with interpreting the partitions resulting from the unedited data, as we may learn something about both the Tapped In network and what a community detection algorithm on an associograms can show us. As it turns out, the partition (#1) forming around R1 has a useful interpretation that can be distinguished from partition 2. 
 4.3 Example Interpretations of Partitions.
 In this section we discuss our interpretation of the highest modularity partitions discovered to illustrate the method. Each of the networks is interpreted for what kind of human network it represents by examining what is known about the actants (actors and artifacts) that are ranked highest by degree within the associated partition, indicating their importance to the activity of that partition. 
 4.3.1 Partition 1.
 The largest modular partition has 8452 vertices (20.87% of the total graph) 29698 edges (12.96% of the total edges). There are 6953 actors, 673 chat rooms, 495 discussions, and 331 files in this partition. We illustrate how this partition is interpreted by examining top ranked actants and the distribution of activity across media. 
 4.3.1.1 Top actants by degree.
 Recall that unweighted degree is a measure of how many other actants a given actant has been associated with, and weighted degree is a measure of how many contact events there have been, i.e., a measure of level of activity. The top 20 actants by unweighted degree in this partition are all rooms. Out-degree indicates the number of persons chatting in each room, and in-degree indicates the number of persons hearing someone chat in each room. Total degree counts ingoing and outgoing associations, so will count someone who both chatted and heard a chat twice. 
 Figure 4. Radial layout of high degree vertices (degree > 282; Gephi does not currently filter by weighted degree). Vertex size is weighted degree. Color is modularity class. 
 The top ranked actants are: R1, the Tapped In Reception: 7173 out, 11,637 in, 18,810 total. R4, the public room for Tapped In’s After School Online (ASO) events: 898 out, 986 in, 1884 total. R10, the Floor Lobby for the Tapped In Groups floor: 489 out, 601 in, 1090 total.  All groups are on the third floor; this is the lobby through which you enter that floor. R5, the personal office of Actor E, a faculty member focusing on teachers’ use of technology, and the active owner of a Tapped In group on teacher education: 313 out, 319 in, 632 total. R9, “ComfyConf”, a public conference room available for use by Tapped In members: 285 out, 310 in, 595 total. R12, the personal office of Actor B, an important volunteer to be described in the next section: 289 out, 296 in, 585 total. The top 20 weighted degree vertices in this community are also all rooms. Weighted out-degree indicates the number of chat events in each room; weighted in-degree indicates the number of events of someone being present when someone else chatted (loosely, “hearing” events); and the total can be taken as a measure of total level of chat interaction in the rooms: R1, the Tapped In Reception: 549,572 out (chatting events) 1,961,485 in (“hearing” events), 2,511,057 total. This room hosts 12.29% of all chat events in the network. R3, the personal office of Actor F, an educational researcher: 24,477 out, 407,429 in, 431,906 total. R4, the ASO Public Room: 51618 out, 253,611 in, 305,229 total. R5, the personal office of Actor E, 19448 out, 213474 in, 232922 total. R11, a group room owned by Actor B: 10872 out, 172479 in, 183351 total. R9, the “ComfyConf”: 16785 out, 106596 in, 123381 total. 
 Most of the rooms on both top-20 lists are public, reception or group rooms. Some these are owned by groups with diverse purposes (Art, Blogging, Math, Portfolios, Robotics, Writing), making it difficult to identify a specific purpose or activity for this partition. But the very fact of this diversity and consideration of other information leads to a clearer interpretation of this partition. Many of the highest ranked rooms (R1, R4, R9 and R10) are owned by Tapped In, and explicitly function either to welcome newcomers and route them to their destinations or as venues for public events open to all. The personal offices involved mostly belong to people who keep open office hours to help others. Also, consider the structural fact that when R1 is deleted, 2178 actor vertices (5.3% of all actants and 10.98% of all actors) become isolated, so were only linked to R1. Of these, 1519 are in partition 1, meaning that 17.97% of actors in partition 1 are there solely because of their association via R1. Furthermore, the distribution of degrees is highly skewed: the average is 9.31 (4.62 for actors), the median 3, and the mode 2. Thus this partition consists of many actors who have weak affiliations within Tapped In, but are bound together by their mutual association with the major entry points and centers of activity for new members or those unaffiliated with tenant organizations: the reception, rooms in which public events take place, and offices of volunteers, as well as some public group rooms for popular topics. We interpret this partition to represent not a separate community with its own purpose or activity, but rather a large network in which more specific communities represented by other partitions are embedded. It may also reflect a phase of participation in which new members are becoming familiar with Tapped In, after which they may or may not deepen their participation in specific groups. (Temporal analysis is a topic for further research.) 
 4.3.1.2 Media distribution.
 The degrees for each artifact type give us a general overview of the distribution of activity in this partition. Unweighted degree summarizes the number of other actants a given actant has some affiliation with. Weighted degree is a better overall measure of level of activity bearing on or emanating from an actant, keeping in mind that events involving chats, discussions and files each are of a different nature. Examining the degrees in Table 2, we can see that the bulk of events involving these 6953 actors are overwhelmingly chatbased. This is consistent with the fact that the top actants are chat rooms, and with the interpretation that this partition gathers together activity related to newcomers and chat sessions popular with participants not strongly active in tenant organizations. 
 Table 2. Media Associations in Partition 1. 
 4.3.2 Partition 2.
 The next largest sub-network (modularity class) has 5826 vertices (14.39% of the actants) and 20459 edges (8.93% of events), with 2485 actors, 782 chat rooms, 1828 discussions, and 731 files. Henceforth we will report the top five actants by weighted degree to indicate where the activity lies, and then add those that are in the top five by unweighted degree to ensure we also include actants with high connectivity. For brevity we now report only total degree in parentheses. The top five weighted degree actants in this partition are as follows: Actor A (weighted 499,998; unweighted 2,355), having a “Normal” account type and not a tenant of any organization, nor affiliated with SRI, is the most active account in the system. The level of activity of this actor and of Actor B/B’ (see below) highlights the importance of committed volunteers in this community. Actor B (weighted 370,604; unweighted 2,892), a volunteer who was given Facilitator status and paid for some (but not all) of her activities. Account B is the second most active account in the system. The real-world actor was given a second account B’ (121,259; 238), so that she could facilitate two events at once. Account B’ is a member of a tenant organization for a partnership of multiple school districts during the time our data was gathered. Taken together, the real word actor B/B’ is as active as Actor A. R13 (441,600; 229), the personal office of a faculty member in the school of education at a community college. R13 does not appear in Figure 4 due to the lower unweighted degree. Actor C (336,002; 305), a middle school technology support staff and a Tapped In help desk volunteer. R6 (221,078; 369), an educational technology group room owned by a university education faculty member. The top five actants by unweighted degree include Actors A and B, and the following: R14 (48,116; 2,355), the personal office of Actor A. This office has nickname “Online Support” and is described as offering “sustained online support”. R15 (31,847; 625), a resource room for a primary school center, owned by Actor G, a primary school teacher and Tapped In help desk volunteer who is involved in many groups. Actor D (166,568; 583), a middle school technology teacher who became a Tapped In help desk volunteer during the period of this study. Most of the actors on this list are help desk volunteers. This suggests that partition 2 has some overlap in function with partition 1: helping support the broader Tapped In community. But scanning ranked lists of chat rooms and discussions, a striking difference emerges: while partition 1 has a mixture of personal offices and group rooms, the top ranked chats and discussions in partition 2 are overwhelmingly group rooms. The topics are diverse, including assessment, climate change, librarianship, online teaching and learning (two groups), music, special education, teachers in training, the WWW (two groups), and a system-wide Tapped In festival. We found that all of these chat rooms have one thing in common: they are the site of regular (repeating) and public After School Online (ASO) events announced in the calendar. Furthermore, actors A and B are known for facilitating or supporting the facilitators of many ASO events (i.e., were present in those rooms when the chatting took place). These facts suggest that partition 2 is the broad public community associated with After School Online, arguably the largest and most significant activity in Tapped In. We acknowledge that some relevant rooms such as R4, the After School Online room, R9, the “ComfyConf” room, and R10, the floor lobby for Tapped In groups, are in partition 1. However, all of these actants ended up in the same partition in the other analysis conducted without R1. Examining the distribution of activity across media in Table 3, most of the events are chat-based, which makes sense given that ASO events are chat based. However, although there is less than half the number of actors as in partition 1, both discussion activity and file sharing are double those in partition 1. These facts are consistent with the interpretation that partition 1 focuses more on brief chat interactions such as when persons enter R1 and are given help, while partition 2 includes topic focused public groups, some of which include asynchronous discussion as well as scheduled chats. Help desk volunteers are involved in both of these activities: they are represented by the rooms where help is given in partition 1, and by themselves as actants participating in various group rooms in partition 2. 
 Table 3. Media Associations in Partition 2. 
 The remaining partitions will be described much more briefly due to limitations of space, but sufficiently to illustrate how modularity partitioning on associograms identifies clearly interpretable communities once the densely connected core has been taken care of. 
 4.3.3 Partition 3.
 The next largest modularity class has 2565 actants (6.33% of total), including 851 actors, 103 chat rooms, 1286 discussion forums, and 325 files. Interpretation of this sub-network is much more straightforward. Sorted by degree, two rooms followed by 40 actors have the highest degree, and all 42 of these actants have tenant affiliation with a public school district of a city in the midwestern US. The top ranked actors by degree are all teachers at various levels in this public school system. When sorted by weighted degree, there is a mixture of rooms, discussions and actors, the majority of which are again associated with this public school system or with a related teacher education center. These results suggest that there was formal involvement of this school system during 2005-2007. We checked this interpretation out with our SRI colleagues. According to Judi Fusco (an SRI researcher on the Tapped In project; personal communication October 2011), Tapped In began working with this school system in 2002 to help them support new teachers online with a communities-of-practice model in which cadres of new teachers worked with an experienced teacher facilitator [22, 29]. The program began with about 175 new teachers and 23 facilitators. The tenant organization is still active and has 704 people working in their online program this year. The weighted degree figures for this partition suggest that this community relies on all media. The chat (272,865 in, 226,561 out) and discussion (355,656 in, 5,976 out) figures are healthy for this number of actors, with the asymmetry suggesting that discussions were used more for dissemination. This network has 1/8 the actors of partition 1, but similar figures for file sharing (5,042 weighted), indicating a proportionally greater importance of file sharing. 
 4.3.4 Partition 4.
 The fourth largest modularity class has 1630 actants (4.03%), including 857 actors, 26 chat rooms, 605 discussion forums and 142 files. The top actants involve a state professional development center (anonymously abbreviated SPD), and a national professional development center (NPD), both located in the southern US. Top actants by weighted degree include a discussion on 21st century learning in the SPD group room (D1 in Figure 4); a chat room owned by a teacher educator; an NPD actor with facilitator status who is a university faculty member, a middle school teacher in the same state as SPD, and another discussion in a SPD group room Adding top five unweighted degree actants, we have three actors, all being secondary school teachers located in the SPD state. Of the top 50 ranked discussions, 35 of them are affiliated with the SPD tenant. Of the remaining 15, the Tapped In Reception hosts 12. This is an online professional development effort involving persons in multiple southern states but with the greatest activity focused in one state. It is possibly a collaboration between the state PD center and the national PD center. Media figures show that this is primarily a discussion-based effort, so it was conducted asynchronously. 
 4.3.5 Partition 5.
 This modularity class has 1251 actants (3.09%), including 112 actors, 35 chat rooms, 1006 discussion forums and 98 files, and 4219 edges (1.84%). Top ranked actants by weighted degree include a room for “final task collaboration” and a room that appears to be for a course on using the internet in K-12 schools. Both are owned by Actor H, a language arts high school teacher in the Midwest, who is the third highest weighted actant. The next two actants are the office of a language arts teacher and the account of a middle school language teacher, both in the same state as Actor H. Sorting by unweighted degree adds three more actors, all teachers in the same Midwestern region. The media usage distribution for these 857 actors is weighted on the chats (104,666 in, 35,111 out). There are low figures for the 2313 discussions—most have only one posting and a few reads. This appears to be a series of events run by Actor H. 
 4.3.6 Partition 6.
 Finally, the sixth largest modularity class has 1037 vertices (2.56%) and 6858 edges (2.99%), with 729 actors, 153 chat rooms, 71 discussions, and 220 files. The affiliation with a university on the west coast (we’ll call it WU) is clear. The top five actants by weighted degree include four personal offices of WU faculty members in education, and one group room owned by a researcher at WU and SRI. Sorting by unweighted degree we add one more WU faculty member office, and two WU public rooms. Almost all of the 100 top ranked actors by weighted degree have WU as their tenant affiliation. The interaction is clearly chat based (sum of weighted degree 4,519,127 compared to only 27,789 for discussions). Heavy use is made of personal offices: 41 of top 50 chats are in personal offices. Asynchronous discussions play a lesser role, but are affiliated with a nearby public school system, suggesting involvement of this university with local public schools. Clearly, this cluster is associated with WU teacher education, possibly in collaboration with local public schools. Judi Fusco confirms that WU was one of the major Tapped In tenants during this time period. They have Masters and EdD programs, and apparently had advanced students doing professional development with teachers in the schools. 
 4.4 Small Communities.
 The visualization of Figure 2 shows that there are also many small peripheral networks embedded in the larger network. We inspected some of the small clusters shown in Figure 5, and found a diversity of actors and topics. (1) One centers around a group room for educational technology students at a liberal arts college with multiple campuses and online programs. This room is owned by an elementary school teacher who is also teaching online for this institution. (2) A cluster of actors is connected by several discussions, all owned by a high school language arts teacher in the Midwest. (3) Another room owned by an education faculty member at a west coast state college is surrounded by many actors. Their user IDs are distributed in a manner suggesting clustered creation over a number of years. Sampling the actors we find a consistent pattern, with accounts being created around the same time in 2004, another cluster in 2005, and another in 2006. All of the sampled actors are educators in nearby school districts, ranging from elementary through high school and including math, science, technology and social studies. It appears that the state college actor owning this room is running a recurring course or program. These small clusters attest to how Tapped In enables participants to create hundreds of small communities embedded in and synergizing with the larger Tapped In network. 
 5. DISCUSSION.
 These communities were detected based purely on structural characteristics. Although our database contains other information such as institutional affiliation, job role, geographic location, and demographics for actors and descriptions for discussions and rooms, none of this information was used in constructing the partition. We only used such descriptive information associated with top-ranked actants in interpreting the partitions after they have been constructed. The fact that we can find a clear interpretation based on the characteristics of actants in each partition attests to the power of the structural method for finding partitions that have some external validity. A caveat: one should not conclude that all actants in these partitions are engaged in the activity identified by our interpretations. Modularity optimization—maximizing intra-group links while minimizing inter-group links—is appealing as a graph-theoretic definition having some correspondence to our understanding of community, but does not capture traditional aspects of community having to do with shared geography, identity or purpose. Each partition discussed above has up to a few hundred highly connected actants, followed by a long tail of other actants that have some weaker or peripheral association with the network partition. The institutional affiliations, job roles, etc. of the core actants ranked highly by degree give us an idea of the activity that resulted in closer affiliations among these actants, but many other actants who did not necessarily participate in the identified activity but are weakly associated with the network will be placed in the corresponding partition by the modularity optimization algorithm if they do not have stronger associations elsewhere. The same point applies to participation of artifacts. 
 Figure 5. Closeup of small peripheral sub-networks, many of which constitute topic-focused communities. 
 Thus, for example, we will sometimes see very large numbers of discussions involved in a partition, but this does not mean that all of these discussions were devoted to the identified activity. Yet, this caveat does not diminish the power of the analytic technique. On the contrary, it shows that we can find both the core purpose of a partition and the extent to which it involves others not directly identifying with this core purpose. Also, such expanded structures illustrate the synergistic power of embedding task-specific activities within a larger “transcendent community” [19, 32], a point on which Tapped In was clearly successful. A limitation of the analysis is the use of a non-overlapping community detection algorithm. Clearly, actors and artifacts may play a role in multiple communities, but many community detection algorithms force each vertex into one community. Recently, various algorithms for overlapping community detection have been proposed, including clusters of k-cliques [27], vertex splitting [17], and approaches that find communities of edges [1, 13]. Ongoing work is evaluating these algorithms with respect to associograms. K-cliques do not apply to bipartite graphs because there are no triads or higher order cliques in a bipartite graph. Edge communities look most promising from both empirical and theoretical standpoints: the results reported in [1] are strong, and the approach is consistent with our theoretical position that relationships between entities are primary rather than entities in isolation (see for example [33, 34]). Once we find an overlapping community detection algorithm that is suitable for associograms, we will redo this work to study how actants (including TI-Reception and highly active volunteer facilitators) may bridge between communities. 
 6. Conclusions.
 These results demonstrate the vibrancy of Tapped In, in which multiple tenant individuals and organizations support their own instrumental purposes and also lead to the emergence of a larger encompassing socio-technical network that is not in the same sense a “community” but that constitutes the synergistic value of the network—what we have termed a “transcendent community” [19]. As can be see in Figure 2 and its decomposition into subnetworks in Figure 3 and our interpretations above, tenant organizations drive significant activity in Tapped In, but they are entangled with participants from other organizations and the larger public sphere of the volunteer-based After School Online series. For the purposes of this paper, these results also demonstrate our method. Social network analysis and its use of the classic sociogram generally take a high level view of user relationships, but do not elucidate how those relationships are formed or mediated. Various micro-analytic methods provide detailed information for different forms of interactions, but do not expose structures of the larger network. This study illustrates a middle level of analysis: associations between participants that take place through digital media. Associograms capture valuable information, abstracting enough for aggregate structure analysis across multiple media while preserving some information on the quantity and directionality of interaction. A “community detection” algorithm applied to the Tapped In associogram found actual communities without applying any knowledge of participant affiliation or other demographics. This further illustrates the importance of considering activity across all available mediational means for interaction, not just limiting research to (for example) chats or discussion alone. 
 7. ACKNOWLEDGMENTS.
 This work was supported by NSF Award 0943147. The views expressed herein do not necessarily represent the views of NSF. The authors thank SRI and Patti Schank for providing us with access to the anonymized data, Judi Fusco for her help with our interpretations, and Nathan Dwyer and Devan Rosen for their prior collaborations on this work.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Multi-mediated Community Structure in a Socio-Technical Network</rdfs:label>
		<dc:subject>socio-technical networks</dc:subject>
		<dc:subject>community structure</dc:subject>
		<dc:subject>distributed learning</dc:subject>
		<dc:subject>networked learning</dc:subject>
		<dc:subject>social network analysis</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kar-hai-chu"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kar-hai-chu"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/30/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/kar-hai-chu"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/31">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Mining academic data to improve college student retention: An open source perspective</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/31/authorlist"/>
		<swrc:abstract>In this paper we report ongoing research on the Open Academic Analytics Initiative (OAAI), a project aimed at increasing college student retention by performing early detection of academic risk using data mining methods. The paper describes the goals and objectives of the OAAI, and lays out a methodological framework to develop models that can be used to perform inferential queries on student performance using open source course management system data and student academic records. Preliminary results on initial model development using several data mining algorithms for classification are presented.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Academic or learner analytics has received significant attention within higher education, including being highlighted in the recently released 2011 Horizon Report [4]. This interest can, in part, be traced to the work at Purdue University which has moved the field of academic analytics from the domain of research to practical application through the implementation of Course Signals. Results from initial Course Signal pilots between fall 2007 and fall 2009 have demonstrated significant potential for improving academic achievement [1]. Despite this early success, academic analytics remains an immature field that has yet to be implemented broadly across a range of institutional types, student populations and learning technologies [2]. The Open Academic Analytics Initiative (OAAI), supported by a grant from EDUCAUSE’s Next Generation Learning Challenges program, is developing and deploying an open-source ecosystem for academic analytics as means to further research into this emerging field. This paper will focus on two of the five primary objectives of the OAAI: (a) research into the “portability” of predictive models for student performance; and (b) the development and initial deployment of an “open source” model. To support real-world adoption, OAAI bases its development on open-source technologies already in widespread use at educational institutions, and on established protocols and standards that will enable an even wider variety of existing open-source and proprietary technologies to make use of OAAI code and practices. The OAAI analyzes student event data generated by Sakai Collaboration and Learning Environment (CLE). The Sakai CLE is an enterprise-level open-source teaching, learning, research, and collaboration software platform initially developed in 2004 by a core group of five institutions (Indiana University, MIT, Stanford University, University of Michigan, and UC Berkeley). Today, Sakai is in use in hundreds of institutions around the world and supported by a vibrant community of developers, designers, educators, and commercial support vendors. Predictive models are developed using the Pentaho Business Intelligence Suite (http://www.pentaho.com/), perhaps the world’s most popular open source BI suite, with integrated reporting, dashboard, data mining, and data integration capabilities. Pentaho includes Weka [8], an open source, Java-based sophisticated data mining tool with growing popularity in the data mining community, which is a central piece in the development and testing of predictive models within the OAAI. An initial set of predictive models for student performance has been developed using Fall 2010 data from Marist’s Sakai system, along with student aptitude and demographic data. These models have been deployed using both open-source Weka and IBM’s SPSS Modeler to help ensure compatibility between data mining tools. At the conclusion of the OAAI, these predictive models will be released under a Creative Commons/open-source license through the OpenEdPractices.org web site for other institutions to use. The models will be published using the vendor-agnostic XML-based standard Predictive Modeling Markup Language (PMML) which will allow for the importing of models into a range of other analytics tools. Over time this will facilitate an open-source community effort to enhance the predictive models using new datasets from different academic contexts as well as new analytic techniques. To explore the effectiveness of the Predictive Model for Student Performance, a series of pilots will be run over spring 2012 at three partner institutions, College of the Redwoods (2-year community college), Cerritos College (2-year community college) and Savannah State University (Historically Black College and University). To support these pilots a Sakai “Student Effort Data” (SED) API has been developed that captures the user activity data Sakai already records to its “event logs” and expose it through a secure standard interface for use by both open-source (Pentaho/Weka) and proprietary external academic analytics tools, such as IBM SPSS Decision Management for Student Performance and SunGard Higher Education Course Signals. Longer-term, Student Information System (SIS) data extraction will be automated and enhanced by leveraging the recently released IMS Global Learning Information Services or LIS standard to facilitate data extraction from Student Information Systems (SIS). 
 Table 1. Correlations between course grades and CMS data.
 One of the initial objectives of the OAAI has been to research into the “portability” of predictive models used in academic analytics to better understand how models developed for one academic context (e.g. large research university), can be effectively deployed in another (e.g. community college). This component of the project is building on the pioneering work of John Campbell whose dissertation research at Purdue University investigated the predictive power of CMS usage data, student aptitude (e.g. SATs and ACT standardized test scores) and student demographic data with regards to student academic success in courses [3]. We have applied similar analytical techniques using Fall 2010 CMS data from Sakai (http://sakaiproject.org), an open-source Course Management System (CMS) started in 2004 and now in use in hundreds of institutions around the world, in production at Marist College to investigate whether similar correlations are found. Although Marist College and Purdue University differ in obvious ways (e.g., institutional type and size) they do share a number of similarities which are particularly pertinent to this study. These include (2010 data) percentage of students receiving federal Pell Grants (Marist 11%, Purdue 14%), percentage Asian/Black/African American/Hispanic students (Marist 11%, Purdue 11%), and ACT composite 25th/75th percentile (Marist 23/27, Purdue 23/29) [6]. Table 1 shows similarities in correlation values between course grades and CMS frequencies when comparing both institutions. Only comparable metrics between CMS systems (Blackboard in the case of Purdue and Sakai at Marist) have been displayed. As in the case of Purdue, all these metrics are found to be significantly correlated with course grade, with rather low correlation values. Thus, our analysis, as detailed above, provides an initial insight with regards to how “portable” models may be with regards to institutional type and size. 
 2. METHODOLOGICAL FRAMEWORK.
 The data mining models considered in our work are based on supervised learning (classification) techniques given that labeled training data is available (data sets used for training purposes carry both input features describing student characteristics and course management system events, as well as student academic performance). The goal is to discriminate between students in good standing and students that are not doing well (a binary classification process) Our methodological framework consists of six phases, namely Collect data, Rescale/Transform Data, Partition Data, Balance Training Data, Train Models, and Evaluate Models using Test Data. The first four phases deal with preparing the input data used to build (train) and subsequently evaluate (test) models. Trained and tested models can then be used to score incoming data. Collect Data: Student demographic data and course enrollment data is extracted from the student records system as well as from Sakai (Open source CMS) . Identifying student information is removed during the data extraction process Sakai logs data of individual course events tracked by each of the tools used by an instructor in a given course shell (e.g. Sessions, Content, Discussion Forums, Assignments, Assessments) as well as scores (grade contributions) on gradable events recorded by the Gradebook tool. Rescale/Transform Data: Data is recoded / processed according to specific needs of the classification model building process. The end product is a data set that collects data of each course taken by each student in a given semester, augmented with student demographic data, CMS event data and partial scores derived from (Gradebook) data. The target (class) variable named Academic Risk establishes a threshold of questionable academic performance (e.g. a cutoff of a C grade or lower defines poor academic performance; a grade above C defines good academic performance). This stage is also concerned with the removal of outliers, handling of missing data, and addressing the issue of variability among courses in terms of assessment and student activity. The aforementioned variability is dealt with by replacing counts with ratios computed with respect to the average metric for the full course. An aggregated score is derived from partial (Gradebook) scores on gradable events. Once again the purpose is to shave variability across courses and compute a metric that can be used to make early predictions on student academic performance a few weeks into the semester. Partition Data: input data is randomly divided in two datasets: a training data set, and a test data set. The training data set is used to build the models. Models are then tested using test data to compute a realistic estimate of the performance of the model(s) on unobserved data. We use a ratio of 70% of the data used for training, and 30% testing, following standard data mining practice. Balance Training Data: The input data used in the binary classification process is typically unbalanced, as there are usually (many) more students in good academic standing than students at academic risk. In such case where class values are present in highly unequal proportions the number of student-at-academic-risk cases may be too small to render useful information from what distinguishes from good students (the dominant class value). Therefore records of students at academic risk in the training data set are oversampled to level the proportion of classes, and therefore improve the performance of the trained model at detecting such cases. The test data set maintains the original proportions of class values. Build Predictive Models: We train different models with the training dataset, using different statistical and machine learning processes. We chose three classifiers for comparison purposes: logistic regression, support vector machines, and C4.5 decision trees as they are state of the art robust classification methods that can deal with both categorical and continuous features. Logistic regression is a highly popular parametric classification method, where the target value is a logit function of the linear combination of the predictor features. The C4.5 [5] decision tree algorithm is a non-parametric classifier that learns rules from data. Support Vector Machines (SVM) are powerful discriminative models initially proposed by Vapnik [7], that classify data in categories by finding an optimal decision boundary that is as far away from the data in each of the classes as possible. Evaluate Models: Trained models are evaluated used test data using measures of predictive performance derived from the confusion matrix that yields counts of true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN). Given the unbalanced nature of classes, the overall accuracy (TP+TN)/(TP+TN+FP+FN) is not a good metric for evaluating the classifier, as it is dominated by the student-in-good-standing class (TN+FP). We therefore appeal to two other accuracy metrics: sensitivity (TP/(TP+FN)), which measures the ability of the classifier to detect the class of interest (academic risk); and specificity (TN/(TN+FP)) that measures the number of false alarms raised by the classifier. 
 3. EXPERIMENTAL SETUP.
 A data sample corresponding to Fall 2010 undergraduate students was gathered from four different sources: Students biographic data and course related data; Course management (Sakai) event data and Sakai’s Gradebook data. Datasets were joined and data was cleaned, recoded, and aggregated to produce an input data file of 3877 records corresponding to courses taken by students. All features considered in the input dataset are listed in Table 2. FTPT is a flag indicating whether the student is full-time or part-time; ACADEMIC _STANDING identifies probation, regular, or Dean’s list students; ENROLLMENT is the course size. RMN_SCORE is the aggregated metric derived from partial (Gradebook) scores described in the previous section. R_SESSIONS and R_CONTENT_READ are Sakai event metrics: R_SESSIONS measures the number of Sakai course sessions opened by a student compared to the course average; R_CONTENT_READ measures the number of content resources viewed by a student compared to the course average. 
 Table 2. Features (predictors and target) in input dataset. 
 Experiments were conducted using Weka 3.6 and IBM SPSS Modeler 14.2. For each of these tools a flow of execution was developed to perform the experiments. The experiments followed these guidelines: (i) Out of the input dataset, generate five different random partitions (70% for training, 30% for testing) by varying the random seed (ii) Balance each training dataset by oversampling records with class ACADEMIC_RISK=1 For each balanced training dataset and each of three classification algorithms (Logistic Regression, C4.5 Decision Tree, SVM), train a predictive model, 5 x 3 = 15 models all in all. For the purpose of this experimental work a radial basis function (RBF) kernel was considered for the SVM algorithm, with a gamma parameter of 0.2; the regularization parameter C was set to 10. (iii) Using each corresponding test dataset , evaluate each classifiers’ performance by measuring their predictive performance (sensitivity, specificity) (iv) Produce summary measures (mean and standard error) 
 4. RESULTS.
 Table 3 displays the assessment of predictive performance of all three classifiers over five different trials. Predictive performance is summarized as a point estimate (mean value) and an error bar. Both the logistic regression and the SVM algorithms considerably outperform the C4.5 decision tree in terms of their ability to detect students at academic risk: the logistic regression classifier attains a mean sensitivity of 87.67% on the test data set, and the SVM yields 82.60%. This means that these algorithms detect, respectively, 87.67% and 82.60% of the student population at risk. In terms of specificity, the logistic regression classifier attains a mean value of 89.51% on the test data set, and the SVM yields 90.51%. This means that these algorithms produce, respectively, 10.35% and 9.51% of false positives on the test data. These values are moderately high, especially when compared with the specificity scores of the C4.5 classifier (97.03%, meaning that less that 3% of the test data are false positives). We performed and assessment of the relative predictive power of the predictors under consideration. This helps to focus the modeling efforts on those predictors that matter most and consider ignoring those with low predictive power. For logistic regression the RMN_SCORE stands in first place, followed closely by ACADEMIC_STANDING and CUM_GPA; in third place R_SESSIONS and SAT_VERBAL. For the SVM classifier RMN_SCORE occupies the first place, followed by CUM_GPA, ACADEMIC_STANDING, R_SESSIONS and SAT_VERBAL. The Decision tree follows a similar pattern, although the relative difference in predictive performance of the predictors under considerations seems to be minimal. This finding, paired with the low sensitivity values exhibited by the decision tree classifier, demands further analysis. ACADEMIC_STANDING and CUM GPA are typical predictors of academic performance, as descried in the literature. The use of the RMN_SCORE metric as a predictor seems promising if partial grades (final grade contributions of gradable events, such as assignments, or tests) are available at prediction time, but its validity and usefulness require further investigation. CMS events appear to be second tier predictors when compared to the performance metrics described above. 
 Table 3. Results of the Classification Performance Analysis. 
 5. CONCLUSION.
 This paper reports on the goals and objectives of the Open Academic Analytic, providing a detailed description of the methodology used to develop predictive models in academic analytics using and open source platform. This research derives its motivation from the need of introducing model development approaches that can be used in practical settings to predict academic performance and carry out early detection of students at risk .The methodology presented in this research has been initially applied on real-world data extracted from Marist College and some preliminary results are reported As the project progresses, this analytical framework for academic success will be deployed using data of other institutions and will overtime be enhanced through open-source community collaboration. The goal is to advance our understanding of technology-mediated intervention strategies by investigating the impact that engagement in an online academic support environment has on student success. We hope that this initiative is imitated by other higher education institutions as a template to facilitate development of predictive models for early detection of academic risk. 
 6. ACKNOWLEDGEMENTS.
 This research is supported by EDUCAUSE’s Next Generation Learning Challenges, funded through the Bill & Melinda Gates Foundation and The William and Flora Hewlett Foundation. It is also partially supported by funding from the National Science Foundation, award numbers 1125520 and 0963365.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Mining academic data to improve college student retention: An open source perspective</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>open source</dc:subject>
		<dc:subject>data mining</dc:subject>
		<dc:subject>course management systems</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/eitel-jm-lauria"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/eitel-jm-lauria"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joshua-d-baron"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joshua-d-baron"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mallika-devireddy"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mallika-devireddy"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/venniraiselvi-sundararaju"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/venniraiselvi-sundararaju"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sandeep-m-jayaprakash"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sandeep-m-jayaprakash"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/31/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/eitel-jm-lauria"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/joshua-d-baron"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/mallika-devireddy"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/venniraiselvi-sundararaju"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/sandeep-m-jayaprakash"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/32">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>The relationship between educational performance and online access routines: analysis of students’ access to an online discussion forum</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/32/authorlist"/>
		<swrc:abstract>A study of the behaviour patterns associated with students accessing an online discussion forum is presented. Data collected on the frequency of access and the duration of sessions is analysed to establish several categories of learners, which depict the differences among the cohort in terms of participation in social learning. A single course run at a British business school for second year undergraduates was studied over two years (i.e. two cohorts) from a course and the results were combined to derive the categories of learner types. We conclude that there are benefits that encourage students to maintain an active online presence such as social bonding and a sense of belonging, but that academic attainment does not seem to be related to their access behaviour necessarily.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Social media is described as a web-based service that allows individuals to construct a profile within an organized framework, to generate a list of other users with whom they share a connection, and to navigate their own list of connections and view those made by others within the system [1]. Thus social networks use the Web to facilitate their communications as one element of what has been termed ‘Web 2.0’ - a term coined by O’Reilly [2] and which has come to be used to describe a wide range of Internet-based information and communication technology (ICT) applications that offer the potential for significantly increased interactivity with a high degree of ‘communication, cooperation, collaboration and connection’ [3] between users. More recently attention has turned to how these new applications can be applied to facilitate educational attainment. It is often recognised that a constructivist perspective is most appropriate for understanding learning through social media. Insights on student behaviour and academic performance however are relatively limited in the research literature. Our intention here is to analyse access patterns to a discussion forum within a VLE (virtual learning environment) to determine a taxonomy of online learners. Often, naively, it is assumed that mere participation in an online discussion session – a form of social media – will lead to better learning, and therefore, raised performance. However, it remains to be seen whether social media in itself can aid learning and attainment through participation. Moreover it is a matter of debate what type of access pattern is most conducive to learning. Are relatively short yet frequent sessions involving directed engagement with a discussion forum to obtain a specific objective more productive in learning terms than relatively long yet infrequent sessions intended to develop understanding through exploration and sustained participation? 
 2. ONLINE IDENTITY and COMMUNITY.
 2.1 Learning Theory.
 Social Learning Theory [4] sees environmental (i.e. social) and psychological factors influencing behaviour with ‘retention’ (remembering what one observes), ‘reproduction’ (recreating that behaviour), and ‘motivation’ (having good reason to recreate that behaviour) as critical cognitive aspects. ‘Motivation’ in these terms resonates with the ideas of Whyte [5] who advocated getting students to accept personal responsibility for their own learning, and, as long as the individual has some inherent ability, the result is that better academic progress is made. However some form of guidance or ‘scaffolding’ is required for learners and this needs to be built into the educational infrastructure, or online learning environment. Social Development Theory [6] argues that social interaction precedes intellectual development, with consciousness and cognition being the end product of social behaviour. Thus the connections between people and the sociocultural context in which they interact are critical for such development [7]. Thus Vygotsky challenges the traditional ‘instructionist’ model of learning whereby teachers ‘transmit’ information to students, and offers in its place learning contexts in which students themselves take an active role in enabling their learning. In this analysis of social constructivism, Vygotsky’s Zone of Proximal Development (ZPD) appears to chime to some extent with the Community of Practice (CoP) concept [8] whereby novices firstly undertake learning from a position of ‘Legitimate Peripheral Participation’ (LPP). Lave and Wenger argue that learning is situated such that it is embedded within activity, context and culture. However intentionality is not necessarily assumed in this model of learning and can be an incidental or vicarious outcome of the interactions occurring within a ‘community of practice’. 
 2.2 Empirical Evidence.
 While there are a number of proponents arguing for the use of ‘social media’ as an educative tool, empirical data on their use is not plentiful but is contentious. A study by Heo et al. [9] examining online interaction amongst of a set of students engaged in project-based learning showed that levels of academic achievement do not always reflect the quantity of online interaction, while the quality of interaction is shown to have a critical relationship with academic outcomes. The fact that social media can act as a distraction to learners is attested to by the work of Junco [10] who finds Facebook usage and scholarship to be negatively correlated: the frequency of engagement with Facebook has a negative correlation with the amount of time spent by students preparing for class. In the same vein Junco and Cotten [11] find that students who spend more time chatting online than their peers report greater levels of academic impairment. Such evidence shows social media holding back rather than supporting student learning. This view chimes with Kubey et al. [12] who find that heavy recreational Internet use highly correlated with impaired academic performance. Additionally loneliness, staying up late, tiredness, and failing to attend class also have correlations with self-reports of Internet-caused academic impairment. Such observations prompted further research and hence we wished to investigate student access patterns to an online discussion board and apparent learning approaches. 
 3. METHODOLOGY.
 The investigation took place over two years (2009-2011) on an undergraduate course on Project Management. Students on the course were enrolled on different degree pathways (Marketing and Management) so the class composition included diverse academic interests as well as diverse demographics (e.g. gender, race, age, etc.). Each session ran for one term (from January to April), with class lectures scheduled on a weekly basis. The task given to the student cohort was to form teams of 4-6 people in order to undertake a project assignment which ran from January until mid-March. Students were informed that the group activity did not attract academic credit per se, but was essential for a subsequent activity which did attract credit. This follow-on activity required students on an individual basis to produce short critical accounts reflecting on their team working experience in the first activity. Thus, without sufficient involvement in the group activity, students would find it difficult to complete the reflective account (the second activity). Communications amongst the team members in the group activity were to be conducted through an online discussion application (Blackboard), with instructions given to students to refer to the discussion forum frequently in order to receive updates from the class instructors as well as to raise queries and to read responses. The first year of the study (2009/10) involved 160 active students on the discussion forum, while the second year (2010/11) involved 143 active students. Only those students who submitted the second assignment were considered to be active. Analysis of the data derived from the discussion forum involved a hierarchical cluster analysis to identify subgroups amongst the class, as well as tests of difference amongst groups (t-tests) and tests of association among factors (Pearson correlation). 
 4. RESULTS.
 An initial hierarchical cluster analysis (Ward’s linkage on Euclidean distance) was performed on the data set for each year separately. 
 Year 2010/11. 
 This procedure distinguished several groups on dimensions related to variables extracted from the analytics data: the average interval between online sessions (AIn); and the average duration of an online session (ADu). We see that there are five major groups of students formed when differentiating on the variables introduced above, with the primary differentiator being ADu at a height of h=7.77 (i.e. a considerable difference amongst the two top-level clusters). The top-level clusters consisted of n=87 students with average durations per session of less than 9 minutes, and n=56 students with average duration per session of greater than 9 minutes (the cut-off point of 9 minutes was chosen from the dendogram by visual inspection). The second level clusters were divided on the basis of average interval between sessions. For the group with relatively shorter ADu, a first division at h=6.94 was made, though a subsequent division at h=4.05 serves to better define three distinct clusters. The height of h=4.05 indicates a relatively high distinction amongst clusters. The sizes of the sub-clusters were n=54, n=26 and n=7, divided at average interval per session of 1-5 days, 5-12 days, and more than 12 days respectively. The alternative group with comparatively longer ADu was divided at h=4.01, with cluster sizes of n=41 for 1-5 days, and n=15 for more than 5 days. No further divisions were made due to small height values which could not provide reliable sub-clusters. We can define the five clusters on the basis of their average interval between sessions and the average duration of a session. 
 Cluster 1, n=54: Short interval between sessions and short duration per session. Cluster 2, n=26: Medium interval between sessions and short duration per session. Cluster 3, n=7: Long interval between sessions and short duration per session. Cluster 4, n=41: Short interval between sessions and long duration per session. Cluster 5, n=15: Long interval between sessions and long duration per session. 
 Note that no clear cluster emerged with medium interval between sessions and long duration per session. 
 A test of difference between means (independent samples ttest) confirmed no significant difference between clusters formed on final assignment mark. A one-way ANOVA test was conducted on the three sub-clusters divided at h=4.05 (Levene’s statistic F=.515, p=.599 confirms homogeneity of variance). Results indicate there is a statistically significant difference between groups (F (2.77) = 5.878, p = .004). A Tukey post-hoc test reveals that the final mark attained in the follow-up assignment is statistically significantly higher for average interval per session of between 1 and 5 days (1<AIn<5) (59.9 ± 10.3 percent) compared to 5≤AIn<11 (53.5 ± 11.8 percent, p = .047) and AIn≥11 (46.87 ± 13.2 percent, p = .010). There was no statistically significant difference between clusters 5≤AIn<11 and AIn≥11 (p = .300). Correlation analysis at the level of clusters indicates some interesting associations. Cluster 1: r = -0.387, p=0.01, n=54. Moderate indirect correlation between access span (period between first and last access) and average interval between sessions. Cluster 2: r = 0.462, p=0.05, n=26. Moderate direct correlation between assignment mark and the average number of messages read per session. Cluster 3: r = 0.875, p=0.01, n=7. Strong direct correlation between assignment mark and access span. r = 0.831, p=0.05, n=7. Strong direct correlation between access span and average messages read per session. Cluster 4: n=41. No significant correlations. Cluster 5: r = 0.555, p=0.05, n=15. Moderate direct correlation between average duration of a session and average number of messages read per session. 
 Year 2009/10. 
 The analysis procedure was repeated on the data for year 2009/10. Analysis revealed several groups distinguished on dimensions related to variables extracted from the analytics data: The average interval between online sessions (AIn); average duration of an online session (ADu); and the average number of discussion messages read during an online session (AMr). As with Year 2010/11, we can define five clusters for Year 2009/10 on the basis of their average interval between sessions and the average duration of a session, though the clusters were not as highly distinct as for Year 2010/11. Cut-off height of h=2.09 to achieve five clusters, but top level h=7.77, and as with Year 2010/11, the value of h (incidentally identical) indicates a considerable difference between the two top-level clusters. However, in contrast to Year 2010/11, the division at h=7.77 is based on average interval per session (AIn) rather than average duration per session (ADu). A test of difference between means (independent samples t-test) revealed no significant difference in final assignment mark between clusters. A one-way ANOVA test was conducted on the three sub-clusters divided at h=2.09 (Levene’s statistic F=.975, p=.423 confirms homogeneity of variance). Results indicate there are no statistically significant differences between clusters 1, 2, and 3 on the basis of final assignment marks. Analysis of analytics data over two years has revealed some consistent results, yet there remain a number of issues that the data are unable to resolve. First we should note that student access patterns can be defined using the two parameters of (a) average interval between sessions, and (b) average duration of each session. A number of distinct clusters can be produced using just these two parameters, though their exact inter-relationship is not clear from the analysis. In one year, average interval was the primary differentiator, whereas for the alternative year, average duration was predominant. The more important concern, though, is the composition of clusters along behavioural patterns, and the relationships between dimensions of behaviour and attainment. Commonalities across the two samples point to common behaviour across years on some dimensions. Clusters 1, 3, and 5 are similar over the two studies, though clusters 2 and 4 describe different behaviours. To help with the organisation of the discussion, we introduce a framework composed of labels to distinguish the different types of learners we observed over the two years of the study. This is shown in Figure 1. 
 5. DISCUSSION.
 The application of Weiner's Attribution Theory [13] allows one to make determinations of the reasons (causes) for the observed behaviour of participants. Analytics data provide the observations from which one can draw conclusions about the students' motivations and intentions, given the assumption that their behaviour is deliberate and, for the most part, stable. Thus, the proceeding discussion is premised on the assumption that observed behaviour is indicative of intention, which permits one to make attributions of cause for the observations. Of course, such attributions remain conjecture and unverified, wherein lies a limitation of the work reported here. 
 Figure 1: Learner types based on access behaviour patterns. 
 An important determinant in the area of social learning is the actions of others, which will in some great part impinge on the experience had by a participant, and consequently, help determine their response (i.e. remain online and observe, post messages, engage in a chat, etc.). Attribution to external causes (i.e. other participants) must feature prominently in an analysis of observed behaviour within social media. It should also be remembered that Social Development Theory [6] would place the emphasis on social interaction for individual development, while Bandura's social learning theory would highlight the role of observation as a valuable component of a vicarious approach to learning from others. Evidence of observation is shown in the predominance of message reading over message posting, with many of the learner types choosing to dip into and out of the social artefacts in order to catch-up on things without significant reciprocal contribution. If we accept both views on learning (i.e. ‘social development’ and ‘social learning’) then there is a place for both interaction and observation in a social learning process. However, as we see from our data, preference for one view over the other seems evident. Learner type 3 (‘inquisitive’) and Learner type 7 (‘committed’) are the two groups that demonstrate some level of adherence to the principles of socially-constructed knowledge. For these two groups of learners, attribution of causes for their sustained levels of participation may be due to internal factors, such as innate high levels of determination. Intrinsic motivation assumes an attitude in the participant that places control over attainment within oneself. Self-agency, therefore, is the driving force that determines the extent to which ‘committed’ and ‘inquisitive’ learners expend effort in influencing the group. By contrast, it may be deduced that the remaining learner types are not determined by a strong sense of self-agency within the context of group work, and consequently, tend not to engage in social development which would require active continual interaction with others. At best, the remaining learner types seem to adopt an observer role in efforts to learn vicariously. One should not forget that social learning is not mediated by observation after the fact (i.e. observation of historic activity), but should be contemporaneous with the authentic activity being learned. Thus, reading a trail of messages is not of itself equivalent to participation in the construction process – primarily this is because such an approach is devoid of any affordances for real-time exchanges with the active members of the society being so observed. With respect to the effectiveness of such illegitimate social learning, an important consideration is cognitive loading [14], which may impact on the observer's ability to comprehend the considerable amount of accumulated knowledge in the form of dozens or even hundreds of message exchanges. Reading alone does not translate into learning, particularly in an electronically-created social environment that is far removed from the realities of face-to-face team exchanges with their informationally-rich cues (e.g. sounds, locations, visual and aural perceptions, etc.). Without the combination of multimodal sensory inputs, textually-based communications lack the potential for yielding contextually-rich integrated knowledge that are conveyed through visual and auditory channels. Thus social learning conducted through a discussion board format consisting of historic collections of message exchanges fails to live up to the expectations and demands of an authentic social learning experience in which learners are able to observe the visual as well as the verbal cues of their peers (and experts), to process those cues progressively without overloading their cognitive capacities, and to re-affirm their understanding through expression and feedback. Although one may be able to recreate a ‘community’ in a social media environment, production of an authentic ‘practice’ is more problematic. Progression within the community of practice is hard to facilitate therefore without continual authentic exchanges intended to share experiences. Those learners who merely ‘visit’ the community to observe would rarely be considered to be participating in the practice, and therefore, could not expect to benefit from the ‘social’ nature of the media. Whatever may be the drawbacks of an electronically-mediated forum, learning can be an incidental product of participation in online discussion, as evidenced by the ‘committed’ learner type (Figure 1). While their academic attainment did not noticeably improve, the strengthening of their social ties and their acceptance by the community as a valuable member of the network are, arguably, important consequences of efforts to ‘enter’ the community through sustained contributions. Whether the behaviour of the ‘purposive’, ‘directed’, ‘strategic’, ‘detached’, and ‘apathetic’ learner types can be explained by their preference for an ‘asocial’ approach to their personal learning, or as being due to other moderating factors such as introverted personalities, insufficient time, social reticence, low level of motivation, non-acceptance in the community, or any other form of ‘action by others’, is not clear from the data. What we do learn is that any attempt to interpret behaviour patterns from analytics data alone is fraught with vagaries that offer multiple competing explanations and which lack adequate support. 
 7. CONCLUSION.
 The study reported on here notes results that point to discernible VLE access patterns, with interpretations of the intentions and motivations of learners of several types. From this analysis a framework is presented that identifies and relates seven types of learners on the dimensions of access span and access duration. The framework recognises that the learner types lie on a two-dimensional spectrum that extends from purely instructivist to fully constructivist at opposite extremes. In reality, all learners lie in between the extremes and demonstrate aspects of both learning approaches. The VLE permits social interaction but interchange amongst participants alone is inadequate as a means of raising performance levels. Improvements in attainment are not clearly determined by behaviour patterns, and intrinsic motivators are perhaps the reason for variations in these patterns. It would appear that few students appreciated the relationship between participation and attainment, which in retrospect proved prescient on their part. In a context devoid of adequate extrinsic motivators, emphasis must be on nurturing intrinsic motivation in order to encourage more active participation where the pedagogic goal is experience of the process of social construction through interaction rather than knowledge of the artefacts thus produced. It should be noted that the results are preliminary in nature and must be considered indicative rather than substantive. Further work should include the development of research instruments to test for the substantive nature of the learner types, and to determine the consequence of each type for learner attainment, group cohesion and the viability of a social media-based learning environment.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>The relationship between educational performance and online access routines: analysis of students’ access to an online discussion forum</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>open source</dc:subject>
		<dc:subject>data mining</dc:subject>
		<dc:subject>course management systems</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tariq-m-khan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tariq-m-khan"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/fintan-clear"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/fintan-clear"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/samira-sadat-sajadi"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/samira-sadat-sajadi"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/32/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/tariq-m-khan"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/fintan-clear"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/samira-sadat-sajadi"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Clustering by Usage: Higher Order Co-occurrences of Learning Objects</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33/authorlist"/>
		<swrc:abstract>In this paper, we introduce a new way of detecting semantic similarities between learning objects by analyzing their usage in a web portal. Our approach does not rely on the content of the learning objects or on the relations between the users and the learning objects but on usage-based relations between the objects themselves. The technique we apply for calculating higher order co-occurrences to create semantically homogenous clusters of data objects is taken from corpus driven lexicology where it is used to cluster words. We expect the members of a higher order co-occurrence class to be similar according to their content and present the evaluations of that assumption using two teaching and learning systems.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 In this paper we present a new way to cluster learning objects into semantically homogenous groups without considering their content or additional semantic metadata. We do so by just taking the usage of the learning objects, i.e. the interaction of the learners with the objects into account. For this purpose, we borrow the technique of calculating higher order co-occurrences from corpus linguistics where it is used to cluster semantically similar words and apply it to the usage of learning objects. 
 In linguistics, first order co-occurrences of a word can be calculated by taking the context of that word into account, e.g. the sentences in which it occurs. Second order co-occurrences can be calculated by taking the co-occurrences of the first order co-occurrences into account and so forth. For example: drink and beer are co-occurrences, as well as drink and wine. Therefore, beer and wine are co-occurrences in the first order co-occurrence class of drink, this means beer and wine are second order co-occurrences. Higher order co-occurrence classes of words tend to be semantically homogenous; this is to say they are similar according to specific attributes, e.g. their direct hypernym. While in linguistics the basic unit is a word that is used in sentences, we transfer this approach to usage data and consider learning objects as basic units that are used in sessions. Therefore, the context of a learning object consists of all sessions the object was accessed in and a session consists of all objects that were accessed in that session. Thus, if two learning objects were used in the same session, they are said to be co-occurrences. In this paper we address the question whether higher order co-occurrence classes of learning objects become semantically homogenous, like the analogue classes of words. This is a non-trivial question; it is by no means necessary that the semantic convergence of words in higher order co-occurrence classes can also be observed for entire learning objects. However, the possibility to derive semantic similarity from usage is highly promising for diverse applications of information retrieval in learning settings and thus the question is worth solving. We evaluate the potential of this approach using the usage data collected in the systems MACE [1] and Travel Well [2]. The rest of the paper is structured as follows: In section 2 we briefly report on the related work in the area of clustering semantically similar data objects such as documents and pictures. In section 3 we introduce higher order co-occurrences in corpus linguistics and describe in section 4 how we adapt this idea to calculate higher order co-occurrences of learning objects. We then present the corresponding investigations using the MACE and the Travel Well dataset as test-beds in order to illustrate our ideas in section 5 and give a summary and an outlook in section 6. 
 2. RELATED WORK.
 Traditionally, representations describing the content of a data object are needed to find semantically similar objects. There exist several approaches to create such profiles and append semantic features to different types of data objects. Some approaches are based on the manual creation of such data, others on the automatic extraction of semantic features. A common approach to automatically extract semantic features from text documents is based on the idea that the content of a text can be represented by a list of characteristic keywords. Thus, by extracting keywords one can construct a shallow semantic representation of texts. A commonly applied measure for extracting keywords is the TF-IDF [3] measure which is based on the assumptions that on the one hand, the more often a term occurs in a document the more representative it is, but that on the other hand, the more often a term occurs in the entire collection of all documents the less relevant it is to discriminate documents. That is, keywords shall both be representative and discriminative. However, a lot of learning object collections do not only contain text documents but also images, videos or audio files. Thus, additional automatic content extraction methods for different media types are required. Pictures for instance can be analyzed using content-based image retrieval (CBIR) methods [4] which take into account the actual content of an image represented by the features color, shape, texture or similar content-related features. The information is extracted using automatic image processing algorithms. Since the extracted information is on a very low level, the results mostly contain only limited semantic information not matching the user’s search queries. Given an image of a specific person, CBIR methods can detect that a person is shown in the picture but can usually not identify who it is. With images showing complex scenes the identification and extraction of semantic features often fails [5]. An alternative to the automatic extraction of semantic features is manually created information. Traditional environments like libraries use the expertise of librarians or archivists to create metadata about their resources. These metadata typically contain information such as title, author and further classifications. Since this procedure requires manual creation of the metadata for all resources, it is lengthy and time consuming and also requires a lot of expertise. Further, maintenance of the data is problematic as well. One way to avoid such problems can be the use of social metadata like ratings, tags or comments about learning objects which are created by a community. The data can be used to create different views on the resources, e.g. by filtering out content which is tagged with a specific keyword or only displaying content which is frequently used or highly rated. Tags in particular provide an effective way to represent user interests and help the user to find documents about a specific topic [6]. Thus, this approach can effectively be used for different media types like images, videos or audio files where it is still difficult to automatically extract the appropriate semantic features [7]. A disadvantage of using such social metadata is that it has to be added by a community and thus often contains ambiguous or synonymic tags which make a comparison difficult. Further, it is not assured that each tag is assigned correctly which can lead to wrong results. Therefore, it seems to be preferable to find a new way to cluster semantically similar objects without considering the content but only their usage. Collaborative filtering approaches [8] employ this method by taking the relations between users and objects into account using implicit and explicit feedback, e.g. if a user bought a product or listened to a song. However, collaborative filtering approaches are not suitable to cluster semantically similar data objects but instead try to overcome semantic niches. Our approach does not rely on the relations between users and objects but on the relations between the objects themselves, i.e. whether they are used in similar contexts. First approaches for clustering objects based on their interrelations were mostly conducted in the area of web mining. Rongfei et al. [9] create object vectors containing the most significant co-occurrences of the respective objects. Thereafter a DBSCAN algorithm is used to cluster the objects based on these vectors. Smith and Ng [10] follow a related approach. They use the sessions an object occurred in to describe it. First, the sessions are clustered into transaction groups. An object (which is represented by an URL) is then characterized in terms of the transaction groups, e.g. an URL was called in seven sessions that belong to transaction group A and in five sessions that belong to group B. The object vectors are then used as input for a self-organizing map for clustering them. In this paper we present an approach where not only the first order co-occurrences of an object are taken into account but higher order co-occurrences as well to cluster semantically related objects. 
 3. BACKGROUND OF HIGHER ORDER CO-OCCURRENCE CLUSTERING.
 3.1 Co-occurrences in Corpus Linguistics.
 If you want to know the meaning of a word, it is a good strategy to look it up in a dictionary where in some cases you might find a helpful definition. However, in most cases a definition (if there is one at all) might not be sufficient to correctly understand the word’s meaning as the word's context is often needed for clarification. The words strong and powerful, for example, have highly related meanings. However, we can say strong tea while we cannot say powerful tea. Powerful drug though is acceptable [11]. Definitions of the word’s meanings will most probably not cover such differences. Therefore, dictionaries usually give contexts in which a word typically occurs to illustrate the actual word usage. Context is considered to be significant for the meaning of a word. Firth [12] says “you shall know a word by the company it keeps”. (See [13] for an overview on the linguistic tradition related to Firth.) The company a words keeps – its co-occurring words – contributes to its meaning. Two words might just co-occur by accident. However, the co-occurrence might also be relatively frequent and thus statistically significant. Statistically significant co-occurrences reveal close relationships between the cooccurring words or their meanings, respectively: they are used to detect multi-word expressions (New York), idioms (kick the bucket [14]) or constructions with a milder idiomatic character (international best practice [11]). When calculating the co-occurrences of a word, one can consider different definitions on how near two words must be to co-occur depending on the purpose of the analysis. It is possible to only consider the direct neighbors of a word, as in the examples above, to use a static frame of words or to consider the whole sentence. The significant co-occurrences form the co-occurrence class of the respective word. For example, the co-occurrence class of the word dog is made up of the words bark, growl and sniff among others. It can then be examined whether words significantly co-occur in co-occurrence classes. These words again form another cooccurrence class, namely a higher order co-occurrence class. For instance, feed and dog are co-occurrences, as well as feed and cat. Therefore, dog and cat are second order co-occurrences. After some iterations the elements in the higher order cooccurrence classes become stable and semantic homogenous. Heyer et al. [15] show this for the co-occurrences of IBM, among other words. Their investigations are based on text corpora collected for the portal wortschatz.uni-leipzig.de (concerning the German treasury of words). The first co-occurrence class is rather heterogeneous, containing words like computer manufacturer, stock exchange, global and so on. After some iterations of computing higher order co-occurrence classes, however, the classes become more homogenous and stable. The co-occurrence class of tenth order only contains names of other computer-related companies like Microsoft, Sony etc. 
 3.2 Significance of Co-occurrences.
 3.2.1 Calculation of Significance Values.
 When calculating the significance of a co-occurrence, its frequency is not sufficient as a measure, but the marginal frequencies of the individual words also have to be taken into account. For example, the bigram is to is one of the highest recurrent word pairs in the Brown corpus with 260 occurrences. However, the word is occurs about 10.000 times and the word to occurs about 26.000 times in the corpus that contains altogether 1 Million words. Therefore, even if words were sequenced randomly we would expect them to co-occur about 260 times together and they are not considered as significant co-occurrences [14]. We assume that the same holds true for the distribution of objects in sessions and apply the same measures as in corpus linguistics to calculate significant co-occurrences of learning objects. There are several measures that can be used to calculate how strong words or objects are attracted by each other. These measures can be divided into measures of effect size that calculate how much the observed co-occurrence frequency exceeds the predefined expected frequency (e.g. MI, Dice, odds ratio) and measures of significance that measure how unlikely the null hypothesis is that the words or objects are independent (e.g. zscore, t-score, simple-ll, chi-squared, log-likelihood). For more details see [16] where more than 30 different measures are discussed. In the following, we present a measure of significance that is based on the Poisson distribution. We choose this measure as it was already successfully applied in corpus linguistics for calculating higher order co-occurrences of words, e.g. by Heyer et al. [15]. The comparison by Bordag of the performance of different significance measures for co-occurrences, namely DICE coefficient, Mutual Information measure, Lexikographers Mutual Information, t-score, z-score, two log-likelihood based measures and two Poisson distribution based measures, supports this choice [17]. Furthermore, a formal proof justifying the assumption of a Poisson distribution for co-occurring words in a corpus, if the frequency of most words is much smaller than the corpus size, is given in [18]. Thus, following Heyer at al. we define the significance for the learning objects A and B based on the Poisson distribution as: 
 (FORMULA_1).
 a = frequency of contexts containing A b = frequency of contexts containing B n = frequency of all contexts 
 Under the assumptions ఒ > 2.5 and k > 10, with k being the frequency of all contexts containing A and B together, formula (1) can be simplified: 
 (FORMULA_2).
 3.2.2 Detection of a Suitable Threshold.
 There are various ways of deciding whether a co-occurrence is significant or coincidental, e.g. by ranking or by using a threshold. Ranking means that the co-occurrences are sorted by their significance values and only the best n co-occurrences are selected. When using a threshold, only co-occurrences with a significance value higher than the threshold are selected. However, there is no standard scale of measurement to draw a clear distinction between significant and non-significant co-occurrences [14]. Therefore the calculation of a suitable n or a suitable threshold (depending on the approach) is an exploratory investigation. 
 4. HIGHER ORDER CO-OCCURRENCE CLUSTERS.
 Sessions are used as input for the calculation of higher order co-occurrences. If timestamps are available, a session is made up of all learning objects accessed by a user without a break longer than an hour between two accesses. After a break of at least an hour, a new session starts. Currently, there are no further lower or upper limits for the size of a session. If only a date is stored for an action, a session comprises all activities of a user at one day. Duplicate learning objects in the sessions are deleted. For example, in the session <A, B, B> the object A was accessed once and the object B was accessed twice. Without deletion, we would consider two co-occurrences between A and B that aroused from one context and this would contort the further calculations. After the sessions are created, they are taken as input to generate significant first order co-occurrences. For calculating the significance of the co-occurrences we use the formula given in section 3.2.2. All significant co-occurrences of a learning object together form its first order co-occurrence class. The first order co-occurrence classes of all learning objects serve as input for the calculation of the second order co-occurrence classes which then form the input for the calculation of the significant third order cooccurrence classes and so forth. To clarify this with an example, let’s take the following sessions S1, S2, and S3 as input: 
 S1 = <A, B, C, D> S2 = <A, B, D> S3 = <D, E, F>.
 The calculation of the first order co-occurrences results in the following (temporary) classes for A, B, C, D, E, and F. The total frequencies of the objects and their co-occurrences are given in brackets, e.g. A and D co-occur 2 times, whereas B occurs 2 times and D 3 times in total. 
 The given frequencies are used to calculate the significance values for each co-occurrence based on the Poisson distribution (see section 3.2.1). For demonstration we use exemplary significance values in this example. (For successfully applying the presented measure of significance, a large collection of sessions is required.) The significance values of the co-occurrences are given in brackets. 
 Given a threshold of 1.3, the final co-occurrence clusters can be calculated by deletion of all co-occurrences with a significance value lower than the threshold. 
 First order co-occurrence class for A: <B, C, D> First order co-occurrence class for B: <A, C, D> First order co-occurrence class for C: <A, B> First order co-occurrence class for D: <A, B> First order co-occurrence class for E: <F> First order co-occurrence class for F: <E> 
 The generated classes can now be used to calculate the second order co-occurrences. This is done the same way as calculating first order co-occurrences but taking the first order co-occurrence clusters as input and not the sessions. This leads to the following second order co-occurrences for A, B, C, D (please note that the second order co-occurrence classes for E and F are empty, so they are not shown here). The total frequencies of the co-occurrences and exemplarily significance values are given in brackets. 
 Using again the threshold of 1.3, the following second order co-occurrence classes arise: 
 Second order co-occurrence class for A: <B, C, D> Second order co-occurrence class for B: <A, C, D> Second order co-occurrence class for C: <A, B, D> Second order co-occurrence class for D: <A, C, D> 
 These classes can now be used as input for the calculation of third order co-occurrences and so forth. The calculation stops when the classes get stable, i.e. they do not change anymore in further iterations. 
 5. CLUSTERING RESULTS.
 5.1 Testbeds.
 5.1.1 MACE.
 The MACE (Metadata for Architectural Contents in Europe) project relates digital learning resources about architecture, stored in various repositories, with each other across repository boundaries to enable new ways of finding relevant information [19]. While interacting with the MACE portal, users are monitored and their activities are recorded as CAM (Contextualized Attention Metadata, [20], [21]) instances. Activities include search, access and metadata provision activities like tagging and rating. The CAM instances used for the evaluation were collected from September 2009 until April 2010 and comprise at least a timestamp as well as a user identifier and an item identifier. The actions considered for a learning object to be part of a session are goToPage, i.e. the user leaves the MACE portal to access the original learning object, and getMetadataForContent, i.e. the user accesses the metadata of a learning object at the MACE portal, e.g. its ratings or user tags. For the calculation of the higher order co-occurrence clusters, we were thus able to take 46.641 events into account that took place in 2.449 sessions. On average, about 11 learning objects were accessed per session. Overall, 3710 distinct learning objects were accessed [1]. MACE stores the metadata representations of the learning objects on a central server. The representations base on the MACE application profile which in turn is based on the Learning Object Metadata (LOM) standard [22]. The MACE application profile comprises several categories that are used to specify a learning object in more detail, such as the general category where basic information about the learning object is stored and the annotation category where comments about a learning object's usefulness for education and the comments’ origins can be stored. We calculate the metadata-based similarity of all pairs of learning objects to get a reference value for evaluating the higher order co-occurrence clusters. Each learning object holds one or more titles and descriptions and is marked with the learning resource types it comprises, e.g. a text containing a figure is marked with the learning resource types narrative text and figure. As these terms belong to a controlled vocabulary, they are comparable. MACE also offers users and domain experts the possibility of editing parts of the metadata, namely tags, classifications and competencies. Tags are free text and can be assigned to learning objects by logged in users. Classifications and competencies are each defined in a controlled vocabulary and can only be set by domain experts. The classification vocabulary is a taxonomy consisting of 2884 terms. The competency vocabulary contains 107 terms to describe the suitability of learning objects for the acquisition of special competencies, e.g. Knowledge of internal environment control and Understanding interaction between technical and environmental issues. To compare the MACE learning objects, document vectors describing them are generated by considering the following assortment of available information: titles, descriptions, learning resource types, user tags, classifications and competencies. Before calculating the metadata similarity, the titles and descriptions are pre-processed. After removing stop words the remaining words undergo a stemming using the Snowball Stemmer [23]. The metadata-based similarity is then calculated using the cosine similarity, i.e. measuring the similarity between two vectors by calculating the cosine of the angle between them [24]. 
 5.1.1 Travel Well.
 The dataset [2] was collected on the Learning Resource Exchange (LRE) portal that makes open educational resources available from more than 20 content providers in Europe and elsewhere. These learning resources exist in multiple languages and conform to a variety of national and local curricula. The registered users, mostly primary and secondary teachers, come from a number of different European countries. The dataset contains information about the rating and tagging behavior of 98 registered users over a period of six months (August 2008 – February 2009). For each user activity, the date, user id, item id and the tag, respectively the rating is stored. As there is no timestamp but only the date, a session comprises all activities conducted by a user in one day. Overall, 14248 events took place in 255 sessions where each session comprises 55 distinct learning objects on average. 75 users rated 1698 unique objects on a scale of 1 to 5 for usefulness; each of these objects was rated 1.3 times on average. Additionally, 79 users tagged 1838 unique objects with 12041 tags in total; consequently each object was assigned with 6.5 tags on average. Information that is available about the users and learning objects is e.g. mother tongue, spoken languages, and subjects the user is interested in as well as title, metadata provider, language, classification keywords, and intended end user age for the learning objects. Similar to the MACE dataset, we calculate the metadata-based similarity of all object pairs to get a reference value for evaluating the higher order co-occurrence clusters. We do so by taking the classification keywords and the tags into account. Since an item cannot be tagged more than once with the same keyword, we created a binary vector for each item and used the Tanimoto coefficient [25] for calculating the metadata-based similarity. 
 5.2 Usage-based Clustering: Results.
 Before clustering, we need to decide how to distinguish whether a co-occurrence is significant or not. This can be done by ranking and choosing the first n co-occurrences or by using a threshold. Since in the given datasets some learning objects can have more learning objects they are similar to than others, we decided to use a threshold. The calculation of a suitable threshold is an exploratory investigation. One possible indication for the quality of a threshold without considering the content is the cluster size and the amount of clusters. Another way to test the quality of a threshold is to manually check some clusters for their semantic consistency. Additionally, even if only available for some learning objects, semantic metadata describing the objects can be used to automatically train a suitable threshold. Here, we used the MACE dataset including semantic metadata to find the best fitting threshold to create meaningful and semantically consistent clusters, which is 1.55. The use of higher thresholds resulted in a lot of very small clusters and the use of lower thresholds resulted in a few small and a few very big clusters (more than 1000 learning objects). Additionally, we used the semantic metadata to manually and automatically validate this choice. We then applied this threshold directly on the Travel Well dataset to test if the threshold is transferable or must be trained new for each environment. 
 5.2.1 MACE.
 Using 1.55 as threshold, the higher order co-occurrence classes became stable after the fifth iteration. This means that in further iterations the classes did not change anymore. The calculations resulted in 184 clusters that contain on average 63 learning objects. 
 Figure 1. Cluster size distribution for MACE. 
 The smallest cluster contains three learning objects and the biggest one 719 learning objects. About 78% of the clusters contain 30 learning objects at maximum and only 17% of the clusters contain more than 100 learning objects (see Fig. 1). 
 Figure 2. Learning object distribution for MACE. 
 The higher order co-occurrence based clustering is not a hard clustering. Therefore, a learning object can belong to more than one cluster, however, about 70% of the learning objects belong to 3 clusters at maximum and there is no learning object that belongs to more than 9 clusters (see Fig. 2). 
 5.2.2 Travel Well.
 As for MACE, the usage classes became stable after the fifth iteration using the threshold of 1.55. The 1838 learning objects were clustered into 100 clusters that contain on average 142 learning objects, see Fig. 3. 
 
 Figure 3. Cluster size distribution for Travel Well. 
 The smallest cluster contains 6 learning objects and the biggest one 420. It is noticeable that the clustering of the Travel Well dataset, where each learning object is contained in about 7 clusters on average (see Fig. 4), resulted in clearly bigger clusters than the MACE clustering. This is due to the fact, that the sessions in Travel Well which contain 55 objects on average are significantly larger than in MACE, where a session only contains 11 learning objects on average. The Travel Well sessions might be this large as there are no timestamps in the dataset but only the date and therefore all activities conducted by a user in a day form one session independent of things such as potential learning breaks. 
 Figure 4. Learning object distribution for Travel Well. 
 5.3 Evaluation.
 The following evaluation answers two questions: (a) Do all the learning objects of the same clusters have a significantly different relatedness than learning objects randomly drawn from the set of all learning objects? (b) If so, does the clustering lead to a significantly lower or higher relatedness and what is the relation of the significantly improved or worsened clusters? These issues will be resolved by applying two statistical methods, namely the Kruskal-Wallis-Test [26] for the first question and a test for the student-t-distribution [27] for the second one. On a first glance the ANOVA [29] seems appropriate to check whether the clustering does have an overall effect. Unfortunately, this approach must be dropped as the ANOVA does have preconditions on the data that were not met. For one, semantic relatedness is not normally distributed as a Kolmogorov-Smirnov Test revealed [28]; secondly the homogeneity of variances within clusters is not given. Semantic relatedness tends to be power-law distributed. Fig. 5 and Fig. 6 which show the distribution of the pair-wise metadata-based similarities in MACE und Travel Well support this claim. 
 Figure 5. Distribution of the pair-wise metadata-based similarities in MACE. 
 Because of these reasons, the non-parametric alternative for the ANOVA, the Kruskal-Wallis-Test [26] was chosen. Kruskal-Wallis is based on ranked data and does not make such strong assumptions as an underlying normal distribution or homogeneity of variances. 
 Figure 6. Distribution of the pair-wise metadata-based similarities in Travel Well. 
 As for the second question and approach, i.e. whether the clustering leads to an improvement of the semantic relatedness, it is assumed that the means of the relatedness values within clusters at least t-distribute around the overall mean-value of the population of MACE respectively Travel Well objects. This is a known fact based on the central limit theorem, according to which the means of samples tend to be normally distributed around the overall mean of the population if the sample size is equal to or greater than 30. For smaller samples the means are student-tdistributed. The according values can be easily computed according to formula 3: 
 (FORMULA_3).
 In this case we take advantage of the fact that the specific values of the population, i.e. mean and standard deviation, can directly be computed and thus do not have to be estimated. This leads to formula 4: 
 (FORMULA_4).
 Here, the estimated standard error is replaced by the computed standard error for the population. To check whether several clusters deviate significantly from the mean of the population the relevant t-value has to be calculated and must then be checked against the t-value needed for significance. As this is not a standard t-test, alpha-errors that would bias the results by chance, do not occur [30]. 
 5.3.1 Kruskal-Wallis-Test.
 The Kruskal-Wallis-Test evaluates whether the medians of certain groups’ ranked data (dependent variable) differ systematically or not. Therefore, the test distinguishes between the entire set of objects and the cluster set of the higher order co-occurrence clustering approach, i.e. it is tested whether the semantic relatedness values of the respective clustering – all partition sets taken together – differ significantly from the overall median of relatedness of the entire MACE respectively Travel Well set. The respective Null-hypothesis (H0) can be formulated as such: 
 H0: The learning objects are taken from the same population. Higher order co-occurrence based clustering does not have an effect on semantic relatedness. 
 Table I displays the result of the tests. It shows that the clustering does have a significant impact on semantic relatedness. 
 Table 1. Kruskal-Wallis-Test. 
 For both datasets, the H0 must be rejected, H1 accepted: H1: The learning objects are not taken from the same population. The higher order co-occurrence clustering has an effect on semantic relatedness. 
 5.3.2 Test for Student-t Distribution.
 The Kruskal-Wallis-Test tests whether the entire clustering has an effect on semantic relatedness. What remains is to compare the individual cluster sets with the overall set of all objects. As in our case the population of all MACE respectively Travel Well objects is available, no additional post-hoc tests must be conducted. To compare individual cluster sets with the overall set, it can simply be tested whether the cluster means of semantic relatedness differ significantly from the overall mean of the population. To this end, the null hypothesis for each single cluster set with the same sample size is formulated: 
 H0: The mean semantic relatedness of a specific cluster set does not differ from the mean semantic relatedness of the whole population of learning objects. 
 H1: The mean semantic relatedness of a specific cluster set systematically differs from the mean semantic relatedness of the whole population. 
 The above described Student t-test to evaluate H0/1 was applied to every cluster set of the clustering. We did a two-sided test on a 5%-significance-level for both datasets, see table II. 
 Table 2. t-distribution test. 
 The semantic density of the clear majority of cluster sets in the MACE dataset as well as in the Travel Well dataset is significantly higher than the semantic density of the overall population (where semantic density is defined as the mean value of semantic relatedness). This means, the chance that the included learning objects show a higher similarity than at a random draw is about 80% for MACE respectively 96% for Travel Well. This is a very good result supporting our initial hypothesis. Furthermore hardly any aggravation can be documented. 
 5.3.3 Discussion of the Results.
 We tested our approach on two different test data sets, MACE and Travel Well. We admit that in both data sets the semantic metadata are not perfectly accurate. Additionally, the definition of a session is not straightforward in the Travel Well test set (in MACE, it is). However, the test data are still comparatively good – large parts of both data sets have been hand-tagged by experts – and we do not find 'better' data sets available. Thus, a 'faute de mieux' argument applies. For the evaluation, we conducted the Kruskal-Wallis-Test to test whether the semantic relatedness values of the higher order co-occurrence classes differ significantly from the overall median of relatedness of the entire MACE respectively Travel Well dataset. Additionally, we checked the cluster means on the student-t-distribution to see which higher order co-occurrence-based clusters have an effect on semantic relatedness. The results clearly indicate that the chosen approach is promising, i.e. semantically similar learning objects are grouped together and learning objects that are not semantically similar are separated. Calculation of effect size is not needed as we compute the amount of significant clusters directly with the given distribution of the population. Effect size thus automatically is given by the significance level of the student-t-distribution. Additionally, the tests show that the threshold calculated for the MACE dataset is transferable to the Travel Well dataset even if they deal with objects from different domains. However, the datasets are still collected in the same area, i.e. Technology Enhanced Learning, and it needs to be tested if the threshold is also transferable to other areas such as web browsing. 
 5.4 Tag Clouds Describing the Clusters.
 We have shown that the objects in the usage-based clusters are semantically similar. Now the question arises what actually makes them similar or – in other words – what are the features that make them similar. Learning objects in MACE and Travel Well can for example be similar according to their topic, their competence level or their type. 
 Table 3. MACE cluster containing objects about energy efficient heating in private buildings. 
 As a first investigation to get a deeper impression about the clusters, we calculate a tag cloud for each cluster. We do so, by taking the user tags into account that were used most in the respective cluster. In this subchapter, we present example clusters for the MACE as well as for the Travel Well dataset. 
 Table 4. MACE cluster containing objects about Spanish construction material. 
 The first MACE cluster comprises 23 learning objects that were assigned with 256 tags in total, whereas the 10 tags that were used most are presented (see Table 3). As these are German tags, the English translation is given in the second column. The third column states how many of the learning objects in the cluster are assigned with the respective tag. As can be seen all learning objects in this cluster deal with energy efficient heating in private buildings. The second MACE cluster (see Table 4) comprises 190 learning objects that were assigned with 2793 tags in total which are mostly in Spanish. Most of the learning objects in that cluster deal with construction material that was produced and used in Spain. 
 Table 5. Travel Well cluster containing objects dealing with English learning material for children. 
 The first example cluster from the Travel Well dataset contains 348 objects that are assigned with 2678 tags in different languages and contains learning objects dealing with English learning material for children (see Table 5). It is noticeable that the Travel Well clusters are not as pure as the MACE clusters. Although they are significantly semantically dense, they often contain objects dealing with two topics. This means, that two clusters were wrongly combined to one cluster. The following cluster for example (see Table 6) contains 136 objects from the information and communication technology and the geographical domain. 
 Table 6. Travel Well cluster containing objects from the ICT and geographical domain. 
 However, tests with varying thresholds show that the threshold calculated for the MACE dataset is also the most suitable threshold for the Travel Well dataset. Most probably the clusters containing objects that deal with two topics arose from the missing timestamps in the Travel Well dataset and the consequential inexact session calculation. 
 6. Conclusion.
 We borrowed the technique of calculating higher order co-occurrences from corpus linguistics and applied this technique to user sessions to group semantically similar learning objects without considering their content. The significant co-occurring objects form the co-occurrence class of the respective learning object. It was examined whether objects significantly co-occur in co-occurrences classes. These objects again form another co-occurrence class, namely a higher order co-occurrence class. We investigated the assumption that - similar to words - the learning objects in the higher order co-occurrence classes become stable and semantic homogenous. We clustered the learning objects contained in the MACE and Travel Well datasets and evaluated the clustering using the Kruskal-Wallis-Test and the student t-distribution test to answer the questions whether all the learning objects of the same clusters have a significantly different relatedness than learning objects randomly drawn from the set of all learning objects. We then asked: does the clustering lead to a significantly lower or higher relatedness and what is the relation of the significantly improved or worsened clusters? Furthermore we created tag clouds describing the clusters to identify the topics of the clusters. The evaluation results clearly indicate that this approach is promising and we will continue on this track. We will further work on identifying an appropriate threshold to separate significant and coincident higher order co-occurrences. To this end, we will conduct additional experiments in other test beds to explore domain-dependent distinctions that need to be addressed. In order to test the usefulness and quality of this clustering approach in real world settings, we will embed the technique in recommender systems. For example, within the MACE portal, a recommender could provide resources thematically related to the ones used in the current session. Furthermore the cluster results can give information about the relationship between courses with regards to content and support the teacher with her teaching activities. 
 7. ACKNOWLEDGMENTS.
 The research leading to these results has received funding from the European Community's Seventh Framework Program (FP7/2007-2013) under grant agreement no 231396 (ROLE project).]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Clustering by Usage: Higher Order Co-occurrences of Learning Objects</rdfs:label>
		<dc:subject>attention metadata</dc:subject>
		<dc:subject>usage data</dc:subject>
		<dc:subject>clustering</dc:subject>
		<dc:subject>higher-order cooccurrences</dc:subject>
		<dc:subject>learning objects</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/katja-niemann"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/katja-niemann"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/uwe-kirschenmann"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/uwe-kirschenmann"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-schmidt"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-schmidt"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tim-krones"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tim-krones"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/katja-niemann"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/uwe-kirschenmann"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-schmidt"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/tim-krones"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/34">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Learning Dispositions and Transferable Competencies: Pedagogy, Modelling and Learning Analytics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/34/authorlist"/>
		<swrc:abstract>Theoretical and empirical evidence in the learning sciences substantiates the view that deep engagement in learning is a function of a complex combination of learners’ identities, dispositions, values, attitudes and skills. When these are fragile, learners struggle to achieve their potential in conventional assessments, and critically, are not prepared for the novelty and complexity of the challenges they will meet in the workplace, and the many other spheres of life which require personal qualities such as resilience, critical thinking and collaboration skills. To date, the learning analytics research and development communities have not addressed how these complex concepts can be modelled and analysed, and how more traditional social science data analysis can support and be enhanced by learning analytics. We report progress in the design and implementation of learning analytics based on a research validated multidimensional construct termed “learning power”. We describe, for the first time, a learning analytics infrastructure for gathering data at scale, managing stakeholder permissions, the range of analytics that it supports from real time summaries to exploratory research, and a particular visual analytic which has been shown to have demonstrable impact on learners. We conclude by summarising the ongoing research and development programme and identifying the challenges of integrating traditional social science research, with learning analytics and modelling.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Information infrastructure embodies and shapes worldviews. The work of Bowker and Star [1] elegantly demonstrates that the classification schemes embedded in information infrastructure are not only systematic ways to capture and preserve—but also to forget, by virtue of what remains invisible. Moreover, the user experience foregrounds certain information, thus scaffolding particular forms of human-computer and human-human interaction, which in turn promotes or obstructs sensemaking [2]. Learning analytics and recommendation engines are no exception: they are designed with a particular conception of ‘success’, thus defining the patterns deemed to be evidence of progress, and hence, the data that should be captured. A marker of the health of the learning analytics field will be the quality of debate around what the technology renders visible and leaves invisible, and the pedagogical implications of design decisions, whether the design rationale is explicit or implicit. In this paper we focus on the challenge of designing learning analytics that render visible learning dispositions and the transferable competencies associated with skillful learning in diverse contexts. These are dimensions of learning that both research and practice are demonstrating to be increasingly important, but which the learning analytics field has yet to engage with deeply. Mastery of discipline knowledge as defined by an explicit curriculum is obviously a critical yardstick in learning, and it is not surprising that currently, this is the dominant focus of most learning analytics research and product development, since this is the dominant paradigm in educational institutions. We know that this is greatly assisted when aspects of the domain and learner can be modelled: user models compare the inferred cognitive model against an ideal model (intelligent tutoring, eg. [3]); presentation layers may tune content dynamically if progress is deemed to be too slow (adaptive educational hypermedia, e.g. [4]); data mining techniques can be deployed, which usually assume the goal is to pass the course (e.g. [5]). In a different part of the learning analytics design space, we see the use of generic learning management systems that are agnostic as to the subject matter (and indeed have only a rudimentary model of the domain, if any). The trend to generic platforms is accompanied by their disaggregation, as open, social platforms, managed by many entities, are used for informal, self-directed learning, sometimes around the edges of formal courses. Learning analytics in these contexts must address a very different learning context, in which the domain, learning objectives, learner cohort and course materials may all be unknown in advance, and may not be controllable (Massive, Online, Open Courses – MOOCs – may be the extreme instance). Converging with these technology-driven trends, is traditional social science research into the personal qualities that enable effective learning across contexts. There is substantial and growing evidence within educational research that learners’ orientation towards learning—their learning dispositions— significantly influence the nature of their engagement with new learning opportunities, in both formal and informal contexts. Learning dispositions form an important part of learning-to-learn competences, which are widely understood as a key requirement for life in the 21st Century. Despite this, employers complain increasingly that many graduates from our school and university systems, while proficient at passing exams, have not developed the capacity to take responsibility for their own learning and struggle when confronted by novel, real world challenges [6]. In this paper we argue that by combining extant research findings from the social science field of education, particularly concerning engagement in learning and pedagogy, with the affordances of learning analytics, we can develop learning platforms that more effectively catalyse the processes of learning for individuals and collectives. We introduce the concept of meta-competencies (§2) as one of several approaches to characterising the demands on learners made by today’s society, and we note the escalating problem of school disengagement (§3). We then summarise some of the core insights in the literature around engagement and learning dispositions (§4), before explaining the use of self-report as a means of gathering dispositional data (§5). In §6 we introduce Learning Power, a multi-dimensional construct for modelling learning dispositions, which has been under development and validation for over a decade, but in this paper we present for the first time the Learning Warehouse platform which underpins it (§7). This generates a visual analytic spider diagram for individuals, which renders the underlying model (§8), plus cohort summary statistics which can inform pedagogical intervention. In §9 we consider qualitative, quantitative and narrative ways to validate dispositional analytics of this sort, including evidence that the visual analytic has pedagogical affordances which build learners’ self-awareness. We also provide examples of how the analytics platform facilitates deeper analyses within and across datasets. §10 summarises four key forms of service that the platform is facilitating which help to close the research-practice gap. We conclude by summarising the contributions that this research makes (§11), and outlining some of the avenues now being pursued (§12). 
 2. META-COMPETENCIES.
 Where formal learning is highly specialised and discipline bound, very often graduates, including those with traditional degrees in ‘vocational’ subjects like engineering or law, find themselves with jobs in which they cannot make much use of whatever specialist knowledge they possess [7]. The acquisition of subject matter knowledge is no longer enough for survival and success in a society characterized by massive data flows, an environment in constant flux, and unprecedented levels of uncertainty (e.g. around how socio-technical complex systems will behave, and around what can or should be believed a true, or ethically sound). What is needed in addition is the ability to identify and nurture a personal portfolio of competencies that enable personal and collective responses to complex challenges. We understand competence as a combination of knowledge, skills, understanding, values, attitudes and desires, which lead to effective, embodied human action in the world, in a particular domain. Skillful accomplishment in authentic settings requires not only mastery of knowledge, but the skills, values, attitudes, desires and motivation to apply it in a particular socio-historical context, requiring a sense of agency, action and value [8]. Writing from the perspective of education, Haste summarises competencies required for 21st century survival. She identifies one overarching ‘meta-competence’ which is the ability to manage the tension between innovation and continuity, and argues that this is constituted in five sub-competences: the ability to (i) adaptively assimilate changing technologies (ii) deal with ambiguity and diversity (iii) find and sustain community links (iv) manage motivation and emotion and (v) enact moral responsibility and citizenship. To be competent in this richer, more expansive sense, the ‘possession’ of knowledge is necessary but not sufficient. Also required are personal qualities and dispositions, a secure-enough sense of identity and purpose, and a range of new skills that enable links to be made across domains and processes. Bauman has argued that deep engagement in learning is particularly important today for two reasons [9]. Firstly, as many school and university teachers will recognise, there is a contemporary search for identity in today’s fluid, globalised society, and secondly, “educational philosophy and theory face the unfamiliar and challenging task of theorising a formative process which is not guided from the start by the target form designed in advance” (p.139). That is, as we transition increasingly to a world where relevant ‘outcomes’ in a real world context can no longer be pre-determined with the confidence of earlier times, and where a learner’s intrinsic capacity to rise and adapt to a challenge is a highly valued trait, we need a theory and practice of engagement in learning that facilitates the formation of identity, combined with scaffolding the processes of knowledgecreation and authentic performance. Thomas and Seely Brown [10] argue for the need to embrace a theory of “learning to become” (p.321) in contrast to theories that see learning as a process of becoming something. They argue that the 20th century worldview shift from learning as transmission to learning as interpretation, is now being replaced by learning as participation, fuelled by structural changes in the way communication happens through new technologies and media. Participation is embodied and experienced, and critically, requires “indwelling”: The potential revolution for learning that the networked world provides is the ability to create scalable environments for learning that engages the tacit as well as the explicit dimensions of knowledge. The term we have been using for this, borrowed from Polanyi, is indwelling. Understanding this notion requires us to think about the connection between experience, embodiment and learning. [10] (p.330) 
 3. LEARNER DISENGAGEMENT.
 The development of the above kinds of competencies presents a challenge for policies and pedagogies that validate learning solely in terms of standardised outcomes—designed (as are all analytics) to facilitate the generation of certain kinds of insight, for certain kinds of stakeholders. An over-emphasis on these indices is in tension with the need to take into account the complexity of learners’ sense of identity and their whole attitude to learning. If learners are, for whatever reason, fundamentally not disposed to learn, then extrinsic drivers around exam performance are unlikely to succeed. As Dewey (1933) observed: Knowledge of methods alone will not suffice: there must be the desire, the will, to employ them. This desire is an affair of personal disposition. [11] (p.30) Rising disengagement is a problem in many developed countries’ education systems. Research undertaken for the English Department for Education [12] reported in 2008 that 10% of students “hate” school, with disproportionate levels amongst less privileged learners (however, highly engaged students from poor backgrounds tend to outperform disengaged students from wealthy backgrounds). The Canadian Education Association regularly surveys student attitudes to school, reporting in 2009 that intellectual engagement falls during the middle school years and remains at a low level throughout secondary school [13]. A 2009 US study across 27 states reported that 49% students felt bored every day, 17% in every class [14]. These disturbing data point to a widening disconnect between what motivates and engages many young people, and their experience of schooling. This is serving as a driver for action research into new models focused on the wholistic design of learning, catalysing academics [15-18] and national schools networks (e.g. the UK’s WholeEducation.org). How can learning analytics research and development engage with this challenge? Certainly, there is a contribution to be made by providing more detailed, more timely information about performance—but while dismal analytics will help educators, their impact on already disengaged learners might be counterproductive. We propose that ‘disposition analytics’ could spark intrinsic motivation by giving learners insight into how they approach learning in general, and how they can become more skillfully equipped for many other aspects of their lives beyond school. We construe this challenge as one of defining, measuring, modelling and formalizing computationally the constructs associated with learning dispositions. 
 4. DEFINING DISPOSITIONS.
 What we are seeking to track, and model for analytics purposes, is a set of dispositions, values and attitudes that form a necessary but not sufficient, part of a learning journey. Figure 1 summarises this conceptualisation of learning dispositions, values and attitudes. This is a complex and embedded journey because it takes seriously the social, historical, cultural and personal resources that shape, and are shaped by, people’s behaviour and dispositions. Learning dispositions are personal, and autogenic. On the one hand they reflect ‘backwards’ (the ‘personal’ left side of Figure 1) to the identity, personhood and desire of the learner, and on the other hand, they can be skilfully mobilised to scaffold ‘forwards’ towards the acquisition of the knowledge, skills and understanding necessary for individuals to develop into competent learners (the ‘public’ right side of Figure 1). Competence in learning how to learn requires agency, intention and desire, as well as the dispositions or virtues necessary to acquire the skills, strategies and knowledge management necessary for making the most of learning opportunities over a lifespan, in the public domain. Although the term ‘disposition’ is imprecise, both theoretically and in practice, it is widely agreed that it refers to a relatively enduring tendency to behave in a certain way [19]. It is a construct linked to motivation, affect and valuing, as well as to cognitive resources [20-24]. Dispositions may be culture specific as well as a relatively enduring feature of personality. A disposition arises from desire, or motivation, which provides the energy necessary for action [17, 25-27]. A disposition can be identified in the action a person takes in a particular situation – for example someone who is disposed to be ‘curious’ will demonstrate this in the manner in which they consistently generate questions and investigate problems. In practice, in education the term is often used interchangeably with ‘competence’ or ‘style’ or ‘capability’, and it is frequently subsumed within the concept of ‘personal development’ as distinct from academic development or attainment. There are many dispositions which are relevant for education – ranging from the specific to the very general, with varying conceptions as to how fixed or malleable they are. Our focus is on malleable dispositions that are important for developing intentional learners, and which, critically, learners can recognise and develop in themselves. 
 5. MEASURING DISPOSITIONS.
 Learning analytics cannot operate without data. For some approaches, this data is a by-product of learner activity, ‘data exhaust’ left in the online platform as learners engage with each other and learning resources. Other approaches depend on users self-disclosing ‘metadata’ about themselves intentionally, knowing that it will be sensed and possibly acted on by people or machines, known and unknown to them. Such ‘intentional metadata’ typically discloses higher order information about one’s state or intentions, which are harder to infer from low-level system event logs. Examples of higher order metadata would include emotional mood during one’s studies, the decision to ‘play’ with an idea or perspective, or setting out to build one’s reputation in a group. These might be disclosed in twitter-style updates, blog posts, comments in a meeting, written work and responses to quizzes/questionnaires. In looking to future research at the end, we signal new work on inferring dispositions from the ‘exhaust’ traces that learners leave in online environments, but the focus of this paper is on self-reported data gathered via a selfdiagnostic ‘quiz’ (the research-validated ELLI survey introduced below). 
 Figure 1: Dispositions as a personal attribute, embedded in a learning journey oscillating between personal and public. 
 Self-report is a standard means of gathering data in the social sciences about an individual’s values, attitudes and dispositions, partly because of the challenges of observation at scale in nondigital environments, and partly because, however astute the observer may be, what a person thinks or feels is by definition idiosyncratic and cannot be confirmed only by the external behaviours and artifacts: take for example an engaged, motivated learner, with low academic ability, who may produce a lower graded piece of work than a bored, disengaged ‘high achiever’ who submits something they have no personal interest in. From the perspective of a complex and embedded understanding of learning dispositions, what learners say about themselves as learners is important and indicative of their sense of agency and of their learning identity (indeed at the personal end of the spectrum in Figure 1, authenticity is the most appropriate measure of validity). 
 6. MODELLING DISPOSITIONS.
 Learning Power is a multi-dimensional construct that has come to used widely in educational contexts in the last ten years. It is derived from literature analysis, and interviews with educational researchers and practitioners about the factors, which in their experience, make good learners. The seven dimensions which have been identified harness what is hypothesised to be “the power to learn” — a form of consciousness, or critical subjectivity [28], which leads to learning, change and growth. 
 I like to learn about things that really matter to me. I like it when I can make connections between new things I am learning and things I already know. I like learning new things when I can see how they make sense for me in my life. 
 Dependence and Fragility: Dependent and fragile learners more easily go to pieces when they get stuck or make mistakes. They are risk averse. Their ability to persevere is less, and they are likely to seek and prefer less challenging situations. The opposite pole of dependence and fragility is ‘resilience’. When I have trouble learning something, I tend to get upset. When I have to struggle to learn something, I think it’s probably because I’m not very bright. When I’m stuck I don’t usually know what to do about it. 
 Creativity: Effective learners are able to look at things in different ways and to imagine new possibilities. They are more receptive to hunches and inklings that bubble up into their minds, and make more use of imagination, visual imagery and pictures and diagrams in their learning. The opposite pole of creativity is ‘being rule bound’. 
 An extensive literature review informed the development of a self-report questionnaire called ELLI (Effective Lifelong Learning Inventory) whose internal structure was factor analysed, and validated through loading against seven dimensions [28]. As detailed later, these dimensions have been since validated with diverse learner groups, ranging in age from primary school to adults, demographically from violent young offenders and disaffected teenagers, to high achieving pupils and professionals, and culturally from middle-class Western society to Indigenous communities in Australia. The term learning power has been used to describe the personal qualities associated with the seven dimensions, particularly by Claxton [29, 30], although in this paper, its meaning is specifically related to the ELLI inventory. 
 Learning Relationships: Effective learners are good at managing the balance between being sociable and being private in their learning. They are not completely independent, nor are they dependent; rather they work interdependently. The opposite pole of learning relationships is ‘isolation and dependence’. 
 The inventory is a self-report web questionnaire comprising 72 items in the schools version and 75 in the adult version. It measures what learners say about themselves in a particular domain, at a particular point in time. A brief description of the seven dimensions is set out below, with three examples from the questionnaire shown for each dimension: 
 Strategic Awareness: More effective learners know more about their own learning. They are interested in becoming more knowledgeable and more aware of themselves as learners. They like trying out different approaches to learning to see what happens. They are more reflective and better at self-evaluation. The opposite pole of strategic awareness is ‘being robotic’. 
 Changing & learning: Effective learners know that learning itself is learnable. They believe that, through effort, their minds can get bigger and stronger, just as their bodies can and they have energy to learn (cf. [22]). The opposite pole of changing and learning is ‘being stuck and static’. I expect to go on learning for a long time. I like to be able to improve the way I do things. I’m continually improving as a learner. Critical curiosity: Effective learners have energy and a desire to find things out. They like to get below the surface of things and try to find out what is going on. The opposite pole of critical curiosity is ‘passivity’. I don’t like to accept an answer till I have worked it out for myself. I like to question the things I am learning. Getting to the bottom of things is more important to me than getting a good mark. Meaning Making: Effective learners are on the lookout for links between what they are learning and what they already know. They like to learn about what matters to them. The contrast pole of meaning making is ‘data accumulation’. 
 I get my best ideas when I just let my mind float free. If I wait quietly, good ideas sometimes just come to me. I like to try out new learning in different ways. 
 I like working on problems with other people. I prefer to solve problems on my own. There is at least one person in my community who is an important guide for me in my learning. 
 If I get stuck with a learning task I can usually think of something to do to get round the problem. If I do get upset when I’m learning, I’m quite good at making myself feel better. I often change the way I do things as a result of what I have learned. 
 7. LEARNING WAREHOUSE PLATFORM.
 Without a learning analytics platform, it is impossible to gather ELLI data globally, with quality and access controls in place, and generate analytics fast enough to impact practice in a timely manner. ELLI is hosted (with other several other researchvalidated tools) within a learning analytics infrastructure called the Learning Warehouse. A mature analytics infrastructure needs not only to gather and analyse data, but orchestrate the tools offered to different stakeholders, and manage data access permissions in an ethical manner. Learners, trainers/educators, researchers, and organisational administrators and leaders are provided with customised organisational portals onto the Learning Warehouse which offers them different tools and levels of permission to datasets as follows: learners sign in to complete the right version of the ELLI questionnaire (e.g. Child or Adult) and receive their personal ELLI visual analytic (detailed in next section); administrators can upload additional learner metadata or datasets; educators/organisational leaders access individual and cohort analytics, scaling to the organisation as a whole if required, in the form of visualised descriptive statistics. Authorised researchers can see all of the above, together with other datasets depending on the bases on which they were gathered. The portals also house learner identity metadata, held separately from the survey data in the Learning Warehouse, and destroyed after use. Learning Warehouse uses the JSR-268 portlet standard1 enabling ELLI profiles to be written and read via external platforms. The researcher interface is provided for querying within and across the anonymised datasets (at time of writing >40,000 cases). Where a data owner requires analysis involving identifiable data, the researchers are given permission to access this from the user portal, held on a different server, for the purposes of the specific project. The researcher interface enables access to aggregated, anonymised datasets over learner cohorts and across tools for researchers, with appropriate permissions within strict ethical guidelines. Researchers are then able to undertake system-wide research on a range of cases, across jurisdictions, instruments and domains, and can curate the data generated from it and make it available for secondary data analysis. Raw data may be downloaded for analysis in Excel and SPSS, with a unique identifier enabling integration of datasets, and in some cases matching with nationally procured datasets. Up to this point, the use of data has fallen within the traditional social science domain in the way that it is used, as well as in the pedagogical domain through providing immediate, visual feedback for learners to appropriate and use in improving their learning power. The next step which we are now beginning to explore is a more integrated researcher experience, which incorporates tools more familiar to the learning analytics world, for example by providing web-based visual analytics tools for querying and interactively exploring data, drawing inspiration from user experiences such as Google Analytics and Gapminder.3 A second development emerges from recent work with collaborative, social applications, which are generating new kinds of data streams at a finer granularity than a complete ELLI questionnaire, and in real time rather than several months apart (e.g. the start and end of a conventional educational research project). We introduce this under future work. 
 8. ELLI VISUAL ANALYTICS.
 Visual analytics are helpful when it comes to comprehending and discussing a 7-dimensional construct such as learning power. On completion of an ELLI web survey, the Learning Warehouse generates a spider diagram (Figure 2), providing a visualization for the learner to reflect on (their own perception of) their learning power. The scores produced are a percentage of the total possible score for that dimension. The spider diagram graphically depicts the pattern and relative strength of individual scores. Note that unlike most spider diagrams, the axes are not numbered, but labeled A little like me, Quite like me, and Very much like me. As discussed shortly, a visual analytic such as this has a number of important properties, which can be both empowering, but also potentially demoralizing, and it is a principle behind the approach that learners are not left to ponder its meaning alone. It is crucial that the learner validates and thus ‘owns’ the profile, a matter for the coaching conversation that follows with a trained mentor. 
 Figure 2: An ELLI spider diagram generated from the Learning Warehouse. The shaded blue region shows the initial profile, while the outer red profile indicates ‘stretch’ on certain dimensions later in the learning project. 
 Data can be aggregated across groups of learners in order to provide a mentor or teacher with a view of the collective profile on all or specific learning power dimensions (Figure 3). 
 Figure 3: Visual analytics on aggregate ELLI data, for all learning power dimensions, and a specific dimension. 
 The spider diagram has been further extended through the use of visual imagery, creating a culturally relevant character to represent each dimension. Examples (Figure 4) include the Simpsons cartoon characters when working with disaffected English teenagers [31], and iconic animals for Australian indigenous young people [32]. 
 Figure 4: Adding visual symbols to Learning Power dimensions to localise them culturally. Top example also shows the addition of metaphorical ‘zones’ to the dimensions, to create mental spaces for learners to inhabit. 
 9. VALIDATION.
 Thorough validation of a learning analytic is a multi-faceted challenge. In this section we describe some of the facets relevant to a dispositional analytic such as ELLI. 
 9.1 Construct validity of ELLI.
 When analysing self-report data there are several ways of ascertaining reliability and validity. Sample size is important, with larger numbers giving greater confidence in standard statistical tests of reliability that explore how the instrument operates in repeated tests. ‘Construct validity’ refers to whether the tool measures what it was designed to measure, for which there exist well established criteria in the social sciences. The reliability and validity of ELLI has been reported in several peer reviewed educational publications [33-35]. 
 9.2 Correlation with standardized attainment.
 Intuitively, one might hypothesise that learners who are curious, resilient, creative and strategic (i.e. in the terms of this paper, demonstrating learning power) should also record higher attainment in traditional tests, because they have, for instance, a much greater desire to learn, and ability to stretch themselves. This argument is made strongly by proponents of learning-to-learn who argue for such approaches to be woven into teaching practice rather than being consigned to be taught as special topics (e.g. [30]). The evidence for this remains inconclusive, to date. Consistent with this line of thought, one would predict ELLI to correlate positively with conventional attainment analytics, and indeed, several studies do report a positive correlation [33, 36, 37]. This is an intriguing finding, but this relationship requires further interrogation: it might also be argued that more developed learning power should correlate negatively with higher test scores. For instance, an analysis of reliability and validity statistics for ELLI (N=10496) in 2008, replicated a 2004 finding that the mean score on students learning power profiles gets significantly lower as students get older [35]. This takes us back to the earlier data reviewed on school disengagement: it points to a widening disparity between the dispositions that reflect learners taking skillful responsibility for their own learning in authentic contexts, and the demands of curricula and associated assessment regimes that focus on test results gathered under artificial conditions. 
 9.3 Pedagogical validity of ELLI profiles.
 In information visualization, visual analytics are judged in terms of the qualities of information design. We would argue that visual learning analytics should go one step further: when they are intended to empower learners we need to understand their pedagogical affordances — the insight yielded for both educators, and learners. In school, workplace and Masters programmes, educators are trained how to use individual and cohort ELLI profiles to shape interventions and classroom practice, but space precludes a detailed report on this. We focus here on the methodological question of how does one validate the pedagogical affordances of the ELLI profile for learners, where the objective is to catalyse changes in dispositions towards learning? Evidence of personal change is gathered using a mixed-methods approach combining quantitative pre- and post-test measures of learning power, plus qualitative and narrative evidence from student interviews. This has proven to be a powerful means of triangulating and validating evidence of impact, and communicating the findings [31, 32, 36, 38-41]. In one 2007 study in a UK school [39], quantitative analysis showed significant changes between pre- and post-measures across a whole year group; qualitative evidence identified key themes, and narrative evidence provided an ‘insiders’ perspective on the experience, such as the following statements from two 15 and16 year old students: It’s (about) understanding – because you can pass exams without understanding…..It’s self growth and achievement…. Our personal experience is important….Learning to tell your own story would make it easier to do all the other things you have to do – learn subjects, get grades etc… When I was a child…I was always much keener to do something if I knew I would get a reward at the end of it….. the performance was important and not the process… and that’s the way the education system works… it’s very results driven… It’s a bit of a trust thing…. they don’t trust you to do it in your own way….its a trust thing… It all ties together – its about self awareness more than anything else ….. self awareness is not even touched upon in the education system… 
 In another project with NEET learners (“Not in Employment, Education or Training”), a 16 year old made significant changes in his pre-post profile through a personalised enquiry project supported by coaching and scaffolding using learning power [31]. In his final debrief for the project he said: It’s opened my eyes quite a bit to learn how to do these things, …and it’s changed what I think I can do. Qualitative evidence from the English Learning Futures project demonstrates that a rich language for describing learning is crucial for deep engagement, as well as an authenticity, agency and identity in learning. In one particular example [42] the ELLI dimensions were identified by a 12 year old boy as the most significant aspect of his learning because: It’s helped me get a long life… Like I never used to know like all this stuff, like strategic awareness, or critical curiosity, I never knew it existed, like changing learning resilience. And as soon as I got it all into my head, I’ve never ever gave up on stuff I need to reach my goal. These examples, drawn from a series of research studies over eight years, show something of the quality of impact on engagement in learning which this visual analytic can have. We are developing a theoretical account of how a visual analytic for dispositions, such as the ELLI profile, operates. The profile provides the framework for a coaching conversation which moves between the self-perception and identity of the learner (Is this like me?) and a projected learning outcome (Where do I need to get to?) [35, 43]. Reflection on the spider diagram is thus a starting point, moving from self-diagnosis to strategy, and forms the basis of authentic enquiry projects that lead to different types of performance outcomes. The seven dimensions of learning power also support personal knowledge construction — for example critical curiosity is a foundation for generating questions, or meaning making is a necessary part of knowledge mapping, both of which are primary forms of knowledge generation [32, 44] Perhaps one of the most powerful aspects of this feedback arises from its position at the interface of identity, purpose and performance. In other words, referring back to Figure 1, the visual feedback facilitates reflection ‘backwards’ to the Self of the learner (the stories that constitute who they are and who they want to become), whilst at the same time providing a scaffolding ‘forwards’ towards a personally chosen outcome. “Do I tend to be a curious person?” is a question of identity, whilst “I use my curiosity to negotiate my way through a learning task, to an authentic performance” is about scaffolding knowledge construction for a particular form of publicly validated outcome. What has emerged from the research programme which has accompanied the model of change we describe here, with the Learning Warehouse at its heart, is substantive evidence that visual literacy is a crucial part of personal and social development and that profound personal change can be achieved, and described, by pedagogical practices (for example coaching for learning) which support such reflection and action [32, 37, 42, 4547]. Let us illustrate this with two examples. An Indigenous Community in New South Wales chose the emu, an animal in sacred stories, as a symbol for the dimension of Critical Curiosity. What we theorise to be happening is that this visual image ‘decentres’ the teacher, the learner projects onto the emu the qualities of critical curiosity, which they then emulate – ‘copy’ or ‘behave like’ the emu – and so begin to experience curiosity as well as being able to describe and deploy it. Furthermore in this particular case study, the young people involved who chose this and six other animals to represent the seven learning power dimensions composed a story, ratified by the community elders, of how the seven animals collaborated to escape from a zoo. This illustrates how symbolism and narrative literacy catalysed by visual analytics have the power to connect learning with communities at a different level (emotional, socio-political) from traditional, didactic teaching [48, 49]. In another Australian case study, a teacher in a remote school in the Northern Territory of Australia copied his spider diagram onto a whiteboard, enriched the spider legs with original art work of traditional sacred animals chosen to represent that community’s understanding of learning power, and is now using this to model learning and change to his community. 
 9.4 Cross-dataset validation.
 Thus far, we have described the instant visual analytics that the Learning Warehouse generates for learners and educators, and we have explored the pedagogical affordances of the ELLI spider diagram. However, the researcher interface onto Learning Warehouse enables deeper analyses to investigate the relationship of learning power to other datasets. This is another form of analytics validation: do we find statistically, pedagogically and theoretically significant patterns when a given analytic is combined with others? - We discussed earlier cross sectional analysis demonstrating that the mean score on students learning power profiles gets significantly lower as students get older [35]. - We have reported positive associations between teachers’ learner-centred practices, their beliefs about learning, students’ learning power, the level of organisational emotional literacy, and attainment by National Curriculum measures [50]. - We have demonstrated distinctive patterns of learning power profiles for under-achieving students, and enhanced the findings with school based, qualitative measures and teacher professional judgment [36]. - A two year study in 2009-11 with fifteen schools participating in the national Learning Futures5 programme, enabled an evaluation of the impact of learning power and enquiry-based learning pedagogies on student engagement [37, 47]. - In 2011 a further analysis enabled the testing of ELLI with an adult sample (N=5762) and an analysis of the differences between age ranges in terms of the internal construction of learning power [51]. 
 Each of these studies discusses the ways in which these results might be interpreted; the larger point is that when the ELLI analytic is combined with other datasets, statistically robust findings emerge which illuminate pedagogical and theoretical debate. This would not be expected from an analytic lacking the other forms of validity discussed. 
 10. CLOSING THE RESEARCHPRACTICE GAP.
 Longitudinal and cross-dataset analytics such as the above have been developed as traditional social science methodologies, but from a learning analytics platform perspective, the point is that such analyses are greatly assisted when learner identities and metadata are curated in an analytics platform, and moreover, enable more timely feedback and interventions. In this section, we describe kinds of services that the Learning Warehouse is being used to deliver. 
 The Learning Warehouse provides portals to organisations including remote Australian communities, schools in China, Malaysia, Germany, Italy, US, and corporates in the UK. The researcher interface for the Learning Warehouse is being used in four major ways to deliver learning analytics to different stakeholders: i. The provision of bespoke organisational analyses as a service (e.g. to schools and corporates) to inform leadership for organisational change. Such information may include the learning characteristics of particular groups, recent examples include a gender cohort in a school; a marketing department in a bank; underachieving students; or measures of change over time in a school. ii. Analysis of data for particular research projects across a cohort of organisations (for example the impact of Learning Futures pedagogies on student engagement in learning [37, 47] iii. Secondary analysis of large-scale datasets for research purposes (see Section 9.2). iv. Collaborative research data analysis service for researchers around the world who wish to use the instruments in their research projects or to avail themselves of secondary datasets. These forms of analysis are traditional in terms of social science research, but there is typically a two-year gap between data collection and feedback in such projects. By automating the capture of learning power data via ELLI, and then aggregating ELLI with other datasets around unique learner identities, the Learning Warehouse accelerates this into immediate feedback in some cases (e.g. cohort aggregate data graphs), or weeks for more complex analysis requiring human intervention. Thus, learning analytics platforms offer new possibilities for the educational research community to demonstrate societal impact, by making possible more rapid, iterative models of engagement from research outputs to impact in practice, and back again. An international community of researchers, practitioners, policymakers and social entrepreneurs has grown around the shared platform provided by the Learning Warehouse, in order to build an evidence base using research validated tools, sharing data within an appropriate ethical framework. 
 11. CONTRIBUTIONS.
 We have described the central role that learning dispositions and transferable skills play in the future learning landscape, and have argued that this has relevance for learning analytics. We propose that the research programme described makes the following specific contributions to learning analytics research and development: 
 1. It is possible to model learning dispositions and transferable skills. The 7-dimensional construct of Learning Power, embodied in the validated design of the ELLI survey instrument, provides a computationally tractable representation of what has, until now, remained an elusive set of personal qualities from a formal modelling (and hence analytics) perspective. 
 2. The spider diagram visual analytic has proven value for learners, mentors and organisations. When visualized, the 7- dimensional profile enables individuals to reflect on their ‘learning self’ and to take responsibility for their own learning, while enabling teachers to assess the learning profile characteristics of groups and individuals. It provides three elements which have been shown to be transformative in some cases: (i) a common language for reflecting on one’s learning dispositions and transferable skills (in this paper we have not had time to describe the programmes of work embedding this language in educational practices); (ii) a visual analytic which through a mentored conversation provokes reflection on one’s relative strengths and weaknesses, and interventions to try; and (iii) through the addition of culturally relevant visual imagery, we have shown that this can provide memorable symbols which motivates young learners, helping them to connect their learning with their sense of identity and their everyday lives. 
 3. The Learning Warehouse exemplifies a collaborative learning analytics platform to acquire, share and analyse datasets. We have given examples of the kinds of analytics afforded by an ELLI dataset drawn from diverse learner populations over years, in combination with other data connected to those learners. Aggregated cohort data informs academic analytics from an institutional perspective. 
 12. FUTURE WORK.
 ELLI’s design rationale from the start was not the creation of a tool for academic analysis (although as described, it has grown to support this). The goal is to provide educators with a practical tool to enable rapid assessment and intervention of a complex quality, to stimulate change in learners. It is this pragmatic driver that now motivates a research programme to ground new forms of online social learning platforms, through end-user tools and underlying analytics, in Learning Power. The long-term goal is to see learners operating in different blended learning configurations (offline/online, synchronous/asynchronous, classroom/mobile, etc.) supported by an underlying infrastructure utilizing static datasets and live data streams to maintain learning power profiles, and recommend possible resources and actions. At all times, however, learners are asked to take increasing responsibility for their own learning process, rather than surrendering control. 
 Reflective learning blog. A suite of Wordpress plugins has been developed for EnquiryBlogger, whose piloting has been positively received [52]. This generates visual analytics reflecting the development of one’s learning power. 
 Online ELLI mentoring. A prototype synchronous tool called ELLIment is in development, for online reviewing of ELLI profiles and recording interventions [53]. 
 Learning Power-based user classification. We are investigating the application of discourse and social analytics as ways to classify online learners and activity against ELLI dimensions. 
 Learning Power-based recommender services. This line of work represents one of the most challenging, but exciting possibilities. ELLI profiles provide a new way for a recommendation engine to connect learners to each other, and to educational resources, based on similar or complementary strengths on different dimensions. ELLI coaches might benefit by finding peers who designed interventions for learners with a given profile and history (there are many reasons why a profile may be as it is, so great care needs to be taken in efforts to automate this). 
 While trained ELLI mentors are an expensive resource, one of the most ambitious future directions is to study, codify and formally model the intervention strategies that mentors use when presented with a given profile and a learner’s history, and investigate if they can inform the design of automated recommender systems. The challenge of modelling sufficiently rich user contextual metadata to do this is significant, but one worth exploring given the huge amounts of data becoming available. It is possible, however, that this cannot be done with great sophistication, just as some might doubt that an artificial agent could replace any but the most rudimentary skills displayed by a good coach or counsellor. The internal structure of Learning Power. As the range of instruments within the Learning Warehouse increases, the possibilities for exploration of relationships and theoretical testing of hypotheses also increase. The application of Structural Equation Modelling (SEM) path analysis, for example, has further illuminated the internal structure of the dimensions in the Adult Learning Power model, not only confirming its validity and reliability, but extending our understanding of how dimensions influence each other, with implications for how we improve practice. For example, SEM appears to confirm that dispositions for learning are a ‘gatekeeper’ construct, on which others depend (e.g. interest, affect and involvement in learning). As the South Australian Department for Education and Childrens’ Services collects data through its ongoing Teaching for Effective Learning project, a combination of data from ELLI and TfEL will enable new analyses. Organisational learning applications in the workplace. A line of work now developing uses ELLI to facilitate organisational self-reflection and learning, with the goal of aligning individual and organisational identity and purpose [51]. In order to advance this research programme, a network has been established [LearningEmergence.net] connecting researchers and practitioners whose interests lie at the intersection of deep learning, complex systems, transformative leadership and knowledge media. We very much welcome the engagement of researchers, practitioners, policymakers and technologists who share our interest in the particular role that analytics can play in building robust learning dispositions and transferable skills. 
 13. ACKNOWLEDGEMENTS.
 The research reported is the fruit of many colleagues’ collaborative efforts, as cited. From our most recent work, we gratefully acknowledge the support of the Paul Hamlyn Foundation in the national Learning Futures programme.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Learning Dispositions and Transferable Competencies: Pedagogy, Modelling and Learning Analytics</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>learning dispositions</dc:subject>
		<dc:subject>learning power</dc:subject>
		<dc:subject>learning how to learn</dc:subject>
		<dc:subject>transferable skills</dc:subject>
		<dc:subject>21st century skills</dc:subject>
		<dc:subject>educational assessment</dc:subject>
		<dc:subject>effective lifelong learning inventory</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ruth-deakin-crick"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ruth-deakin-crick"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/34/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/ruth-deakin-crick"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/35">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Social Learning Analytics: Five Approaches</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/35/authorlist"/>
		<swrc:abstract>This paper proposes that Social Learning Analytics (SLA) can be usefully thought of as a subset of learning analytics approaches. SLA focuses on how learners build knowledge together in their cultural and social settings. In the context of online social learning, it takes into account both formal and informal educational environments, including networks and communities. The paper introduces the broad rationale for SLA by reviewing some of the key drivers that make social learning so important today. Five forms of SLA are identified, including those which are inherently social, and others which have social dimensions. The paper goes on to describe early work towards implementing these analytics on SocialLearn, an online learning space in use at the UK’s Open University, and the challenges that this is raising. This work takes an iterative approach to analytics, encouraging learners to respond to and help to shape not only the analytics but also their associated recommendations.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 The field of learning analytics has its roots in the appropriation of business intelligence concepts by educational institutions: the earlir terms ‘academic analytics’ [1] and ‘action analytics’ [2] refer respectively to the capture and report of data by educational administrators, and to the need for benchmarking to increase the effectiveness of educational institutions. Learning analytics shift the perspective from that of the institution gathering data about learners in order to inform organisational objectives, to that of providing new tools for the learner and teacher, drawing on experience from the learning sciences with the intention of understanding and optimizing not only learning but also the environments in which it takes place. As part of this shift to learner-centred design, we propose that Social Learning Analytics (SLA) can be usefully thought of as a subset of learning analytics, which draws on the substantial body of work evidencing that new skills and ideas are not solely individual achievements, but are developed, carried forward, and passed on through interaction and collaboration. A socio-cultural strand of educational research demonstrates how language is itself one of the primary tools through which learners construct meaning, and its use is influenced by the aims, feelings and relationships of their users, all of which shift according to context [3] (as will be seen, discourse and context are two foci of the SLA we propose). Another strand of research emphasises that learning cannot be understood by focusing solely on the cognition, development or behaviour of individual learners; neither can it be understood without reference to its situated nature [4, 5]. As groups engage in joint activities, their success is related to a combination of individual knowledge and skills, environment, use of tools, and ability to work together. Understanding learning in these settings requires us to pay attention to group processes of knowledge construction – how sets of people learn together using tools in different settings. The focus must be not only on learners, but also on their tools and contexts. Viewing learning analytics from a social perspective highlights types of analytic that can be employed to make sense of learner activity in a social setting. This does not require the development of a completely new set of tools; this paper cites numerous examples of related work in context. Instead, it groups a range of pre-existing and new tools and approaches to form the basis of a coherent set. In doing so, it identifies ways in which analytics may be developed and implemented in order to identify social behaviours and patterns that signify effective process in learning environments. The aim is to use analytics not only to identify these but also to render them both visible and actionable. The paper is organized as follows. We introduce the broad rationale for focusing on social learning (§2), which is then developed specific to several forms of analytic which are inherently social (§3), or which have social dimensions (§4). We then describe progress towards the implementation of these analytics in a social learning space (§5), consider some of the challenges that we are encountering (§6), before concluding (§7). 
 2. WHY SOCIAL LEARNING ANALYTICS?.
 The focus on SLA reflects shifts in the broader cultural, technological and business landscapes, which together are reshaping the educational landscape platforms. We see these as a set of drivers for the growing importance of online social learning, and hence, for SLA. 
 2.1 Social media.
 No review of the forces shaping the educational landscape can ignore the digital revolution. Only very recently have we had the right infrastructural ingredients to provide almost ubiquitous internet access in wealthy countries and mobile access in many more. In addition, we now have user interfaces that have evolved through intensive use, digital familiarity from an early age, standards enabling interoperability and commerce across diverse platforms, and scalable computing architectures capable of servicing billions of real-time users, and mining that data. With the rise of very large social websites such as Facebook, YouTube and Twitter, plus the thousands of smaller versions and niche applications for specific tasks and communities, we have witnessed a revolution in the way that people think about online interaction and publishing. Such social media platforms facilitate the publishing, indexing and tracking of user-generated media, provide simple to learn collaboration spaces, and enable a new set of social ‘gestures’ that are becoming ubiquitous, and expected by the current generations: friending, following, messaging, microblogging, ‘liking’, rating, etc. Potential implication: As ubiquitous access to social networks become a critical part of learners’ online identity, and an expected part of learning platforms, social learning analytics should provide tools to provoke learning-centric reflection on how interpersonal relationships and interactions reflect learning, or can be more effectively used to advance learning. 
 2.2 Open/free content and data.
 There has been a huge shift in expectations about access to digital content. Learners expect increasingly to find reasonable quality information on the Web for free, to the point where they often feel aggrieved when confronted by a request for money, and will seek free avenues first. Within education, the Open Educational Resource (OER) movement has been a powerful vehicle for making institutions aware of the value of making quality learning material available, not only for free, but in formats that promote remixing, in an effort to reap the benefits seen in the open source software movement. This has by no means proven to be a simple matter, since educational staff, materials and institutions are different in important respects from open source programmers, source code and developer networks, but OER has made huge progress, and is gaining visibility at the highest levels of educational policy. Free and open learning resources are mirrored by efforts within the open and linked data communities to make data open to machine processing as well as human interpretation. This requires both the shift in mindset by data owners (which OER has had to effect within education), as well as the laying of technological infrastructure to make it possible to publish data in useful formats. 
 Potential implication: A consequence of the information overload that now confronts learners is the need for more effective filtering and navigation, and it is here that social networks are playing an increasing role, as a means to maximize the increasingly scarce resource at a learner’s disposal: focused attention. SLA should augment learners’ capacities to build effective social learning networks. 
 2.3 Society increasingly values participation.
 Technology is always appropriated to serve what people believe to be their needs and values. Beyond what we can observe for ourselves informally, there is a significant body of research indicating that the period in which we find ourselves is moving towards a set of values mirrored in the affordances of social media. In 1997, the World Values Survey covered 43 societies, representing 70% of the world’s population. Inglehart [6] argued that the shift to ‘postmaterialism’ (a finding from earlier surveys) was confirmed and he offered a new ‘postmodernization’ framework. He suggested that modernization helped society move from poverty to economic security, and that the success of this move led to a shift in what people want out of life. In postmodernity, as he used the term, people value autonomy and diversity over authority, hierarchy, and conformity. According to Inglehart, ‘postmodern values bring declining confidence in religious, political, and even scientific authority; they also bring a growing mass desire for participation and self-expression.’ We find these results interesting: on the one hand it is easy to recognise this shift in wealthy nations, but this shift seems also to be reflected even in the less developed regions surveyed, where poverty is still clearly a daily reality. Another perspective on the shift in social value is the view that, since 1991, we have lived in the ‘knowledge age’ – a period in which knowledge, rather than labour, land or capital, has been a key wealth-generating resource [7]. This shift has occurred within a period when constant change in society has ben the norm, and it is therefore increasingly difficult to tell which specific knowledge and skills will be required in the future [8]. These changes have prompted an interest in ‘knowledge-age skills’ that will allow learners to become both confident and competent designers of their own learning goals [9]. Accounts of knowledge-age skills vary, but they can be broadly categorized as relating to learning, management, people, information, research/enquiry, citizenship, values/attributes and preparation for the world of work [10]. From one viewpoint they are important because employers are looking for ‘problem-solvers, people who take responsibility and make decisions and are flexible, adaptable and willing to learn new skills’ [11, p5]. More broadly, knowledge-age skills are related not just to an economic imperative but to a desire and a right to know, an extension of educational opportunities, and a ‘responsibility to realize a cosmopolitan understanding of universal rights and acting on that understanding to effect a greater sense of community’ [12, p111]. Potential implication: Research evidencing the growing desire in many societies for civic participation and self-expression provides another driver for social learning. Another is within education, with the perceived need to move away from a curriculum based on a central canon of information, towards learning that develops skills and competencies for coping with complexity and novel challenges. SLA should augment learners’ capacities to assess themselves on 21st Century skills. 
 2.4 Innovation depends on social connection.
 The conditions for online social learning are also related to the pressing need for effective innovation in organisational life. In a succinct synthesis of the literature, Hagel, et al. [13] argue that social learning is the only way in which organisations can cope with the unprecedented turbulence they now face. They invoke the concept of ‘pull’ as an umbrella term to signal some fundamental shifts in the ways in which we catalyse learning and innovation, and argue that the world is changing so rapidly that useful knowledge/understanding (in contrast to data or information) is rarely well codified, indexed or formalized, while socially transmitted knowledge is growing in importance as a source of timely, trustworthy insight. This leads them to highlight quality of interpersonal relationships, tacit knowing, discourse and personal passion as key capacities to foster, as we move in business from mere transactional relationships, to building and sustaining more meaningful relationships. Potential implication: These business trends serve as another driver for social learning, and invite opportunities for SLA to augment personal and collective capacities by investigating how we can make visible representations of “quality of interpersonal relationships, tacit knowing, discourse and personal passion.” 
 2.5 Summary.
 We have reviewed some of the ‘tectonic forces’ reshaping the learning landscape. These are ‘signals’ that many futures analysts and horizon scanning reports on learning technology have highlighted as significant. If taken together, these are shaping a radically new context for learning, then by extension, learning analytics must be reframed accordingly to place online social interaction and the social construction of knowledge at the heart of their models. We now introduce five categories of analytic whose foci are driven by the implications of the drivers reviewed above. The first two categories are inherently social, while the other three can be ‘socialized’, ie. usefully applied in social settings: - social network analytics — interpersonal relationships define social platforms - discourse analytics — language is a primary tool for knowledge negotiation and construction - content analytics — user-generated content is one of the defining characteristics of Web 2.0 - disposition analytics — intrinsic motivation to learn is a defining feature of online social media, and lies at the heart of engaged learning, and innovation - context analytics — mobile computing is transforming access to both people and content. 
 We do not present these five categories as an exhaustive ‘taxonomy’, since this would normally be driven by, for instance, a specific pedagogical theory or technological framework, in order to motivate the category distinctions. We are not grounding our work in a single theory of social learning, nor do we think that a techno-centric taxonomy is helpful. The social learning platform and analytics we are developing is in response to the spectrum of drivers reviewed above, drawing on diverse pedagogical and technological underpinnings which will be introduced with each category. 
 3. INHERENTLY SOCIAL TYPES OF LEARNING ANALYTIC.
 3.1 Social learning network analytics.
 Networked learning uses ICT to promote connections between learners, tutors, communities and resources [14]. These networks consist of actors – both people and resources – and the relations between them. A tie describes the relationship between these actors and can be classified as strong or weak, depending on its frequency, quality or importance [15]. People make use of weak ties with people they trust when accessing new knowledge or engaging in informal learning, and go on to make increasing use of strong ties with trusted individuals as they deepen and embed their knowledge [16]. Social network analysis investigates ties, relations, roles and network formations, and a social learning network analysis is concerned with how these are developed and maintained to support learning [15]. Because of its focus on the development of relationships, and its view that technology forms part of this process, this type of analysis offers the possibility of identifying interventions that are likely to increase a network’s potential to support the learning of its actors. Social network analysis can be approached from the perspective of an individual or of the entire network. An egocentric approach may identify the people who support an individual’s learning, the origin of conflicts in understanding, and some of the contextual factors that influence learning. On the other hand, a wholenetwork view provides insight into the interests and practices of a set of people, identifying elements that hold the network together [17]. It also has the potential to help with the identification of groupings within a network that can support learning, for example communities and affinity groups [18, 19]. As social network analysis is developed and refined in the context of social learning, it has the potential to be combined with other types of social learning analytic in order to define what counts as a learning tie and thus to identify interactions which promote the learning process. It also has the potential to be extended in order take more account of socio-material networks, identifying and, where appropriate, strengthening and developing indirect relationships between people which are characterised by the ways in which they interact with the same ‘objects of knowledge’ [20]. 
 3.2 Social learning discourse analytics.
 The ties between learners in a network are typically established or strengthened by their use of dialogue. These interactions can be studied using the various forms of discourse analysis that offer ways of understanding the large amounts of text generated in online courses and conferences. For example, Schrire [21] used discourse analysis to understand the relationship between the interactive, cognitive and discourse dimensions of online interaction, examining initiation, response and follow-up (IRF) exchanges. More recently, Lapadat [22] has applied discourse analysis to asynchronous discussions between students and tutors, showing how groups of learners create and maintain community and coherence through the use of discursive devices. Corpus linguistics, the study of language based on examples of real-life use, is a method of discourse analysis that relies heavily on electronic tools and computer processing power. This method employs software to facilitate quantitative investigation of vast corpora including millions of words of both speech and text [23]. Educational success and failure have been related to the quality of learners’ educational dialogue [24]. Social learning discourse analytics can be employed to analyse, and potentially to influence, dialogue quality. The ways in which learners engage in dialogue indicate how they engage with the ideas of others, how they relate those ideas to their personal understanding and how they explain their own point of view. Mercer and his colleagues distinguished three social modes of thinking used by groups of learners in face-to-face environments: disputational, cumulative and exploratory talk [25-28]. Disputational dialogue is characterised by disagreement and individual decisions; in cumulative dialogue speakers build on the contributions of others without critiquing or challenging them. Educators typically consider exploratory dialogue the most desirable because it involves speakers explaining their reasoning, challenging ideas, evaluating evidence and developing understanding together. Learning analytics researchers have built on this work to provide insight into textual discourse in online learning [29, 30], providing a bridge to the world of social learning analytics. A related approach to social learning discourse analytics employs a structured deliberation/argument mapping platform to study what learners are paying attention to, what they focus on, which viewpoints they take up, how learning topics are distributed amongst participants, how learners are linked by semantic relationships such as support and challenge, and how learners react to different ideas and contributions [31]. This approach to overlaying discourse network models on social network models exemplifies what makes social learning analytics distinctive from generic social network analytics, which examine topology but take no account of the quality of stakeholder interactions. 
 4. SOCIALIZED LEARNING ANALYTICS.
 In this section, we consider three kinds of analytic, which although meaningful for an isolated learner who is making no use of interpersonal connections or social media platforms, take on significant new dimensions in the context of social learning. 
 4.1 Social learning content analytics.
 ‘Content analytics’ is used here as a broad heading for the various automated methods used to examine, index and filter online media assets for learners. (Note that this not identical to content analysis, which is concerned with description of the latent and/or manifest elements of communication [32].) These analytics may be used to provide recommendations of resources tailored to the needs of an individual or a group of learners. This is a very fast-moving field, and the state of the art in textual and video information retrieval tools is displayed annually in competitions such as the Text Retrieval Conference [see 33 for a review]. One example is visual similarity search, which uses features of images in order to find material that is visually related, thus supporting navigation of educational materials in a variety of ways, including identifying the source of an image, finding items that offer different ways of understanding a concept, or finding other content in which a given image or movie frame is used [34]. This takes on a social learning aspect when it makes use of the tags, ratings and other data supplied by learners. An example is iSpot, a ‘citizen science’ social media site that helps learners to identify anything in the natural world [35]. When a photo is first uploaded to the site, it usually has little to connect it with other information. The addition of a possible identification by another user ties the image to other sets of data held externally. In the case of iSpot, this analysis is not solely based on the by-products of interaction; each user’s reputation within the network is used to weight the data that they add. This suggests one way in which content analytics can be combined with social network analytics to support learning. Other approaches to content analytics are more closely aligned with content analysis. These involve examination of the latent elements that can be identified within transcripts of exchanges between people learning together online. This method has been used to investigate a variety of issues related to online social learning, including collaborative learning, presence and online cooperation [36]. These latent elements of interpersonal exchanges could also support sentiment analysis, revealing learners’ emotions such as happiness and frustration. It is also possible to draw on the manifest information about user activity and behaviour that is provided by tools such as Google Analytics and userfly.com as well as by the tools built into virtual learning environments (VLEs) such as Moodle and Blackboard. This is the approach taken by LOCO-Analyst, which uses content analysis to establish and investigate semantic relations between different learning resources and to provide feedback for content authors and teachers that can help them to improve their online courses [37]. 
 4.2 Social learning disposition analytics.
 Learners who are prepared to learn and are open to new ideas have the potential to make good use of these resources and tools. A well established research programme has identified, theoretically, empirically and statistically, a seven-dimensional model of learning dispositions, termed ‘learning power’ [38]. These dispositions can be used to render visible the complex mixture of experience, motivation and intelligences that make up an individual’s capacity for lifelong learning and influence responses to learning opportunities [39]. Learning dispositions are not ‘learning styles’, which have been critiqued on a variety of grounds, including lack of contextual awareness [40]. In contrast, an important characteristic of learning dispositions is that they have been validated as varying according to a range of variables [41]. As detailed in [41], a learning analytics platform and visual analytic has been developed to model and assess such dispositions and transferable skills. This visual analytic is used to reflect back to learners their self-perception on these dimensions, providing an explicit language for describing dispositions, catalysing changes in their engagement, activities and approach to learning. From a social learning perspective, three elements of disposition analytics are particularly important. Firstly, they draw learners’ attention to the importance of relationships and interdependence as one of the seven key learning dispositions. Secondly, they can be used to support learners as they reflect on their ways of perceiving, processing and reacting to learning interactions. Finally, they play a central role in an extended mentoring relationship. This type of relationship has an important role in online social learning, especially when learning is informal and not teacher-led. Mentors motivate, encourage, challenge and counsel learners, and can also provide opportunities to rehearse arguments and increase understanding [42, 43]. 
 4.3 Social learning context analytics.
 All these types of social learning analytic can be applied in a wide variety of contexts, including formal settings such as universities, informal contexts in which learners choose both the process and the goal of their learning [44] and in the many situations in which learners are using mobile devices [45]. In some cases, many learners are simultaneously engaged in the same activity, and in other cases learning takes place in asynchronous environments, where the assumption is that is that learners will be participating at different times [30]. They may be learning alone, in a network, in an affinity group, in communities of inquiry, communities of interest or communities of practice [18, 46-48]. ‘Context analytics’ are the analytic tools that expose, make use of or seek to understand these contexts. These analytics may be used alone, or may be employed as higher-level tools, pulling together data produced by other analytics. For example, if network analysis indicates that student Rebecca is on the edge of a community, and dispositions analysis shows that she is currently working on her collaboration skills, then a context focused recommendation might suggest that she could join a teamwork skills group and use analytics visualizations to monitor her position within the group. Several weeks later, she might be prompted to reflect on her collaboration skills and to rate the group. She might receive this prompt directly from the system, or the system could recommend her teacher, mentor or group leader to engage with her and to make the recommendation. 
 5. DESIGN & IMPLEMENTATION.
 Having identified different types of social learning analytic, the challenge is to employ these to analyse learners’ behaviour and to offer visualizations and recommendations that can be shown to spark and support learning. This section focuses on progress towards the implementation of these analytics in a social learning space developed by The Open University, a UK-based university with a strong emphasis on open and distance learning. SocialLearn is a social media space tuned for learning. It has been designed to support online social learning by helping users to clarify their intention, to ground their learning and to engage in learning conversations [49]. The system’s architecture includes a Recommendation Engine, a pipeline designed to process data and output it in a form for analysis by SocialLearn recommendation web services. The second element of SocialLearn’s architecture is the Identity Server that supplies, with the learner’s informed consent, data to the Recommendation Engine. These data include learners’ profiles and activities within SocialLearn, selected elements of their activity at The Open University, and selected elements of their activity and interactions on social media sites such as LinkedIn, Twitter and sites employing OpenID. The SocialLearn Analytics and Delivery Channels depend on the Identity Server to maintain a unified user profile. The final element of SocialLearn’s architecture is the SL Delivery Channel, which includes sites for both input and output. Data are collected, with the learner’s informed consent, from use of the SocialLearn website, from use of the SocialLearn ‘backpack’ (browser toolbar) while elsewhere on the web, from use of SocialLearn applications embedded on external sites and, where applicable, from calls on the SocialLearn application programming interface (API). At the same time, data that has been analysed by the Recommendation Engine may be presented to learners in the form of recommendations or visualizations available on the SocialLearn website, via the SocialLearn backpack, within embedded applications or by ways of calls on the API. Reactions to these visualizations and recommendations, together with options for feedback by learners, make this an iterative process because these responses are fed back via the SL Delivery Channel and influence subsequent output. The architecture is designed to be flexible, so that new algorithms can be added at any time, and analytics can be trialed, developed, combined or set aside without disruption. Sections 4.1-4.6 describe progress towards the implementation of different types of social learning analytic, including work carried out at The Open University and elsewhere that supports the development of social learning analytics, recommendations and visualizations. Work on some types of social learning analytic is still at the stage of planning how work carried out elsewhere might be adapted. In other cases, mock-ups and wireframes are in place or pilot studies are underway. 
 5.1 Implementing social learning network analytics.
 In the case of social learning network analytics, the SocialLearn team is considering the possibilities offered by SNAPP (Social Networks Adapting Pedagogical Practice), a freely available network visualization tool that analyses forum contributions and presents them as a network diagram. Its architects identify uses for such diagrams from the point of view of teachers, including: 
 - identifying disconnected students - identifying key information brokers within a class - indicating the extent to which a learning community is developing within a class [50] 
 In the case of SocialLearn, the intention is to deploy social learning network analytics to exploit data in the Identity Server, in order to support both individual and group recommendations. For example, individuals might see: - One of your key search terms is ‘learning analytics’. This has been mentioned five times in the ‘Future Developments’ thread. View thread? - John Smith has been identified as a key information broker in your network, View John’s most recent posts? - Ten people you have replied to list ‘social learning’ as a key search term. Add this to your key search terms? 
 In learning groups that are not formally led by a teacher, members may share responsibility for welcoming newcomers, engaging all members and encouraging meaningful participation. Social learning network feedback for a group or moderator will seek to use what is known about effective group structure and dynamics and feed this back for reflection [51]. For example: 
 Research shows that effective learning groups tend to be structured like this <network diagram> whereas your group currently looks like this <group diagram>. 
 5.2 Implementing social learning discourse analytics.
 In order to support meaningful participation, SocialLearn is developing two sets of discourse analytics – the first based on the work of Neil Mercer and his colleagues around exploratory dialogue [27], and the second building on development of Contested Collective Intelligence and Concept Mapping to scaffold structured deliberation and argument mapping [52]. Key characteristics of exploratory dialogue include challenge, evaluation, reasoning and extension. Initial research suggests that these are signaled in forum interaction by key words and phrases [29]. For example: ‘alternative’, ‘but if’ and ‘I don’t believe’ suggest challenge; ‘good point’, ‘important’ and ‘how much’ suggest evaluation; ‘next step’, ‘it’s like’ and ‘relates to’ suggest extension, and ‘does that mean’, ‘my understanding’ and ‘take your point’ suggest reasoning. Figure 1 shows how a visualization of these elements of dialogue could be presented to learners, together with recommendations. The coloured shapes in Figure 1 indicate comparative levels of use of different types of dialogue. In this case, indicators of reasoning, evaluation and extension appeared several times within the learner’s discussion and are represented by green squares. Only one challenge was detected, and this lower level is represented by a yellow circle. The final sentence, ‘Positive challenges…’ suggests ways of increasing indicators of exploratory dialogue. 
 Figure 1: Visualization of learner’s use of indicators of exploratory dialogue. 
 This example focuses on a single learner. A group version of the visualization could be used to represent the dialogue of a group or a thread, with the aim of achieving a more widespread shift in the quality of the learning dialogue. Explicit semantic networks provide a computational system with a more meaningful understanding of the relationships between ideas than natural language. Following the established methodological value of Concept Mapping [53], the mapping of issues, ideas and arguments extends this to make explicit the presence of more than one perspective and the lines of reasoning associated with each. In a comprehensive review of computer-supported argumentation [54], Scheuer et al concluded that studies have demonstrated ‘more relevant claims and arguments… disagreeing and rebutting other positions more frequently… and engaging in argumentation of a higher formal quality.’ However, appropriate tools need to be part of an effective learning design: “The overall pedagogical setup, including sequencing of activities, distributions of roles, instruction on how to use diagramming tools, usage of additional external communication tools, and collaboration design, has an influence on learning outcomes” [54] 
 On this basis, Cohere is being developed to interoperate with SocialLearn. As preliminary results show [31], this holds the promise of giving the platform access to proxy indicators of participants’ attitudes towards the topic under discussion, and of the roles they play within the group (e.g. forging meaningful links between peers’ contributions, or a tendency to challenge others). This provides the representational basis to automate recommendations that encourage new approaches to a given subject, either by providing links to resources that challenge or extend learners’ point of view, or by providing links to other groups talking about the same subject or resources but in different ways. 
 5.3 Implementing social learning content analytics.
 When viewing online resources, SocialLearn’s ‘Backpack’ – a toolbar of apps and resources – can be used on any Internet site. The Backpack currently includes the basic components of social learning content analytics. Clicking on the Backpack’s light bulb icon provides the option of viewing the keywords, hotlinks or images connected with the open web page (as in the large box on the right of Figure 2). This information about images can be combined with visual similarity search to identify and recommend other resources that make use of these images, for instance: - This image appears to be The Mona Lisa, and is used twice in this Renaissance 101 lecture webcast [view] - This image appears to be Steve Jobs, and is used in the following blogs by academics in Design faculties [view] 
 Figure 2: The SocialLearn Backpack open at the foot of a BBC News page, showing a list of images on the page. 
 As SocialLearn develops, it will be possible to refine these recommendations, based on the number of users following or recommending links or on the relevance of key words on a site to the key words associated with individual learners or groups of learners. In the case of learners who are developing the learning dispositions of resilience and critical curiosity (the desire and capacity to be taken out of one’s comfort zone, and to dig beneath the surface), these analytics could recommend online resources that both stretch a learner to a new level and which are rated as rewarding investigative skills. 
 5.4 Implementing social learning disposition analytics.
 Theoretical and empirical evidence in the learning sciences substantiates the view that deep engagement in learning is a function of a complex combination of learners’ identities, dispositions, values, attitudes and skills. When these are fragile, learners struggle to achieve their potential in conventional assessments, and critically, are not prepared for the novelty and complexity of the challenges they will meet in the workplace, and the many other spheres of life which require personal qualities such as resilience, critical thinking and collaboration skills. As detailed in an accompanying LAK paper [41], learning dispositions can be modelled as a multi-dimensional construct called Learning Power, currently assessed by learner self-report via a web questionnaire called ELLI (Effective Lifelong Learning Inventory), whose data warehouse platform supports a range of analytics. ELLI has been extensively validated, and is now being piloted within The Open University [55]. ELLI generates a spider diagram visual analytic which is used to support self-reflection and change. Figure 3 suggests how these meta-cognitive processes could be supported within SocialLearn. The ELLI profile generated by completing the self-report questionnaire appears at the top of Figure 3. In this case, the learner saw herself as fairly strong on changing and learning, learning relationships and meaning making, but her resilience was low at that point. The central text indicates that, within a mentored discussion, she agreed that she would work on this area. Working to develop resilience involves accepting that learning can be hard for everyone, taking on a challenge and persisting even when the outcome and the way ahead are uncertain. The ELLI Spider at the foot of Figure 3 visualizes her progress since the mentored discussion. Red triangles would indicate no activity on a dimension, yellow squares signal some activity and green circles indicate the learner has been very active in an area. The ELLI Spider is fed by self-report data (for example, within a learning blog [56]) and by data about activity and interactions that is processed within the Recommendation Engine and provided via the Identity Server and the Delivery Channel. We are now operationalising the dimensions against candidate activity traces that could signify them. For example, indicators of growing resilience that could be fed back to the learner, rendering the Recommendation Engine’s rationale transparent: - You seem to be making progress in building your learning resilience. Last time you declared yourself Stuck on a path you did not return to it. This time you returned to Step 3 on Photosynthesis 101 after a week and, after requesting help, solved the problem. 
 Figure 3: ELLI Profile (top) visualizing results of most recent self-report questionnaire. ELLI Spider and summary (bottom) highlighting recent work on dispositions. 
 5.5 Implementing social learning context analytics.
 Within SocialLearn, dispositions analysis and subsequent activity are among the data items that feed into the Identity Server and Delivery Channel, Together, the server and channel provide data about a learner’s current context, including goals, activities, group membership and learning roles. A future SocialLearn app will make use of this data, adding geolocation to the mix – to produce recommendations tailored to the learner. Figure 4 shows a mock-up of the SocialLearn app, currently under development, which will recommend and provide access to learning materials in response to search terms. The app will allow resources to be rated and recommended to individuals or to groups. If users choose to make their location data available, this can be used to influence recommendations, For example, if Simon is working on ‘Climate Change’ the app might suggest a podcast on coastal defences when he visits a seaside resort, and could provide a map showing a local site where Simon would be able to view the effects of erosion. 
 5.6 Different dashboard views.
 Because SocialLearn is designed to work in a wide variety of contexts, users are likely to move between roles while using it. At some points they will be learners, at others mentors or teachers and at others group leaders or administrators. In many cases this will involve tailoring recommendations and visualizations to take into account these different roles. The intention is to provide different dashboard views of analytics. Figure 5 (below) shows what an individual learner’s dashboard could look like – providing ‘Kris Mann’ with an overview of different analytics and recommendations. If Kris were mentoring someone, he would also have agreed access to elements of their dashboard and could rate the system-generated recommendations and add his own. As a teacher, he would need an overview of his pupils’ analytics and recommendations, with clear visualizations and teacher recommendations helping him to find his way through these. In the role of group leader or administrator he would need an overall view of group activity and dialogue, without needing a breakdown of individual learners’ activities. The challenge is to provide sufficient dashboard options to meet users’ needs without overburdening them with possibilities. 
 Figure 4: Mock-up produced by designer of SocialLearn app making use of social learning context analytics. 
 Figure 5: Mock-up of SocialLearn dashboard for an individual learner. 
 6. CHALLENGES AND POSSIBILITIES.
 All the social learning analytics described here are currently under development. An initial and ongoing challenge is to gain learners’ informed consent to their data being used to support these analytics. Data harvesting on websites generally goes unnoticed, it is often only by looking at the list of cookies stored on our computer that we realise how much information about our activity is being gathered, analysed, bought and sold. In the context of education, analytics are likely to include sensitive information about identity, status, background and achievements. Ethical use therefore involves making users aware of the data that is being collected, how it is being used and who has access to it. This is difficult to do clearly and concisely, giving users sufficient privacy options without overwhelming them. At this early stage, a second challenge is to experiment with and refine these analytics, while continuing to provide a supportive experience for learners. In the case of social learning discourse analytics, for example, the correlation of words and phrases with elements of exploratory dialogue needs to be investigated in more detail. In the case of social learning dispositions analytics, we need to be clear which levels of activity should prompt colour changes in the visualization, signaling a move from low to high levels of activity. Each area of social learning analytics requires further research in order to optimise support for learners.A final challenge is to ensure that these analytics, recommendations and visualizations spark and support learning. There is a danger that learners could be overwhelmed and discouraged by the amount of information presented to them, confused by being presented with too many visualizations, or misled by system-generated analytics. The SocialLearn research programme therefore works in the case of each analytic from a theory of how learning can be triggered or improved. It then develops an appropriate analytic and monitors what happens when it is implemented, looking not only for the predicted positive changes, but also for any significant changes. At this stage, the challenge is to improve on or refine the analytic and how it is presented to learners. 
 7. CONCLUSION.
 Social learning analytics make use of data generated by learners’ online activity in order to identify behaviours and patterns within the learning environment that signify effective process. The intention is to make these visible to learners, to learning groups and to teachers, together with recommendations that spark and support learning. In order to do this, these analytics make use of data generated when learners are socially engaged. This engagement includes both direct interaction – particularly dialogue – and indirect interaction, when learners leave behind ratings, recommendations or other activity traces that can influence the actions of others. Another important source of data consists of users’ responses to these analytics and their associated visualizations and recommendations At present, we are focusing on the five broad categories of social learning analytic described in this paper: network analytics, discourse analytics, content analytics, dispositions analytics and context analytics. The Open University is currently developing these within SocialLearn, which provides a technical architecture enabling different analytics and recommenders to be deployed. Their initial deployment in 2011-12 is part of a research programme at the university, focused on the effective use of social learning analytics through evaluation of both their use and their effects. In addition, the research programme is beginning to address some of the important challenges relating to the ethical use of data to support learning. 
 8. ACKNOWLEDGMENTS.
 We gratefully acknowledge The Open University for making this work possible by resourcing the SocialLearn project.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Social Learning Analytics: Five Approaches</rdfs:label>
		<dc:subject>social learning</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>discourse analytics</dc:subject>
		<dc:subject>learning how to learn</dc:subject>
		<dc:subject>transferable skills</dc:subject>
		<dc:subject>21st century skills</dc:subject>
		<dc:subject>educational assessment</dc:subject>
		<dc:subject>social learning analytics</dc:subject>
		<dc:subject>sociallearn</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/35/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/36">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Learning analytics – Challenges, paradoxes and opportunities for mega open distance learning institutions</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/36/authorlist"/>
		<swrc:abstract>Despite all the research on student retention and success since the first conceptual mappings of student success e.g. Spady [12], there have not been equal impacts on the rates of both student success and retention. To realise the potential of learning analytics to impact on student retention and success, mega open distance learning (ODL) institutions face a number of challenges, paradoxes and opportunities. For the purpose of this paper we critique a ‘closed’ view of learning analytics as focusing only on data produced by students’ interactions with institutions of higher learning. Students are not the only actors in their learning journeys and it would seem crucial that learning analytics also includes the impacts of all stakeholders on students’ learning journeys in order to increase the success of students’ learning. As such the notion of ‘Thirdspace’ as used by cultural, postmodern and identity theorists provide a useful heuristic to map the challenges and opportunities, but also the paradoxes of learning analytics and its potential impact on student success and retention. This paper explores some of these challenges, paradoxes and opportunities with reference to two mega ODL institutions namely the Open University in the UK (OU) and the University of South Africa (Unisa). Although these two institutions share a number of characteristics, there are also some major and important differences between them. We explore some of the shared challenges, paradoxes and opportunities learning analytics offer in the context of these two institutions.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 One of the basic premises of learning analytics is that if higher education institutions optimise and analyse the data they hold on their students, they can identity and (more) effectively and appropriately address the challenges that students face, whether they are at risk, underprepared or high performance students. Siemens [10] suggests that learning analytics refers to ‘student-produced data and analysis models to discover information and social connections, and to predict and advise on learning’ (emphasis added). It is true that students produce data and leave trails that higher education institutions may not fully exploit. To focus only on the data trails which students produce may result in the incomplete assumption that they are the primary actors in their learning journeys. Students’ trails and data regarding their activities, actions or non-actions are a useful baseline, but often institutional decisions, efficiencies and non-action on the side of the institution impact equally on students’ choices, and their actions or non-action. This latter perhaps falls into the category of academic rather than learning analytics, though both approaches have many overlapping elements and both are relevant here. We do not see learning analytics as the panacea which will solve all the complexities in understanding student success, attrition or retention. Several authors [7, 8] have cautioned that learning analytics can very easily serve to increasingly bureaucratise students’ learning even further, or serve a panoptical purpose and culture of increasing surveillance rather than empowering students and their institution to facilitate more appropriate choices. In this paper we present two case studies and propose that learning analytics can at least support student success if we consider that both students and institutional data trails are found in the ‘student walk’ as the space where these two actors meet in a ‘Thirdspace’ (as described by various authors) [2, 4, 6, 9, 11]. Our discussions of the potential of learning analytics to help map and engage with this ‘Thirdspace’ are set against the concerns expressed by Tinto [15] who bemoans the fact that, despite all the research done since the first conceptual mappings of student success and retention, the impact on success and retention rates has been minimal. Tinto [14, 15] and others [5] suggest that student departures are more of a ‘puzzle’ than we (currently) accept, and that knowing why students fail does not give us an equal understanding of why students persist or stay despite failing. In their attempt to unravel the ‘student departure puzzle’ [5, 13] indicate that student success and retention is a multidimensional phenomenon where a number of interrelated and often interdependent variables meet in complex relationships. In their socio-critical model for understanding and predicting student success, retention and throughput, Subotzky and Prinsloo [13] aim to provide a conceptual map to identity measureable and actionable data in contrast with data which may ostensibly shape student success, but which are outside the locus of control of both students and the institution. They propose that three interrelated and often interdependent levels of factors impact on student success namely: individual (academic and attitudinal attributes, and other personal characteristics and circumstances); institutional (quality and relevance of academic, non-academic, and administrative services); and supra-institutional (macro-political and socio-economic factors) [13]. Subotzky and Prinsloo [13] propose a number of constructs that underscore their socio-critical conceptual model; and which we find useful as a shared point of departure for our comparative analysis and discussion of the role of learning analytics in mega open distance learning institutions. The constructs are as follows: 1. Students and the institution as situated agents: ‘Success is seen as the outcome of the mutually inﬂuential activities, behaviours, attitudes, and responsibilities of students and the institution’. The situatedness of their agency relates to the ‘structural conditions of their historical, geographical, socio-economic, and cultural backgrounds and circumstances’. Within these structural constraints, both students and the institution are agents, and not merely passive recipients or providers of services. 2. The second construct is that of the ‘student walk’ which embodies the mutually constitutive interactions and relations between students and the institution. 3. The notion, amount and role of ‘capital’ – whether social, epistemological, intellectual or other forms of capital – provides a basis for understanding the socio-critical nature of the ‘student walk’ where mutual engagement and transformation are shaped by engagement and exchanges. 4. The fourth construct refers to the impact of habitus on the agency of both students and the institution, where habitus refers to refers to socially acquired, embodied systems of dispositions and/or predispositions [3]. 5. The fifth construct is the notion that both students and the institution have inter and intra-relational aspect shaping their agency. Students’ intrapersonal relations are shaped by self-efficacy, attribution and locus of control, while their interpersonal relations are the multiple networks impacting and shaping students’ learning. Self-efficacy, attribution and locus of control also apply to the institution within three different domains namely academic, administrative, and non-academic social domains of institutional life. 6. Student success, as a final construct is more broadly constructed than just course success, but also refers to students’ satisfaction with their learning journeys, optimal ‘fit’ between their aspirations and abilities and the institution’s offerings. Student success can also imply not graduating or completing their initial educational aims. Though the detail of the mapping of students’ journeys differs between the OU and Unisa, the constructs developed by Subotzky and Prinsloo [13] encompass, from our understanding, a shared basis for our continued exploration. In determining the potential for analytics to help us make sense of students’ journeys through a ‘Thirdspace’, we must also accept that it is not always feasible, from a student or from an institutional perspective, to act on what the data may be telling us. 
 2. THE STUDENT JOURNEY AS ‘THIRDSPACE’.
 The conceptual model described above illustrates a ‘Thirdspace’, a mostly temporary nexus where students and the institution engage. In a certain sense, this nexus of engagement is a temporary diasporic space for both students and institution. The notion of ‘Thirdspace’, ‘liminal’ or ‘diasporic’ space is used in a range of contexts such as identity, multicultural, phenomenological geography and identity theories discourses by authors such as Bhabha [2], Brah [4] and Soja [11]. Soja [11] describes the Firstspace as the material world in which individuals and communities live; Secondspace as their mental world of beliefs, assumptions and epistemologies. Thirdspace is the space where these two worlds merge and become one temporal space. In the work of Bhabha [2], third space functions as a space where individuals negotiate and renegotiate their assumptions, beliefs, identities in a constant space of becoming. The notion of ‘Thirdspace’ is not commonly used in describing the engagement between students and institutions, except for by Burnapp [6], Whitchurch [16] and in an indirect sense, Barnett [1]. Burnapp [6] uses the notion of the ‘Thirdspace’ in describing international student experiences whilst Whitchurch [16] uses it to describe the fluidness of academic identity in a digital age. Barnett [1] refers to the notion of a ‘third world’ where students find themselves in their trajectories of ‘being and becoming’. In this so-called ‘third world or Thirdspace, students have left the known pre-enrolment spaces and move into a space where their identities, epistemologies and ontologies are shaped by their engagement with academic and professional discourses. A student enrolling in higher education moves from often a highly structured ‘place’ to an undefined and liminal and unstructured space’ [6]. In this ‘Thirdspace’ students are caught in a liminal space between what they were and what they are becoming. They may be labelled as ‘underprepared’, ‘at risk’, ‘illiterate’, or ‘deficient’ – and blamed for not ‘fitting in’ into the world of higher education. Early conceptual models attempting to understand and map student success and retention disproportionately emphasised the responsibility of students to fit in, to prepare for and ensure that they are sufficiently assimilated and integrated into the epistemologies and ways of being required by the higher education of their choice (see for example [5], [13]). The ‘student walk’ as Thirdspace is a temporary space where yet another identity construct and role are imposed on students. This new identity shapes and is shaped by their other identities as mothers, professionals, etc. Students and especially distance education students in ODL settings do not leave their other identities ‘outside’ of their learning, but rather find them in ever-increasing networks of identity constructs. On the other hand, students’ engagement with their studies and institution has the potential to shape their multiple identities in often profound ways. This ‘Thirdspace’ also has implications for the institution which provides learning based on students choices, prior knowledge and aspirations. The success of the ability of the institution to match the aspirations, prior knowledge and levels of preparedness of students has a profound impact on the success of students, attrition and throughput rates. Although this ‘Thirdspace’ is actually, in the context of ODL, a ‘non-place’ or a space of ‘placelessness’ [6], students and other institutional stakeholders leave traces which, if harvested, can help us to understand the complexities of student success, attrition and throughput. Using the actionable intelligence provided by learning analytics allows this ‘Thirdspace’ to be a safe and critical ‘non-place’ of becoming. We suggest that the notion of a ‘Thirdspace’ provides useful pointers for understanding the potential of learning analytics in higher education institutions and more particularly, in mega ODL institutions. We now turn briefly to providing short overviews of two different ODL contexts as basis for our exploration of the challenges, paradoxes and potential of learning analytics. 
 3. ANALYTICS AT THE OPEN UNIVERSITY: A SHORT CASE STUDY.
 The OU supports around 200,000 students each year and collects vast amounts of data about its students, the majority of which is been collated and disseminated to academic units and support departments by a central unit. This unit provides several services in support of the University in supplying external reports and in helping internal staff to better understand student cohorts: 
 - Providing information systems and easy access to student retention and progression data, and demographic profiles. - Delivering and reporting internal and external institutional surveys (student feedback). - Disseminating institutional data and information analysis. - Collaborating internally to undertake ad-hoc projects aimed at enhancing the quality of the student experience. - Supporting internal review processes and external audits. 
 Academic teams typically make use of faculty or module level information to inform curriculum design, for example, by using feedback from surveys sent to students at the end of their module. Other datasets relating to points of withdrawal and student such information have been used to create, for example, a single University-wide model of vulnerability based on historical shared student characteristics. At a very broad level then, the OU has made good use of ongoing data to make adjustments to curriculum design and to form a view of how to provide effective student support. This understanding is well communicated and has provided a shared understanding of a model of support as a generic ‘good fit’ for all students. Since 2005, the OU has captured all outward and inward communications with students and tutors. Currently, over 7.5 million contacts have been recorded, each categorized to reflect the nature of the contact and the resultant outcomes. Until recently, this dataset has largely been a repository for student information and has not been widely exploited to extract cohort information, patterns of behaviour or useful insights into commonalities between programmes of study, approaches to assessment and modes of delivery. In the last two years, greater use has been made of this information and data captured at registration, to develop a fuller understanding of the reasons which lead to student contact and the triggers for student behaviours, which can then be matched to a variety of anticipatory support behaviours. In addition, much work has been invested in the OU’s ability to interrogate its Moodle-based VLE system to track student behaviour and engagement on and between modules. The OU is now moving toward a tailored, at scale and largely automated approach to student support that does not assume that a single model of support fits all, but allows curriculum-based support teams to provide the most time effective, appropriate support for their own student cohort. 
 4. ANALYTICS AT THE UNIVERSITY OF SOUTH AFRICA: A SHORT CASE STUDY.
 Unisa reports to various national higher education and legislative networks on student throughput, module success, and attrition. Most of the data required relates to programme cohort analysis, though analyses regarding student profiles and success in individual modules are also available. Analyses are also available on request by departments, schools or individual lecturers. Until recently, most of the analyses were used by institutional structures for operational planning purposes, and, to a lesser extent, by departments and/or individual lecturers in planning module specific interventions or teaching strategies. Up to 2010, academic and learning analytics at Unisa remained fragmented. There was no coherent and shared understanding of student success as a phenomenon, nor any committee or task team that were either representative of all stakeholders involved in the development, delivery and support of teaching and learning; nor having access to appropriate analyses of institutional and module (course)-specific trends. Different departments responded in their individual capacities to increase student success and retention. Compounding the impact of this fragmented approach was the fact that the analyses conducted focused more on cohort analyses in programmes and institution-wide trends, and not necessarily at module level. In addition, institution-wide interventions and strategies impacted on individual modules with no input from the academic and tutoring staff involved in those modules. However, 2010 saw a major change in the institutional comprehension of the role and impact of learning analytics. Three major developments emerged, namely 
 1. The development and formal acceptance of the socio-critical conceptual model [13] has provided Unisa with an integrated and shared framework for understanding and predicting student success and retention. While there was a general understanding of the notion of the ‘student walk’ or ‘student journey’, there was no clear understanding of the complexities facing both students and the institution in their reciprocal engagement in a ‘Thirdspace’. Successful implementation of a framework will hugely depend on the role and function of learning analytics. Currently the main centralised sources of student data are: 
 - Information provided by students during the application and registration processes - Submission of assignments - Financial interactions with the University - Student activity on the learning management system 
 Other data sources, for example, interactions with tutors or support staff, are not centrally recorded. 2. The second major development in the context of realizing a future for learning analytics is the development and piloting of a ‘student tracking system’. The aim is to map student risk on all currently held historical data. This system will eventually house and track all interactions between students and the institution and generate automated (where appropriate) and personal proactive and reactive responses. 3. The third and final development realizing the potential of learning analytics is the formation of a Student Success Committee. This comprises the major role-players dealing with student retention and success ranging from Senate to administrative, professional and academic departments. 
 5. CHALLENGES, PARADOXES AND POINTERS.
 From the above case studies, the following issues emerge: - Both institutions (like most other higher education and ODL institutions) have huge student datasets. - Perversely, the sheer volume of available data can act as a constraint rather than as an enabler of better understanding both student and institutional behaviours. - At present it is not clear whether the two institutions in question fully understand or have a conceptual map of how the data is used, by whom, and for what purpose. - Both institutions provide analytical services to a range of customers but may need a meta-picture of how data is used and the impact of different strategies based on analyses. - How do overarching institutional goals, for example, widening participation and open access act with or against messages provided by analyses? That is, mega ODL institutions are often balancing conflicting drivers. - While the two institutions in question have different structures and different approaches to the notion of cohort, it is not clear whether there is an institutional perspective which makes sense of cohort and module specific trends. - It is not clear how the results of analyses flow through the organisation, that is, to individuals or support departments and back? - Monitoring and evaluation of support systems based largely on the output of an analytics approach needs to be ongoing for support systems to remain effective and optimal. Such analyses are time intensive. - Both institutions encourage the scholarship of teaching and learning to increase evidence-based approaches to interventions aimed at improving student retention and success. How best then to capture and integrate scholarship practices into institutional sense making processes? - Academics involved in such scholarship may find their efforts to change delivery and teaching strategies based on found evidence frustrated by a lack of institutional support. - Although the data may suggest tailoring, it is not practical for mega ODL institutions to have a multitude of differentiated support systems in place. 
 There are however also some pointers for consideration. The use of analytics at all levels would be more successful if founded on a shared and institutionally-accepted conceptual understanding of the nexus or ‘Thirdspace’ of student and institutional interaction. Analytics should provide an integrated, coordinated and holistic platform for all stakeholders to make sense of and find their own way in supporting student learning and institutional efficiency recognising interrelations and interdependencies. It falls outside of the scope of this paper to argue for a centralised or decentralised approach to analytics, but rather to point to a need for an integrated, coordinated and holistic approach involving all stakeholders who can contribute or use the analyses. 
 6. CONCLUSIONS.
 Learning analytics aims to help us to teach more effectively by providing us timely and appropriate actionable data on which to make choices regarding pedagogy, assessment strategies, student support interventions and use of technology to mention but a few. Using the notion of ‘Thirdspace’ to describe the space where students and institution meet, we explored some of the challenges, paradoxes and potential for learning analytics to better support learning outcomes and student success. If learning analytics is considered only as a tool, then simply having more information about our students may not necessarily change the way we teach. There would be a danger that learning analytics might become part of the broader bureaucratisation of student learning. If however, learning analytics is embedded in organisational culture, systems, and processes, there is the potential to really impact and shape our approaches to student needs, whether as individuals or as groups. Learning analytics is an essential tool for mega ODL institutions for personalising learning as far as possible for very diverse groups of students with even more diverse prior experiences, contexts, aspirations and futures.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Learning analytics – Challenges, paradoxes and opportunities for mega open distance learning institutions</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>open university (ou)</dc:subject>
		<dc:subject>student walk</dc:subject>
		<dc:subject>thirdspace</dc:subject>
		<dc:subject>university of south africa (unisa)</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/paul-prinsloo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/paul-prinsloo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-slade"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-slade"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/fenella-galpin"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/fenella-galpin"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/36/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/paul-prinsloo"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-slade"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/fenella-galpin"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/37">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Predicting failure: A case study in co-blogging</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/37/authorlist"/>
		<swrc:abstract>Monitoring student progress in homework is important but diﬃcult to do. The work in this paper presents a method for monitoring student progress based on their participation. By tracking participation we can successfully create a model that predicts, with very high accuracy, if a student is going to score a low grade on her current assignment before it is completed, thus enabling selective interventions.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Monitoring student progress on homework is important. It is however time consuming and not always accessible – by deﬁnition homework is done at home, away from most monitoring methods. As early as possible, the instructor wants to detect those students who are not doing their homework, identify why, and hopefully help the student resolve the issue preventing her from successfully completing it. One indicator that a student is doing well is her participation in class. However, even if a student participates in class, she may be too busy or lack the motivation to apply herself to the homework. In-class exercises enable the teacher to do some observation of each student’s progress, but as classsize increases, ﬁnding struggling students becomes diﬃcult – they get lost in the crowd. The ideal situation would be to somehow automatically monitor student progress on homework and give timely feedback to the instructor or student about possible problems. In a regular classroom, it is diﬃcult to imagine how one could automatically monitor student progress. For students engaged in online learning activities, several options open up. For example, the work of Campell et al [3] develops data mining techniques, that look for students that are struggling or at-risk of failing, which trigger alerts of potential problems to instructors. This approach to predicting failure enables teachers to quickly sort out students who are having problems. While this kind of automatic assistance for early identiﬁcation of struggling students is helpful it still falls short of what would be ideal; rather than waiting until the grades come in, the system should monitor and report progress while the students work on the assignment. The work presented in this paper develops a model of student progress based on student participation in the an online collaborative learning activity. Participation and collaboration can be keys to online learning [8, 10] and tracking it could thus be a predictor of success or failure. Participation can be directly tracked from an activity log of an online learning system. However, For some learning activities, a student can do a lot of work oﬄine that is not visible to an online activity logger. What kind of activity is a good measure of participation and a good predictor of performance? In other words, what kinds of participation can I observe and are they important and predictive? This short paper will present a method for monitoring student eﬀort while working on a homework assignment by tracking participation. A multiple regression model is presented. It combines the current level of participation for an assignment and prior grades to predict if each student is on a path to succeed or fail with her current assignment. We describe student co-blogging as a source of data – during a semester-length course, students did their homework in a blogosphere. 
 2. BACKGROUND.
 Information technology (IT) is widely used in the classroom; in some communities students expect the classroom to include IT [16]. A range of technical inclusions can have impact, from the simple introduction of internet resources into the classroom to more advanced technology – like computersupported intentional learning communities [12] – that expand the learning context while enhancing lessons [11]. The most relevant content is available in the course material, but these other resources and modalities of learning have additional value. Availability does not necessarily translate into eﬀective use. For example, searching the Internet for additional content requires skill [15]. The successful integration of an online learning environment into the ﬂow of a course depends on more than just technical skill. The payoﬀ for students spending time online has to signiﬁcantly exceed the costs of getting them online. Once the students are online, it is important to monitor their performance so that students who are not succeeding can be identiﬁed. This problem exists oﬄine too, but for an online learning activity alternate automatic or semi-automatic methods could be implemented to track student performance. Early detection of failure is an important problem for which a learning analytic approach may prove to be useful [1, 3]. Participation is a metric that can be used to gauge student progress. It is an important factor in collaborative online learning environments [6]; lack of participation is a risk factor for failure [7]; promoting participation is an eﬀective method for improving student outcomes [11]. Learning environments that enable students to collaborate depend on grounding, mutual understanding and background [2, 5]. In order to track user participation in an online learning environment we need reliable participation data. The collaborative environment is that platform because its content is not available oﬄine. Our study uses one type of collaborative learning environment [4], student co-blogging. 
 3. CASE STUDY.
 3.1 The co-blogging activity.
 The data we present was collected from a course on Human Computer Interaction (HCI). There were about 50 students in the class: a mix of undergraduates and Masters students. In a student co-blogging community, each student had her own blog. During the semester, each student regularly posted to her blog. Students also browsed in the course blogosphere, read peer contributions and commented on them. Homework assignments were weekly blog posts about various methods used in HCI. Students, for example, did needs analysis, generated data gathering plans, or did expert reviews. The assignment was the same for all the students, but each student applied the methods to a diﬀerent website, software, or device of her choice and posted about it in the course blogosphere. Students were encouraged to read freely in the blogosphere throughout the semester. While working on an assignment, students were allowed to review the posted work of other students and revise their own posts up until the deadline. In this manner, the co-blogging environment is a platform for peer tutoring, peer assessment, and cooperative learning [14]. This has some similarity to peers getting together and discussing homework but then separately writing their own solution. After the submission deadline, the TA assigned to each student two posts to critique; the critiques were then due a few days later. Students were also encouraged to do additional commenting and respond to comments on their own posts. The critique part of the process is more about selfassessment [13]; a survey of the class suggested that students found writing critiques more useful than getting them. There was incentive to create high quality posts in the blogosphere. Earlier posts could be used as a reference for later assignments. Learning how to apply the methods from previous assignments saved time when doing a later assignment. By only having low quality earlier posts to go back to, much of the work would have to be done again. The student gains less by not adequately learning the material when they are ﬁrst exposed to it, and later activities beneﬁt less from blogosphere content because the quality is, as a result, not as good. 
 3.2 The technology.
 The co-blogging system was developed by the primary author of this paper. Users can preview a post by hovering over its title and open a post to view its contents and any comments it has accrued – these are all diﬀerent kinds of participation. Students could ﬁlter blog posts by users (view all posts by the selected user), assignment tags (each homework assignment had a diﬀerent tag associated with it) or view only the posts that were top-rated by the instructors. An author was notiﬁed if his post received comments. Most of the time students would browse the most recently updated posts. This meant that both good and bad posts were regularly read and were likely to receive comments regardless of their quality. 
 3.3 Grading.
 Posts and critiques were graded by several TA’s on the scale of 0-3 for posts and 0-2 for critiques, where 0 means the assignment was not completed, 1 indicates not good , 2 is good, and 3 is exemplary work. The scale for critiques was 0 for not completed, 1 for not constructive, and 2 for a constructive and good critique. 
 4. PREDICTING FAILURE.
 We regard reading content generated by a peer to be a form of peer learning and as such it has tremendous educational value. There were many ways in which having access to the progress of other students could prove valuable. By reading in the blogosphere, a student could get help in interpreting the homework requirements, she could get started on a diﬃcult part of the assignment, she could look at formatting and presentation, or she could verify or check her answer. For these reasons, it was assumed that reading in the blogosphere was the most signiﬁcant form of participation. It is also worth noting that a student cannot be oﬄine and leverage peer content eﬀectively. Doing so would require the student to save and maintain a synchronized version of the online content. The eﬀort of opening a post and saving it would be logged as participation anyway. An automatic version of this process would still be more eﬀort than simply accessing the content in the blogosphere itself. For these reasons reading in the blogosphere is a reliable measure. Writing is not as good a predictor because, for example, some students draft their work outside of the blogosphere. In what follows, grades are paired with participation. The relationship between average participation and average grades is explored for all assignments and each assignment. A multiple regression model that combines participation with average previous grades is presented. The application of the model shows a signiﬁcant positive relationship between participation and grades: the more a student participates (reads) the more likely it is that she will receive a high grade on the homework assignment. This relationship holds for all the assignments together, and any individual assignment. Thus the relationship is positive, signiﬁcant and consistent. 
 4.1 The Relevant Student Participation.
 We tracked student activity while the students were writing their posts to predict success before their homework was turned in for grading. We used the simplest form of participation where we counted the number of times each user read a post created by another user. Regardless of the time spent reading that post (time until next link click happens) or how many times a particular post was read. Clicking a link to open a post written by a peer is just counted as one read in terms of participation in the blogosphere. User participation at any time is the total number of reads. Reading enables a student to make use of content generated by his peers. The user can discard the content as useless or accept it as helpful to whatever purpose the user had in mind when clicking the link. Posts can be previewed which means that users might have some idea about the content of the blog post before actually clicking the link. This preview is not counted as participation for the purpose of our model. 
 4.2 Pairing participation with grades.
 The relationship of interest is between participation and grades. We hypothesize that students who learn from the contributions of other students, as measured by their participation, produce higher quality solutions, as measured by their grade. Using this data, we then want to predict, before they are graded, if they are going to succeed or fail. The data we explore is from the ﬁrst six homework assignments for the class. It is generated by scanning the coblogging activity log which stores the user name associated to each action, the URL requested, and the date of request. The log is counted for reads by each user, in each homework assignment, for every post that has a matching homework tag, and was written by another user. This gives each user a participation number for every homework assignment that is paired with the grade of that same assignment. The distribution of the participation numbers was skewed towards several very high activity users so the log transform of the number was taken to remove the outlier eﬀect. The resulting participation distribution was close to being normally distributed (mean=1.5, standard deviation=0.36). 
 5. CREATING A MODEL.
 Participation is the number of times each student reads a post that was written by another student. Average grade is the combined grade of post and comments for all assignments; the maximum grade for any assignment was 5, combined 3 for a post and 2 for a critique. Average participation was highly positively related with average grade (r=0.7, p=0.00000003). Participation for individual assignments was also signiﬁcantly related with the grade for each assignment (lowest r=0.41 with p=0.0038) (see Figure 1). 
 6. APPLYING THE MODEL.
 We use existing data to create a multiple linear regression model to predict failure for the homework the student is currently working on. We use linear models composed of current participation data and previous grades to calculate the expected grade. The hypothesis is that previous grades and the participation in the current homework can predict the grade received for that assignment. For example, if the student is working on homework number 3, the average grades of the previous two assignments and the student’s participation for homework number 3 are used as predictors in the multiple linear regression models (see Figure 2). 
 Figure 1: Individual assignment model. 
 We create one model for each homework. The models use current participation and the average of previous grades. Prior grades do not have a big impact in the ﬁrst couple of regression models but by the third prediction model previous grades start to have predictive value. Participation remains an important part of the models throughout. It is important to note that no alarms about grades were sent to students. If a notiﬁcation would have been sent to students with a predicted grade of less than 3 out of 5, which is close to a failing grade of <2.5, then the intervention would have had the following eﬀects: - Average alarm rate: 26% – Percentage of students that are predicted to score less than 3 out of 5. These are the number of students that would receive a notiﬁcation of possible homework failure. - False alarms: 10% – Students that would have been notiﬁed but even without the intervention, did not fail. - Average miss rate: 6% – Students that the model did not predict failing and without the intervention actually scored a low grade. - Unexplained miss rate: 0.83% – Out of 240 grade predictions only two students scored a low grade and would not have been notiﬁed. The explained misses were students that only submitted a partial assignment (usually forgot to give critiques). There are diﬀerent kinds of alarms that can go oﬀ for students nearing deadline without having a submission so we can only consider them to be possible misses by our model. 
 Figure 2: Grade and Participation model coeﬃcients and p-value. 
 7. CONCLUDING REMARKS.
 Participating in a collaborative learning environment enables the exploration of peer created content. A student posts drafts in the blogosphere, reads drafts of the same homework by other students and has the option of leaving comments and marking those she especially likes. By exploring how peers approach the assignment, format the answer or solve a particular problem, she is contributing to a common resource – a non-rivalrous resource [9] – that is usable later in the semester to solve diﬀerent problems with the same method. We have explored learning and preventing failure and found a promising method for notifying students before they get a bad grade. The study presented in this paper explored student participation in an online collaborative discourse coblogging community while they worked on creating content that they could later use as a resource for doing their term project. By tracking participation and relating it to earlier grades, we successfully created a model that predicted, with very high accuracy, if a student was going to score a low grade on the assignment she was currently working on. Further research could explore the use of peer assessment as a replacement for teacher grades in the model. The approach to failure prediction developed in this work may also prove useful for reducing the size of the grading task for large classes. For a large class, weekly grading can be much too labor intensive. Spot checking and rotating through the class can provide some feedback, but the addition of a failure prediction mechanism that works for each assignment as it is being done, could help the instructors to be more selective about which students to grade more closely. The approach to failure detection we have presented can potentially be reversed, in which case, the instructor would have an early detection mechanism for identifying students who are doing well on the current assignment. After veriﬁcation, the instructor could choose to notify the rest of the class of these examples of good work, so as to steer the weaker students in the right direction.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Predicting failure: A case study in co-blogging</rdfs:label>
		<dc:subject>co-blogging</dc:subject>
		<dc:subject>participation</dc:subject>
		<dc:subject>predicting failure</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/bjorn-levi-gunnarsson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/bjorn-levi-gunnarsson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-alterman"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-alterman"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/37/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/bjorn-levi-gunnarsson"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-alterman"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/38">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Using computational methods to discover student science conceptions in interview data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/38/authorlist"/>
		<swrc:abstract>A large body of research in the learning sciences has focused on students’ commonsense science knowledge—the everyday knowledge of the natural world that is gained outside of formal instruction. Although researchers studying commonsense science have employed a variety of methods, one-on-one clinical interviews have played a unique and central role. The data that result from these interviews take the form of video recordings, which in turn are often compiled into written transcripts, and coded by human analysts. In my team’s work on learning analytics, we draw on this same type of data, but we attempt to automate its analysis. In this paper, I describe the success we have had using extremely simple methods from computational linguistics—methods that are based on rudimentary vector space models and simple clustering algorithms. These automated analyses are employed in an exploratory mode, as a way to discover student conceptions in the data. The aims of this paper are primarily methodological in nature; I will attempt to show that it is possible to use techniques from computational linguistics to analyze data from commonsense science interviews. As a test bed, I draw on transcripts of a corpus of interviews in which 54 middle school students were asked to explain the seasons.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Much of the recent interest in learning analytics has been driven by the great surge in the amount and kinds of data that are available. This paper, in contrast, applies learning analytic techniques to a type of data that has a long history, and that predates recent technological advances. For the last few decades, a large body of research in the learning sciences has focused on students’ commonsense science knowledge—the everyday knowledge of the natural world that is gained outside of formal instruction. Although researchers studying commonsense science knowledge have employed a variety of methods, one-on-one clinical interviews have played a unique and central role. The data that result from these interviews take the form of video recordings, which in turn are often compiled into written transcripts, and coded by human analysts. In my team’s work on learning analytics, we draw on this same type of data, but we attempt to automate its analysis. In this paper, I describe one part of this work. The automated analyses I present here are not intended to code the data using categories developed by human analysts. Rather, these analyses are employed in an exploratory mode, as a way to discover student conceptions in the data. Furthermore, my goal in this paper is not to contribute new results to research on commonsense science. Rather, my aims are primarily methodological in nature; I will attempt to show that it is possible to use relatively simple techniques from computational linguistics to analyze the type of data that is typically employed by researchers in commonsense science. As a test bed, I draw on transcripts of a corpus of interviews in which 54 middle school students were asked to explain the seasons. It should be emphasized that it is not at all obvious that it should be possible to analyze data of this sort using simple computational techniques. Unlike some other applications in learning analytics, the total amount of data I have is relatively small. Furthermore, the speech that occurs in commonsense science interviews can pose particular difficulties for comprehension. Student utterances are often halting and ambiguous. Furthermore, gestures can be very important, and external artifacts such as drawings are frequently referenced. However, our analysis algorithms only have access to written transcripts of the words spoken by participants. Even with all of this complexity, my general approach is to go as far as possible with simple methods, before proceeding to more complex methods. Thus, the analyses I describe here make use of extremely simple methods from computational linguistics— methods that are based on rudimentary vector space models and simple clustering algorithms. 
 2. LITERATURE REVIEW.
 2.1 Commonsense Science.
 It is now widely accepted that many of the key issues in science instruction revolve around the prior conceptions of students. This focus on commonsense science leads to a perspective in which the central task of science instruction is understood as building on, adapting, and, when necessary, replacing students’ prior knowledge. One outcome of this focus has been the growth of a veritable industry of research on students’ prior conceptions. The bibliography compiled by Pfundt and Duit [1], which lists literature on the science conceptions of teachers and students, provides one measure of the scale of this effort. As of early 2009, the bibliography had over 8300 entries, spanning a wide range of scientific disciplines, including, for example, what students believe about the shape of the earth [2], evolution [3], and nutrition [4]. In discussing the literature on commonsense science, it has become commonplace to distinguish two theoretical poles. At one extreme is the theory-theory perspective. According to this perspective, commonsense science knowledge consists of relatively well-elaborated theories [5]. At the other extreme, is the knowledge-in-pieces (KiP) perspective. In this perspective, it is assumed that: (a) commonsense science knowledge consists of a moderately large number of elements—a system—of knowledge and (b) the elements of the knowledge do not align in any simple way with formal science domains [6, 7]. I believe that the computational methods described in this paper should be of interest to a broad range of researchers who study commonsense science, and adopt a range of theoretical perspectives. However, the exploration of computational methods presented in this paper was biased by my own theoretical perspective, which lies closer to the KiP pole. As I hope will become evident, my exploration of computational methods has been driven by a desire to get at the more basic knowledge—the pieces—that I believe comprise commonsense science knowledge. And I have attempted to capture the dynamics that unfold as students construct explanations during an interview. 
 2.2 Vector space models and their applications in education research. 
 Generally speaking, the goal of this work is to attempt to use computational techniques in order to “see” student conceptions in transcripts of commonsense science interviewers. There are many techniques from computational linguistics that could be employed in this way. The techniques I will use are based primarily on a type of vector space model [8]. In vector space models, the meaning of a block of text—a word, paragraph, essay, etc.—is associated with a vector, usually in a high dimensional space. So, two blocks of text have the same meaning to the extent that their vectors are the same. In this way, a vector space analysis makes it possible to compute the similarity in meaning between any pair of words or blocks of text. In Section 4, I will describe, in some detail, the algorithms employed in the particular analyses used in this work. One particular variant of vector space model, Latent Semantic Analysis (LSA), has had increasing prominence across a range of disciplines and applications [9-11]. LSA incorporates several innovations that distinguish it from the most basic form of vector space analysis; most centrally, it makes use of an auxiliary training corpus that provides information about the wider contexts in which terms appear, and it reduces the dimensionality of the vector space, which has the effect of uncovering latent relations among terms. Vector space methods have seen increasing use in educational research. These applications have been greatly dominated by uses of LSA. In fact, outside of information retrieval, some of the earliest and most persistent uses of LSA have been in applications related to education [12]. These applications have been of two broad types. First, LSA has been used as a research tool by educational researchers—that is, as a means of analyzing data, in order to study thinking and learning. Second, LSA has been used as a component of intelligent instructional systems. The majority of these educational applications, across both types, have been focused on the teaching of reading and writing. For example, LSA-based systems have been employed to automatically score essays written by students [10, 13]. In a number of applications, students are asked to summarize a passage or document that they have just read, and an LSA-based system is used to evaluate these summaries. In one such application, Shapiro and McNamara [14] had students read and summarize portions of psychology textbooks. Using LSA, these summaries were then compared both to the text the students read, and to model essays composed by experts. Similarly, Magliano and colleagues conducted a wide range of studies in which LSA was used to assess the strategies employed by readers and their reading skill, more broadly [15, 16]. In many of these uses of LSA, the data consisted of written text produced by participants in the research. However, in some instances, LSA has been applied to transcriptions of verbal data. For example, in their study mentioned above Shapiro and McNamara [14] found that LSA could be applied successfully both to written summaries of the textbook and to transcriptions of verbal summaries given by students. Similarly, Magliano and Millis [15] applied LSA to think-aloud protocols that students produced as they read passages of text. As mentioned above, LSA has been used as a component of intelligent instructional systems. For example, intelligent systems have been constructed that provide feedback to students on summaries that they write of a given text passage [17, 18]. One LSA-based system, AutoTutor, is of particular interest here because it has been applied to teach science-related subject matter [19, 20]. AutoTutor teaches physics by first posing a problem or question. The student responds by typing a response into the system. The system then evaluates that response by using LSA to compare the student’s text to a set of predefined expectations and misconceptions; the expectations are pre-specified components of a correct response and the misconceptions are possible erroneous ideas that might be expressed by the student. Based on this analysis, the system responds by posing further questions to the student, either to help correct the misconceptions, or to draw out more components of a complete answer to the original problem. I want to say a bit about where the work described in the present paper fits within the space of uses of vector space models in education. First, in this work, SNLP is used as an analytic tool for researchers; I will not be describing an LSA-based system that is used by students. Second, I apply my analyses to verbal data. As mentioned above, many applications of vector space models in education use text that is typed by a student, either in the form of an essay or short responses. Furthermore, prior research that has worked with verbal data has employed data that is very different than that employed in the present work. For example, the work by Shapiro and McNamara [14] and Magliano and colleagues [15, 16], which I mentioned above, employed a more constrained type of think-aloud protocol, focused on passages of text that were just read. In contrast, the verbal data employed in this work consists of relatively free-flowing discussions involving back-and-forth between an interviewer and interviewee. Third, in all these applications, answers given by students, whether in written or verbal form, were evaluated by comparison to a predefined model. This model might be, for example, some portion of the text just read, or an ideal answer constructed by the researcher. In contrast, as mentioned above, I will describe techniques for automatically inducing a set of conceptions from the data itself. Finally, I want to emphasize one other respect in which this work differs from prior work in education that made use of LSA; namely, I am not using LSA! As noted above, I believe it makes sense to begin with simpler techniques, and then to pursue more sophisticated methods as it seems necessary. 
 3. THE INTERVIEWS.
 3.1 Subject matter and interview design.
 (The data used in this work was drawn from a larger corpus collected by the NSF-funded Conceptual Dynamics Project (CDP).1 For the present work, I draw from a set of 54 interviews in which students were asked to explain Earth’s seasons [21]. The seasons have long been a popular subject of study in research on commonsense science, and a significant number of studies have set out to study student and adult understanding in this area [22-26]. Our seasons interview always began with the interviewer asking “Why is it warmer in the summer and colder in the winter?” After the student responded, the interviewer would, if necessary, ask for elaboration or clarification. The interviewer had the freedom, during this part of the interview, to craft questions on-the-spot in order to clarify what the student was saying. Next, the student was asked to draw a picture to illustrate their explanation. Then, once again, the interviewer could ask followup questions for clarification. Our interviewers were also prepared with a number of specific follow-up questions to be asked, as appropriate, during this part of the interview. Some of these questions were designed as challenges to specific explanations that students might give. 
 3.2 Overview of student responses.
 In prior work with our seasons data, Conceptual Dynamics researchers have adopted a strongly KiP perspective [21]. We assume that students possess a system consisting of many knowledge elements—the “pieces”—that may potentially be drawn upon as they endeavor to explain the seasons. When a student is asked a question during an interview, some subset of these elements are activated. The student then reasons based on this set of elements, and works to construct an assemblage of ideas in the service of explaining the seasons. We refer to this assemblage of ideas as the dynamic mental construct or DMC, for short. For the purpose of the present work, it is not a bad approximation to think of a DMC as a student’s current working explanation of the seasons. So, throughout this manuscript, I will use the terms “DMC” and “explanation” interchangeably. The explanations of the seasons given by the students we interviewed varied along a number of dimensions. But it is helpful, nonetheless, to begin with a number of reference points, in the form of a few categories of explanations (DMCs). The first category, closer-farther, is illustrated by the diagram in Figure 1a. In closer-farther explanations, the earth is seen as orbiting (or moving in some other manner) in such a way that it is sometimes closer to the sun and sometimes farther. When the earth is closer to the sun then it experiences summer; when it’s farther away it experiences winter. The second category of DMC, side-based, is illustrated in Figure 1b. Side-based explanations are usually focused on the rotational motion of the earth, rather than its orbital motion. In side-based explanations, the earth rotates so that first one side, then the other, faces the sun. The side facing the sun at a given time experiences summer, while the other side experiences winter. 
 Figure 1. Closer-farther, side-based, and tilt-based DMCS. 
 The third and final category of DMC, tilt-based, is depicted in Figure 1c. Tilt-based DMCs depend critically on the fact that the earth’s axis of rotation is tilted relative to a line connecting it to the sun. In a tilt-based explanation, the hemisphere that is tilted toward the sun experiences summer and the hemisphere that is tilted away experiences winter. This category includes the normative scientific explanation, as well as some non-normative explanations. As discussed in Sherin et al. [21], during an interview, students tend to move among DMCs. In some cases, students do begin the interview with what appears to be a fully-formed explanation. In other cases, a student might construct an explanation during the interview, slowly converging on an explanation they find reasonable. Finally, students can be to seen to shift from one DMC to another, sometimes in response to a challenge from the interviewer. 
 3.3 Example interviews.
 Now I will briefly discuss a few example interviews. These examples will play a role as important reference points when I discuss the automated analysis. In this first example, a student, Edgar, began by giving an explanation focused on the fact that the Earth rotates, and he stated that light would hit more directly on the side facing the sun. He made the drawing shown in Figure 2, as he commented: E: Here’s the earth slanted. Here’s the axis. Here’s the North Pole, South Pole, and here’s our country. And the sun’s right here [draws the circle on the left], and the rays hitting like directly right here. So everything’s getting hotter over the summer and once this thing turns, the country will be here and the sun can't reach as much. It's not as hot as the winter. After a brief follow up question by the interviewer, Edgar seemed to recall that the Earth orbited the sun, in addition to rotating. He then shifted to a closer-farther type explanation: E: Actually, I don't think this moves [indicates Earth on drawing] it turns and it moves like that [gestures with a pencil to show an orbiting and spinning Earth] and it turns and that thing like is um further away once it orbit around the s- Earth- I mean the sun. 
 I: It’s further away? E: Yeah, and somehow like that going further off and I think sun rays wouldn’t reach as much to the earth. 
 In addition, initial attempts by Gregory Dam and Stefan Kaufmann to apply LSA to my research team’s data proved promising, and thus justified further exploration [28]. Dam and Kaufmann employed techniques based on one variant of LSA to apply a given coding scheme to an earlier subset of this corpus. 
 Thus Edgar’s interview illustrates a case in which a student began with a side-based explanation and transitioned to a closer-farther explanation. It is also worth noting that Edgar’s language was halting, imprecise, and made significant use of gestures and his drawings. These are features that might well pose difficulties for automated analysis. 
 Figure 2. Edgar's drawing. 
 I want to briefly introduce interviews with two other students from the corpus, both of whom gave variants of tilt-based explanations. The first example is from an interview with Caden. I: So the first question is why is it warmer in the summer and colder in the winter? C: Because at certain points of the earth’s rotation, orbit around the sun, the axis is pointing at an angle, so that sometimes, most times, sometimes on the northern half of the hemisphere is closer to the sun than the southern hemisphere, which, change changes the temperatures. And then, as, as it’s pointing here, the northern hemisphere it goes away, is further away from the sun and get’s colder. I: Okay, so how does it, sometimes the northern hemisphere is, is toward the sun and sometimes it’s away? C: Yes because the at—I’m sorry, the earth is tilted on its axis. And it’s always pointed towards one position. 
 Note that, in Caden’s explanation, the tilt of the earth affects temperature because the hemisphere tilted toward the sun is closer to the sun, and the hemisphere tilted away is farther from the sun. (This is not correct.) In contrast, another student, Zelda gave a tiltbased explanation, but her explanation made use of the fact that the tilt of the earth causes rays to strike the surface more or less directly, and this is what explains the seasons. Z: Because, I think because the earth is on a tilt, and then, like that side of the Earth is tilting toward the sun, or it’s facing the sun or something so the sun shines more directly on that area, so its warmer. Thus, Caden and Zelda both gave tilt-based explanations, but they differed in how exactly the tilt of the earth affected the seasons. For Caden the tilting causes one hemisphere or the other to be closer to the sun. For Zelda, the tilting causes parts of the earth to receive the sun’s rays more or less directly. This illustrates some of the types of features we would like the automated analysis to resolve. 
 4. VECTOR SPACE ANALYSIS OF THE SEASONS CORPUS.
 In order to captured students’ conceptions expressed in the seasons interviews, my team explored the use of techniques from statistical natural language processing. In particular, we explored the use of vector space models, augmented with cluster analysis. These choices make sense for a number of reasons. As mentioned above, one type of vector space model, LSA, has already been employed, with some success, in applications that are in some respects close to my own [10, 14-17, 19, 27]. 
 The work described in this manuscript extends the work of Dam and Kaufmann in several respects. First, Dam and Kaufmann’s analysis did not discover student conceptions in the data corpus. Instead, it began with the conceptions identified by human analysts and used those conceptions to code transcript data. Second, unlike Dam and Kaufmann, I will be exploring the use of simpler vector space models, rather than LSA. Third, Dam and Kaufmann were primarily concerned with coding at the level of students. Each student was coded by the computer in terms of just one of three possible explanations of the seasons. The success of this analysis was judged by comparison to an analysis of these same transcripts by human coders, restricted to the same set of three explanations. However, this type of analysis represented a drastic simplification over our earlier qualitative analyses of the corpus. As exemplified in the description of Edgar’s interview above, the explanations given by students over the course of an interview were quite clearly dynamic. Thus, assigning a single code to each manuscript was often a dramatic simplification. In this new work, all of my analysis is done at a finer time scale; I look to identify student ideas only in small segments of text. In the rest of this section, I describe an exploratory analysis of our data. Here, I restrict myself to one pathway through the analysis, using one set of parameters and algorithms. In Section 5, I briefly describe the results I obtain when employing different parameters and algorithms. 
 4.1 The basics: Converting text to vectors.
 The central idea underlying any vector space model of text meaning is relatively simple: Every passage of text—whether it is a word, sentence, or essay—is mapped to a single vector. The direction in which this vector points is taken to be a representation of the meaning of the passage. More precisely, the similarity between two passage vectors is quantified as the cosine of the angle between the two vectors (or, equivalently, the dot product of the vectors if we assume the vectors are of unit length). 
 Table 1. Partial vocabulary and sample counts. 
 The question, of course, is how we go about converting a passage of text to a vector. In the most rudimentary forms of vector space models, this mapping is accomplished in a rather straightforward manner. First, we look across the entire corpus of text that we wish to include in our analysis, and we compile a vocabulary, that is, a complete list of all of the words that appear somewhere in the corpus. This vocabulary is then pruned using a “stop list” of words. This stop list consists primarily of a set of highly common “non-content” words, such as the, of, and because. For the corpus used in this work, this resulted in a vocabulary consisting of 647 words. (The stop list used contained 782 terms.) If the vocabulary is sorted from the most common to least common words, the top 10 words correspond to the list shown in the left hand column of Table 1. This vocabulary can be used to compute a vector for a passage from an interview transcript as follows. First, we take the transcript and remove everything except the words spoken by the student. Any portion of the remaining text can now be converted to a vector. To do so, we go through the entire vocabulary, counting how many times each word in the vocabulary appears in the text being analyzed. When this is done, we get a list of 647 numbers. If, for example, we process the portion of Caden’s transcript presented above, we obtain the values listed in the middle column of Table 1 for the 10 most common words in the larger corpus. Finally, in most vector space analyses, the raw counts are modified by a weighting function. In the analyses reported on in this section, I replaced each count with (1 + log(count)). This has the effect of dampening the impact of very frequent words. (Raw counts of 0 were just left as 0.) Appropriately weighted values are shown in the third column of Table 1. 
 4.2 Using passage vectors to discover meanings in the data corpus. 
 We now have a means of mapping a passage of text to a vector consisting of 647 numbers. This capability can now be used to discover units of meaning that exist across the 54 interviews that comprise my data corpus. This process involves four steps which I will now discuss: (1) preparing and segmenting the corpus, (2) mapping segments to vectors, (4) clustering the vectors, and (5) interpreting the results. 
 4.2.1 Preparing and segmenting transcripts.
 First, as discussed above, the transcripts are reduced to that they include only the words spoken by the student during the interview. Next, recall that, in our earlier analyses of this corpus conducted by my research team, we found that students could be seen to construct explanations of the seasons out of large number of knowledge resources, and that their explanations could shift as an interview unfolded. We thus need a way to attach meanings to small parts of an interview transcript. This requires a means of segmenting a transcript into smaller parts. In keeping with my goal of using simple methods, I segmented the transcripts by breaking each transcript into 100-word segments. In order to lessen problems that might be caused by the fact that this introduces arbitrary boundaries, I chose to employ overlapping 100-word segments, with the start of each segment beginning 25 words after the start of the preceding segment. So the first segment of a transcript would include words 1-100, the second words 26-125, the third 51-150, etc. When all of the 54 interview transcripts were segmented in this manner, I ended up with 794 segments of text. These specific choices for segment size and step size are, of course, somewhat arbitrary. In Section 5, I will briefly present results with different values of these parameters. 
 4.2.2 Mapping segments to vectors.
 The next step in the analysis is to map each of these 794 segments to a vector. To accomplish this, I employ precisely the method described above. The result is 794 vectors, each consisting of a list of 647 numbers. However, here I must introduce one complication. There is one inherent problem with applying vector space models to an analysis of this sort of data. Vector space models such as LSA were originally developed as a means to find documents in a large corpus that pertain to a given topic. They were thus not developed for finding fine distinctions in meaning among documents pertaining to very similar topics. However, all of the documents involved in my analysis are about very similar subject matter; they all explain the seasons, and they almost all do so by talking about the position and motion of the earth in relation to the sun. In fact, the clustering analysis (described in the next section) does not produce meaningful results if I use the raw document vectors that are produced by the method described above. (I will say more about this problem in Section 5.) Instead, I need a means of modifying the vectors so that they highlight their more unique features—the features that, on average, tend to differentiate the segment from the other 793 segments of text. For that purpose, I compute what I call deviation vectors. To compute the deviation vectors for two vectors V1 and V2, I first find their average, and then break each vector into two components, one that lies along the average, and another that is perpendicular to the average (refer to Figure 3). The perpendicular components, V1' and V2', are the deviation vectors. If we use these deviation vectors in place of the original vectors, the result is that V1 and V2, have each been replaced by the component that defines its unique piece – a piece that characterizes how it differs from the average. The same procedure can be employed with any number of vectors. For the next steps of the analysis, I replaced the 794 segment vectors in just this way; I found their average, and then replaced each vector with its deviation from this average. 
 Figure 3. How to compute deviation vectors. 
 4.2.3 Clustering the vectors.
 Now each of the 794 segments has been mapped to a vector that we understand as representing the meaning of that segment. The next step is to identify common meanings amongst these segments. To do that, we look for natural clusterings of the 794 vectors. To cluster the transcript vectors, I employed the very general technique called hierarchical agglomerative clustering (HAC). In HAC, we begin by taking all of the items to be clustered, and placing each of these items in its own cluster. Thus, we begin with a number of clusters equal to the total number of items. Then we pick two of those clusters to combine into a single cluster containing two items, thus reducing the total number of clusters by one. The process then iterates; we again pick two clusters to combine, and the total number of clusters is decreased by one. This repeats until all of the items are combined into a single cluster. The result is a list of candidate clusterings of the data, with each candidate corresponding to one of the intermediate steps in this process. A central issue in applying this algorithm is determining which clusters to combine on each iteration. In practice, there are many rules that can be applied. Throughout my discussions here, I will present results that were obtained using a technique called centroid clustering. At each step in the iteration, I first find the centroid of each cluster (the average of all of the vectors currently in the cluster). Then I find the pair of centroids that are closest to each other, and merge the associated clusters. An explanation of centroid clustering, including its application to vector space models, can be found in [29]. 
 4.2.4 Determining the number of clusters.
 The result of the clustering analysis can be thought of as a table with 794 rows. At the top is a row in which each segment is in a single cluster. At the bottom is a row in which all of the segments are in a single cluster. Table 2 displays the results for just a part of this large table. The bottom row, for example, shows the results when the segments are grouped into three clusters that contain 271 segments, 279 segments, and 244 segments respectively. As you move up the table the number of clusters grows, and the size of each cluster shrinks. In each row of Table 2, clusters contain segments that have been grouped together because, from the point of view of our vector space model, they have similar meanings. This means that each row in Table 2, constitutes a candidate coding scheme—it is a scheme for sorting segments into categories. The puzzle, of course, is which row to select. 
 Table 2. Sizes of clusters for selected clusterings. 
 Unfortunately, there is no simple answer to this question. In general, there is a tradeoff. When the number of clusters is high, we obtain a better fit to the data. However, we get this better fit at the expense of a more complex model. Because each cluster is described by a list of 647 values, each additional cluster represents a dramatic increase in model complexity. Here, as elsewhere, I make my choice in a heuristic manner. Across multiple analyses, I have found that working with a set of about 7 clusters strikes a workable balance. With 7 clusters, it is possible to resolve interesting features of the data, while producing results (in the form of graphs) that are not overly difficult to interpret. 
 4.2.5 What do the clusters mean?.
 We now have grouped the 794 segments into 7 clusters, each containing between 44 and 211 segments (refer to Table 2). The next question we must answer is: What do these clusters mean? Each of the 7 clusters can be thought of as defined by its centroid vector—the average of all of the vectors that comprise the cluster. These centroids each, in turn, are described by a list of 647 entries, each of which corresponds to one of the words in the vocabulary. One way to attempt to understand the meaning of the clusters, then, is to look at the words that have the largest value in each centroid vector. When this is done I obtain the results shown in Figure 4. For each cluster, I list the 10 words that are most strongly associated with that cluster, ignoring words that appeared less than 30 times in the overall corpus. In addition, the second column in each table has the value from the centroid vector corresponding to this word. The third column in each table lists the total number of times that the word appears across the entire corpus. 
 4.2.6 Interpreting the clusters based on the word lists. 
 In many respects, the lists of words shown in Figure 4 clusters are suggestive. First, several of the clusters seem to align with the three broad classes of seasons explanations listed in Section 3. For example, it seems natural to associate Cluster 1, which starts with the words tilted, towards, and away, with tilt-based explanations. Similarly, it seems natural to associate Cluster 4 (side, facing) with side-based explanations, and Cluster 7 (farther, closer) with closer-farther explanations of the seasons. 
 Figure 4. Top words associated with each cluster. 
 For this reason, if I use traditional measures for determining the appropriate number of clusters (e.g., Bayesian information criterion or Akaike information criterion), the terms corresponding to the model complexity always dominate, and the model with the smallest number of clusters prevails. 
 But these clusters are not supposed to necessarily align with fullfledged explanations of the seasons. They are clusters of segments, which it is hoped can align with smaller conceptual units that, when combined, form the basis of a constructed explanation. And, indeed, the additional clusters do seem to offer the possibility of an analysis of that sort. For example, we should expect tilt-based explanations to often be seen in concert with talk about the Earth’s hemispheres (Cluster 3). And recall that tiltbased explanations invoke different mechanisms by which the changing tilt of the earth impacts temperature. For example, Caden argued that the tilting of the Earth causes parts of the earth to be alternately closer or farther from the sun. In contrast, Zelda’s explanation focused on the impact of the Earth’s tilt and how it impacts the angle and directness of the sun’s rays. We should thus be able to see these ideas in combination, when we look at individual interviews. Similarly, we should expect to see side-based explanations (Cluster 4) in tandem with clusters having to do with the rotation of the Earth. Ideas about the rotation of the Earth seem to appear in Cluster 2 and Cluster 6. Cluster 2 seems to truly be focused on the spinning of the Earth. Cluster 6, in contrast, seems to be more about day and night. Not surprisingly, talk about the rotation of the Earth was often combined with talk about the day/night cycle. 
 4.3 Application to segmented transcripts.
 The clusters shown in Figure 4 thus seem to have reasonable interpretations in terms of our understanding of the data corpus. We have thus identified a set of “common underlying ideas.” A next step I can take is to apply this set of ideas back to the original transcripts. I want to use these ideas—these units of meaning—to interpret individual student interviews. In order to accomplish this, I begin by preparing each of the interview transcripts precisely as before; the transcripts are reduced so that they include only the words spoken by a student, then they are broken into 100-word segments using a moving window that steps forward by 25 words. Next I compute the vector for each of these segments, again using the same techniques described earlier. Finally, each of these vectors for the segments is compared to the 7 centroid vectors corresponding to the 7 clusters (by taking the cosine of the angle between the vectors and each centroid). I begin my discussion of the results with Zelda, since her analysis produces a graphic that is relatively easy to read. In Figure 5 we see that Zelda’s transcript has been broken into 5 overlapping segments. Each of these segments is associated with 7 bars, one bar for each of the 7 clusters. For all of the segments, Cluster 1 is the clear winner. This makes sense since, as discussed earlier, Zelda gave an answer that was very close to the accepted scientific explanation of the seasons. Note, also, that the bar for Cluster 5 is slightly elevated in three of the segments. Cluster 5 had to do with rays striking the earth at an angle. Again, this makes sense given what we can read in Zelda’s transcript. 
 Figure 5. Segmenting analysis of Zelda's transcript. 
 The interview with Caden provides an interesting contrast. When Caden’s transcript is analyzed using the segment centroids, we get 8 segments with the bars shown in Figure 6. Like Zelda, we understood Caden as giving an explanation that emphasized the tilt of the Earth. But, in Figure 6, we see that Cluster 3 dominates—the cluster having to do with the Earth’s hemispheres —although there are hints of Cluster 1 (tilted-toward) in the earlier segments. The predominance of Cluster 3 is not too surprising. As I noted earlier, tilt-based explanations should be closely associated with discussion of the two hemispheres of the earth. Indeed, glancing at the portion of Caden’s transcript presented earlier, there is an emphasis on the different effects on the northern and southern hemispheres. 
 Figure 6. Segmenting analysis of Caden's transcript. 
 Both Zelda and Caden were relatively stable in the explanations that they gave. We would now like to see if this analysis can capture shifts that occur as an interview unfolds. To see this, we can now return to the interview with Edgar. Looking at Figure 7 it seems clear that the interview has a two major parts. The first part is dominated by Cluster 5, which has to do with rays striking the Earth’s surface. The latter part is strongly dominated by Cluster 7, which is the closer-farther cluster. Thus, once again, it seems possible to interpret the automated analysis in a manner that is consistent with our qualitative analysis of the interview. 
 Figure 7. Segmenting analysis of Edgar’s transcript. 
 5. Alternative analysis methods.
 To this point, my exploration has been limited in a particular way; I have looked at some of the results produced by my analysis, but all of these results were produced by a single set of algorithms, and with a single set of input parameters. In particular, my analysis followed the following plan: (1) The transcripts were pruned so that they only contained the words spoken by the interviewee; (2) the resulting documents were broken into 100word segments, with a step size of 25 words; (3) a vector was computed for each segment, using a weight function of (1 + log(count)), and ignoring words in my stop list; (4) the resulting vectors were replaced with their deviation vectors; and (5) the vectors were clustered using hierarchical agglomerative clustering. These choices of algorithms and parameters were chosen, in part, because they produced interpretable results. In this section, I want to briefly give a sense for the results produced by alternative approaches, including some that did not produce interpretable results. 
 5.1 Alternative parameters.
 There are many ways in which the above analysis could be altered, while still employing a method that is very similar in outline. For example, the composition of the stop list could be changed, and the transcripts could be “pruned” in a different manner. For example, in pruning the transcripts, I needed to decide what to do about word fragments, whether to leave them, delete them, or complete them. More dramatically, I could have opted to stem words, that is, to reduce them to their base or root. For the most part, these smaller changes produced similar interpretable results across a large range of variations. For illustration, I will present the results obtained if the transcripts are segmented into 50 word segments, with a step size of 10 words (rather than 100 and 25). When this is done, I end up with 2320 segments. When these segments are clustered, I obtain the results shown in Table 4. Like Table 3, this table shows the sizes of the clusters that are produced during some of the latter steps in the clustering. Note that when the segments are grouped into 7 clusters, one of the clusters contains only 1 segment. For that reason, it makes to sense to look at the next row in the table, where the segments are grouped into 6 clusters. Figure 8 shows the word lists associated with these clusters, which were produced in the same method used to produce the lists in Figure 4. 
 Table 3. Sizes of clusters for selected clusterings. 
 5.2 Alternative algorithms.
 Of course, it is possible to make much more substantial changes to the analysis presented in Section 4. Here I will consider changes related to one unusual feature of my analysis, the use of deviation vectors. Recall that I introduced the deviation vectors as a means of addressing the fact that there was substantial overlap in the vectors that are produced by my initial computation of document vectors. Table 4 shows the results that are produced when I do not compute deviation vectors before clustering the segments. Note that in each of the candidate clusterings shown, I obtain one very large cluster, containing most of the segments, and several very small clusters. When I look at earlier stages of the clustering, I see the following behavior: initially the segments are all clustered into a large number of relatively small changes, then the these small clusters begin to agglomerate, one at a time, onto one large cluster. In the final stages, which we see in Table 4, some small remaining clusters are swept up into the large cluster. In short, this analysis does not seem to discover a small number of moderatelysized clusters that we can associate with conceptions. 
 Table 4. Sizes of clusters for selected clusterings. 
 Now we can compare the lists in Figures 8 and 4. Although there are many differences, it is not difficult to discern an alignment. Clusters 1, 3, 4, and 5 in the new analysis seem to be similar to the corresponding clusters in the original analysis. Cluster 6 seems to be similar to Cluster 7 in the original analysis. Finally, Cluster 2 in the new analysis is similar to both Cluster 2 and Cluster 6 in the original analysis. (Note that it makes sense for Clusters 2 and 6 in the original analysis to be grouped together since they both pertain to the rotation of the Earth.) Thus, while there are certainly differences, the qualitative picture produced by this new analysis seems to bear a close resemblance to the original analysis. 
 Figure 8. Top words associated with each cluster. 
 The question remains as to whether there are other more standard approaches that might improve on the results shown in Table 4. In particular, it is standard practice to use judiciously-chosen weighting functions as a means of accentuating the differences among documents. Recall that the counts in my vectors were all weighted by (1 + log(count)), where “count” is the number of times a word appears in a given document. We can modify this function so that it weights words that appear across many documents less strongly than words that appear in in just a few documents. I tried several such weighting functions, including variants of the so-called tf-idf weighting. In all cases, I obtained results that looked like Table 4. 
 6. Discussion.
 6.1 Summary.
 I began this paper with the observation that research on commonsense science knowledge typically focuses on data derived from one-on-one clinical interviews. To date, researchers in this field have generally used humans as “instruments” for analyzing this data. I believe that we have done so because of some tacitly-held beliefs: we have tended to assume that, to make sense of clinical interview data, it is necessary to have an instrument with an ability to understand natural language. We have also assumed that it is necessary to have access to as much of the interaction as possible. We need not just the words spoken; we also need gestures, facial expressions, drawings, etc. It also seems to require the ability to make leaps that look across the breadth of a data corpus. The task of analysis is also, in some respects, complicated by the theoretical position I adopted in this work. I believe that, in many cases, it is simply not possible to understand a student as expressing a single model of the seasons. Instead, students construct and shift explanations—DMCs—as the interview unfolds. I want to capture this movement in explanations. Nonetheless, this work set out to explore how much can be accomplished with a relatively simple suite of techniques from statistical natural language processing. Stated crudely, the statistical techniques rely primarily on “counting words.” Furthermore, from among the “bag of words” models that are employed by linguists, I chose to begin with one of the simplest possible models. In short, there was every reason to think that the types of analysis described here would not be very successful. Nonetheless, these results are at least suggestive that these simple techniques can give meaningful results. The clustering algorithm produced a set of clusters that seemed to have meaningful interpretations— interpretations that made sense given earlier qualitative analyses of the same corpus. And, when these clusters were employed to produce a segmented analysis of individual transcripts, they produced a narrative analysis of the transcript that aligned with the descriptions produced by human analysts. That said, most of my presentation in this manuscript has had an exploratory character. My goal has been only to begin to map the boundaries of what might be possible with a family of relatively simple computational techniques. In this final section of the paper, I reflect on what we can conclude, and I discuss caveats. 
 6.2 What do these computational techniques buy us?.
 What role might these computational techniques play in the toolkit of researchers, especially researchers who use clinical interviews to study the conceptions of science students? I presented many results that were intriguing, but my results were only intended to be about the methods themselves; I didn’t use the methods in the service of any scientific agenda. So what, in the long term, might these techniques buy us? One question to ask is whether computational techniques can and should replace human analysts, or at least reduce the work required. Whether or not this may ultimately be possible, I should be clear that the analyses presented in this paper were still highly dependent on human interpretation. For example, I had to make a judgment about the appropriate number of clusters to work with. Even more importantly, the analysis required me to make sense of the lists of words that were associated with each of the clusters. Nonetheless, the computational techniques described here might play a useful role in our toolkits, even in the short term. In particular, I believe that the biggest and most immediate contribution will be in the support computational techniques can provide for traditional kinds of analysis. I believe that the primary contribution of the computational techniques will be in their ability to provide a type of triangulation that helps us to establish the validity of our analyses. This point is worth some elaboration. When two humans code the same data in order to establish the reliability of a coding procedure, they are, in a fundamental way, doing the same thing. Thus, when two humans code, we have two sets of measurements, both essentially performed with the same type of experimental apparatus. In contrast, if we can find a way to obtain confirmatory results, using a very different type of apparatus, then that should more profoundly increase our confidence in the validity of our results. It is that type of support that I believe is the biggest potential contribution of the computational techniques. 
 6.3 Open issues and next steps.
 A large number of problems remain unsolved, some of which I have been careful to highlight, others which I have glossed over. Here I will mention a few. 
 6.3.1 Additional subject matter.
 One obvious next step is to try some of these same analyses on different interview data, about topics other than the seasons. It is possible that there is something special about the seasons as subject matter. For example, it might be that, in this territory, a small number of key words (e.g., “tilt”) can do a lot of the work of discriminating among explanations. 
 6.3.2 Systematic comparison to human analysis.
 My presentation in this manuscript was, in places, selective and anecdotal. I did not discuss the segmented analysis of every transcript; I just selected a few to give the flavor of the analysis produced, and I relied on the reader’s intuition to judge whether the automated and human analyses were in accord. That’s in keeping with the exploratory approach adopted in this manuscript. But, ultimately, I want to perform an analysis in which I systematically compare the output of the automated analysis to codes produced by human analysts. As mentioned earlier, Dam and Kaufmann [28] performed such an analysis in which each transcript was given a single code. However, their analysis did not capture the dynamic features of interviews. An analysis focused on a smaller grain size is in greater accord with the theoretical perspective I have adopted, and I believe it will be possible to obtain good agreement between human and automated codes. 
 6.3.3 Systematic investigation of alternative analysis methods. 
 In Section 5, I very briefly talked about the results I obtained when using different methods and parameters. In future work, I want to extend this exploration of alternative analysis methods so that it is both more deep and more broad. I believe that it is important to have a more deep understanding of why some methods work and others do not. And I want to look more broadly and systematically at alternative techniques, including some that begin to depart significantly from the methods discussed here, including latent semantic analysis [9-11], probabilistic latent semantic indexing [30], and latent Dirichlet allocation [31]. 
 6.3.4 Why does this work?.
 Finally, perhaps the greatest puzzle raised by this research is the question of why these techniques work at all. Where is the magic? In my view, this question is almost, on its own, worthy of a program of research. Are gestures and diagrams really so unimportant to understanding the explanations given in interviews of this sort? Are a few key words enough to understand what students are saying? Why did the clustering analysis pick out precisely the same set of categories as our human coders? Answering these questions may do more than tell us something about this new class of methods, it might lead to a deeper understanding of the very phenomena about thinking and learning that were are seeking to study. Ultimately this question will need to be a focus of future research. 
 7. ACKNOWLEDGMENTS.
 This work was funded in part by NSF grant #REC-0092648.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Using computational methods to discover student science conceptions in interview data</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>conceptual change</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/bruce-sherin"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/bruce-sherin"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/38/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/bruce-sherin"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/39">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Student Success System: Risk Analytics and Data Visualization using Ensembles of Predictive Models</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/39/authorlist"/>
		<swrc:abstract>We propose a novel design of a Student Success System (S3), a holistic analytical system for identifying and treating atrisk students. S3 synthesizes several strands of risk analytics: the use of predictive models to identify academically at-risk students, the creation of data visualizations for reaching diagnostic insights, and the application of a case-based approach for managing interventions. Such a system poses numerous design, implementation, and research challenges. In this paper we discuss a core research challenge for designing early warning systems such as S3. We then propose our approach for meeting that challenge. A practical implementation of an student risk early warning system, utilizing predictive models, must meet two design criteria: a) the methodology for generating predictive models must be ﬂexible to allow generalization from one context to another; b) the underlying mechanism of prediction should be easily interpretable by practitioners whose end goal is to design meaningful interventions on behalf of students. Our proposed solution applies an ensemble method for predictive modeling using a strategy of decomposition. Decomposition provides a ﬂexible technique for generating and generalizing predictive models across diﬀerent contexts. Decomposition into interpretable semantic units, when coupled with data visualizations and case management tools, allows practitioners, such as instructors and advisors, to build a bridge between prediction and intervention.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Improving student retention, graduation, and completion rates is a fundamental challenge in improving educational delivery.[?] S3 is intended as a practical end-to-end solution for identifying which students are at risk, understanding why they are at risk, designing interventions to mitigate that risk, and ﬁnally closing the feedback loop by assessing the success. Current approaches to building predictive models for identifying at-risk students are stymied by two serious limitations. First, the predictive models are one-oﬀ and, therefore, cannot be extended easily from one context to another. We cannot simply assume that a predictive model developed for a particular course at a particular institution is valid for other courses. Can we devise a ﬂexible and scalable methodology for generating predictive models that can accommodate the considerable variability in learning contexts across diﬀerent courses and diﬀerent institutions? Secondly, current modeling approaches, even if they generate valid predictions, tend to be black boxes from the standpoint of practitioners. The mere generation of a risk signal (e.g. green, yellow, red) does not convey enough information for the purpose of designing meaningful personalized interventions on behalf of students. As a way of overcoming these limitations we devise a modeling strategy that begins with a generic measure called Success Index which is decomposed initially into ﬁve indices: Preparation, Attendance, Participation, Completion, and Social Learning. (As we proceed with the implementation of S3 we anticipate discovering and adding other indices.) Each index is itself a composite expressing a number of relevant activity-tracking variables. These tracking variables are measured on diﬀerent scales, primarily in terms of the frequency with which a particular action or task is performed or the time spent on-task. These indices or semantic units serve as the foundation for applying an ensemble method for predictive modeling. 
 2. RELATED WORK.
 In education predictive models for identifying at-risk students was pioneered by John Campbell and the Course Signals Project at Purdue University.[?] Similar work has been underway at Capella University, Rio Salado College and other institutions. In this section we discuss methodological limitations with current risk modeling approaches in education. In Section 3 we provide a quick overview of S3. In Section 4 we propose how to overcome current methodological limitations in risk modeling. 
 2.1 Predictive Models in Education.
 The Course Signals system and recent research studies [?] provide early evidence that student elearning activities are predictive of academic success. Regression modelling such as logistic regression has been applied to build a best-ﬁt coursebased predictive models. Such models incorporate the most signiﬁcant LMS variables such as total number of discussion messages posted, total number of mail messages sent, and total number of assessments completed. The mathematical aspects of this modeling strategy is brieﬂy described in Sec 4.1. Macfadyen and Shane [?] discuss the limitations of this work in terms of its overall generalizability and interpretation. In particular, the generalizability of such models can be limited by the sample courses used for model ﬁtting, or by focussing on fully online courses within one institution. A core problem in current approaches, as applied in Course Signals-type systems, is that a single hypothesis/model that best ﬁts a collection of course data, is chosen from the space of all possible hypotheses, and then applied to make predictions across diﬀerent courses in diﬀerent programs and institutions. There are potential sources of bias in this solution. This methodology is expected to work well when courses on which the model is applied have a relatively consistent instructional model with the courses used to discover the best-ﬁt model, but otherwise lead to a risk of systematic errors in predictions, i.e. relatively high bias. The limitations of this modeling strategy, in terms of generalizability and interpretability, critically hinder the wideranging deployment of discovered models to educational institutions in a meaningful way. Hence, it limits the potential beneﬁts that institutions can draw from their data through the development of predictive analytics capabilities for modeling learner success. In this work, we propose a predictive modeling strategy that aims at closing this gap. We focus on providing a highly-generalizable modeling strategy that is well-suited for supporting wide-ranging needs of educational institutions and for taking full advantage of predictive analytics. We propose an adaptive framework and a stacked-generalization modeling strategy whereby intelligent data analysis can be applied at all levels and graciously combined to express higher-level generalizations. A second key problem is that current predictive modeling systems do not provide diagnostic information. For example, Course Signals generates a prediction that indicates the identiﬁed level of risk; however, there is no direct insight into the speciﬁc causes, thus making a recommended remediation diﬃcult to specify. Furthermore, the system does not incorporate human insight that can be leveraged via model tuning, if needed. If a system is designed to facilitate interpretability and self-explanation, a by-product is the ability to support a meaningful tuning functionality, thus taking the insight of business domain experts into account. To enable an eﬀective synthesis of machine intelligence and human insight, the proposed S3 provides an interpretable model and data visualizations. In particular, we focus on developing an interpretable modeling strategy, intuitive human experience and powerful interaction with the data and models. Furthermore, for predictive analytics to be successfully applied at an institution, it needs to be deeply integrated into business process, where decision makers can use it in their natural workﬂow every day. Another issue with a Signals-type model is that it ignores potentially key aspects of learning. One such example is social learning. For example, in [?], social network analysis plays a key role in providing insights into the student learning community and the patterns of peer interactions. In S3, a social network analysis and visualization is incorporated to capture and explain the social learning aspect. Similarly, the treatment of content comprehension is limited to tracking the number of content topics visited. On the other hand, intelligent tutoring systems and related work [?] develop specialized data analysis and domain knowledge representation to model learner behavior and abilities in relation to content usage and knowledge acquisition. In S3, we propose an ensemble strategy whereby a domainspeciﬁc decomposition allows for the development and integration of specialized models and algorithms that are best suited for diﬀerent aspects of learning. In particular, in S3, the proposed decomposition provides an abstraction of learning behavior into semantically meaningful units. Prediction ensembles provide a powerful and ﬂexible paradigm for enhancing the relevance and generalizability of predictive analytics. It can also be viewed as enabling a collaborative platform, whereby institution can plug their own proprietary model as part of the ensemble. Thus, it enables an open, community-driven R&D platform for the application of predictive models to advance learning analytics as well as institutional analytics capabilities. 
 3. STUDENT SUCCESS SYSTEM.
 In this section we provide a functional overview of S3. This will serve as background for the modeling strategy described in Section 4. The overview is not intended to be comprehensive. Our aim is to provide enough context for stating the research problem and our proposed solution. 
 3.1 S3 Functionality.
 The workﬂow for S3 is analogous to the workﬂow associated with the steps in a patient-physician relationship. When a patient sees a physician the basic workﬂow is: a) understand the problem; b) reach a diagnosis; c) prescribe a course of treatment; d) track the success. S3 follows a similar workﬂow. First, upon login to S3 an advisor (a possible role in S3) is presented with a pictorial list of her students. Associated with each student is a risk indicator: green indicates not at-risk, yellow indicates possibly at-risk, and red means at-risk. The advisor can immediately click on a particular student or view the screen showing the list of students in a particular category (e.g. high risk). Next, associated with each student is his Student Proﬁle Screen. The Student Proﬁle Screen provides an overview of the student’s proﬁle, including projected risk at both the course and institution level. The screen also serves as a gateway to other screens, including Course Screens which provide views into course-level activity and risks. The Notes Screen provides case notes associated with the student while Referral Screen provides all the relevant referral options available at the institution. 
 3.2 Data Visualizations.
 As the user of the S3 navigates through the various success indicators, the underlying models and data are presented in an intuitive and interpretable manner, going from one level of aggregation to another. S3 contains a number of visualizations for diagnostic purposes. These include: Risk Quadrant, Interactive Scatter Plot, Win-Loss Chart, and Sociogram. For illustrative purposes we provide a representation of the Interactive Scatter Plot and the Win-Loss Chart. A user of S3 is able to explore the data that make up the predictive model by selecting the success indicators associated with each domain and visualize patterns such as cluster structures and relations between diﬀerent indicators and measures of performance. The chart is also dynamic in the sense that data can be animated to visualize paths/trails depicting changes in learner behaviors and performance over time. 
 Figure 1: Visualization - Interactive Scatter Plot. 
 Another example of the charts available in S3 is the WinLoss Chart. As shown below, one can see at glance how the student compares to peers in the overall success indicator and along each of the sub-indicators. Values above, within, or below average are indicated by green, orange and red bars. Option is provided to compare current indicators with the student’s own history. This option help visualize changes in student’s own learning behavior over time. 
 Figure 2: Visualization - Win-Loss Chart. 
 4. ENSEMBLE MODELING STRATEGY.
 The idea of prediction ensemble is to enable the selection of a whole collection, or ensemble, of hypotheses from the hypothesis space and combine their predictions appropriately. A key rationale is that various indicators of learning success and risks can be found by analyzing diﬀerent aspects of the learning and teaching processes, the educational tools and instructional design, the pre-requisite competencies, the dynamics of a particular course, program or institution, as well as the modality of learning being fully online, live, or hybrid. We argue that there is a need for the discovery and blending of multiple models to eﬀectively express and manage complex and diverse patterns of the elearning process. Ensemble methods are designed to boost the predictive generalizability by blending the predictions of multiple models [?, ?, ?]. For example, stacking, also referred to as blending, is a technique in which the predictions of a collection of base models are given to a second-level predictive modeling algorithm, also referred to as a meta-model. The secondlevel algorithm is trained to combine the input predictions optimally into a ﬁnal set of predictions. Classiﬁer ensembles allow solutions that would be diﬃcult (if not impossible) to reach with only a single model [?]. Stacking, data fusion, adaptive boosting, and related ensemble techniques have successfully been applied in many ﬁelds to boost prediction accuracy beyond the level obtained by any single model [?]. S3 represents a particular instance of the ensemble paradigm. It employs aspects of data fusion as explained in Sec 4.1 to build base models for diﬀerent learning domains. Furthermore, the system utilizes a stacked generalization strategy as explained in 4.2. A best-ﬁt meta-model takes as input predictors the output of the base models and optimally combine them into an aggregated predictor, referred to as a success indicator/index. In this type of stacked generalization, optimization is typically achieve by applying EM (Expectation Maximization) algorithm [?]. 
 4.1 Base Models.
 The data fusion model is useful for building individual predictive models that are well suited for sub-domains of an application. In the context the S3, these models correspond to each data-tracking domain and represent diﬀerent aspects of the learning process. That is, each model is designed for a particular domain of learning behaviour. An initial set of domains are deﬁned as: Attendance, Completion, Participation, and Social Learning. Consider the attendance domain: learner tracking data reﬂecting online attendance is collected, including for example, number of course visits, total time spent, average time spent per session, in addition to other administrative aspects of the elearning activities such as number of visits to the grade tool, number of visits to the calendar/schedule tool, number of news items/announcements read. A simple logistic regression model, or a generalized additive model, is suitable for this domain. On the other hand, in the case of the social learning domain, social network analysis SNA techniques would need to be applied. The work by [?] demonstrates the key importance for specialized analysis of this aspect of the elearning process. In fact, SNA, in conjunction with text mining on learners discourse, is needed for the extraction of meaningful risk factors and success indicators. In other words, the logistic regression model described above for the attendance domain is considered insuﬃcient for meaninful predictive analysis of the social learning domain. In S3 predictive models for each domain are built independently. Each generate an abstracted success sub-indicator represented as a predicted class and an associated probability estimate (ˆ, p), where p = p(Y = y |X), and X denotes domain-related activities being tracked. 
 4.2 Combining Model Ouputs.
 A key design aspect of ensemble systems is the combining process. Combination strategies for ensemble systems are characterized along two dimensions [?]: (1) trainable versus non-trainable rules, and (2) applicability to class labels versus class-speciﬁc probabilities. By selecting a trainable rule, the blending weights associated with the prediction of individual models are optimized to obtain a best-ﬁt meta-model. By selecting a non-trainable combination rule, the business user is able to adjust the weight of the base predictions. For example, in a hybrid course where emplasis on discussion and social learning are primarily conducted face-to-face, the instructor can choose to dampen the eﬀect of the social learning model from the overall prediction. The proposed ensemble system takes advantage of the estimated probabilties in combining the base predictions. In S3, there are three risk-levels, and each base model generates as output a vector of three probability values corresponding to estimated probability for each of the levels “At-Risk”, “Potential Risk”, “Success”. Let {g1 , g2 , . . . , gL } denote the learned prediction functions of L predictive models with gi : X i → (Y, p ∈ [0, 1]c )), ∀i, where Y are the risk categories, p is the associated probablity vector, and c is the number of risk categories, i.e. c = 3. For the described instance of S3 we have L = 4 corresponding to each of the data-tracking domains, at the course grouping/template level. The meta-model takes as input a matrix G with c = 3 columns represent the risk categories and L = 4 predictive models, where gij represents the probablity of risk-level j according to predictive model gi . It also takes as input the corresponding true outcomes y in the training dataset. A simple non-trainable combining process would be to average the values gij for each column of G. Normalization to add to 1 over all categories may be applied. Then, the maximum likelihood principle is applied by selecting the risk category with maximum posterior probability as the aggregated success indicator. Alternatively, the outputs of the base models are used as input to ﬁnd the best-ﬁt secondlevel mapping between the ensemble outputs and the correct outcome (risk level) as given in the training dataset. Typically, to ﬁnd the best-ﬁt meta-model, an iterative k-fold cross validation process is applied [?]. The training dataset is divided into k = L blocks and each of the ﬁrst level model is ﬁrst trained on L − 1 blocks, leaving one block for the second-level model, at each iteration through the L blocks. The process is designed to achieve a reliable model ﬁtting. Linear regression stacking seeks a blended prediction function b represented as b(x) = i wi gi (x), ∀x ∈ X, where a key advantage of this linear model is that it lends itself naturally to intepretation. Furthermore, the computational cost involved in ﬁtting such a model is modest. 
 5. CONCLUSIONS.
 We proposed a holistic ensemble-based analytical system S3 for tracking student academic success. From a design perspective, the unique synthesis of using predictive models to identify at-risk students, creating data visualizations to reach diagnostic insights, and incorporating a case-based methodology for managing interventions provides a just-intime mechanism and personalized approach to improving student retention and student success. From a research perspective, an ensemble-based approach to predictive modeling using semantic decomposition overcomes two signiﬁcant shortcomings in current approaches, namely generalizability and interpretability. Ensemble methods are designed to boost the predictive generalizability by blending the predictions of multiple models. In S3, a stacked generalization strategy is applied to combine the predictions of a collection of base models via a second-level predictive modeling algorithm, a meta-model. The second-level algorithm is trained to combine the input predictions optimally into a more informed set of predictions. To facilitate model interpretability, abstraction of the elearning process into meaningful domains in conjunction with data visualization, interactive and intuitive interface are all part of S3. Furthermore, S3 can be tuned by business experts to best suit their needs. Future work will apply ensemble techniques to real datasets to demonstrate the full power of this methodology.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Student Success System: Risk Analytics and Data Visualization using Ensembles of Predictive Models</rdfs:label>
		<dc:subject>learner success</dc:subject>
		<dc:subject>predictive analytics</dc:subject>
		<dc:subject>data visualization</dc:subject>
		<dc:subject>predictor ensembles</dc:subject>
		<dc:subject>data fusion</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/alfred-essa"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/alfred-essa"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hanan-ayad"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hanan-ayad"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/39/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/alfred-essa"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/hanan-ayad"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/40">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>The Learning Registry: Building a Foundation for Learning Resource Analytics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/40/authorlist"/>
		<swrc:abstract>We describe our experimentation with the current implementation of a distribution system used to share descriptive and social metadata about learning resources. The Learning Registry, developed and released in a beta version in October 2011, is intended to store and forward learning-resource metadata among a distributed, de-centralized network of nodes. The Learning Registry also accepts social/attention metadata—data about users of and activity around the learning resource. The Learning Registry open-source community has proposed a schema for sharing social metadata, and has experimented with a number of organizations representing their social metadata using that schema. This paper describes the results and challenges, and the learning-resource analytics applications that will use Learning Registry data as their foundation.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Learning resources are available from many agencies: federal and national governments, state/provincial agencies, local school districts, post-secondary institutions, and for-profit, and not-for-profit organizations. These resources are distributed across a variety of repositories, using a variety of metadata standards and access mechanisms. Open learning resources should be made publicly available to all—and in principle they are—but in practice users must visit each organization’s repository individually and contend with their various interfaces. Harvesting these resources for use elsewhere can involve complicated metadata crosswalks, and metadata often is incomplete and out of date. A searcher should be able to search across repositories for all of the learning resources available on a particular topic or find ones that have been authored by a particular person or institution. Federated search or registries that collect metadata could provide a single access point to these repositories but require a centralized authority to maintain and prove difficult to upkeep. Updates to descriptive metadata could be obtained from information published on the web by scraping websites, use of microformats on learning resource pages [1], from search terms entered into repositories, or from analysis of online curriculum that uses embedded resources. Yet even if federated search and automated collection of metadata were achievable, searching for learning resources based on descriptive metadata alone (e.g., keywords, author, publication date) may not yield adequate results. Newer methods of locating relevant items take into account characteristics of the searcher to provide recommendations. Online commerce and social networking sites using analytics have demonstrated the value and efficiency of recommendations, but these require that the search mechanism have access to both the properties of the object of search (metadata), and the properties and actions of the searchers and their community. In education, this richer set of properties and actions is increasingly coming from user interactions with portals and repositories, and includes data such as counts of use (in the classroom or online), contexts of use (e.g., what sorts of classrooms/students/teachers), and reflections by users (e.g., ratings, descriptions). However, this rich social metadata— teachers downloading, favoriting, rating, commenting upon—is, at present, locked inside portals and cannot be shared across a broad network of education stakeholders (including researchers). The Learning Registry provides a unique opportunity to share data across these information siloes The Learning Registry is an infrastructure that supports learning resource discovery, sharing, and amplification. Consider the case of an open educational resource (OER)1 about robotic planet exploration (i.e., rovers) developed by a design museum. This design OER could be hosted on the museum’s website in their education corner and include alignment to standards. As the OER becomes better known, links to it may appear in teacher portals, resource repositories/aggregators, or national or state-level curriculum. Related resources, for example, about an actual rover sent to Mars, may be available. This rover-design OER may also be mentioned in blogs, discussion forums, and social media sites. Data about this OER (including related OERs) is scattered ever more widely as it grows in popularity. Learning Analytics research and development efforts will build learning environments that capture different kinds of data about this rover resource: aligned to a standard; used in an online design course; commented upon by an educator. A widely crowd-sourced dataset is necessary to conduct the types of analytics needed for educational improvement. Where does all of this disparate data get aggregated? In the absence of a solution such as the Learning Registry, one option is to centralize all activity about a resource in one site so that attention/social metadata can be captured there. Another option is to create n-way connections among metadata and social metadata sources to share data. A third option might be to release a browser plugin that acts as a sensor to help track user activity and engagements with and around learning resources (e.g., with a learning management system, or LMS). Web crawlers could scrape websites for (static) information about resources; harvesters could extract metadata from repositories; or programs could harvest attention metadata via APIs to social media sites. Yet even with all these approaches, the “big data” still needs to go somewhere. A for-profit company could collect this data and monetize it, but the Learning Registry is working to foster the alternative: that individuals and groups will contribute data to this distributed and vast timeline-oriented dataset for use by researchers and developers to improve teaching and learning with online resources. The Learning Registry makes possible answering empirical research questions such as: how effective is a resource and for whom and what? Learning Registry data could be used to answer questions from analytics: if Learner A uses resources U, V, W; Learner B uses U, V, X; and Learner B shows more competence, then resource X may be worth noting. 
 2. DESIGN OF THE LEARNING REGISTRY.
 The Learning Registry was envisioned as a store-and-forward network based loosely on the NNTP model (e.g., see [2], [3]): a network to which providers of learning resources, metadata and social metadata can distribute information for consumption and amplification by the community. To support these providers, the design must accommodate a large volume of data expressed in a variety of metadata standards. In this section, we provide a brief overview of the design of the Learning Registry. More detailed information can be found in [4] and at the starting points at www.learningregistry.org. 
 2.1 Storing and Distributing Learning Resource Information.
 The Learning Registry accepts, stores, and provides access to learning resource descriptions—metadata or social metadata—as documents expressed in JSON notation. The storage and access mechanism used internally is CouchDB (couchdb.apache.org), a lightweight, open-source document-based database. CouchDB provides data access in the form of views generated by MapReduce functions written in Javascript. Couch’s replication mechanisms make it easy to stand up a network of Couch nodes, which serves the goal of decentralizing the Learning Registry and eliminating single points of failure. Replication can also be leveraged by high-volume users to create their own local repository containing all or some of the Learning Registry data. On top of CouchDB, the Learning Registry provides a layer of services (written in Python) as APIs to publish data, to query documents, and to retrieve documents. These services are the principle public interfaces to the Learning Registry, though developers are welcome (and encouraged) to provide other services in addition to or on top of these services. 
 2.2 Submitting Descriptive Metadata.
 By design, the Learning Registry defines a loose format for the submission of metadata about resources, and does not specify what metadata schema should be used. The Resource Data Description (RDD) document can be used to submit metadata or social metadata (either linked to via a URI or embedded directly as the payload), and can contain a reference to a URI that gives the schema for the metadata. The RDD is a thin wrapper around the submitted metadata; in this way, the Learning Registry can be agnostic about metadata formats and, as the field develops, about formats for social metadata. Early users have used the National Science Digital Library (NSDL) Dublin Core schema [5] or IEEE LOM [6], but nothing precludes the use of newer schemas such as LRMI (lrmi.net) or custom ones (e.g., [7]). We expect that services built on top of the Learning Registry can provide extraction or crosswalk services across RDDs that use disparate standards, or can assemble metadata fields from different schemas into custom views. Documents submitted to the Learning Registry are assigned a unique document ID. This document ID is not a unique identifier for the resource described by the document: multiple documents in the Learning Registry can describe the same resource. We have found it challenging to create a unique way to identify a resource so that these descriptions (metadata or social metadata) can be used for analytics. The document ID of a metadata description is specific to just that one submission of metadata, and, indeed, social metadata may be submitted for a learning resource before the resource itself has been registered with a metadata submission. For the present, the Learning Registry uses the resource’s URL as this global identifier (called resource_locator). Using URL this way is already posing difficulties because in practice multiple URLs might refer to the same resource. (e.g., PBS Learning Media, www.pbslearningmedia.org, desired to redirect its users to local PBS affiliates, turning the URL into something like ca.pbslearningmedia.org). While a solution such as OpenURL could provide a standard URL to match resource metadata with social metadata, it requires a central service. Over time, the Learning Registry community may develop more consistent URL conventions, adopt OpenURL, provide translation services, or simply live with “islands” of data created by non-uniform naming. The Registry may also eventually support assertions that could allow community members to make formal statements to the effect: “I assert that this URL, x, and that URL, y, refer to the same Learning Resource.” and thus allow combined analytics for x and y. Uniquely identifying resources was also a difficulty. In our experimentation, we found that publishers assumed that their local URL was the canonical version. For example, a site such as learnalot.com would submit http://learnalot.com/resource-1, which is where the reference to the learning resource is hosted on their site. However the resource may have originated from http://federalagency.gov/resource-1) and, in order to aggregate all paradata for this resource, a canonical locator must be used. OpenURLs, which are used in many learning contexts such as Google Scholar, rely on a query mechanism to locate a context-sensitive URL for a resource, relative to the requestor. There is currently no way to express an OpenURL in a canonical format such that all context-sensitive copies can have an association expressed within the Learning Registry 
 2.3 Submitting Social Metadata.
 The NSDL Com_para format, created for the STEM Exchange prototype [8], was an early starting point for the social metadata, or “paradata” format. After discussion with the “attention” metadata developers (e.g., [9]), the Com_para format was modified. Then, in an effort to move to a JSON-based format, the Activity Streams format (activitystrea.ms) was investigated and ultimately extended for use in the Learning Registry: the extension was to allow aggregations of data across actors or resources. Our intent is to collect any and all data we can at any grain size, without enforcing a specific vocabulary. Because we are building a data store for analytics, in principle we are not concerned with data volume or expiration. The current plan is to solicit ratings data, such as rated, favorited, and bookmarked. We are also encouraging the submission of usage data, such as downloaded, viewed, or aligned to a standard. Other activity data could be enhancing descriptive metadata, such as commenting and tagging. The schema for social metadata is [actor], [verb], [object] with [modifiers] allowed for [verb] that are most commonly used for measures and dates. A challenge in the submission of social metadata is that the characterization of the actor is left to the publisher of the data. This potentially creates a bias in the data and may limit the ability to do relationship analysis. For example, if a publisher asserts “a student watched video Y,” later analysis cannot determine, from the generic “student,” grade-level, cultural background, or municipality. Our strategy is to build early rudimentary analytics to show the community what can be done with them, and to encourage best practices in social metadata expression. 
 2.4 Retrieving Stored Data.
 The Learning Registry offers two core functions for data retrieval: obtain and harvest. Obtain is used to gather all RDDs at a node, or a subset based on the resource_locator present in the RDD. Harvest is based on OAI-PMH Harvest [10] to gather RDDs or payload data for specific date ranges. Additionally, the Slice service allows users to retrieve documents based on properties of RDDs. Slice is not a deep search of full metadata, paradata (another name for “social metadata”), or the resource itself, but a means of querying high-level properties included in the RDD “wrapper” around such payloads including (1) identity of the resource owner, metadata author, or submitter; (2) date of submission, and (3) “tags” (including keywords, schema format, and the resource data type, i.e., paradata or metadata). We face challenges in our application of CouchDB’s views in storing data. Because CouchDB is not a relational database, it relies on extensive indexing to populate views. As such, users would submit data and not see it immediately via the Slice interface, due to the slowness of updating views. This issue can be addressed by managing user expectations or by developing alternatives to Slice that update their views more quickly. 
 2.5 Special Tools and Issues.
 The Learning Registry community has expressed interest in building out tools and services to connect various data collection platforms to the Learning Registry: e.g., Basic Learning Tool Interoperability (BLTI) interfaces for Learning Management Systems (LMS); usage data from LMSs; social metadata from portals and repositories; and descriptive metadata of interest to K-12 teachers (e.g., alignment to the Common Core). We anticipate these being built out over the coming year. As an example, the Learning Registry has created an OAI-PMH “data pump” to support a one-time extraction of Dublin Core and NSDL_DC metadata from repositories that support an OAI-PMH harvest end point. This script extracts, from the harvested metadata, various field values to create tags (called “keys” in the RDD) for the submitted metadata. An issue for the Learning Registry is ensuring that valid information is submitted. The Learning Registry thus requires that documents submitted must be digitally signed using a key signature. Our approach to identity [11] relies, at present, on a PGP-based signature attached to the document. (Signing submissions turned out to be difficult for some users, and we also found the need to create a PGP key store for some users.) 
 3. INITIAL EXPERIMENTATION.
 Our first experiment with the Learning Registry replicated an existing linkup between NSDL (nsdl.org) and a teacher portal. NSDL provided an OAI-PMH protocol to transfer data to a teacher content management system (CTE Online, cteonline.org) that could harvest math resources and send usage data back. This basic exchange was replaced with Learning Registry functionality to demonstrate feasibility. This early proof of concept led us to create the OAI data pump, which became a useful tool for data extraction. The Learning Registry beta version was opened to an early-adopter community in the September-October 2011 timeframe. This was done to ensure that the concept was viable, to see what problems people faced as they published and extracted their data, and to learn how expressive they found the paradata specification. To demonstrate the utility of data submitted, we created an implementation of a search amplified by Learning Registry data. The AMPS Chrome browser extension2 inspects the results of an Internet search performed on google.com and then injects related activity data and standards alignment data from the Learning Registry. AMPS utilizes a simple mapping from resource URLs to data stored/referenced in the Learning Registry (emphasizing the importance of canonical URLs). AMPS provided a good-proof-of-concept for showing how the Learning Registry can be used to connect data from disparate sources. For example, ISKME submits Achieve rubric data on alignment of open resources to the Common Core Standards, and ratings, e.g., for quality of assessment and interactivity. These data can be correlated, via canonical URL, to ratings data from a distant teacher portal and surfaced in a search, using this extension. Submitters wanted a visual way to see connections among disparate data submitted. The Learning Registry Visual Browser (demolearningregistry.sri.com/browse) provides a browser-based interface to explore data. The user can enter a search term and the browser queries the Learning Registry for documents containing that term as a tag or identity. A list of summaries of these documents is displayed, as an ordinary search engine might, but the browser also displays a cloud of related terms, which allows the user to easily explore the (semantically) nearby “space” of documents in the Learning Registry. (The visual browser for different early-adopters can be accessed from various community pages on the learningregistry.org website.) 
 4. LEARNING ANALYTICS ON LR DATA.
 The alternatives to a system such as the Learning Registry are for searchers to visit individual websites and repositories, for organizations to build federated search or to make n-way connections among resource providers, or for web crawlers to scrape for descriptive and social metadata. As we have described, these are all barriers to resource locating, sharing, and amplifying. The Learning Registry data store provides a unique value to learning-resource analytics that is now not possible. In this section, we describe applications for learning analytics, and for each, how Learning Registry data could be used. - Relationship Mining: The Learning Registry could surface relationships between people based on their attention to resources. What institutions, portals, or groups of users have shared/curated the same resource? - User knowledge modeling: Learning Registry data could be used to compute what a student might be expected to know. In contrast to the coarse characterization of ranges of grade levels present in most metadata, Learning Registry assertions could be specific about the grade or level at which resources were successfully used. - User experience modeling (Are users satisfied?): Learning Registry ratings social metadata could be used to compute satisfaction and thus provide feedback to developers. - User profiling (What groups do users cluster into?): Inverting the [actor] [verb] [object] syntax, we could compute the types of actors who use resources. Submitters of social metadata can also be clustered in categories based on how and when they submit. - Domain modeling (How is content decomposed into components and sequenced?): Curriculum construction tools could gather data on sequencing of learning resources or alignment to standards. - Trend analysis (What changes over time and how?): Trends in attention to different resources could be computed if a sufficiently fine-grain size of social metadata is submitted, because the social metadata specifies a date or date range. - Recommendations (What next actions/resources can be suggested for the user?): The Learning Registry can support recommendations by clustering users or by building a social network graph and then recommending resources among a cluster or network. - Feedback, Adaption, and Personalization (What actions should be suggested for the user? How should the user experience be changed for the next user?): Learning Registry data could provide feedback to developers about the utility of their resources, about who adapts them and how, and could eventually cause “widespread sharing” of learning resources to learners at the appropriate time. 
 The Learning Registry community is now creating analytics-based applications such as these using its unique timeline-organized dataset. 
 5. ACKNOWLEDGMENTS.
 SRI International’s work on this project is supported by the US Department of Education (ED-04-CO-0040/0010). The opensource Learning Registry project was conceived by Steve Midgley, US Department of Education, and Dan Rehak, Advanced Distributed Learning Initiative (ADL), U.S Department of Defense. Susan Van Gundy, UCAR and NSDL, developed the first paradata specification and Aaron Silvers (ADL) amplified it based on the activity streams specification. Joe Hobson of Navigation North worked out many of the bugs in submitting metadata and paradata into the Learning Registry. Pat Lockley conceived of the Chrome search plugin. Technical support is provided by Lockheed Martin Global Training and Logistics under contract to the US Dept. of Defense Many other people and organizations contributed input and submitted data for the early proof-of-concept. A complete list of community members can be found at www.learningregistry.org. The Learning Registry code is stored in an open GitHub repository, https://github.com/LearningRegistry/.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>The Learning Registry: Building a Foundation for Learning Resource Analytics</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>attention metadata</dc:subject>
		<dc:subject>social metadata</dc:subject>
		<dc:subject>learningresource analytics</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/marie-bienkowski"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/marie-bienkowski"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/john-brecht"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/john-brecht"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-klo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-klo"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/40/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/marie-bienkowski"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/john-brecht"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-klo"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/41">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Challenges and Opportunities for Learning Analytics when formal teaching meets social spaces</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/41/authorlist"/>
		<swrc:abstract>Social networking is revolutionizing the world in ways few imagined just a few years ago. The power of social networking technology can also be leveraged to improve education and enhance the instructor and learner experience. Unlike conventional learning management systems, social software environments such as Athabasca Landing provide a persistent space and are flexible enough to support social and learner-led methods of informal, non-formal, and formal learning. Analytics can be used to effectively track and measure personal progress and help uncover extra-curricular factor affecting learner success such as network formation and growth. The paper reports on an attempt to explore this problem through analysis of student behaviour on the Athabasca Landing site within the context of a course. Its findings, explanation, and potential implications are listed. Effects of social learning on learners, based on the learner's behaviour before, during, and after the course are described and discussed. Finally, features of an open source tool created for this analysis, LASSIE is presented.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 This paper describes the rationale and design of LASSIE (Learning Analytics for Social Systems in Institutional Education) an open source analytics tool for Athabasca Landing, a social site implemented at Athabasca University, which supports both formal and informal learning as well as many other social and practical applications. The paper begins by exploring salient differences between the Landing and a learning management system. We go on to discuss LASSIE’s objectives, design decisions, architecture and functionality, followed by descriptions of its data extraction, graphing, trend discovery, and statistical features. We describe the value of the system and discuss its limitations and future plans for development. 
 1.1 Learning Spaces.
 Learning management systems (LMSs) such as Moodle (www.moodle.org), BlackBoard (www.blackboard.com), and Desire2Learn (www.design2learn.com) have, historically, tended to model and replicate traditional classroom and institutional processes and consequently tended to embed institutional processes and forms such as courses, formal assessments, timetables, classes and hierarchies of control. They do not lend themselves well to different, more learner-centric approaches and are the cause of increasing dissatisfaction among educators (e.g.[1-4]). In recent years, alternatives that go beyond the LMS have begun to enter the field, with potential for greater learner control and rich tools for media creation and sharing. (e.g. [5-16]). These can present new challenges for learning analytics (LA) as less formal structure in a learning space leads to less easily analysable forms of data. Social media are soft technologies from which emerge patterns and usages that are not part of the hard design of the system but that are overlaid on top of it. Without clearly demarcated courses, lessons, learning outcomes and so on, there is a need for a flexible analytics toolset that can be adapted constantly to cater for many different structures of data. Athabasca University (AU) is an entirely distance-based university. To support its distributed population, we have built Athabasca Landing, an Elgg-based beyond-the-LMS social system. The Landing connects AU staff and students who are distributed over a vast geographical region, providing a soft space with rich tools and a comprehensive infrastructure for sharing and connecting with others, including blogs, wikis, bookmark sharing, photo sharing, event scheduling, group formation, social tagging, file sharing, podcasting, video sharing, profile creation, social networking and much more. It is more of a social construction-kit than a purpose-driven space. It supports many social forms including explicit groups, social networks, and set-based categorizations. It is as much as possible owned, shaped and controlled by the people who use it. It is a persistent space not defined by course or program temporal boundaries. There is no prescribed way of structuring courses in Landing. Most Landing users are not using it to support or attend a specific course and have joined it to communicate and network with other users, and those who are taking courses may engage with the site in many other ways. Thus the activity of a user taking a course on Landing is neither course specific nor is his or her course activity restricted to the course. 
 1.2 Learning Analytics.
 LMSs are, typically, course-centric and follow a rigid structure, thereby focusing and delimiting the scope of analytics. Courses have defined durations and students are only active within those time frames and can only interact within the confines of the course structure. Social learning environments such as the Landing provide enormous flexibility to instructors and learners alike and the analytics of this data can potentially explain the influence of extra-curricular activities on learner success such as the influence of a learner’s social network and extra-course activities. However, the blurred edges and flexible nature of the system presents a problem for those attempting to analyse how it is used. There is no strict definition of a course, course structure, a diverse set of learning objects can be used in innovative ways, and the instructor may choose to use the social space partially or fully for his course. Analytics applied to course delivery on the Landing poses numerous challenges when compared with that using a traditional LMS but also provides unique opportunities not possible in the formally bounded space. Students’ activity can be analyzed inside and outside the context of the course so, for example, friends network of a student and its correlation to student success can be analyzed and, in principle, it should be possible to observe diffusion of knowledge and connections beyond a fixed set of formal learning transactions. 
 1.3 Related Work.
 Data from a database can be extracted in several different contexts and tools have been proposed for different sorts of data extraction. [16, 27]. DeLeS [27] offers useful analytics but the analytics is limited to the functions programmed in the system. AAT [25] is not only limited to pre-programmed analytics queries but also allows users to specify data, create queries, and design reports using a graphical user-interface. Neither DeLeS not AAT is designed to work with social learning environments. SNA software (Social network analysis) is frequently used to analyse social systems data and many studies have been conducting on social systems using SNA. SNA numerically or visually describe features of a network for the purpose of quantitative or qualitative analysis. SNAs predate social networking and most are designed to analyse a network rather than a social network. Cytoscape (www.cytoscape.org) is a powerful network graphing and analysis tool that has been used in anything from analysing social networks and web networks to biological networks. Sonivis (www.sonivis.org) is geared more toward wiki spaces. Recently, SNAs geared towards social networking such as SocNetV [23], SNA-network [24], and Statnet (www.statnet.org) have also appeared on the market but they only provide analysis from a network perspective. To date, we have not found a software tool designed to analyse formal learning within a social network, leading to the decision to develop LASSIE. 
 2. LASSIE.
 LASSIE, available as a standalone software tool and as an Elgg plugin (with limited features), enables the analysis of user behaviour within and outside the context of formal learning. LASSIE is capable of extracting data, graphing it, and performing statistical calculations. It could be used to view data, trends, and correlate data to facilitate interpretation. Data are displayed in sortable tables and can also be downloaded in CSV format for further analysis using tools such as spreadsheet software, SNAs or R-language statistical packages. It can be used to observe activity of an individual user, a group of people, or the overall activity in different time slices. Furthermore, it supports time slicing, allowing users to view data from specific periods of interest. LASSIE also provides a REST-based web service support for future integration with SNAs and other software tools. Initially, LASSIE was designed as an Elgg plugin.. To cater for more data sources, LASSIE’s development was forked into a standalone application while maintaining the Elgg plugin as a cut-down version for simpler real-time and ad hoc queries. Figure 1 shows a screenshot of the standalone application. 
 Figure 1. Screenshot of LASSIE. LASSIE provides a wizard-based user-friendly GUI with plenty of documentation. 
 To cater for difficulties in defining course boundaries and user activity related to courses, LASSIE allows users to define these boundaries themselves. For example, a user may define course activity to be all activity in a group and/or all activity by a group of persons within a time period, and/or to filter according to specified tags, or relationships with others such as the teacher. LASSIE provides powerful and customizable reporting to empower users to make informed and intelligent interpretations of the data. Definitions of course, user activity, and learning objects can be specified for a single analysis or globally re-used. 
 3. RESULTS AND DISCUSSIONS.
 Analytics of social systems is data rich, noisy, influenced by many external factors and is therefore often difficult to interpret. For example, loss of activity or significant reduction in activity of a user for an extended period could be due to changes in personal life, tragedies, changing priorities, or due to a technical hiccup in the system. LASSIE provides analytics in the form of raw, numerical, and visual data for users, entities, entity groups, analytics of the entire site and correlation between any of these. Interpretation of data is left to humans. The system allows detailed and summarized tracking of user activity on Landing and the usage of different content types over a period of time. For example, Figure 2 shows a timeline of the collective activity of a group of users who took a course. The two high peaks correspond to first and last week of the course. As expected, there is high level of activity at the start and end of the course with some variation in the middle. The activity during the course is higher than the activity before or after the course. Activity on Landing continues to be higher after the course than before the course but only with selected tools such as blogs. Not surprisingly, spikes in activity in this course correspond to assessments and teacher-specified activities. LASSIE continues to provide very useful and insightful statistics about the Landing, its content, and how it is used. Discussing all the statistics and their interpretations is not possible in this paper, but we provide a few examples to illustrate the kind of analyses that are enabled by the system. Analyses show that a small subset of students who were not active before a course become active users during the course and remain active users past the course end date. Students who do not participate in extracurricular activities outside their courses tend to become inactive upon termination of the course. Course designs that favour social engagement will usually favour those who like to engage socially and the time on task that results from this may be the cause of greater motivation or the result of it. 
 Figure 2. Collective blog, file and page posting activity before after and during the course. Two spikes corresponds to first and last assignments of the course. 
 Figure 3 shows correlation between membership increase and addition of new pages to Landing. It also shows a constant decrease in the average content posted per user. Around 18 – 21 percent of the users are active users. It is interesting to observe, however, that different tools encourage quite different patterns of engagement: wiki pages, for instance, are intended to be used for collaborative content development while blogs are used for discussions on different topics. An active user is difficult to define. Many users become highly active or completely inactive for different periods of time. Many users are active readers and viewers but hardly post anything on the site. For the purpose of this discussion, an active user is someone who posts at least one content entry per month. Significant numbers of users continue to have a one-way relationship with Landing. They read and view regularly but do not participate with comments, votes, postings, or messages. 
 Figure 3. Most of the pages are posted by a smaller group of users. As the number of members increases, this becomes more apparent. Decimal average was multiplied by 1000 to show the line in the graph. 
 Users have tool and content preferences. Most users tend to use one or two tools (e.g. bookmarks, pages) most often. They tend to participate more in courses where their preferred tools part of the course structure. Group activity spikes are generally reflected in activity of users and spikes in activities of users are generally reflected in one or more groups. 78% of Landing content was created by 12% of the users. 91% of them are either directly linked or one link apart. A user following someone or in a shared group is a direct link. If a user’s direct link is linked to another user that he is not linked to, then the user is one link distance apart. Generally, users who follow many other users tend to be more active than people who follow fewer users. Many different networks can be constructed from social systems. Network of people following, network by groups, and content-posting network are some examples. Since most networks are based on some kind of activity, active users tend to be part of larger networks. Many networks such as networks of followers and followed users are scale-free networks. Degree distribution of scale-free networks follows the Power Law. If a node is removed from the network at random, there would be no significant change in connectivity, however, if the a hub node is removed, the loss of connectivity would be significant. Since the Landing project started, only one relatively inactive member has closed the account and three were banned, making it difficult to observe Power Law properties. However, LASSIE allows us to step back in time, remove a node and observe the effects. The findings confirmed the assumption that the networks are scale-free and that their degree distribution is governed by Power Law. Median and least active users in a course tend to become inactive after course completion. Active users tend to remain active in Landing after course completion date. Users participating in extra-curricular activities on the Landing during a course tend to continue participating in those activities past the course completion date. LASSIE provides valuable information about users over a timeline. Currently, the meaning behind the details in a timeline could be anyone’s guess, however, as the correlation capabilities of LASSIE improves and predictive modeling capabilities are added, it would be possible to identify in course and extracurricular behavior which leads to success in a course. Furthermore, this information would be highly valuable in counseling learners and designing courses that would facilitate learner success. The ultimate goal of a learning analytics software system is to be capable of clearly identifying activities and behaviour, which would lead to greater levels of success in a course. Such a software tool has yet to be invented for traditional or digital classrooms. Currently LASSIE provides invaluable information and tools to help us interpret the data. Many interesting correlations have been uncovered, however, caution has been taken not to jump to conclusions without further investigation, which requires adding additional capabilities to the software tool. Offering a course on a social system is not sufficient to improve learner success. The course structure needs to take the course material and user preferences into account. This cannot be accomplished without analytics software that can show how users are behaving in a social system and how they are responding to course structure and their online social environment. LASSIE provides this information but it is up to us to interpret the analytics data accurately. 
 4. CONCLUSION.
 In this paper, the LASSIE analytics software tool has been introduced and some examples provided of its use. It is a powerful, user-friendly and flexible tool designed to analyse learner and user activity in a social networking system. LASSIE is sufficiently flexible to allow course designers and instructors to explore beyond the course group area, allowing them to evaluate the performance of courses, course structures, as well as learner responses to new learning objects. It allows them to better understand their students and their activities. This information serves to help learning designers and instructors adapt, extend, and revise the course material and activities to achieve the pedagogical goals. As it develops, the tool becomes softer, more flexible, more capable of connecting disparate data. The softness and flexibility of the tool, like the Landing itself, allows many creative and unforeseeable uses. This means that greater care must be taken not to jump to conclusions without cross-checking correlations and their possible interpretations. Our next major challenge is to extend LASSIE to be able to work more effectively with the inverse set of those activities that are not course related yet which lead to learning. Analysis of a soft system, in which many of the technical and organisational processes are not embodied in software but in external systems and the minds of the users of that software requires a slightly different approach from that used for systems where goals, needs, methods and processes are clearer. Softer technologies need softer analytics LASSIE, like the eponymous sheepdog famed in Hollywood movies, has begun to herd and make sense of the information we need.]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Challenges and Opportunities for Learning Analytics when formal teaching meets social spaces</rdfs:label>
		<dc:subject>social systems</dc:subject>
		<dc:subject>formal learning</dc:subject>
		<dc:subject>informal learning</dc:subject>
		<dc:subject>analytics</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nazim-rahman"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nazim-rahman"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-dron"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-dron"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/41/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/nazim-rahman"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-dron"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/42">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/proceedings"/>
		<dc:title>Does the Length of Time Off-Task Matter?</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/42/authorlist"/>
		<swrc:abstract>We investigate the relationship between a student’s time off-task and the amount that he or she learns to see whether or not the relationship between time off-task and learning is a more complex model than the traditional linear model typically studied. The data collected is based off of students’ interactions with Cognitive Tutor learning software. Analysis suggested that more complex functions did not fit the data significantly better than a linear function. In addition, there was not evidence that the length of a specific pause matters for predicting learning outcomes; e.g. students who make many short pauses do not appear to learn more or less than students who make a smaller number of long pauses. As such, previous theoretical accounts arguing that off-task behavior primarily reduces learning by reducing the amount of time spent learning remain congruent with the current evidence.</swrc:abstract>
		<led:body><![CDATA[

 1. INTRODUCTION.
 Today, students are interacting with technology of various forms more than ever during learning. One context where this is occurring is in middle school and high school mathematics classes, where learning increasingly occurs from students using educational software in a classroom with a teacher present. One such form of educational software which is used progressively more in the United States is Cognitive Tutor software [11], where student learning is individualized based on assessment of the student’s current learning and the factors leading to a specific student error. Cognitive Tutors are now used in more than 6% of U.S. secondary schools. Cognitive Tutors have been shown to enable individual students to learn at their own pace, while empowering teachers to spend instructional time in one-on-one teaching episodes with the students who are struggling most [16]. Educational software like Cognitive Tutors provide extensive logs of student performance [12], enabling not only more effective learning for individual students, but supporting analysis of student learning over time, using methods from learning analytics [17] or educational data mining [3]. In this paper, we study the relationship between a student’s learning gain and his or her off-task behavior. One key model for this relationship is Carroll’s Time-On-Task hypothesis. This hypothesis argues that off-task behavior reduces learning by reducing the amount of time spent on-task [7]. However, there are several factors that may complicate this relationship. In particular, it is possible that taking a short break may improve cognition afterwards [cf. 13]. Hence, short pauses may impact learning differently than longer pauses. In addition, it is possible that a qualitative difference may be seen between students who go offtask for a break, and students who are more fundamentally disengaged; hence, students who are off-task for large proportions of time may have greater reduction of learning than could be anticipated through a simple linear model. For this reason, we investigate the differences between several models of the relationship between off-task behavior and learning, leveraging both quantitative field observations of off-task behavior [cf. 2] and an automated detector of off-task behavior [cf. 4] to measure both overall prevalence of off-task behavior and the duration of individual episodes. We looked at the percent of time off-task and the number of brief and lengthy off-task episodes to study the relationship between these factors and learning gain. Through the use of statistical analyses on medium-sized educational data sets, a form of learning analytics [cf. 17], we can better understand how off-task behavior influences learning and under what conditions off-task behavior influences learning differently. A learning analytics analysis using similar methods includes research on student activities during writing [cf. 5]. Past studies conducted on students using educational software have generally shown a negative correlation between off-task behavior and learning during the use of the software [8, 9, 15]. A similar pattern has been seen when studying these relationships outside of technology, generally finding a negative relationship between learning and off-task behavior (see [6] for an extensive review of this literature). However, these studies have typically not explored non-linear relationships. A study done by Karweit and Slavin, however, found that changing the length of observation periods affected the strength of the relationships between off-task actions and learning [10]. This supports the notion that the length of off-task episodes may be predictive of student learning, as well as the overall quantity of time spent offtask. 
 2. DATA.
 The students in this experiment used educational software in the domain of scatterplots, a subject taught in the data analysis portion of middle school mathematics in the United States. Initially, students took a pre-test to determine how well they knew the material at hand. Afterwards, the students interacted with a Cognitive Tutor lesson teaching this topic [1], for approximately 80 minutes apiece. Finally, a post-test assessment was given to evaluate the students’ progress. Full details on the assessments are given in [1]. 186 students completed the pre-test, the tutor activity, and the post-test. These students were drawn from multiple previous studies in separate years [cf. 8], but each used the same tutor software under the same conditions (in some studies, these students served as the control condition which was compared to a modified version of the tutor – students using modified versions of the tutor are not analyzed in this paper). Data on student off-task behavior was gathered using two methods. While students engaged in the cognitive tutor classroom [11], two observers recorded student’s behaviors using quantitative field observations [2]. The students were observed using peripheral vision in order to decrease potential observer effects, in a sequence of 20-second long observations cycled across students. In each observation, the student’s behavior was noted in terms of whether it involved off-task behavior [2, 4], in order to compute the percentage of time each student was offtask. Off-task behavior was defined as any of the following: offtask conversation (talking about anything other than the subject material), off-task solitary behavior (any behavior that did not involve the tutoring software or another individual, such as reading a magazine or surfing the web), and inactivity (such as staring into space, or the student putting his/her head down on the desk, for at least 20 seconds – brief reflective pauses by a student actively using the software were not counted as off-task). Other behaviors such as actively working in the software, collaborating with other students, and gaming the system (intentionally misusing the software in order to successfully complete problems [cf. 2]) were not counted as being off-task. The second method used an automated detector of off-task behavior developed using data mining [4], built using the field observations as ground truth. The off-task detector is a latent response model used to infer exactly when off-task behavior occurs, from features of individual student actions and recent student behavior before those actions. The detector was shown to achieve a correlation over 0.5 to the proportion of off-task behavior observed, under student-level cross-validation. As such, we use the detectors’ inferences as components in further analysis, a process termed “discovery with models” [3]. As offtask episodes can sometimes be caught by the detector from the behavior occurring shortly after the actual off-task episode (in which case they are identified by very quick actions coming after the a long pause) [4], we label each off-task pause with the length of the longest pause in the sequence of 5 student actions considered by the detector when making an inference. 
 3. ANALYSIS.
 3.1 Percent of Time Off-Task.
 The first relationship that we considered was the percentage of time the student spent off-task while using the cognitive tutor. Post-test score was the indicator used as an assessment of the student’s eventual learning. 
 Figure 1: The predicted post-test score (from the non-linear model below) compared to the percent of time-off-task. 
 With the analyses in this section, we measure each student’s proportion of off-task behavior, using the field observations, as this is the most standard method for assessing off-task behavior, used by researchers for decades [e.g. 2, 4, 6, 9, 10]. A student’s proportion of off-task behavior, is statistically significantly negatively correlated to their post-test score, r= -0.229, F(1,184) = 10.150, p<0.01. The student’s pre-test score was statistically significantly positively correlated to the post-test as well, r= 0.299, F(1, 184) = 18.007, p<0.001. In order to study the relationship between off-task behavior and learning, we can analyze the relationship between off-task behavior and the post-test, while controlling for the pre-test. The best-fitting linear model of this relationship is: 
 Post = 0.273Pre - 0.394OT + 0.617.
 Within this model, the off-task term was significantly different than chance, t(185)= -2.412, p=0.017. We can also investigate a non-linear model (shown in Figure 1), including the percentage of time off-task, squared, which produced the best-fitting equation: 
 Post = 0.275Pre - 0.848OT2 + 0.598.
 For the off-task squared term, t(185)= -2.295 and p=0.023. Hence, both linear and quadratic models based on off-task behavior are significant predictors of student off-task behavior. In order to investigate whether one model is significantly better than the other, we compare the models using the Bayesian Information Criterion for Linear Regression, BIC’ [14]. BIC’ is a formula used to see how well a specific model predicts the data given the number of parameters (e.g. models with more parameters should achieve better fit simply by chance). It can also be used to compare two non-nested models of the same dependent measure, so we can use it to compare the models predicting post-test using the proportion of off-task behavior in a linear fashion, and the proportion of off-task behavior, squared. In the first case, where pre-test and percent of time off-task predict post-test, the BIC’ produced a result of -12.685; while pre-test and percent of time off-task squared produced a result of -12.255. Although the regression model that uses time off-task squared produces a higher r value, the difference in the BIC’ of the two models is only 0.430. This indicates that the two models are not statistically different, which would be indicated by a difference of 6 or greater [14]. Hence, there is not a strong justification for preferring a non-linear model of the relationship between off-task behavior and learning, to a linear model, although there is a trend in that direction. 
 3.2 Number of Brief/Lengthy Times Off-Task.
 A second question is whether lengthy pauses impact learning in a different fashion than brief pauses. It has been shown that breaks in the workplace can reduce mental fatigue and ultimately lead to better employee performance on cognitive tasks [13]. Thus, it is possible that the number of times a student has either brief or long pauses affects their learning gain. Within the analyses in this section, we use the automated detector of off-task behavior rather than the field observations of off-task behavior. The type of field observation used when the data was first collected – round-robin observations of an entire classroom by one or more field observers – gives a useful representation of the total proportion of time each student is off-task, but it does not shed light on how long individual episodes of off-task behavior are. By labeling every student action across the entire session (and gaps between actions) as to whether it is off-task or not, automated detectors allow us to analyze the length of individual episodes of off-task behavior. An analysis predicting learning using the total number of times off-task gave the following results. These two variables had a correlation of 0.053, F(1,184)=0.519 and p=0.472. When pre-test was included as a covariate, there was not a substantial difference for the term indicating the number of times off-task: t(185)= 0.027, p= 0.979. Hence, these results show that the total number of off-task episodes as indicated by the detector is not predictive of learning; however, when broken into brief and lengthy episodes there may be a significant relationship. To investigate the difference between lengthy and brief pauses, we split the off-task episodes, as assessed by the detector, by their length in two fashions. First, a median split was conducted in terms of the length of an off-task episode. Episodes that were shorter than the median 65.9 seconds were classified as “brief” off-task episodes, whereas episodes that were longer than the median were classified as “lengthy” off-task episodes. We also conducted a quartile split, and compared the shortest-time quartile (less than 26.0 seconds) of the off-task episodes to the longest-time quartile (longer than 124.9 seconds) of the off-task episodes. In this manner, we can examine the difference in the correlation between learning gain and the number of times a student spent off-task, between brief off-task episodes and lengthy off-task episodes. We first analyze the median split. The relationship between the number of off-task behavior episodes shorter than 65.9 seconds and the post-test was not significant, r= -0.028, F(1,184)=0.142, p=0.706. The relationship between the number of off-task behavior episodes longer than 65.9 seconds and the post-test was surprisingly also not significant, r= -0.056, F(1,184)=0.586, p=0.445. These patterns remained non-significant even when included the pre-test as a covariate. There was essentially no difference between these two models, BIC’ = 0. A similar pattern is seen when comparing the top quartile and bottom quartile. The relationship between the number of off-task behavior episodes shorter than 26.0 seconds and the post-test was not significant, r= -0.101, F(1,184)=1.897, p=0.170. The relationship between the number of off-task behavior episodes longer than 124.9 seconds and the post-test was surprisingly also not significant, r= -0.077, F(1,184)=1.098, p=0.296. This pattern remained non-significant even when pre-test was included as a covariate. The difference between these two models was not significant, BIC’ = -1.501 
 4. DISCUSSION AND CONCLUSIONS.
 In general, this paper replicated previous findings showing a negative relationship between off-task behaviors assessed using field observations and learning. Our results showed that there was a significant relationship between the percentage of time the student spent off-task (assessed by the field observations) and learning. There was also evidence for a relationship between the proportion of off-task behavior squared, and learning. However, the difference between the quadratic and linear models of this relationship was not significant; suggesting that more complex models than the model hypothesized by Carroll may not be justified. There was also not strong evidence that brief off-task episodes impact learning differently than longer off-task episodes. One surprise in the findings was that the number of episodes identified by the off-task detector was not predictive of learning, either for brief episodes or lengthy episodes. Previous analyses have found a significant relationship between the proportion of off-task behavior identified by the detector and learning gains [e.g. 3]. Those analyses were conducted in a broader data set, including data from other versions of the same learning software, and other tutor lessons. In this paper, we analyzed a more focused data set, in order to explore different models in detail without needing to consider this type of factor. But it is possible that features specific to this sub-set of the data or the associated tutor lesson led to the null result seen here. Therefore, it may be valuable to replicate these analyses in a larger data set. In summary, the data in this study appeared to accord with Carroll’s time on-task hypothesis [7]. Currently, there is not sufficient evidence to suggest that a more complex relationship exists between learning gain and off-task behavior, in terms of the temporal aspects of off-task pauses. However, this issue may be worth further investigation in additional data, before this result can be considered conclusive. 
 5. ACKNOWLEDGEMENTS.
 We would like to thank Art Graesser for helpful comments and suggestions, Angela Wagner for her help collecting the data re-analyzed here, and National Science Foundation, award #SBE-0836012, "Toward a Decade of PSLC Research: Investigating Instructional, Social, and Learner Factors in Robust Learning through Data-Driven Analysis and Modeling".]]></led:body>
		<swrc:month>May</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Does the Length of Time Off-Task Matter?</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>off-task behavior</dc:subject>
		<dc:subject>intelligent tutoring systems</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-roberge"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-roberge"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/anthony-rojas"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/anthony-rojas"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/42/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-roberge"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/anthony-rojas"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
	</rdf:Description>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/american-institutes-for-research">
		<rdfs:label>American Institutes for Research</rdfs:label>
		<foaf:name>American Institutes for Research</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/darren-cambridge"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kathleen-perez-lopez"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/american-university-in-bosnia-and-herzegovina">
		<rdfs:label>American University in Bosnia and Herzegovina</rdfs:label>
		<foaf:name>American University in Bosnia and Herzegovina</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/denis-smolin"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/apollo-group">
		<rdfs:label>Apollo Group</rdfs:label>
		<foaf:name>Apollo Group</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-barber"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mike-sharkey"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/athabasca-university">
		<rdfs:label>Athabasca University</rdfs:label>
		<foaf:name>Athabasca University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dragan-gasevic"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/liaqat-ali"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/george-siemens"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nazim-rahman"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-dron"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sabine-graf"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/cindy-ives"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/arnold-ferri"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/ball-state-university">
		<rdfs:label>Ball State University</rdfs:label>
		<foaf:name>Ball State University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/brian-j-mcnely"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/paul-gestwicki"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/j-holden-hill"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/philip-parli-horne"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/erika-johnson"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/beuth-university-of-applied-sciences">
		<rdfs:label>Beuth University of Applied Sciences</rdfs:label>
		<foaf:name>Beuth University of Applied Sciences</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/brandeis-university">
		<rdfs:label>Brandeis University</rdfs:label>
		<foaf:name>Brandeis University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/johann-ari-larusson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/bjorn-levi-gunnarsson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-alterman"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/brunel-business-school">
		<rdfs:label>Brunel Business School</rdfs:label>
		<foaf:name>Brunel Business School</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tariq-m-khan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/fintan-clear"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/samira-sadat-sajadi"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/desire2learn-incorporated">
		<rdfs:label>Desire2Learn Incorporated</rdfs:label>
		<foaf:name>Desire2Learn Incorporated</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/alfred-essa"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hanan-ayad"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology">
		<rdfs:label>Fraunhofer Institute for Applied Information Technology</rdfs:label>
		<foaf:name>Fraunhofer Institute for Applied Information Technology</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/katja-niemann"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/uwe-kirschenmann"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/maren-scheffel"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/fzi-research-center-for-information-technologies">
		<rdfs:label>FZI Research Center for Information Technologies</rdfs:label>
		<foaf:name>FZI Research Center for Information Technologies</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/veronica-rivera-pelayo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/valentin-zacharias"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lars-muller"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/simone-braun"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/graz-university-of-technology">
		<rdfs:label>Graz University of Technology</rdfs:label>
		<foaf:name>Graz University of Technology</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/barbara-kump"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/christin-seifert"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/stefanie-n-lindstaedt"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-schon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-ebner"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/georg-kothmeier"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/kuleuven">
		<rdfs:label>K.U.Leuven</rdfs:label>
		<foaf:name>K.U.Leuven</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-luis-santos"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sten-govaerts"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/know-center-gmbh">
		<rdfs:label>Know-Center GmbH</rdfs:label>
		<foaf:name>Know-Center GmbH</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/guenter-beham"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/marist-college">
		<rdfs:label>Marist College</rdfs:label>
		<foaf:name>Marist College</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/eitel-jm-lauria"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/joshua-d-baron"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mallika-devireddy"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/venniraiselvi-sundararaju"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sandeep-m-jayaprakash"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/northwestern-university">
		<rdfs:label>Northwestern University</rdfs:label>
		<foaf:name>Northwestern University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/bruce-sherin"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/open-universiteit">
		<rdfs:label>Open Universiteit</rdfs:label>
		<foaf:name>Open Universiteit</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/schreurs-bieke"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/de-laat-maarten"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherland">
		<rdfs:label>Open University of the Netherland</rdfs:label>
		<foaf:name>Open University of the Netherland</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherlands">
		<rdfs:label>Open University of the Netherlands</rdfs:label>
		<foaf:name>Open University of the Netherlands</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/wolfgang-greller"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/purdue-university">
		<rdfs:label>Purdue University</rdfs:label>
		<foaf:name>Purdue University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kimberly-e-arnold"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/matthew-d-pistilli"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/saarland-university">
		<rdfs:label>Saarland University</rdfs:label>
		<foaf:name>Saarland University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-schmidt"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tim-krones"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/simon-fraser-university">
		<rdfs:label>Simon Fraser University</rdfs:label>
		<foaf:name>Simon Fraser University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/melody-siadaty"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksandar-giljanovic"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/marek-hatala"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/solbridge-international-school-of-business">
		<rdfs:label>SolBridge International School of Business</rdfs:label>
		<foaf:name>SolBridge International School of Business</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sergey-butakov"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/south-orange-county-community-college-district">
		<rdfs:label>South Orange County Community College District</rdfs:label>
		<foaf:name>South Orange County Community College District</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-bramucci"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-gaston"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/sri-international">
		<rdfs:label>SRI International</rdfs:label>
		<foaf:name>SRI International</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/marie-bienkowski"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/john-brecht"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-klo"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/tallinn-university">
		<rdfs:label>Tallinn University</rdfs:label>
		<foaf:name>Tallinn University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tobias-ley"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/the-open-university">
		<rdfs:label>The Open University</rdfs:label>
		<foaf:name>The Open University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/doug-clow"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/haiming-liu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-slade"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/fenella-galpin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-de-liddo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michelle-bachler"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/elpida-makriyannis"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/the-open-university-in-scotland">
		<rdfs:label>The Open University in Scotland</rdfs:label>
		<foaf:name>The Open University in Scotland</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ronald-macintyre"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/the-university-of-sydney">
		<rdfs:label>The University of Sydney</rdfs:label>
		<foaf:name>The University of Sydney</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/walter-christian-paredes"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kon-shing-kenneth-chung"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/trinity-college-dublin">
		<rdfs:label>Trinity College Dublin</rdfs:label>
		<foaf:name>Trinity College Dublin</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/john-mcauley"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/alexander-o-connor"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dr-dave-lewis"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-">
		<rdfs:label>Universitat Oberta de Catalunya (UOC)</rdfs:label>
		<foaf:name>Universitat Oberta de Catalunya (UOC)</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/german-cobo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-monzo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-melenchon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/josep-grau-valldosera"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/julia-minguillon"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-carlos-iii-of-madrid">
		<rdfs:label>University Carlos III of Madrid</rdfs:label>
		<foaf:name>University Carlos III of Madrid</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/derick-leony"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/abelardo-pardo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/luis-de-la-fuente-valentin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/david-sanchez-de-castro"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-belgrade">
		<rdfs:label>University of Belgrade</rdfs:label>
		<foaf:name>University of Belgrade</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jelena-jovanovic"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nikola-milikic"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/zoran-jeremic"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-bristol">
		<rdfs:label>University of Bristol</rdfs:label>
		<foaf:name>University of Bristol</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ruth-deakin-crick"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-california,-berkeley">
		<rdfs:label>University of California, Berkeley</rdfs:label>
		<foaf:name>University of California, Berkeley</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/brandon-white"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-hawaii">
		<rdfs:label>University of Hawaii</rdfs:label>
		<foaf:name>University of Hawaii</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kar-hai-chu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/devan-rosen"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/victor-miagkikh"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-leeds">
		<rdfs:label>University of Leeds</rdfs:label>
		<foaf:name>University of Leeds</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ahmad-ammari"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lydia-lau"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vania-dimitrova"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-louisville">
		<rdfs:label>University of Louisville</rdfs:label>
		<foaf:name>University of Louisville</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/leyla-zhuhadar"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-michigan">
		<rdfs:label>University of Michigan</rdfs:label>
		<foaf:name>University of Michigan</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/steven-lonn"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-e-krumm"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/r-joseph-waddington"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/stephanie-d-teasley"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tim-mckay"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kate-miller"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jared-tritz"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-saskatchewan">
		<rdfs:label>University of Saskatchewan</rdfs:label>
		<foaf:name>University of Saskatchewan</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-greer"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carl-gutwin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carrie-demmans-epp"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/greg-logan"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-south-africa">
		<rdfs:label>University of South Africa</rdfs:label>
		<foaf:name>University of South Africa</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/paul-prinsloo"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/utrecht-university">
		<rdfs:label>Utrecht University</rdfs:label>
		<foaf:name>Utrecht University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/diederik-m-roijers"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/johan-jeuring"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ad-feelders"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/western-kentucky-university">
		<rdfs:label>Western Kentucky University</rdfs:label>
		<foaf:name>Western Kentucky University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rong-yang"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute">
		<rdfs:label>Worcester Polytechnic Institute</rdfs:label>
		<foaf:name>Worcester Polytechnic Institute</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-roberge"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anthony-rojas"/>
	</foaf:Organization>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/darren-cambridge">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/american-institutes-for-research"/>
		<rdfs:label>Darren Cambridge</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Darren</foaf:firstName>
		<foaf:lastName>Cambridge</foaf:lastName>
		<foaf:mbox_sha1sum>0a47e651d5bffd8d4fd6d89f77d89301d68db34b</foaf:mbox_sha1sum>
		<foaf:name>Darren Cambridge</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/14"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kathleen-perez-lopez">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/american-institutes-for-research"/>
		<rdfs:label>Kathleen Perez-Lopez</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Kathleen</foaf:firstName>
		<foaf:lastName>Perez-Lopez</foaf:lastName>
		<foaf:mbox_sha1sum>b6b16b424843bc9768993b6a9f39d576d2201c08</foaf:mbox_sha1sum>
		<foaf:name>Kathleen Perez-Lopez</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/14"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/denis-smolin">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/american-university-in-bosnia-and-herzegovina"/>
		<rdfs:label>Denis Smolin</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Bosnia_and_Herzegovina"/>
		<foaf:firstName>Denis</foaf:firstName>
		<foaf:lastName>Smolin</foaf:lastName>
		<foaf:mbox_sha1sum>57f0ce1ccba3dffd5d93e9cf90595afba1d1cb89</foaf:mbox_sha1sum>
		<foaf:name>Denis Smolin</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/15"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mike-sharkey">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/apollo-group"/>
		<rdfs:label>Mike Sharkey</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Mike</foaf:firstName>
		<foaf:lastName>Sharkey</foaf:lastName>
		<foaf:mbox_sha1sum>d5412876c38269dfe20933f7ed43aa04611a9a31</foaf:mbox_sha1sum>
		<foaf:name>Mike Sharkey</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/18"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rebecca-barber">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/apollo-group"/>
		<rdfs:label>Rebecca Barber</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Rebecca</foaf:firstName>
		<foaf:lastName>Barber</foaf:lastName>
		<foaf:mbox_sha1sum>8dba37139a7e8bb22ed78814bb130c3884586978</foaf:mbox_sha1sum>
		<foaf:name>Rebecca Barber</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/18"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nazim-rahman">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/athabasca-university"/>
		<rdfs:label>Nazim Rahman</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Nazim</foaf:firstName>
		<foaf:lastName>Rahman</foaf:lastName>
		<foaf:mbox_sha1sum>66725a7ebaab80a8d8b8baa19842570ef1931fa7</foaf:mbox_sha1sum>
		<foaf:name>Nazim Rahman</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/41"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/george-siemens">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/athabasca-university"/>
		<rdfs:label>George Siemens</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>George</foaf:firstName>
		<foaf:lastName>Siemens</foaf:lastName>
		<foaf:mbox_sha1sum>b2e59dbe6857c502ab9718888118ed4b64b98d94</foaf:mbox_sha1sum>
		<foaf:name>George Siemens</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/9"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jon-dron">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/athabasca-university"/>
		<rdfs:label>Jon Dron</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Jon</foaf:firstName>
		<foaf:lastName>Dron</foaf:lastName>
		<foaf:mbox_sha1sum>46e35b04de688c4100cbb8c70f009fcf0edeb555</foaf:mbox_sha1sum>
		<foaf:name>Jon Dron</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/41"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/liaqat-ali">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/athabasca-university"/>
		<rdfs:label>Liaqat Ali</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Liaqat</foaf:firstName>
		<foaf:lastName>Ali</foaf:lastName>
		<foaf:mbox_sha1sum>72b3aad6b5c1047332c2e4b93cbad6f4c07faefb</foaf:mbox_sha1sum>
		<foaf:name>Liaqat Ali</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dragan-gasevic">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/athabasca-university"/>
		<rdfs:label>Dragan Gasevic</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Dragan</foaf:firstName>
		<foaf:lastName>Gasevic</foaf:lastName>
		<foaf:mbox_sha1sum>c1028e73bff38983ae75c03050e6d970e25c9008</foaf:mbox_sha1sum>
		<foaf:name>Dragan Gasevic</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/erika-johnson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/ball-state-university"/>
		<rdfs:label>Erika Johnson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Erika</foaf:firstName>
		<foaf:lastName>Johnson</foaf:lastName>
		<foaf:mbox_sha1sum>56d42b4fa929ce70cf0d712772be4916d7e8eba4</foaf:mbox_sha1sum>
		<foaf:name>Erika Johnson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/12"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/brian-j-mcnely">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/ball-state-university"/>
		<rdfs:label>Brian J. McNely</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Brian</foaf:firstName>
		<foaf:lastName>J. McNely</foaf:lastName>
		<foaf:mbox_sha1sum>7526257bdf26cacbb4d7cd7885811b5d89cf9614</foaf:mbox_sha1sum>
		<foaf:name>Brian J. McNely</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/12"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/paul-gestwicki">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/ball-state-university"/>
		<rdfs:label>Paul Gestwicki</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Paul</foaf:firstName>
		<foaf:lastName>Gestwicki</foaf:lastName>
		<foaf:mbox_sha1sum>9e667e3b4662d112eb78289b850524175a01400a</foaf:mbox_sha1sum>
		<foaf:name>Paul Gestwicki</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/12"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/j-holden-hill">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/ball-state-university"/>
		<rdfs:label>J. Holden Hill</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>J.</foaf:firstName>
		<foaf:lastName>Holden Hill</foaf:lastName>
		<foaf:mbox_sha1sum>9b68b68d1b141316470d8f7dc466c77138590213</foaf:mbox_sha1sum>
		<foaf:name>J. Holden Hill</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/12"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/philip-parli-horne">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/ball-state-university"/>
		<rdfs:label>Philip Parli-Horne</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Philip</foaf:firstName>
		<foaf:lastName>Parli-Horne</foaf:lastName>
		<foaf:mbox_sha1sum>99777b63d088d4b71cbc9ff5f8c576e62ba2d572</foaf:mbox_sha1sum>
		<foaf:name>Philip Parli-Horne</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/12"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/agathe-merceron">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/beuth-university-of-applied-sciences"/>
		<rdfs:label>Agathe Merceron</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Agathe</foaf:firstName>
		<foaf:lastName>Merceron</foaf:lastName>
		<foaf:mbox_sha1sum>1010dc911334fd66797dbb9ca71b0ea6daeea65b</foaf:mbox_sha1sum>
		<foaf:name>Agathe Merceron</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/25"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/bjorn-levi-gunnarsson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/brandeis-university"/>
		<rdfs:label>Bjorn Levi Gunnarsson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Bjorn</foaf:firstName>
		<foaf:lastName>Levi Gunnarsson</foaf:lastName>
		<foaf:mbox_sha1sum>2d227d7e8d4bad5ee53cdf0c2c00082d16197b62</foaf:mbox_sha1sum>
		<foaf:name>Bjorn Levi Gunnarsson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/37"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/richard-alterman">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/brandeis-university"/>
		<rdfs:label>Richard Alterman</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Richard</foaf:firstName>
		<foaf:lastName>Alterman</foaf:lastName>
		<foaf:mbox_sha1sum>5a4a38ec340e39671e7694bd1a5d4ecad69a2cc3</foaf:mbox_sha1sum>
		<foaf:name>Richard Alterman</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/37"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/johann-ari-larusson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/brandeis-university"/>
		<rdfs:label>Johann Ari Larusson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Johann</foaf:firstName>
		<foaf:lastName>Ari Larusson</foaf:lastName>
		<foaf:mbox_sha1sum>f417832817f76061ddf7e09f04d0a8291260e720</foaf:mbox_sha1sum>
		<foaf:name>Johann Ari Larusson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/8"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/fintan-clear">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/brunel-business-school"/>
		<rdfs:label>Fintan Clear</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Fintan</foaf:firstName>
		<foaf:lastName>Clear</foaf:lastName>
		<foaf:mbox_sha1sum>f519f17cb7bee3eebe658f1cb28e7de940bad286</foaf:mbox_sha1sum>
		<foaf:name>Fintan Clear</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/32"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/samira-sadat-sajadi">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/brunel-business-school"/>
		<rdfs:label>Samira Sadat Sajadi</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Samira</foaf:firstName>
		<foaf:lastName>Sadat Sajadi</foaf:lastName>
		<foaf:mbox_sha1sum>d4150dfab83fd01a7057d9a5b3cb2cb870c0bc6a</foaf:mbox_sha1sum>
		<foaf:name>Samira Sadat Sajadi</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/32"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tariq-m-khan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/brunel-business-school"/>
		<rdfs:label>Tariq M. Khan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Tariq</foaf:firstName>
		<foaf:lastName>M. Khan</foaf:lastName>
		<foaf:mbox_sha1sum>9ffb653ab57104f1048123ba7edcd7e84dfa3a56</foaf:mbox_sha1sum>
		<foaf:name>Tariq M. Khan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/32"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/alfred-essa">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/desire2learn-incorporated"/>
		<rdfs:label>Alfred Essa</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Alfred</foaf:firstName>
		<foaf:lastName>Essa</foaf:lastName>
		<foaf:mbox_sha1sum>4913911a559933ee18e3878ec6da7d5773538608</foaf:mbox_sha1sum>
		<foaf:name>Alfred Essa</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/39"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hanan-ayad">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/desire2learn-incorporated"/>
		<rdfs:label>Hanan Ayad</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Hanan</foaf:firstName>
		<foaf:lastName>Ayad</foaf:lastName>
		<foaf:mbox_sha1sum>e14939965caf7ba804d4eae29ef78592be579f96</foaf:mbox_sha1sum>
		<foaf:name>Hanan Ayad</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/39"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/uwe-kirschenmann">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology"/>
		<rdfs:label>Uwe Kirschenmann</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Uwe</foaf:firstName>
		<foaf:lastName>Kirschenmann</foaf:lastName>
		<foaf:mbox_sha1sum>003d6d949f5a65e352bb2cef51d3982ea975de05</foaf:mbox_sha1sum>
		<foaf:name>Uwe Kirschenmann</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/martin-wolpers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology"/>
		<rdfs:label>Martin Wolpers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Martin</foaf:firstName>
		<foaf:lastName>Wolpers</foaf:lastName>
		<foaf:mbox_sha1sum>61798a03ba9da369aafe3e416ecda9685c4e13ab</foaf:mbox_sha1sum>
		<foaf:name>Martin Wolpers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/katja-niemann">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology"/>
		<rdfs:label>Katja Niemann</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Katja</foaf:firstName>
		<foaf:lastName>Niemann</foaf:lastName>
		<foaf:mbox_sha1sum>bf1266883936a49888fbb3c464baeff7a05561f5</foaf:mbox_sha1sum>
		<foaf:name>Katja Niemann</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology"/>
		<rdfs:label>Hans-Christian Schmitz</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Hans-Christian</foaf:firstName>
		<foaf:lastName>Schmitz</foaf:lastName>
		<foaf:mbox_sha1sum>33e2d0fb50cce2b0894ca84b6c7c14e8226cd23f</foaf:mbox_sha1sum>
		<foaf:name>Hans-Christian Schmitz</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/simone-braun">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fzi-research-center-for-information-technologies"/>
		<rdfs:label>Simone Braun</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Simone</foaf:firstName>
		<foaf:lastName>Braun</foaf:lastName>
		<foaf:mbox_sha1sum>42e8b51095d0da198a5b73c74129ea3fc85c2b31</foaf:mbox_sha1sum>
		<foaf:name>Simone Braun</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/29"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/veronica-rivera-pelayo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fzi-research-center-for-information-technologies"/>
		<rdfs:label>Veronica Rivera-Pelayo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Veronica</foaf:firstName>
		<foaf:lastName>Rivera-Pelayo</foaf:lastName>
		<foaf:mbox_sha1sum>ecfd203470e3ae9f516b3e4dd044489fe9dcff0f</foaf:mbox_sha1sum>
		<foaf:name>Veronica Rivera-Pelayo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/29"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/valentin-zacharias">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fzi-research-center-for-information-technologies"/>
		<rdfs:label>Valentin Zacharias</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Valentin</foaf:firstName>
		<foaf:lastName>Zacharias</foaf:lastName>
		<foaf:mbox_sha1sum>5af18695bea6ad7274ba2468228786e42b8a4971</foaf:mbox_sha1sum>
		<foaf:name>Valentin Zacharias</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/29"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/lars-muller">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fzi-research-center-for-information-technologies"/>
		<rdfs:label>Lars Muller</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Lars</foaf:firstName>
		<foaf:lastName>Muller</foaf:lastName>
		<foaf:mbox_sha1sum>46c785c1abfd534155b61609e1f9030f407ecaae</foaf:mbox_sha1sum>
		<foaf:name>Lars Muller</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/29"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/martin-schon">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/graz-university-of-technology"/>
		<rdfs:label>Martin Schon</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Austria"/>
		<foaf:firstName>Martin</foaf:firstName>
		<foaf:lastName>Schon</foaf:lastName>
		<foaf:mbox_sha1sum>21e4b3aae3f36f2721d356db6ed1a50b19050494</foaf:mbox_sha1sum>
		<foaf:name>Martin Schon</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/26"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/stefanie-n-lindstaedt">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/graz-university-of-technology"/>
		<rdfs:label>Stefanie N. Lindstaedt</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Austria"/>
		<foaf:firstName>Stefanie</foaf:firstName>
		<foaf:lastName>N. Lindstaedt</foaf:lastName>
		<foaf:mbox_sha1sum>0ded1f3a672e03c667fe44d49904f807584651f2</foaf:mbox_sha1sum>
		<foaf:name>Stefanie N. Lindstaedt</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/16"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/barbara-kump">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/graz-university-of-technology"/>
		<rdfs:label>Barbara Kump</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Austria"/>
		<foaf:firstName>Barbara</foaf:firstName>
		<foaf:lastName>Kump</foaf:lastName>
		<foaf:mbox_sha1sum>7fec42452039b66c078555c001e69622c436d524</foaf:mbox_sha1sum>
		<foaf:name>Barbara Kump</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/16"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/martin-ebner">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/graz-university-of-technology"/>
		<rdfs:label>Martin Ebner</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Austria"/>
		<foaf:firstName>Martin</foaf:firstName>
		<foaf:lastName>Ebner</foaf:lastName>
		<foaf:mbox_sha1sum>5d31ab5ab1b3850fc57dd2472a72f99c7898a361</foaf:mbox_sha1sum>
		<foaf:name>Martin Ebner</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/26"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/christin-seifert">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/graz-university-of-technology"/>
		<rdfs:label>Christin Seifert</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Austria"/>
		<foaf:firstName>Christin</foaf:firstName>
		<foaf:lastName>Seifert</foaf:lastName>
		<foaf:mbox_sha1sum>7dd4192a8816900e810a796cf786b081e3fb4181</foaf:mbox_sha1sum>
		<foaf:name>Christin Seifert</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/16"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/georg-kothmeier">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/graz-university-of-technology"/>
		<rdfs:label>Georg Kothmeier</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Austria"/>
		<foaf:firstName>Georg</foaf:firstName>
		<foaf:lastName>Kothmeier</foaf:lastName>
		<foaf:mbox_sha1sum>21ba2146187090accd67a2bc7cf7c715df57bc66</foaf:mbox_sha1sum>
		<foaf:name>Georg Kothmeier</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/26"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/katrien-verbert">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/kuleuven"/>
		<rdfs:label>Katrien Verbert</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Belgium"/>
		<foaf:firstName>Katrien</foaf:firstName>
		<foaf:lastName>Verbert</foaf:lastName>
		<foaf:mbox_sha1sum>610730a9705def3b48bc4b17449cc5ac317d7bf8</foaf:mbox_sha1sum>
		<foaf:name>Katrien Verbert</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/28"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/erik-duval">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/kuleuven"/>
		<rdfs:label>Erik Duval</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Belgium"/>
		<foaf:firstName>Erik</foaf:firstName>
		<foaf:lastName>Duval</foaf:lastName>
		<foaf:mbox_sha1sum>33b04e59663928cd5da8044a9475463e09adfa3c</foaf:mbox_sha1sum>
		<foaf:name>Erik Duval</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/28"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jose-luis-santos">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/kuleuven"/>
		<rdfs:label>Jose Luis Santos</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Belgium"/>
		<foaf:firstName>Jose</foaf:firstName>
		<foaf:lastName>Luis Santos</foaf:lastName>
		<foaf:mbox_sha1sum>0d320207c6d6e61c526f877b78ed3bb3bd83539d</foaf:mbox_sha1sum>
		<foaf:name>Jose Luis Santos</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/28"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sten-govaerts">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/kuleuven"/>
		<rdfs:label>Sten Govaerts</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Belgium"/>
		<foaf:firstName>Sten</foaf:firstName>
		<foaf:lastName>Govaerts</foaf:lastName>
		<foaf:mbox_sha1sum>7c0604220cb336cde3a5a4da5b762537b6dbf573</foaf:mbox_sha1sum>
		<foaf:name>Sten Govaerts</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/28"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/guenter-beham">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/know-center-gmbh"/>
		<rdfs:label>Guenter Beham</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Austria"/>
		<foaf:firstName>Guenter</foaf:firstName>
		<foaf:lastName>Beham</foaf:lastName>
		<foaf:mbox_sha1sum>34c0ee76752bade7f46d78a32fce7b167a863265</foaf:mbox_sha1sum>
		<foaf:name>Guenter Beham</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/16"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joshua-d-baron">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/marist-college"/>
		<rdfs:label>Joshua D. Baron</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joshua</foaf:firstName>
		<foaf:lastName>D. Baron</foaf:lastName>
		<foaf:mbox_sha1sum>d43af47dbc38a0d1bb3e01e5e7e3224b55326577</foaf:mbox_sha1sum>
		<foaf:name>Joshua D. Baron</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/31"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mallika-devireddy">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/marist-college"/>
		<rdfs:label>Mallika Devireddy</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Mallika</foaf:firstName>
		<foaf:lastName>Devireddy</foaf:lastName>
		<foaf:mbox_sha1sum>9147b48fed80e3ca4ea3c4948e36ed2e4d176c43</foaf:mbox_sha1sum>
		<foaf:name>Mallika Devireddy</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/31"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/venniraiselvi-sundararaju">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/marist-college"/>
		<rdfs:label>Venniraiselvi Sundararaju</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Venniraiselvi</foaf:firstName>
		<foaf:lastName>Sundararaju</foaf:lastName>
		<foaf:mbox_sha1sum>f35ffe409b0b107d7ac21b6631012e052c1d5c45</foaf:mbox_sha1sum>
		<foaf:name>Venniraiselvi Sundararaju</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/31"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sandeep-m-jayaprakash">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/marist-college"/>
		<rdfs:label>Sandeep M. Jayaprakash</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Sandeep</foaf:firstName>
		<foaf:lastName>M. Jayaprakash</foaf:lastName>
		<foaf:mbox_sha1sum>28a9c63cfd93fe8a31a89630042dc072ac059b47</foaf:mbox_sha1sum>
		<foaf:name>Sandeep M. Jayaprakash</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/31"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/eitel-jm-lauria">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/marist-college"/>
		<rdfs:label>Eitel J.M. Lauria</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Eitel</foaf:firstName>
		<foaf:lastName>J.M. Lauria</foaf:lastName>
		<foaf:mbox_sha1sum>16509f461bf256d4040f4f6d54a288a3c68b15a5</foaf:mbox_sha1sum>
		<foaf:name>Eitel J.M. Lauria</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/31"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/bruce-sherin">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/northwestern-university"/>
		<rdfs:label>Bruce Sherin</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Bruce</foaf:firstName>
		<foaf:lastName>Sherin</foaf:lastName>
		<foaf:mbox_sha1sum>30dc2eaae2973a1ef4d31aaf7f718d142941d29b</foaf:mbox_sha1sum>
		<foaf:name>Bruce Sherin</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/38"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/schreurs-bieke">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/open-universiteit"/>
		<rdfs:label>Schreurs Bieke</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Schreurs</foaf:firstName>
		<foaf:lastName>Bieke</foaf:lastName>
		<foaf:mbox_sha1sum>4876f16b22727c7025a7e066c5cdaca72d984dfc</foaf:mbox_sha1sum>
		<foaf:name>Schreurs Bieke</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/7"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/de-laat-maarten">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/open-universiteit"/>
		<rdfs:label>De Laat Maarten</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>De</foaf:firstName>
		<foaf:lastName>Laat Maarten</foaf:lastName>
		<foaf:mbox_sha1sum>b09555ec48436cfe3c394803a8d134e72d4d5f86</foaf:mbox_sha1sum>
		<foaf:name>De Laat Maarten</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/7"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherland"/>
		<rdfs:label>Hendrik Drachsler</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Hendrik</foaf:firstName>
		<foaf:lastName>Drachsler</foaf:lastName>
		<foaf:mbox_sha1sum>a0f12a89fc8bf9cfbd44b49a9e1bbcec40106def</foaf:mbox_sha1sum>
		<foaf:name>Hendrik Drachsler</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/10"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/wolfgang-greller">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherlands"/>
		<rdfs:label>Wolfgang Greller</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Wolfgang</foaf:firstName>
		<foaf:lastName>Greller</foaf:lastName>
		<foaf:mbox_sha1sum>b76d0dbeb6f156274d5086116ff55a44083d86ef</foaf:mbox_sha1sum>
		<foaf:name>Wolfgang Greller</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/10"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kimberly-e-arnold">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/purdue-university"/>
		<rdfs:label>Kimberly E. Arnold</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Kimberly</foaf:firstName>
		<foaf:lastName>E. Arnold</foaf:lastName>
		<foaf:mbox_sha1sum>0a1c3a54be2becd632978690811b51dd9c49eb3a</foaf:mbox_sha1sum>
		<foaf:name>Kimberly E. Arnold</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/13"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/matthew-d-pistilli">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/purdue-university"/>
		<rdfs:label>Matthew D. Pistilli</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Matthew</foaf:firstName>
		<foaf:lastName>D. Pistilli</foaf:lastName>
		<foaf:mbox_sha1sum>6e0be50e89d13b85a1e63cb1e200fa9b1bdf1702</foaf:mbox_sha1sum>
		<foaf:name>Matthew D. Pistilli</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/13"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/anna-schmidt">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/saarland-university"/>
		<rdfs:label>Anna Schmidt</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Anna</foaf:firstName>
		<foaf:lastName>Schmidt</foaf:lastName>
		<foaf:mbox_sha1sum>dca53d2addb09f74999a2901b22869d44a19992b</foaf:mbox_sha1sum>
		<foaf:name>Anna Schmidt</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tim-krones">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/saarland-university"/>
		<rdfs:label>Tim Krones</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Tim</foaf:firstName>
		<foaf:lastName>Krones</foaf:lastName>
		<foaf:mbox_sha1sum>13660e5f56ca434624637b5f33461bc2ae967645</foaf:mbox_sha1sum>
		<foaf:name>Tim Krones</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/33"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/melody-siadaty">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/simon-fraser-university"/>
		<rdfs:label>Melody Siadaty</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Melody</foaf:firstName>
		<foaf:lastName>Siadaty</foaf:lastName>
		<foaf:mbox_sha1sum>c6f71904094948ff7b6a129122123cb56f40a480</foaf:mbox_sha1sum>
		<foaf:name>Melody Siadaty</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/aleksandar-giljanovic">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/simon-fraser-university"/>
		<rdfs:label>Aleksandar Giljanovic</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Aleksandar</foaf:firstName>
		<foaf:lastName>Giljanovic</foaf:lastName>
		<foaf:mbox_sha1sum>f4a4216ee772142fe7ee3a2a2b0a6fbf6855e914</foaf:mbox_sha1sum>
		<foaf:name>Aleksandar Giljanovic</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/marek-hatala">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/simon-fraser-university"/>
		<rdfs:label>Marek Hatala</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Marek</foaf:firstName>
		<foaf:lastName>Hatala</foaf:lastName>
		<foaf:mbox_sha1sum>c9e93ef04b4e0d9b8b946fcb693b1a9d7bf68e42</foaf:mbox_sha1sum>
		<foaf:name>Marek Hatala</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sergey-butakov">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/solbridge-international-school-of-business"/>
		<rdfs:label>Sergey Butakov</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/South_Korea"/>
		<foaf:firstName>Sergey</foaf:firstName>
		<foaf:lastName>Butakov</foaf:lastName>
		<foaf:mbox_sha1sum>9234c4faff7a7fec50101b6d5f3ab8a9b68d32e5</foaf:mbox_sha1sum>
		<foaf:name>Sergey Butakov</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/15"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/robert-bramucci">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/south-orange-county-community-college-district"/>
		<rdfs:label>Robert Bramucci</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Robert</foaf:firstName>
		<foaf:lastName>Bramucci</foaf:lastName>
		<foaf:mbox_sha1sum>252c1bfa8ae691de7e19044df90a4f50e099cd38</foaf:mbox_sha1sum>
		<foaf:name>Robert Bramucci</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/20"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jim-gaston">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/south-orange-county-community-college-district"/>
		<rdfs:label>Jim Gaston</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jim</foaf:firstName>
		<foaf:lastName>Gaston</foaf:lastName>
		<foaf:mbox_sha1sum>838e5f67a9bc54bfddd942265324c676ab903ae4</foaf:mbox_sha1sum>
		<foaf:name>Jim Gaston</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/20"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/marie-bienkowski">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/sri-international"/>
		<rdfs:label>Marie Bienkowski</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Marie</foaf:firstName>
		<foaf:lastName>Bienkowski</foaf:lastName>
		<foaf:mbox_sha1sum>27538f6147c9b9d6cbf3673b695f078c30d8b5e2</foaf:mbox_sha1sum>
		<foaf:name>Marie Bienkowski</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/40"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/john-brecht">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/sri-international"/>
		<rdfs:label>John Brecht</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>John</foaf:firstName>
		<foaf:lastName>Brecht</foaf:lastName>
		<foaf:mbox_sha1sum>42a1e67d2bf08e81c0a4e2ffc9773b04eed838f0</foaf:mbox_sha1sum>
		<foaf:name>John Brecht</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/40"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jim-klo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/sri-international"/>
		<rdfs:label>Jim Klo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jim</foaf:firstName>
		<foaf:lastName>Klo</foaf:lastName>
		<foaf:mbox_sha1sum>bd519294e7f755c5edaf06d087cf35e3857c471b</foaf:mbox_sha1sum>
		<foaf:name>Jim Klo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/40"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tobias-ley">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tallinn-university"/>
		<rdfs:label>Tobias Ley</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Estonia"/>
		<foaf:firstName>Tobias</foaf:firstName>
		<foaf:lastName>Ley</foaf:lastName>
		<foaf:mbox_sha1sum>4cd47d541d76e21c7e4cbc36d5b52b236bd2d1c0</foaf:mbox_sha1sum>
		<foaf:name>Tobias Ley</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/16"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/fenella-galpin">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Fenella Galpin</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Fenella</foaf:firstName>
		<foaf:lastName>Galpin</foaf:lastName>
		<foaf:mbox_sha1sum>f4a41b89dcea7bae3a4521e7a3a52ee66303719b</foaf:mbox_sha1sum>
		<foaf:name>Fenella Galpin</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/36"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Rebecca Ferguson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Rebecca</foaf:firstName>
		<foaf:lastName>Ferguson</foaf:lastName>
		<foaf:mbox_sha1sum>9a2a32c74e31c9e3a338d550c2619bae7273ec6d</foaf:mbox_sha1sum>
		<foaf:name>Rebecca Ferguson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/27"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/35"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/doug-clow">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Doug Clow</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Doug</foaf:firstName>
		<foaf:lastName>Clow</foaf:lastName>
		<foaf:mbox_sha1sum>50032dfb2365c3d7db9b3868e57e5b236f60f742</foaf:mbox_sha1sum>
		<foaf:name>Doug Clow</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/19"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Rebecca Ferguson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Rebecca</foaf:firstName>
		<foaf:lastName>Ferguson</foaf:lastName>
		<foaf:mbox_sha1sum>9a2a32c74e31c9e3a338d550c2619bae7273ec6d</foaf:mbox_sha1sum>
		<foaf:name>Rebecca Ferguson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/27"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/35"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Simon Buckingham Shum</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Simon</foaf:firstName>
		<foaf:lastName>Buckingham Shum</foaf:lastName>
		<foaf:mbox_sha1sum>bbaf2c40d77ba4a21d7c6f19c0b973ed3624cdc5</foaf:mbox_sha1sum>
		<foaf:name>Simon Buckingham Shum</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/34"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/35"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Simon Buckingham Shum</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Simon</foaf:firstName>
		<foaf:lastName>Buckingham Shum</foaf:lastName>
		<foaf:mbox_sha1sum>bbaf2c40d77ba4a21d7c6f19c0b973ed3624cdc5</foaf:mbox_sha1sum>
		<foaf:name>Simon Buckingham Shum</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/34"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/35"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/haiming-liu">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Haiming Liu</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Haiming</foaf:firstName>
		<foaf:lastName>Liu</foaf:lastName>
		<foaf:mbox_sha1sum>eee27ddfad442a161b5761c1d7bb38071edbfe2d</foaf:mbox_sha1sum>
		<foaf:name>Haiming Liu</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/27"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sharon-slade">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Sharon Slade</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Sharon</foaf:firstName>
		<foaf:lastName>Slade</foaf:lastName>
		<foaf:mbox_sha1sum>3260e0cb8452388c0ed91830426f8b34af161d46</foaf:mbox_sha1sum>
		<foaf:name>Sharon Slade</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/36"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ronald-macintyre">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university-in-scotland"/>
		<rdfs:label>Ronald Macintyre</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Ronald</foaf:firstName>
		<foaf:lastName>Macintyre</foaf:lastName>
		<foaf:mbox_sha1sum>6bd9cdfbf9e59f5e10ad8b44587292ca4cc112e4</foaf:mbox_sha1sum>
		<foaf:name>Ronald Macintyre</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/27"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/walter-christian-paredes">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-university-of-sydney"/>
		<rdfs:label>Walter Christian Paredes</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Walter</foaf:firstName>
		<foaf:lastName>Christian Paredes</foaf:lastName>
		<foaf:mbox_sha1sum>19033360bce7e2171d8982f0e7ae49f3fb006051</foaf:mbox_sha1sum>
		<foaf:name>Walter Christian Paredes</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/1"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kon-shing-kenneth-chung">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-university-of-sydney"/>
		<rdfs:label>Kon Shing Kenneth Chung</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Kon</foaf:firstName>
		<foaf:lastName>Shing Kenneth Chung</foaf:lastName>
		<foaf:mbox_sha1sum>dd639ffb620d7b66be12a3ecac46d79705dc2795</foaf:mbox_sha1sum>
		<foaf:name>Kon Shing Kenneth Chung</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/1"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/alexander-o-connor">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/trinity-college-dublin"/>
		<rdfs:label>Alexander O'Connor</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Ireland"/>
		<foaf:firstName>Alexander</foaf:firstName>
		<foaf:lastName>O'Connor</foaf:lastName>
		<foaf:mbox_sha1sum>b5abdda471d5f81789eeb20fbe10fcc6d372dd1f</foaf:mbox_sha1sum>
		<foaf:name>Alexander O'Connor</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/23"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dr-dave-lewis">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/trinity-college-dublin"/>
		<rdfs:label>Dr. Dave Lewis</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Ireland"/>
		<foaf:firstName>Dr.</foaf:firstName>
		<foaf:lastName>Dave Lewis</foaf:lastName>
		<foaf:mbox_sha1sum>59d7a180cd22321520e12eb45f2b62f0d70a7eed</foaf:mbox_sha1sum>
		<foaf:name>Dr. Dave Lewis</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/23"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/john-mcauley">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/trinity-college-dublin"/>
		<rdfs:label>John McAuley</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Ireland"/>
		<foaf:firstName>John</foaf:firstName>
		<foaf:lastName>McAuley</foaf:lastName>
		<foaf:mbox_sha1sum>45da45bbed6b59e92c2c80bf2c48105d23ea2a76</foaf:mbox_sha1sum>
		<foaf:name>John McAuley</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/23"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Eugenia Santamaria</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Eugenia</foaf:firstName>
		<foaf:lastName>Santamaria</foaf:lastName>
		<foaf:mbox_sha1sum>1219cb57c84f36856cc57b239c9fa24c2607ee07</foaf:mbox_sha1sum>
		<foaf:name>Eugenia Santamaria</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carlos-monzo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Carlos Monzo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Carlos</foaf:firstName>
		<foaf:lastName>Monzo</foaf:lastName>
		<foaf:mbox_sha1sum>ad957c6070739420fbef495ba5d8d7a6cef0f976</foaf:mbox_sha1sum>
		<foaf:name>Carlos Monzo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/german-cobo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>German Cobo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>German</foaf:firstName>
		<foaf:lastName>Cobo</foaf:lastName>
		<foaf:mbox_sha1sum>89c03475c8426f8dd9dd21a3777eb7a6b09af20d</foaf:mbox_sha1sum>
		<foaf:name>German Cobo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Jose Antonio Moran</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Jose</foaf:firstName>
		<foaf:lastName>Antonio Moran</foaf:lastName>
		<foaf:mbox_sha1sum>787b5ef9d20cee9cbd2dfccc6363acda72bb55d7</foaf:mbox_sha1sum>
		<foaf:name>Jose Antonio Moran</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/javier-melenchon">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Javier Melenchon</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Javier</foaf:firstName>
		<foaf:lastName>Melenchon</foaf:lastName>
		<foaf:mbox_sha1sum>b6a774da22673bddde32bf2e325e3e8f45130fb6</foaf:mbox_sha1sum>
		<foaf:name>Javier Melenchon</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>David Garcia-Solorzano</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>David</foaf:firstName>
		<foaf:lastName>Garcia-Solorzano</foaf:lastName>
		<foaf:mbox_sha1sum>2328848dd86f7d66ec73dabe5f07dd051b078fac</foaf:mbox_sha1sum>
		<foaf:name>David Garcia-Solorzano</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carlos-monzo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Carlos Monzo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Carlos</foaf:firstName>
		<foaf:lastName>Monzo</foaf:lastName>
		<foaf:mbox_sha1sum>ad957c6070739420fbef495ba5d8d7a6cef0f976</foaf:mbox_sha1sum>
		<foaf:name>Carlos Monzo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>David Garcia-Solorzano</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>David</foaf:firstName>
		<foaf:lastName>Garcia-Solorzano</foaf:lastName>
		<foaf:mbox_sha1sum>2328848dd86f7d66ec73dabe5f07dd051b078fac</foaf:mbox_sha1sum>
		<foaf:name>David Garcia-Solorzano</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Jose Antonio Moran</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Jose</foaf:firstName>
		<foaf:lastName>Antonio Moran</foaf:lastName>
		<foaf:mbox_sha1sum>787b5ef9d20cee9cbd2dfccc6363acda72bb55d7</foaf:mbox_sha1sum>
		<foaf:name>Jose Antonio Moran</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/javier-melenchon">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Javier Melenchon</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Javier</foaf:firstName>
		<foaf:lastName>Melenchon</foaf:lastName>
		<foaf:mbox_sha1sum>b6a774da22673bddde32bf2e325e3e8f45130fb6</foaf:mbox_sha1sum>
		<foaf:name>Javier Melenchon</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/german-cobo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>German Cobo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>German</foaf:firstName>
		<foaf:lastName>Cobo</foaf:lastName>
		<foaf:mbox_sha1sum>89c03475c8426f8dd9dd21a3777eb7a6b09af20d</foaf:mbox_sha1sum>
		<foaf:name>German Cobo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Eugenia Santamaria</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Eugenia</foaf:firstName>
		<foaf:lastName>Santamaria</foaf:lastName>
		<foaf:mbox_sha1sum>1219cb57c84f36856cc57b239c9fa24c2607ee07</foaf:mbox_sha1sum>
		<foaf:name>Eugenia Santamaria</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/6"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/17"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/derick-leony">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-carlos-iii-of-madrid"/>
		<rdfs:label>Derick Leony</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Derick</foaf:firstName>
		<foaf:lastName>Leony</foaf:lastName>
		<foaf:mbox_sha1sum>7d333b815aac797ba069af0c3ab0d03006a22133</foaf:mbox_sha1sum>
		<foaf:name>Derick Leony</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/21"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/abelardo-pardo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-carlos-iii-of-madrid"/>
		<rdfs:label>Abelardo Pardo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Abelardo</foaf:firstName>
		<foaf:lastName>Pardo</foaf:lastName>
		<foaf:mbox_sha1sum>5d52785ab83243bb537a2acaae36b12fb2abe935</foaf:mbox_sha1sum>
		<foaf:name>Abelardo Pardo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/21"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/luis-de-la-fuente-valentin">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-carlos-iii-of-madrid"/>
		<rdfs:label>Luis de la Fuente Valentin</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Luis</foaf:firstName>
		<foaf:lastName>de la Fuente Valentin</foaf:lastName>
		<foaf:mbox_sha1sum>b5996521dd0ff7f4407e23591214ddf5661a521f</foaf:mbox_sha1sum>
		<foaf:name>Luis de la Fuente Valentin</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/21"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/david-sanchez-de-castro">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-carlos-iii-of-madrid"/>
		<rdfs:label>David Sanchez de Castro</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>David</foaf:firstName>
		<foaf:lastName>Sanchez de Castro</foaf:lastName>
		<foaf:mbox_sha1sum>c3e474eb743e5385d1c3c45980dfb376ab4c266d</foaf:mbox_sha1sum>
		<foaf:name>David Sanchez de Castro</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/21"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-carlos-iii-of-madrid"/>
		<rdfs:label>Carlos Delgado Kloos</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Carlos</foaf:firstName>
		<foaf:lastName>Delgado Kloos</foaf:lastName>
		<foaf:mbox_sha1sum>1f8adb88df12d259e28c0e7aa8a84511ef753e39</foaf:mbox_sha1sum>
		<foaf:name>Carlos Delgado Kloos</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/21"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nikola-milikic">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-belgrade"/>
		<rdfs:label>Nikola Milikic</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Serbia"/>
		<foaf:firstName>Nikola</foaf:firstName>
		<foaf:lastName>Milikic</foaf:lastName>
		<foaf:mbox_sha1sum>b2dad131dd796c863a073eaa023f2e4d55a20cd9</foaf:mbox_sha1sum>
		<foaf:name>Nikola Milikic</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/zoran-jeremic">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-belgrade"/>
		<rdfs:label>Zoran Jeremic</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Serbia"/>
		<foaf:firstName>Zoran</foaf:firstName>
		<foaf:lastName>Jeremic</foaf:lastName>
		<foaf:mbox_sha1sum>77c4fa1c9d4d0438706201aea3b0e68fe58761b6</foaf:mbox_sha1sum>
		<foaf:name>Zoran Jeremic</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jelena-jovanovic">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-belgrade"/>
		<rdfs:label>Jelena Jovanovic</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Serbia"/>
		<foaf:firstName>Jelena</foaf:firstName>
		<foaf:lastName>Jovanovic</foaf:lastName>
		<foaf:mbox_sha1sum>6753445d5d2cb241850b6bf141da146191915775</foaf:mbox_sha1sum>
		<foaf:name>Jelena Jovanovic</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/3"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ruth-deakin-crick">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-bristol"/>
		<rdfs:label>Ruth Deakin Crick</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Ruth</foaf:firstName>
		<foaf:lastName>Deakin Crick</foaf:lastName>
		<foaf:mbox_sha1sum>32f8e6bfe8f3b85fe55a838996208c569a0b4e80</foaf:mbox_sha1sum>
		<foaf:name>Ruth Deakin Crick</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/34"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/brandon-white">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-california,-berkeley"/>
		<rdfs:label>Brandon White</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Brandon</foaf:firstName>
		<foaf:lastName>White</foaf:lastName>
		<foaf:mbox_sha1sum>5776a5138001bd1bc816baace86c13edce323cdf</foaf:mbox_sha1sum>
		<foaf:name>Brandon White</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/8"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dan-suthers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-hawaii"/>
		<rdfs:label>Dan Suthers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Dan</foaf:firstName>
		<foaf:lastName>Suthers</foaf:lastName>
		<foaf:mbox_sha1sum>acb5c9152cb21e56d54b1575ef1bfb36618e69ce</foaf:mbox_sha1sum>
		<foaf:name>Dan Suthers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/30"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kar-hai-chu">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-hawaii"/>
		<rdfs:label>Kar-Hai Chu</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Kar-Hai</foaf:firstName>
		<foaf:lastName>Chu</foaf:lastName>
		<foaf:mbox_sha1sum>54a6bfec531c4ab76ef12bfcaed723ed6837c27e</foaf:mbox_sha1sum>
		<foaf:name>Kar-Hai Chu</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/30"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/vania-dimitrova">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-leeds"/>
		<rdfs:label>Vania Dimitrova</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Vania</foaf:firstName>
		<foaf:lastName>Dimitrova</foaf:lastName>
		<foaf:mbox_sha1sum>de2be8d3ed476f083642963fd7c07c5bbb9d0686</foaf:mbox_sha1sum>
		<foaf:name>Vania Dimitrova</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/2"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ahmad-ammari">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-leeds"/>
		<rdfs:label>Ahmad Ammari</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Ahmad</foaf:firstName>
		<foaf:lastName>Ammari</foaf:lastName>
		<foaf:mbox_sha1sum>6e6a33f9159c5a1160ffeeaa5ae9b3611ca0d575</foaf:mbox_sha1sum>
		<foaf:name>Ahmad Ammari</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/2"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/lydia-lau">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-leeds"/>
		<rdfs:label>Lydia Lau</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Lydia</foaf:firstName>
		<foaf:lastName>Lau</foaf:lastName>
		<foaf:mbox_sha1sum>e935cf1f0c2c46b383e6bb5891f189d9cb17b6d5</foaf:mbox_sha1sum>
		<foaf:name>Lydia Lau</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/2"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/leyla-zhuhadar">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-louisville"/>
		<rdfs:label>Leyla Zhuhadar</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Leyla</foaf:firstName>
		<foaf:lastName>Zhuhadar</foaf:lastName>
		<foaf:mbox_sha1sum>49092236119f5f8dc1662ee309d6b0f3a9893646</foaf:mbox_sha1sum>
		<foaf:name>Leyla Zhuhadar</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/5"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tim-mckay">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-michigan"/>
		<rdfs:label>Tim McKay</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Tim</foaf:firstName>
		<foaf:lastName>McKay</foaf:lastName>
		<foaf:mbox_sha1sum>b34e1b1925c099065a55c6d79ec879bc8d66f164</foaf:mbox_sha1sum>
		<foaf:name>Tim McKay</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/22"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/steven-lonn">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-michigan"/>
		<rdfs:label>Steven Lonn</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Steven</foaf:firstName>
		<foaf:lastName>Lonn</foaf:lastName>
		<foaf:mbox_sha1sum>699272f08c9647d1487a026be802e85760fa61cd</foaf:mbox_sha1sum>
		<foaf:name>Steven Lonn</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/11"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kate-miller">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-michigan"/>
		<rdfs:label>Kate Miller</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Kate</foaf:firstName>
		<foaf:lastName>Miller</foaf:lastName>
		<foaf:mbox_sha1sum>989adb74b151d584e6c4c3b8af605afa99530e1b</foaf:mbox_sha1sum>
		<foaf:name>Kate Miller</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/22"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/andrew-e-krumm">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-michigan"/>
		<rdfs:label>Andrew E. Krumm</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Andrew</foaf:firstName>
		<foaf:lastName>E. Krumm</foaf:lastName>
		<foaf:mbox_sha1sum>538b3063071b35bc78d7015210b3afc5791988a4</foaf:mbox_sha1sum>
		<foaf:name>Andrew E. Krumm</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/11"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jared-tritz">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-michigan"/>
		<rdfs:label>Jared Tritz</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jared</foaf:firstName>
		<foaf:lastName>Tritz</foaf:lastName>
		<foaf:mbox_sha1sum>0d321fcaac8cad72fbf873513d322686157f3927</foaf:mbox_sha1sum>
		<foaf:name>Jared Tritz</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/22"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/r-joseph-waddington">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-michigan"/>
		<rdfs:label>R. Joseph Waddington</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>R.</foaf:firstName>
		<foaf:lastName>Joseph Waddington</foaf:lastName>
		<foaf:mbox_sha1sum>4e73987d86c2f05a20603fd7d03abc0477167fd4</foaf:mbox_sha1sum>
		<foaf:name>R. Joseph Waddington</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/11"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/stephanie-d-teasley">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-michigan"/>
		<rdfs:label>Stephanie D. Teasley</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Stephanie</foaf:firstName>
		<foaf:lastName>D. Teasley</foaf:lastName>
		<foaf:mbox_sha1sum>16bc90b1cd9ae3c57954ba24066eea419ebb2377</foaf:mbox_sha1sum>
		<foaf:name>Stephanie D. Teasley</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/11"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-saskatchewan"/>
		<rdfs:label>Christopher A. Brooks</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Christopher</foaf:firstName>
		<foaf:lastName>A. Brooks</foaf:lastName>
		<foaf:mbox_sha1sum>83d2ec499449a83cd1952800a1849bbb2118683b</foaf:mbox_sha1sum>
		<foaf:name>Christopher A. Brooks</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/24"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jim-greer">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-saskatchewan"/>
		<rdfs:label>Jim Greer</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Jim</foaf:firstName>
		<foaf:lastName>Greer</foaf:lastName>
		<foaf:mbox_sha1sum>da266e394507fe8b78f3f05dfd2dd5f8c1657cef</foaf:mbox_sha1sum>
		<foaf:name>Jim Greer</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/24"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carl-gutwin">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-saskatchewan"/>
		<rdfs:label>Carl Gutwin</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Carl</foaf:firstName>
		<foaf:lastName>Gutwin</foaf:lastName>
		<foaf:mbox_sha1sum>63b4028010434a2183902cd7844095cc61932b76</foaf:mbox_sha1sum>
		<foaf:name>Carl Gutwin</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/24"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/paul-prinsloo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-south-africa"/>
		<rdfs:label>Paul Prinsloo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/South_Africa"/>
		<foaf:firstName>Paul</foaf:firstName>
		<foaf:lastName>Prinsloo</foaf:lastName>
		<foaf:mbox_sha1sum>61452492c70072062ccc019ac115ebcc22457725</foaf:mbox_sha1sum>
		<foaf:name>Paul Prinsloo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/36"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/diederik-m-roijers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utrecht-university"/>
		<rdfs:label>Diederik M. Roijers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Diederik</foaf:firstName>
		<foaf:lastName>M. Roijers</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Diederik M. Roijers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/4"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/johan-jeuring">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utrecht-university"/>
		<rdfs:label>Johan Jeuring</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Johan</foaf:firstName>
		<foaf:lastName>Jeuring</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Johan Jeuring</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/4"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ad-feelders">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utrecht-university"/>
		<rdfs:label>Ad Feelders</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Ad</foaf:firstName>
		<foaf:lastName>Feelders</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Ad Feelders</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/4"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rong-yang">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/western-kentucky-university"/>
		<rdfs:label>Rong Yang</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Rong</foaf:firstName>
		<foaf:lastName>Yang</foaf:lastName>
		<foaf:mbox_sha1sum>63b641a4a1c6dbb4c782d53640c5f2240a3dbc41</foaf:mbox_sha1sum>
		<foaf:name>Rong Yang</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/5"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Ryan S.J.d. Baker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ryan</foaf:firstName>
		<foaf:lastName>S.J.d. Baker</foaf:lastName>
		<foaf:mbox_sha1sum>188538b9d7ab9a3c2883dc5640511f67db9e3aab</foaf:mbox_sha1sum>
		<foaf:name>Ryan S.J.d. Baker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/9"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/42"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Ryan S.J.d. Baker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ryan</foaf:firstName>
		<foaf:lastName>S.J.d. Baker</foaf:lastName>
		<foaf:mbox_sha1sum>188538b9d7ab9a3c2883dc5640511f67db9e3aab</foaf:mbox_sha1sum>
		<foaf:name>Ryan S.J.d. Baker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/9"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/42"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/daniel-roberge">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Daniel Roberge</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Daniel</foaf:firstName>
		<foaf:lastName>Roberge</foaf:lastName>
		<foaf:mbox_sha1sum>6a61c6dcc8c35d1cbe9100549441a21a5846b6b5</foaf:mbox_sha1sum>
		<foaf:name>Daniel Roberge</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/42"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/anthony-rojas">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Anthony Rojas</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Anthony</foaf:firstName>
		<foaf:lastName>Rojas</foaf:lastName>
		<foaf:mbox_sha1sum>57f238383c48d69067a1c418756bd0523c2970c2</foaf:mbox_sha1sum>
		<foaf:name>Anthony Rojas</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2012/paper/42"/>
	</foaf:Person>
</rdf:RDF>