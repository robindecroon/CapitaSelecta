<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:dcterms="http://purl.org/dc/terms/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:ical="http://www.w3.org/2002/12/cal/ical#"
    xmlns:swrc="http://swrc.ontoware.org/ontology#"
    xmlns:bibo="http://purl.org/ontology/bibo/"
    xmlns:swc="http://data.semanticweb.org/ns/swc/ontology#"
    xmlns:led="http://data.linkededucation.org/ns/linked-education.rdf#"
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" >
	<swc:ConferenceEvent rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011">
		<swc:completeGraph rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/complete"/>
		<swc:hasAcronym>LAK2011</swc:hasAcronym>
		<swc:hasRelatedDocument rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<rdfs:label>First Conference on Learning Analytics and Knowledge 2011</rdfs:label>	<ical:dtend rdf:datatype="http://www.w3.org/2001/XMLSchema#date">2011-03-01</ical:dtend>
		<ical:dtstart rdf:datatype="http://www.w3.org/2001/XMLSchema#date">2011-02-28</ical:dtstart><foaf:homepage rdf:resource="https://tekri.athabascau.ca/analytics/"/>
	</swc:ConferenceEvent>
	<swrc:Proceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings">
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/43"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/44"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/45"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/47"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/48"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/49"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/51"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/52"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/53"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/54"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/55"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/56"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/58"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/59"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/61"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/62"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/63"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/64"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/65"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/66"/>
		<swc:relatedToEvent rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011"/>
		<swrc:booktitle>Proceedings of 1st Learning Analytics and Knowledge (LAK2011), Feb 28 - Mar 1, 2011</swrc:booktitle>
		<swrc:month>Mar</swrc:month>
		<swrc:series></swrc:series>
		<swrc:year>2011</swrc:year>
	</swrc:Proceedings>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/43">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Learning Analytics To Identify Exploratory Dialogue within Synchronous Text Chat</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/43/authorlist"/>
		<swrc:abstract>While generic web analytics tend to focus on easily harvested quantitative data, Learning Analytics will often seek qualitative understanding of the context and meaning of this information. This is critical in the case of dialogue, which may be employed to share knowledge and jointly construct understandings, but which also involves many superficial exchanges. Previous studies have validated a particular pattern of ‘exploratory dialogue’ in learning environments to signify sharing, challenge, evaluation and careful consideration by participants. This study investigates the use of sociocultural discourse analysis to analyse synchronous text chat during an online conference. Key words and phrases indicative of exploratory dialogue were identified in these exchanges, and peaks of exploratory dialogue were associated with periods set aside for discussion and keynote speakers. Fewer individuals posted at these times, but meaningful discussion outweighed trivial exchanges. If further analysis confirms the validity of these markers as learning analytics, they could be used by recommendation engines to support learners and teachers in locating dialogue exchanges where deeper learning appears to be taking place.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 Learning resources are being uploaded to the Internet at such a rate that is increasingly likely that individuals will find themselves adrift in an ‘ocean of information’ [1, p136]. Resources that extend over time, such as conference recordings, videos and real-time dialogue capture are difficult to scan or assess quickly and so learners and teachers must rely on basic, often misleading, cues such as title, keyword and producer when deciding whether to make use of a resource. Analytics are therefore needed to distinguish between resources that extend over time, and to identify those that support learning. This paper investigates the use of key words and phrases to identify sections of Elluminate® online conference sessions that have inspired participants to engage in knowledge building through dialogue in the associated synchronous text chat. 
 In other contexts, various approaches have been used to identify and classify forms of learning dialogue and academic dialogue but these are typically dependent on the use of grammatically correct, carefully punctuated and formally structured text [2,3]. Synchronous textual dialogue is likely to be more akin to speech than to formally constructed prose [4]. It is therefore relevant to look at how people build knowledge together through speech. In face-to-face settings, Mercer and his colleagues [5-9] have distinguished three social modes of thinking used by groups of learners: disputational, cumulative and exploratory. Of the three, exploratory dialogue is the type considered most educationally desirable by teachers [10]. Mercer and Littleton [8, 62] provide a clear description of its use in a school environment: Exploratory talk represents a joint, coordinated form of co-reasoning in language, with speakers sharing knowledge, challenging ideas, evaluating evidence and considering options in a reasoned and equitable way. The children present their ideas as clearly and as explicitly as necessary for them to become shared and jointly analysed and evaluated. Possible explanations are compared and joint decisions reached. By incorporating both constructive conflict and the open sharing of ideas, exploratory talk constitutes the more visible pursuit of rational consensus through conversation. Exploratory dialogue is a form of discourse that may be found in both online and offline learning environments [4,11], where it can be taken as an indication that learning is taking place and that learners are going beyond a simple accumulation of ideas. The research reported here therefore asks: Could the identification of exploratory dialogue within the synchronous textual chat associated with online resources help to identify resources and sections of resources that support learning? Data collection and preparation In order to investigate these questions, data were collected from Elluminate, a web conferencing tool that supports chat alongside video, slides and presentations. The focus was on the synchronous discussion related to a two-day online teaching and learning conference. The Elluminate text chat in four conference sessions, each between 150 and 180 minutes in length (24,530 words in total) was investigated. During these four sessions, 233 participants logged in to the Elluminate sessions at one or more times. The majority of these participants were higher education researchers and practitioners from around the world, although most were based in the UK. Analysis presented in this paper focuses mainly on the afternoon session of 22 June, when 120 people logged in to the Elluminate discussion and 67 actively participated in the Elluminate synchronous text chat. Of these participants, 47 were female (26 contributed to the text chat), 54 were male (34 contributed to the text chat) and the gender of 19 is unknown (7 contributed to text chat). The conference timetable was used to subdivide the four main conference sessions into smaller units, including pre-session chat, post-session chat, conference introduction, groups of short talks, longer talks, moderated discussion and keynotes. 
 The four conference sessions were all archived and made public by the organizers. Sociocultural discourse analysis [12] was used to identify words that could be indicative of exploratory dialogue. These included: 
 - Challenges		eg But if, have to respond, my view - Critiques		eg However, I’m not sure, maybe - Discussion of resources	eg Have you read, more links - Evaluations		eg Good example, good point - Explanations		eg Means that, our goals - Explicit reasoning	eg Next step, relates to, that’s why - Justifications	eg I mean, we learned, we observed - Others' perspectives	eg Agree, here is another, take your point 
 Ninety-four words and phrases were identified in this way. Some words, phrases and punctuation, which initially appeared to be good indicators, were discarded because they were often used for finding out more about the conference, its tools and participants, rather than its content. For example, interrogatives and question marks were often associated with comments such as ‘Can you still hear?’ or ‘what’s everyone doing for coffee???’ Once exploratory markers had been identified, the Elluminate chat was pasted into Microsoft Word, where a simple ‘find and replace’ Apple Script program was used to highlight the key words and phrases. The data was then transferred to Excel for more detailed analysis. Table 1 gives an example of a section of data in which six of nine consecutive postings were coded as exploratory. By way of contrast, Table 2 gives an example of nine consecutive postings in the data, none of which was coded as exploratory. 
 Table 1: Dialogue coded as exploratory (real names removed from all data samples). Each row of the table represents one contribution. Words in bold have been highlighted by the analysis. 
 Table 2: Dialogue not coded as exploratory. Each row of the table represents one contribution. 
 Once key words had been highlighted, the postings were divided according to the timings on the official conference timetable, and the use of exploratory dialogue in each section was calculated. As postings are short and clearly delineated, the posting was taken as the unit of analysis, and so an entire posting containing one or more markers of exploratory dialogue would be coded as exploratory. The conference included two morning sessions and two afternoon sessions. The first phase of analysis focused on identifying the periods containing the greatest concentration of exploratory markers. The four main sessions were first rated on the amount of turns in the conversation rated as exploratory (those containing one or more of the words/phrases indicating exploratory talk). The total number of exploratory turns was divided by the number of people contributing to the posting dialogue, to give an average number of exploratory posts per person. Total exploratory turns in dialogue = average no of exploratory posts per person Number of people contributing to posted dialogue In addition, the number of words in the turns considered exploratory were totaled and divided by the number of people contributing to the posted dialogue, to give an average number of exploratory words per person. The results are shown in Table 3. 
 Total words in exploratory turns/Number of people contributing to posted dialogue = average no of exploratory words per person 
 Table 3: Comparing exploratory turns per person in main conference sessions. 
 In both cases the afternoon session of 22 June pm, which contained one of the two keynote sessions, appears to have inspired the most exploratory dialogue. On the other hand, the morning session that day appears to have inspired the least learning dialogue. As the four conference sessions differed in length, it is possible that these differences were related to having more or less time in which to post. Table 4 therefore indicates the average number of exploratory turns posted per minute, and the average number of words in exploratory turns posted per minute. 
 Table 4: Comparing exploratory turns per person in main conference sessions. 
 Once again, the afternoon of 22 June contained the most exploratory dialogue. However, the least exploratory dialogue now appears to have taken place on the afternoon of 23 June. Further analysis is required in order to investigate which of these measures is most relevant. As all measures indicate that the afternoon of 22 June contained the highest concentration of exploratory markers, the following analysis concentrates on that session. During that afternoon, the Elluminate session was divided into four sections: a set of short talks, moderated discussion, keynote, and then chat between the scheduled end and the actual close of the Elluminate session. 
 Table 5: Comparing contributions to the synchronous Elluminate text chat during one continuous afternoon conference session (22 June). 
 Table 5 presents a summary of analysis of that afternoon’s Elluminate chat. As the length of the sessions ranged from 8 to 75 minutes, contributions were first classified by time. This showed that the most posts per minute took place during the informal chat session, whereas the most exploratory posts were contributed while the keynote was in progress. The series of short talks at the beginning of the afternoon appeared to be associated with the lowest levels of talk, whether exploratory or not. On the basis of markers of exploratory dialogue, it therefore appears that conference participants engaged in the highest levels of knowledge construction on the afternoon of 22 June, and that these levels were highest during the 75-minute keynote discussion. It is not possible to represent the whole Elluminate session here, but Table 7 provides a one-minute extract. If a recommendation engine were to use these markers to identify sections of the 12-hour conference where knowledge construction took place, it would recommend sections like those shown in Table 6. This minute contains 6 exploratory turns (mean for the keynote was

 1.6 per minute), it contains 192 words (mean for the keynote was 67.5 per minute) and it contains 81 words in the exploratory turns (mean for the keynote was 31.1 per minute). Using the markers has clearly not identified all the exploratory turns, but it has successfully identified a section including challenge (line 5), critique (line 7), discussion of resources (lines 2 and 5), evaluation (line 12), explicit reasoning (line 4) and consideration of the perspective of others (line 9, among others). The discussion moves fast, contains contributions from nine different participants, and is grounded by references to two online resources as well as to examples drawn from three separate higher education establishments. It clearly relates to the presentation that is taking place in the audio channel, which is referenced in lines 1 and 2, and it moves this discussion on by posing questions and relating the discussion to personal experience. 
 Table 6: One minute of Elluminate text chat, with exploratory markers highlighted in bold. 
 As Elluminate identifies who has posted each comment in the text chat, it was also possible to consider the postings of individuals (only the participants who made some contribution to the live chat were included in this analysis). Once again, a large amount of exploratory activity was evident during the keynote on 22 June, whereas the many contributions during the informal chat were found to be short and lacking in exploratory talk. When analysed in this way, participants were seen to be contributing longer, more thoughtful posts during the short talks at the beginning of the afternoon but, once again, the exploratory dialogue was less evident during these short talks than during the moderated discussion or the keynote. Table 7 summarises analysis of the activity by individuals during the keynote talk and compares the contributions of the five people who posted the most contributions during this session. The moderator (M) was very active, posting 32 times. For all these individuals who posted a large number of posts, more than a fifth of their words were in contributions containing exploratory markers. However, there are notable differences within these groups, and C stands out as a high-volume poster with 75% of her total words in posts containing exploratory markers. These figures were typical of those in other sessions – the moderator was consistently one of the most active contributors. Although individuals’ interest and attention clearly fluctuated according to session, C was often among those with the highest percentage of exploratory posts in a session. 
 Table 7: Analysis of the contributions of the five individuals who contributed the most posts during the keynote session. 
 A recommendation engine using markers of exploratory dialogue to search for people engaged in learning would therefore have highlighted C. Her contributions, such as the one below, incorporated important elements of knowledge building, including explanation, explicit reasoning and discussion of resources. In addition, many of her contributions, like this one, were addressed to a named participant in the conference, indicating that she was engaging in dialogue, rather than adding didactic comments or personal reflection. Learning should be an active process, [named participant]. Lectures, like television, tend to be more passive. Not all lectures are like that, but most are. Recording them and putting them on the web doesn’t make them any better. That’s why social knowing (John Seely Brown) is so much better, where people come together to construct knowledge through their conversations and interactions with each other. Alternatively, engaging in projects or experiments can be useful. 
 Discussion.
 Preliminary analysis of the data suggests that markers of exploratory dialogue can be used to distinguish meaningfully between Elluminate sessions and to support evaluation of those sessions. The markers proved to be a more nuanced tool than generic analytics, such as simply counting the numbers signed in for an Elluminate session, or contributing to the text chat. Peaks of posting activity were associated with the end of Elluminate sessions, when many participants were thanking speakers and saying goodbye, while others were discussing what they had learned. Peaks of exploratory activity, on the other hand, were associated with periods set aside for discussion and keynote speakers. Fewer individuals posted at these times, but meaningful discussion outweighed trivial exchanges. Exploratory markers indicate the importance of context when assessing learning dialogue. When several speakers were presenting in close succession, posting activity was relatively low, but increased as the presentations came to an end. However, when speakers had time to engage in discussion as part of their allotted timeslot – as was the case with the keynote speaker – meaningful exchanges peaked. Unscheduled chat at the beginning of Elluminate sessions tended to be primarily social in nature, while unscheduled chat was likely to include many more exploratory exchanges. This has implications for those scheduling online conferences – clearly flagged discussion sessions related to presentations will be easier to find in the archives than discussions that overrun into other sessions. Discussion continues after scheduled sessions, so it could prove useful to leave Elluminate sessions open for chat for some time after the end of the scheduled presentation. Not all exploratory dialogue related to conference content – there was considerable discussion of online conferences and of social issues. A future set of exploratory markers should identify keywords such as ‘mike’, ‘sound’ and ‘how are you’ that would signal a move away from discussion of content. At this stage, though, analysis suggests that time needs to be set aside for these exchanges, to avoid distraction or cognitive overload when presentations begin. 
 Areas for further investigation.
 Data analysis covered two complete days of online conference, and only a representative sample can be presented in a paper of this length. However, the analysis to date is clearly limited in its scope and there is a pressing need for evaluation of the reliability and validity of these presumed markers of exploratory dialogue – both individually, and as a set. If this set, or an amended set, of markers can be shown to be reliable and valid it will be important to attend to both context and practicalities. Exploratory dialogue is not necessarily focused on learning about content – individuals and groups are also likely to be learning about the tools they use (such as Elluminate) and the people with whom they are interacting. This type of learning dialogue is of less interest for people participating after the event, as they are neither using the same tools in the same way nor interacting with the same people. From a practical perspective, the current analysis is mainly carried out manually and in future it will be necessary to investigate how this process can be carried out automatically in order to benefit both learners and educators. It will also be useful to investigate the relationship between the text chat and the audio and video channels. Compared to other computational linguistics approaches to text analysis, the approach presented in this paper is very simple; we are testing the limits of the simple exploratory dialogue markers described. In parallel, however, we are also beginning to test more complex forms of computational rhetorical analysis as described by Sándor [13,2], as a way to detect linguistic phenomena associated with the making of knowledge-level claims around open educational resources, on which we hope to report in future work. 
 Conclusion.
 Although the conference sessions studied here are freely available as open online resources, they are both difficult and time-consuming for users to navigate. The published timetable of the conference gives some guidance, but is limited because a few sessions were reorganized, started late or overran. Some provoked little debate, whereas others inspired discussion which extended far beyond the scheduled time period. The conference also included set-up sessions and breaks, during which talk turned to the practicalities of microphone use, and the absence of virtual biscuits. There is therefore a need for analytics that will allow learners to locate sections of an Elluminate session that clearly support learning. At the same time, both learners and educators can benefit from tools that allow them to use Elluminate and other, similar, resources more effectively. Analytics can be used to distinguish different types of contribution to text chat, and to support learners who wish to engage in more fruitful learning discussion. They can be used to help educators schedule events in order to support discussion, and to model exploratory dialogue within that discussion.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Learning Analytics To Identify Exploratory Dialogue within Synchronous Text Chat</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/43/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/44">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Social and Semantic Network Analysis of Chat Logs</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/44/authorlist"/>
		<swrc:abstract>Multi-user virtual environments (MUVEs) allow many users to explore the environment and interact with other users as they learn new content and share their knowledge with others. The semi-synchronous communicative interaction within these learning environments is typically text-based Internet relay chat (IRC). IRC data is stored in the form of chatlogs and can generate a large volume of data, posing a difficulty for researchers looking to evaluate learning in the interaction by analyzing and interpreting the patterns of communication structure and related content. This paper describes procedures for the measurement and visualization of chat-based communicative interaction in MUVEs. Methods are offered for structural analysis via social networks, and content analysis via semantic networks. Measuring and visualizing social and semantic networks allows for a window into the structure of learning communities, and also provides for a large cache of analytics to explore individual learning outcomes and group interaction in any virtual interaction. A case study on a learning based MUVE, SRI’s Tapped-In community, is used to elaborate analytic methods.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 Multi-user virtual environments (MUVEs) are used for many different purposes in a number of contexts, but the interaction within these environments can often lead to learning outcomes and resource sharing, and there is an increase in their use for learning communities. Communicative interaction within these environments is commonly conducted via Internet relay chat (IRC). Text boxes displaying IRC has been a successful tool for allowing for communicative interaction. However, IRC poses a difficulty for researchers seeking to analyze and interpret the communicative interaction since data is stored in the form of chatlogs, which can produce large volumes of text data. This paper discusses and applies procedures for the representation and analysis of chat interaction in learning based MUVEs, or learning taking place in any type of MUVE, as social and semantic networks. A description of the social and semantic network approach to human communication is presented followed by a review of parallel methodological techniques. Elaboration of methods presented in this research is covered along with sample outputs from a case study on SRI’s Tapped-In (tappedin.org) community [1]. Finally, applications and future research possibilities are offered. 
 This work was supported by NSF Award #0943147. The views expressed herein do not necessarily represent the views of NSF. 
 2 Concepts.
 2.1 Communicative Interaction in MUVE.
 Although MUVEs have a wide array of uses, communicative interaction within the environment is often conducted through Internet Relay Chat (IRC). IRC is conducted in a semi-synchronous way, where comments posted appear almost instantly for other users to view and respond to. IRC is a much more real time mode of computermediated communication than listserv messages, bulletin boards, and email. Much like instant messaging (IM), IRC allows users to select a username that appears before each comment they post, allowing multiple users to comment and maintain conversational interaction. IRC interaction is conducted within a chat-box that displays all users’ comments along with their username in a log file. In addition to IRC interaction being semisynchronous, it is also persistent. The persistence of these interactions allow for the storage of all data as chatlogs, which can in turn be used for analyses of the users’ interaction. However, the nature of chatlogs as a dynamic, non-threaded interaction introduces some methodological hurdles regarding analysis. Posts to IRC conversations are generally quite short, usually a few words to several lines allowing the IRC interaction to allow for multi-participant semi-synchronous interaction, similar to face-to-face (FtF) conversation in the sense that many people can interact in the same communicative space [2]. However, IRC interaction departs from FtF in how interactional coherence is achieved, and users adapt to CMC interaction in interesting ways [3]. One of the principle differences is adjacency relevance, where an utterance in FtF interaction is generally relevant to the one before it. In IRC, this constraint is loosened: an utterance may be relevant to the one that appeared several lines before it. Herring [3] showed that users invent devices to manage interactional coherence in spite of the fact that IRC contributions can arrive out of sequence, violating conventions from in face-to-face conversation concerning the sequential organization of contributions. Since we cannot assume that adjacent contributions are relevant to each other, as they may be relevant to contributions a little while back, we need an estimate of this relevance. Estimates of who was talking to who are generated from the concept of chat proximity: these people are co-present, and contributions may be relevant to any of these things said recently. The constraint that contribution C_i is relevant to C_(i-1) is loosened to a temporal window. The algorithm introduced in the Methods section below is motivated by this unique interaction, and uses a temporal window to capture such non-adjacent utterances. 
 Smith, Farnham, & Drucker [4] investigated the social life of small graphical chat spaces by analyzing Microsoft’s V-Chat systems. The VChat research illustrates the usage patterns of graphical chat systems, illuminating the ways physical proxemics are translated into social interactions in online environments. Krikorian, Lee, Chock, and Harms [5] developed methods to study user proximity in graphical chat rooms, and found that various perceived demographics influenced the social “distance” of avatars in the graphical chat environment. In addition to the spatial analysis, there have also been methodological advancements regarding the communicative content of virtual environments. Sack [6] generated conversation maps of newsgroup postings and described very large conversations by visualizing large amounts of interaction in newsgroups. Suthers, Dwyer, Medina, and Vatrapu [7] developed a framework for representing and analyzing distributed interaction within multi-user virtual environments, including some structural representation of interaction in sequential records of events. Rosen, Woelfel, Barnett, and Krikorian [8] explicated a methodology for semantic network analyses of IRC interaction in virtual worlds. Rosen & Corbit [9] developed network analytic techniques for the measurement and representation of the structure of networks from IRC interaction. Understanding the structure and content of the interaction provides an in-depth and unique window into MUVEs along several lines. First, network position can be used to identify network roles, similar to Turner et al. [10], identifying roles such as answer person and question person. Second, network analytic techniques can be employed on the subsequent data. Network visualizations can be generated allowing for visual and representational analyses, elements that have traditionally important to community research [11]. Finally, network analytics and representations can be used in cohort with semantic network analysis for a more complete understanding of the learning environments and interactions. 
 2.1 Social Networks.
 Social network perspectives focus on the structure of social systems. Individual characteristics are only part of the story: people influence each other, and ideas and materials flow throughout the network [13]. From the network perspective, the social environment can be expressed as patterns or regularities in relationships among interacting units. This section elaborates some of the network concepts and terminology used in the subsequent methods for the analysis of MUVEs. The form of network that will be utilized herein is a communication network. Communication networks are generally defined as the patterns of contact that are created by the flow of messages among communicators through time and space [see 2]. However, these flows are not clear in IRC interaction from an adjacency approach, and the algorithmic solution presented in this paper is way of deal with this problem. Communication network analysis identifies the communication structure, or communication flow. Relation ties (linkages) between actors are channels for the transfer (flow) of either material or nonmaterial resources, or for an association between actors. The ties that exist between the nodes can vary along several elements, including direction, reciprocity, and strength. 
 Ties between actors can be measured as being either directional, or non directional. Ties that are directional indicate the movement from one point to another, such as the number of phone calls one person makes to another, or the degree of liking one person has for another. Additionally, these links can also be symmetrical or asymmetrical. If the link is directional and the relation has different values in each direction then the link is asymmetrical and lacks reciprocity. Non-directional links simply indicate an association of two actors in a shared partnership, such as two students being part of the same class. There are many measures of centrality for individual nodes, as well as how connected the entire network is; select measures are discussed below. 
 Degree Centrality. The degree measure of centrality is calculated by counting the number of adjacent links to or from an actor in a network [12]. Freeman [14] conceptualized this measure as an indicator of individual activity, yet it does not capture system-wide properties of the network like density and centralization, discussed below. It does, however, represent the number of alternatives available to an individual in the network. While a relatively straightforward measure, degree centrality provides insight into individual contributions to the interconnectedness of the overall network [14]. 
 Betweenness Centrality. Betweenness centrality measures the relative brokerage of an individual node i by indicating the number of nodes j that need to go through i to get to other nodes k that could otherwise not be reached. Betweenness centrality is calculated by the proportion of all geodesics linking j & k that pass through i, Σ for all nodes. 
 Density. Density is used to measure the completeness of the relations in a network, also called connectedness. Measured as the ratio of total links to possible links, density can identify networks as being sparse (relatively disconnected) or dense (relatively well connected). 
 Centralization. Centralization measures the disparity, or variation of the individuals’ centrality (which can be betweeness or degree centrality) in a given network. The higher a networks degree centralization is the more likely it is that few individuals are well connected while others are less connected. Conversely, the more decentralized a network is, the more equal the members’ centrality scores are. 
 2.2 Semantic Networks.
 In semantic network analysis, a specific text is analyzed to generate a measure of the degree to which words are associated. The association is then used to infer something about their meaning or the meaning of the context they were used in. One of the more common approaches is to generate the amount of co-occurrence between word-pairs within a particular set of text. Then, the co-occurrence measure of relatedness across a particular set of words can be used to group, cluster, or scale the words (or a specific subset, such as frequently occurring words. The groups or clusters can be used for analysis, or used to obtain additional measures for use in other analyses, or bases for formal content analysis [15, 16]. 
 3 Method.
 3.1 Social Network Analysis.
 The structure of the communicative interaction within a MUVE may be examined through network analysis. Network analysis is a set of research procedures for identifying structures in social systems based on the relations among the system’s components, and is the methodology used to operationalize the network approach to interaction, discussed above. The basic network data set is an n x n matrix S, where n equals the number of nodes in the analysis. A node is the unit of analysis; in the current research a MUVE participant will be considered a node. Each cell, Sij, indicates the strength of the relationship, which would typically represent the amount of communication from person i to person j. Since there is no inherent direct communicative relationship between individuals in IRC interaction, the relationship used herein assigns relational strength by capturing temporal proximity of contributions in IRC. Relationships in networks are analyzed as directional when possible, and in the current study direction is established based on the ordering of contributions within a temporal window in IRC. This method provides the directional differences between all analyzed parties, representing the communication matrix. Network mapping procedures are used to generate sociogram maps that visually represent the networks created using above procedures. These will allow for the visual analysis of other network data, as well as elaborate clique’s and network roles that can remain cloaked when only analyzing numerical outputs. 
 MUVE Communication Matrix Formation. 
 To generate the n x n matrix used in the analysis of MUVE interaction, a process was developed that extracts the strength of the relationship between each cell, Sij. Since IRC is logged temporally based on the sequential comments of participants, methods can be used to generate relational strength based on proximity in the interaction. The algorithm includes several parameters to generate relational data from IRC interaction. Using the time stamp that accompanies all posts, a temporal parameter was used to help insure that a user is not considered connected to all users that posted after their post. This parameter can be set for use based on the context, as some interactions are faster moving than others; the current study used a limit of 120 seconds before a users connection was reset. See Table 1 for Pseudocode of the algorithm used. The algorithm is O(n), where n is the number of records in the chat dataset. Each record contains timestamp, userid, and contribution. The algorithm assumes that chat records in the dataset are sorted by timestamps. The algorithm was implemented in the Java programming language. The window size parameter was set to 120 seconds. 
 Table.

 1. Pseudocode of the algorithm used for communication matrix formation 
 3.2 Semantic Network Analysis.
 The method used in this study adapts and implement neural-based content analysis software to observe Internet communication patterns in chat rooms [8]. This implementation uses Catpac™ [17], a developed and proven semantic network analysis package that has the capability to extract word patterns and clusters. Clusters are extracted by sliding a text-window through the text and associating each word in the window with a neuron in an artificial neural network. Using a proprietary variation of an interactive activation and competition algorithm, connection strengths or weights are generated as a function of the coactivation patterns among the neurons. These weights in turn serve as the basis of cluster analysis and Galileo mapping™ [18]. Catpac has been used for the study of traditional text [20], such as articles and long response questionnaires. It has been successful in revealing clusters of associated words in text that provided helpful quantitative data to support qualitative interpretations. One of the most important aspects of the method used in the procedure discussed in this paper is the ability to analyze data based on set parameters. For this, an algorithm has been developed that parses chat data into separate and interrelated files used to determine individual, group, and systematic organizational patterns over time. This becomes useful when combined with a qualitative analysis where the researcher has an ethnographic understanding of the community members, whereas there is a "name file" that allows for directed analysis and the labeling of contributions. For example, if the learning community were associated with a large undergraduate class, the teacher would have the ability to observe semantic clusters extracted from any designated groups’ communication (e.g. freshmen, non-majors, etc.). If the analysis was on a mentor-based learning community one could observe the difference between communication originating from mentors/teachers as compared to student users. Other uses bridge to industry, where virtual task groups' general learning interactions could be parsed, revealing both potentially positive and negative trends in the interaction. 
 4 Outputs.
 Outputs below were generated from SRI’s Tapped In [1], a virtual organization that hosts the content and activities of many thousands of education professionals annually in more than 8,000 user-created spaces that include IRC, threaded discussions, shared files and URLs, and other tools to support collaborative work. Education agencies and institutions of higher education use Tapped In to meet the needs of their students and faculty. Also, approximately 40-60 community-wide activities per month are explicitly designed to help connect members, and groups are often formed after members meet in these activities. Analytic outputs below were generated from a single chat session of a Tapped In user group that deals with the use of wikis in the classroom. The session was 1 hour long with 62 participants. 
 4.1 Social network analytics and visualization.
 The analytics employed in the current methodological explication are Degree Centrality, Betweenness Centrality, Density, and Centralization. The degree centrality for the users in the Tapped-In group can be found in Table 2, and the visualization of the network can be found in Figure 1. Network centralization is 6.372% (Outdegree) and 4.104% (Indegree), indicating a decentralized network. Network density is 46.20, indicating a fairly dense network. The centrality measures presented in Table 2 offer interesting insights into the user interaction within the chat. First, the degree centrality indicates the number of incoming and outgoing connections via chat posts. The values have been normalized relative to the number of users. The in-degree measures indicate the number of message posts within the time window that were pointed back to that user, and the out-degree indicates the number of messages that user posted pointing back to other users within the window. A few users were the most active, with some slight differences between their in- and out-degree values. However, since the centralization measure was very low, the distribution of interaction was indeed spread through the network, without a core group of users that were substantially outweighing a periphery. The betweenness centrality scores indicate that there are indeed several users that have very high scores, and thus act as bridges of information in the network. These users connect other individuals that could not otherwise reach many of the people in the network. This bridging role can be seen in the visualization of the network in Figure 1. It is clear that user 7, 44, 37, and 27, who have the highest betweenness centrality, are structurally positioned in the network between many other users that would otherwise be disconnected. The sociogram also reveals several cliques (i.e. clusters) with in the network, as well as a few people that are not very well connected, often connected only to one other user, such as 14 and 56. 
 Table 2. Degree and Betweenness centrality rankings for users. Degree centralites below 1.0 and betweenness centralities below 0.5 have been removed from table. User ID’s correspond to sociogram in Figure 1, and have been anonymized from original user logon names 
 Fig. 1. Sociogram of chat-based user interaction generated using algorithm in Table 1. Thickness of lines indicated tie strength and arrows indicate direction of flow. 
 Using network analytic measures, such as centrality and density, provide for structural analyses of IRC interaction based on chat proximity. These measures and visualizations can thus be used to decipher effects along different levels of granularity; at the micro level, user roles, such as bridging or leader roles, can be identified, at the meso level one can identify group formation through clique detection [see 21 for elaboration], and at the macro level overall network centralization and density can be calculated. These measures, however, represent only a small subset of possible analytic approached afforded by network analysis, and measures should be chosen that help explain the phenomena being explored. 
 4.2 Semantic network visualization.
 Using the neural network engine in the CATPAC package allows for the semantic analysis of parsed content. The plot in Figure 2, produced using ThoughtView [19], contains a multi-dimensional scaling representation of the top 40 words from the entire 1-hour of IRC from the group analyzed. Usernames were automatically stripped from the data for the analysis of the complete interaction. There are some contextual issues regarding automated textual analysis, like the occurrence of errors in user typing, abbreviations, and icons. Additional outputs available, but not included here, include dendograms, frequency lists, and two-dimensional plots [19].Additional parsing of content into individual text files is also possible with the algorithm, allowing for analysis of specific user content based on other parameters such as demographics, individual network metrics (e.g. centrality), learning outcomes, etc. Extracting semantic clusters from user activity in IRC can allow for further exploration of contributions from specific users identified in the social network analysis as relevant to research agendas. For example, there may be interest in analyzing the content of contributions from very central participants, or from participants that became more central over time (if longitudinal data is used). The individualized outputs are not included in this paper due to space limitations. 
 Fig. 2. Multi-dimensional plot of word clusters. 
 5 Conclusion.
 This paper presented two approaches for the analysis of learning communities using IRC, and learning interaction in IRC. A social network approach for structural analysis is paired with a semantic network approach for content analysis. An algorithm was introduced for the formation of network matrices from IRC interaction. Similar versions of the social and semantic approaches discussed above have been introduced separately in earlier papers [8, 9], but the algorithm used for the network analysis is introduced here for the first time, as well as implications of combining the two procedures. One of the shortcomings of using the proposed algorithm to create social network ties in IRC from a temporal approach is that ties are an abstraction from chat interaction, rather than the traditional bilateral connections between actors. Unfortunately, some online learning environments offer little other evidence of social connectivity, and the chat proximity analysis offers a window into the social structure of chat interaction. One of the strengths of the technique is that latent or informal networks can be discovered from the interaction that may have otherwise been cloaked from analysis. Evidence of social network ties in learning communities can exist (such as direct messaging), but these connections represent intentional ties where users are choosing to be connected to each other. There is potential utility in uncovering informal network connections that may represent bridging between otherwise disconnected social groups, as well as pivotal moments where an idea or discussion has migrated across community boundaries. Communities can exist informally, and future research should employ clustering analysis and clique detection to enable automatic community detection.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Social and Semantic Network Analysis of Chat Logs</rdfs:label>
		<dc:subject>social network analysis</dc:subject>
		<dc:subject>semantic network analysis</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>virtual environments</dc:subject>
		<dc:subject>internet relay chat (irc)</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/devan-rosen"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/devan-rosen"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/victor-miagkikh"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/victor-miagkikh"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/44/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/devan-rosen"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/victor-miagkikh"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/45">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Variable Construction for Predictive and Causal Modeling of Online Education Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/45/authorlist"/>
		<swrc:abstract>We consider the problem of predictive and causal modeling of data collected by courseware in online education settings, focusing on graphical causal models as a formalism for such modeling. We review results from a prior study, present a new pilot study, and suggest that novel methods of constructing variables for analysis may improve our ability to infer predictors and causes of learning outcomes in online education. Finally, several general problems for causal discovery from such data are surveyed along with potential solutions.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 Scientists and engineers at the Apollo Group are developing an Individualized Learning Platform (ILP) for online education, the broad overview of which is illustrated by Fig. 1 [1]. The ILP is being constructed using insight from domain experts in cognitive and learning sciences while deploying a data-driven Intelligence Engine that takes input data or “signals” from the ILP and provides appropriate guidance to administrators, faculty, and students to better customize and individualize the online learning experience. A wide variety of information is provided to and recorded by the ILP, including information about learner and faculty context, aspects of curriculum, and so on. Coupling insight from educational theory with the Intelligence Engine will allow the Apollo Group to enhance learner satisfaction while improving learning outcomes and supporting other institutional goals. Ann Brown’s inﬂuential work [2] calls for a designbased empiricism, and the reader will ﬁnd Fig. 1 only slightly adapted from a diagram in that work. The ILP is designed around several core principles, one of which is that guidance be evidence-based [1]. This paper focuses on a candidate methodology to achieve this core objective, focusing on the discovery of causes of positive learning outcomes in the online education environment. The learning management system of the ILP will track student progress and activity in online courses. A central challenge is to determine the predictors, and especially causes, of student learning outcomes given these records of their activities and interactions. 
 Fig.

 1. Illustration of Apollo Group’s Individualized Learning Platform, reproduced from [1]. 
 Predictive models are useful for purposes such as identifying students likely to withdraw from courses or otherwise have negative learning outcomes; such models rely on discovering “symptoms” in student behaviors and activity to predict likely outcomes. If we can identify, from passive observation, students “at risk” for negative learning outcomes, instructors can “ﬂag” students to target existing resources toward them to rectify problems a student may be having. However, “symptoms” and predictors of learning outcomes need not identify causes of learning outcomes. When we acquire causal knowledge, we acquire the ability to predict values of variables post-intervention. Traditional statistical methods focus on predictive tasks, allowing us to forecast or classify from observed attributes of a unit (e.g., student) but not to reliably do so after manipulation of the environment (e.g., online courseware, methods of instruction, etc.). If we are able to identify causes, we can better design interventions to drive learning gain and other positive outcomes for students, knowing that post-intervention these changes will drive better learning outcomes. Further, such knowledge can lead to the development and engineering of better online learning platforms and environments. However, there are several hurdles to overcome to achieve such insight. We focus on the complexity of data collected in the online environment and begin to address the dearth of literature focusing on transforming log-style, transactional data collected in online courseware for use with causal discovery procedures (1). In the next section we sketch an extant framework for causal discovery from non-experimental datasets. In Section 3, we survey a past application of this framework in the online education domain. We outline the multi-faceted complexity of data collected in online education environments in Section 4. In Section 5, we describe a pilot study of data from an online graduate economics course and suggest in Section 6, based on the results of the pilot study, that we need to construct new measures of student behavior from underlying “raw” variables. In Section 7 we outline three remaining general problems for causal discovery in the online education domain and provide concluding remarks in Section 8. 
 2 Causal Discovery from Observational Data.
 Data collected in log ﬁles and databases that underlie online courseware are non-experimental, historical data. As a result, we are rarely in the position to learn causal relations in the paradigmatic manner of the sciences: namely, from randomized experimental data. Despite natural diﬀerences in courses from offering to oﬀering and from instructor to instructor, we as investigators cannot reach into the past to intervene and experiment with courseware or other aspects of the online education experience. Over the past twenty years, there has been a large research program by philosophers, statisticians, and computer scientists to develop many diﬀerent methods for the discovery of causal relations from observational datasets. Much of this research has focused on causal interpretations of probabilistic graphical models, speciﬁcally directed acyclic2 graphs (DAGs) with associated probability distributions, called Bayesian networks ([7], [8]). Within this formalism, variables are represented by nodes in a graph with edges between nodes representing direct causal relationships between variables. Consider the graph of Fig. 2, modeling qualitative, hypothetical causal relationships among attributes of students in an imaginary online course. Several attributes might be particularly salient for non-traditional students to whom online education and degree programs are appealing. We model relationships between hypothetical measures of employment, size of family (familySize), time obligations not related to a student’s education (obligations), time spent studying, motivation, length of messages in an online discussion forum (messageLength), academic ability, and ﬁnal exam scores in a course (ﬁnal ). In our hypothetical model, student ﬁnal exam performance has two direct causes: student ability and studying. Further, the model represents relationships among the determinants of a student’s studying behavior. 
 (1) A noteworthy exception for educational data is ([3]), though their analysis is not directed speciﬁcally at causal discovery. 
 (2) Feedback cycles (in time) can be modeled within the Bayes nets framework by, for example, deploying variables indexed by time. Some literature ([4], [5], [6]) focuses on the discovery of cyclic graphical models, though work on this topic is underdeveloped compared to the Bayes nets formalism. 
 Fig. 2. Graphical representation of hypothetical causal relationships for students in an online course 
 If two crucial (usually reasonable) assumptions hold-the Causal Markov Axiom and the Causal Faithfulness Condition [7]3 - then the causal structure encoded in the Bayesian network graph implies a set of probabilistic (conditional) independence relations between the variables. The Causal Markov Axiom asserts4 that, assuming there are no unmeasured common causes of variables we consider, a variable is probabilistically independent of its non-descendents (noneﬀects) conditional on its direct causes. The assumption of Faithfulness asserts that all probabilistic independencies that are actually observed occur only because of an absence of a direct causal relation. That is, conditional independence between variables does not occur by accident (via canceling out settings of parameters, for example). 
 (3) There is substantial philosophical literature about the Causal Markov Axiom and the Causal Faithfulness Condition (e.g., [9], [10], [11], [12], [13], [14], [15]). I pass over this controversy as these assumptions are standard in the causal learning framework deployed here. 
 (4) Assuming it is possible to represent the underlying causal structure as a directed acyclic graph 
 Fig. 3. Hypothetical illustration of causal relations that could lead to a faithfulness violation 
 To illustrate how a violation of this assumption might occur, consider a slight modiﬁcation to hypothetical causal relations among three variables from Fig. 2 in Fig. 3, in which the association represented by each arrow is positive or negative. Suppose we posit that increased family size has a negative impact on employment; the student is likely to work less as the size of his or her family increases but that employment and family size both contribute to increased noneducational time obligations. The negative eﬀect of familySize on employment combined with the positive eﬀect of employment on obligations, given appropriate (perhaps unlikely) parameter values (representing strength of causal relations), may exactly “cancel out” the positive eﬀect of familySize on obligations. Such “canceling out” parameter values could lead us to believe that familySize and obligations are independent, despite the fact that there is a direct causal relation between the two. This hypothetical judgment of independence despite a direct causal relation is a violation of faithfulness. While a causal Bayesian network implies conditional independencies, this “graphs → independencies” mapping is many-one: for any particular graph G, there will typically be other graphs, though not all graphs, that predict the same (conditional) independencies, and so are observationally indistinguishable from G. We are all familiar with the old maxim that “correlation does not imply causation.” For example, if verbosity in online message forums (messageLength) and studying are correlated, this can be explained by messageLength → studying, studying → messageLength, or studying and messageLength sharing a common cause (as they do in Fig. 2, motivation), or a combination of these explanations. Multiple graphs can imply the same observed correlations and/or independencies. We can use observational data to learn a set of (observationally indistinguishable) causal structures: namely, exactly the set of possibilities that could have produced the observed pattern of independencies in the data. Causal Bayesian network structure learning algorithms, e.g., the PC algorithm [7] and GES ([16], [17]), under suitable assumptions, will identify (the set of observationally equivalent graphs containing) the correct DAG in the large sample limit. Two rough intuitions illustrate basic principles of search for graphical causal models from conditional independence relations. The ﬁrst concerns “screening oﬀ” relations whereby, to consider a simple three variable example, two variables, say messageLength and studying, are correlated but become independent when we condition upon a third variable, motivation; this conditional independence claim tells us that messageLength and studying are not directly causally related. Assuming there are no unmeasured common causes of messageLength, studying, and motivation, this conditional independence claim is explained by three possible causal structures: messageLength → motivation → studying; studying → motivation → messageLength; or messageLength ← motivation → studying. If we lack background knowledge, these three graphs are indistinguishable from observational data. However, if we assume student motivation to be inherent or at least temporally prior to their enrollment in a program and behavior in a course, then we can infer that motivation is a common cause of messageLength and studying. The second intuition has us consider two independent variables that share a common eﬀect. Suppose that a student’s level of motivation and non-educational, time-consuming obligations are independent. We expect that each of these student attributes share a common eﬀect in the amount of time a student devotes to study. Unconditionally the instructor cannot infer anything about a student’s motivation level from the knowledge that a student has many time-consuming obligations outside of the course in which they are enrolled; the instructor, however, can make inferences about a student’s motivation when the student (honestly) reports to the instructor how much they study. If we know that a student is studying a lot while juggling many obligations, we infer something about the student’s motivation level, namely that is high. We can similarly infer from a student’s report that they are highly motivated and yet are not studying as much as they would like that they are likely dealing with many outside obligations. In both cases, when we condition on a common eﬀect two otherwise independent variables now provide information about each other. In the graph of Fig. 2, this is represented as what is called a “collider,” where arrows from motivation and obligations meet at studying (motivation → studying ← obligations). Assuming (however unlikely) that we omit no common causes, there are no other graphical structures that explain this constraint on conditional independencies (or dependencies). That is, we can orient edges into a “collider” when such circumstances arise as we search over conditional independence relations in a larger dataset. Since we also assume that graphs are acyclic, having oriented colliders, we can often orient edges in such ways that avoid creating “colliders” where they were not discovered via tests for conditional independence. Thus, we can often orient many edges to make causal inferences from observational data alone. A constraint-based algorithm such as PC simply systematizes search over (sets of) variables in a data set to determine the conditional independence relations that hold among the variables and produces the set of graphical structures that imply those relations. 
 3 A Simple Causal Model of Outcomes in an Online Learning Environment 
 The work below is certainly not the ﬁrst to deploy methods for causal discovery in the online education domain. Scheines et al. ([18]), for example, focused on a set of variables relevant to students in an online causal and statistical reasoning course, including measures of: 
 – student background knowledge (pre: a measure of pretest abilities derived from GRE items) – behavior in online courseware (print: a measure of the extent to which students print out online course material, and volqs: a measure of the number of interactive, voluntary understanding questions attempted within the courseware), and – learning outcomes (quiz : an average of quiz scores over several course modules, and ﬁnal : ﬁnal exam grade). They then used several causal Bayes net learning algorithms to develop a path analytic model (Fig. 4) for these variables. They found interesting links between the background variable as well as their behavioral variables and learning outcomes. 
 Fig. 4. Linear path analytic model of student behavior and outcomes in an online causal and statistical reasoning course [18]; marginally signiﬁcant edges are dashed 
 Examining this model, student ﬁnal exam performance is well-predicted by the extent to which students “check their understanding,” and printing out reading material is negatively associated with these self-checks. Controlling for other possible mediators they still ﬁnd a negative eﬀect (though it is only marginally signiﬁcant) of greater printing behavior on ﬁnal exam score. They cautiously suggest an interpretation of this eﬀect as due to diﬀering study habits. Students who print out material are less likely to engage the voluntary, interactive questions during studying while students who did not print out material may be more likely to do so. Student behavior with respect to printing course material may also be indicative of other study habits, though those habits were unmeasured in this analysis. Thus, we ﬁnd a fruitful deployment of causal discovery methods to ﬁnd behavioral and background attributes of students in the online environment that are predictive of (and more cautiously, causally related to) learning outcomes. 
 4. The Complexity of Collected Data and Variable Construction.
 Having brieﬂy explored basic principles of causal discovery, we consider a second, more distinctive challenge to discovering causal models that arises from the complexity of the data collected from online courseware systems. Most online courseware collects an enormous amount of data about a multitude of diﬀerent phenomena, which leads to very high-dimensional datasets. Variables collected fall into three rough categories: 
 1. purely observed variables with (relatively) straightforward interpretations or meanings 2. measured indicators of underlying “latent” phenomena, and 3. “raw” variables that require some form of construction to be interpretable or meaningful. 
 The ﬁrst two categories are well-treated in the literature on causal discovery as well as multivariate analysis in general. Further, latent variable modeling is an active area of research in the social science methodology (e.g., psychometrics) community. We also brieﬂy discuss procedures for the discovery of latent variable models later in this work. There is, however, little literature dealing with the third category of variables in a principled way with respect to causal discovery. Natural and social scientists construct variables frequently, but the approach taken is usually either based on signiﬁcant, richly detailed background theory or ad-hoc guesswork. Consider, for example, weather forecasting, in which prognostications are made frequently in terms of “high” pressure and “low” pressure weather systems. While these systems cover large geographic regions, their features-strength, size, speed, etc.-are constructed from a multitude of directly observed barometric pressure readings spread out over large geographic regions. Meteorologists’ high and low pressure systems are instances of constructed variables, while individual, localized barometric pressure readings are the raw variables from which such constructions arise. In general, a host of situations call for principled, data-driven methods for constructing variables from underlying “raw” data. In many cases, we measure many variables but it is not clear just what the causal variables of interest should be. While latent variable modeling is one way to potentially reduce the dimensionality of data, in some situations it is more appropriate to seek dimensionality reduction methods whereby we construct new measured variables as deterministic functions of “raw” measured variables. This diﬀerence between latent variable modeling (i.e., group 2 above) and the construction of variables via deterministic functions of “raw” variables is partly illustrated in Fig. 5. The larger rectangle of Fig. 5 illustrates a discovery and estimation problem within the framework of latent variable models. When deploying a latent variable model, the modeler must ﬁrst decide (or discover) the appropriate causal structure relating latent variables to their manifest eﬀects and then estimate the parameters (“factor loadings”) quantifying the nature of these causal relationships. Conditional upon the latent variable, each of its noisy, measured (or manifest) indicators (X1, X2, and X3 ) is independent of the other measures. Whether this condition is appropriately tested or assumed, this assumption is usually called that of “local independence.” 
 Fig. 5. Illustrative example of the diﬀerence between latent variable modeling and deterministic variable construction. The larger rectangle envelops a discovery and estimation problem, the smaller rectangle a construction problem. 
 Contrast this estimation problem with the heuristic illustration of a variable construction problem in the smaller rectangle of Fig. 5. Here, we call X1, X2, and X3 our “raw” variables and deterministically construct a new variable called scale from these raw variables. Since we are using a latent variable model to motivate the illustration, there are no direct connections between X1, X2, and X3, but this need not be the case in general. Whether or not the “raw” variables are unconditionally independent, they remain or become dependent when conditioning upon the deterministically constructed new variable. Consider the situation in which we construct scale as the sum of only X1 and X2, and assume X1 and X2 to be unconditionally independent. Given information that scale takes on the value 10 (conditioning on scale) and that X2 takes on the value 7, then we know the value of X1 to be 3. Thus, conditioning on scale, X2 provides us information about X1, so the two components of scale are conditionally dependent. In situations in which latent variable models are deployed, scales like this are often constructed as well, and this is just one special case of the general problem at hand. Of course in general, not just any constructions will do. The problem that we face is to reduce a set of “raw” variables {R1 , ..., Rn } to some smaller set of variables {C1 , ..., Ck } via deterministic functions {f1 , ..., fk } of the “raw” variables in order to achieve some objective or goal. In the online education domain, we focus on predicting and identifying causes of learning outcomes as assessed by exam scores, course grades, or perhaps even another constructed variable5 incorporating several aspects of learning outcomes. This search problem is clearly intractable; the search space must be constrained by some combination of background knowledge and a guiding/focal objective function. Background knowledge may be rather general. For example, we might know that the relevant constructed variables will be linear combinations of the raw variables. Other forms of background knowledge may be domain- or application-speciﬁc, such as providing a speciﬁcation of which “raw” variables are relevant for particular constructed variables and which may be disregarded. Objective functions, similarly, may take on a multitude of forms. We might seek functions of raw variables that lead to the best prediction of a particular target variable. Alternatively, we might seek functions of raw variables that lead to greatest amount of causal knowledge with respect to some target variable. Any number of other objective functions may be appropriate in any number of situations, but note that the “best” constructed variables can change depending on the objective function. Once some sensible combination of background knowledge and an objective function have been speciﬁed, we will (hopefully) have a space of functions that is searchable. Thus, we can search for variable constructions for particular purposes. We later consider a speciﬁc example of message forum data from an online learning environment to ﬂesh out some possibilities for this program of research. 
 5 A Simple Pilot Study.
 One might plausibly wonder whether variable construction is actually required for successful prediction. We thus ﬁrst demonstrate that predictions about a set of students enrolled in an online, several month graduate economics course can be improved through the use of constructed variables. We note at the outset, however, that an unqualiﬁed causal interpretation of the resulting DAG is tricky at best, though further research to ﬁnd more plausible or appropriate “causal constructions” is ongoing. Nevertheless, a (graphical) representation of the probabilistic dependence structure for these variables can signiﬁcantly improve predictions. We focus on variables in three rough semantic categories provided in Table 1. Our categorization provides a rough time ordering. Background (including demographic) variables are those upon which we cannot in principle intervene but that might prove useful for predictive and/or classiﬁcation purposes. Behavioral variables measure aspects of students’ interaction with online courseware and are vital to the purpose of discovering the behavioral causes of student learning outcomes. 
 (5) In this work we principally focus on constructing predictors and causes of a given target variable, rather than constructing the target variable itself; the latter problem is brieﬂy discussed later. 
 Table 1. Description of measured and constructed variables included in pilot study 
 These are also the variables most likely to require construction to be meaningfully interpreted. Learning outcome variables are assessed at speciﬁc times within the course in our example. Two individual assignments are graded during the course while an individual ﬁnal exam is assessed at the end of the course. Final course grades are calculated including both individual assessments and assessments of a student’s work with a group of other students. Data from a sample of 815 students are provided to the PC algorithm6 along with time-ordering background knowledge. The algorithm returns a set of DAGs which imply the conditional independence relationships judged present in the data via statistical test. One DAG is chosen, and a path analytic model is estimated according to that structure. This model is provided as Fig. 6 (7). Both the structure of the model and estimated parameters characterize qualitative and quantitative relationships among the variables under consideration. As we are especially concerned with discovering the predictors and causes of learning outcomes, we focus on two particular learning outcome variables. The ﬁrst is student ﬁnal course grade (grade points). The model provides us with something of a “sanity” check of our method in this case. Among the variables directly connected to grade points are variables which constitute the basis by which the instructor assigns the ﬁnal grade, including both assignment scores and the ﬁnal exam score. Other inﬂuencers are GPA and both counts of messages, the instructor’s number of private messages to the student as well as the count of the student’s public and group messages. If we take GPA to be a proxy for general student ability in online courses of this sort (which we implicitly do by taking GPA to be a background variable as opposed to an outcome), then this seems like a sensible picture of the predictors of the ﬁnal course grade. However, the ﬁnal course grade may not be our best target for determining the causes of learning outcomes. 
 (6) Algorithms deployed in this work are all implemented in the freely distributed Tetrad IV suite of algorithms available at http://www.phil.cmu.edu/projects/tetrad. 
 (7) The model of Fig. 6 is judged to ﬁt the data by a relevant statistical test comparing the implied covariance matrix of this model with the sample covariance matrix [19]. 
 Fig. 6. Estimated linear path analytic model from our pilot study. Rectangles are placed around two learning outcomes on which we focus. 
 After all, the same instructor provides grades for assignments as well as the ﬁnal assessment via the course grade, and the ﬁnal course grade is really (in part) just a function of these components. Further, the ﬁnal course grade includes assessments of a student’s group work, so an independent, objective assessment of individual learning outcomes would be helpful. This we ﬁnd in our second learning outcome variable of interest, ﬁnal exam points. Students in this sample had diﬀerent instructors, but all took a ﬁnal exam, provided by a textbook manufacturer, that was independently graded. This provides us with a relatively clean, objective instrument to assess learning outcomes with respect to the material of this online economics course. However, the set of variables directly connected to ﬁnal exam points is relatively small. We ﬁnd that the unmediated predictors of a student’s ﬁnal exam score are sex, GPA, and average score on other MBA course ﬁnal exams. This may provide support for our use of the latter two variables as proxies for ability, but we ﬁnd no direct connections between this independent learning outcome assessment and behavioral variables we consider in this analysis. This, of course, does not mean that behavior and learning outcomes are unrelated. Perhaps we have just not constructed and included the appropriate sets of behavioral variables. We must explore the intriguing possibility that we failed to appropriately “carve up” our behavioral raw data. One crucial way that student behavior is captured in this environment is through messages that students post in an online forum. We need to carefully consider ways in which we can construct variables out of this data. Our ﬁrst pass included ad hoc constructions of studentPublicGroupMessageCount and instructorPrivateMessageCount; we did not ﬁnd signiﬁcant, unmediated links to ﬁnal exam score, though some interesting relationships between demographic features, message behavior, and other variables were discovered. Given the richness and importance of student and instructor interactions via these messages, we choose forum message data as the illustrative example of the problem of variable construction. (8) 
 6 The Construction of New Variables.
 Real, deployed online courseware collects data for every forum message posted by students in a given course. We focus here on only a handful of these characteristics to motivate the problem of variable construction search. For each message we know: 
 - message creator - message timestamp - message content - the forum in which the message was posted, and - whether the course facilitator judged the message as “substantive” or not. 
 These messages can be organized by student9 (as message creator, excluding messages posted by course instructors), and raw variables can be created that correspond to the message attributes. In a course with 50 students, the most proliﬁc of whom posted 100 messages, this scheme results in 400 “raw” variables. A schema of the resulting data set is shown in Table 2. The form of the data (binary, real valued, text, etc.), as well as background knowledge, informs the space of functions over which we might search. A plausible objective function is to optimize predictions for each student of measured learning outcomes, such as score on a ﬁnal exam or course grade. We seek useful, meaningful constructed variables to incorporate into causal and/or predictive models. Numerous potential constructed variables arise even out of our simple toy example. A simple example involves word counts from the content ﬁelds for each student. We let C denote the indices of content ﬁelds in our dataset (C = {2, 6, 10, 14, 18, ..., 398}). 
 
 (8) viewChapterCount is also an ad hoc construction from separate logs in the online courseware that could just as easily be the target of investigation for principled variable construction and search. 
 (9) The reader might sensibly inquire why we choose to organize message data by student (as opposed to, for example, organizing the data by message). This choice provides an organization in which most variables in the data are roughly independently and identically distributed (or i.i.d.). This is necessary for us to proceed with causal discovery techniques we deploy. The data would not be i.i.d. were it organized by message, having a variable representing the message creator. 
 Table 2. Hypothetical table for forum message “raw” variables organized by student. 
 FORMULA(1).
 where wordCount is a function that count words given a ﬁeld of text as an argument, and I is the indicator function (taking value 1 when the content ﬁeld is not empty, 0 otherwise). Letting F denote the set of indices for variables that identify the particular forum in which students posted (i.e., F = {3, 7, 11, ..., 399}), we can reproduce a variable from our pilot study: 
 FORMULA(2).
 Letting S denote the set of indices for variables containing the binary “substantive message” ﬂag, which takes value 1 when a message is substantive and 0 otherwise, (i.e., S = {4, 8, 12, ..., 400}), another simple example is: 
 FORMULA(3).
 We might consider many alternatives. We can deploy any number of functions just on the content and timing of messages. Guided by background knowledge and the form of the data, we iteratively search over potential variable constructions and judge them via resulting models in which they are used. We seek variable constructions and models incorporating them that maximize our ability to predict a student’s ﬁnal exam score and infer causes of learning outcomes. Given often lacking background knowledge tying together education domains with causal inference from data obtained from online courseware, as well as the dimensionality, granularity, and complexity of the latter, we are forced not only to search over potential causal structures that explain the data but also to search for the variables that take part in that modeling. Search for more, better variable constructions is currently ongoing and a topic for further research. 
 7 Future Directions for Causal Discovery from Online Education Data.
 Having delved into the problem of variable construction in considerable depth, we focus on three further problems for predictive and causal inference from online educational data. The resolution of these problems may aﬀect approaches to variable construction and search, but they are general problems for predictive and causal inference even when variables for analysis are given. Roughly these problems are: 1. the treatment of certain categorical variables for causal structure search 2. measurement of target variables intended to capture “learning outcomes”, and 3. inferring the presence of unmeasured (latent) common causes. We provide a brief overview of each of these problems and suggest some potential solutions. Getting a handle on approaches to these three problems in addition to the problem of variable construction will certainly advance the state-of-theart with respect to causal discovery in online education settings as well as for learning analytics in general. 
 7.1 The Treatment of Certain Categorical Variables for Causal Structure Search 
 A variety of phenomena in our pilot study are best represented by discrete variables; we include binary variables, for example, representing gender and Pell Grant eligibility. Other discrete and categorical phenomena cannot be ignored in attempts to discover causal structure. For example, learners may come from various demographic and ethnographic groups, and instructors may have various levels of preparation and credentials. To produce satisfying causal explanations of student learning, we must attempt to control for diﬀerences arising because of these categorical phenomena. If we treat the set of learners who, for example, had a particular instructor for a course as a separate sample for analysis, we consider a much smaller sample, leading to concerns for small eﬀect sizes and the power of statistical tests deployed for causal structure search. Further, discrete variables may pick out genuinely diﬀerent populations of learners having diﬀerent underlying causal structures. We focus on diﬀerences between instructors. Certain aspects of the educational experience of the student may be independent of the instructor (e.g., student messaging behavior may be unrelated to a particular instructor or course oﬀering); other aspects may be dependent on the instructor, and the underlying causal structure we attempt to model may be diﬀerent from instructor to instructor. In our pilot study, we included a measure of the instructor’s peer review score as a ﬁrst attempt to control for the instructor of each course, but this approach is sub-optimal as this variable takes on the same value for every student within a given class and is thus not independent and identically distributed (or i.i.d.) (10). 
 (10) This illustrates a general problem for deploying methods that rely on statistical tests for conditional independence when we have mixed data (including both categorical and continuous variables). A variety of techniques can be used to mitigate this problem. We might, for example, discretize continuous variables or treat binary or ordinal categorical data as if it were continuous (our strategy for treating binary variables and instructor avg pr in our pilot study). Diﬀerent situations will suggest diﬀerent approaches. 
 Further, conditional independence relations that obtain in distributions for individual instructors may not obtain in the distribution created by the combination of data sets for all instructors. Also, information common to distributions for every instructor can be destroyed even if instructors’ courses are well-represented by the same causal structure (cf. [20], [21]). We need some way of investigating what are potentially diﬀerent causal structures among instructors to provide causal insights into the dynamics of online courseware on a relatively large scale. An intuition about how to attack this problem comes from considering what we might call “invariant edges.” We begin by stratifying our full dataset into datasets for each individual instructor. We call “invariant edges” those that are discovered by graphical search procedures when applied to each instructor’s respective data set. We should expect, for example, that the edges oriented into grade points will remain invariant across instructors, as the ﬁnal course grade is a (noisy) function of various components included in the model. We know the “causes” of the ﬁnal course grade in this sense, but we might also discover other features of the underlying causal structure by ﬁnding other invariant edges. However, invariance over all data sets might be too strict a standard. Ramsey et al. [21] attack a similar methodological problem; they seek to discover what we might call “approximately invariant edges” from brain imaging (fMRI) studies in cognitive neuroscience. Seeking to discover qualitative causal relations amongst regions-of-interest (ROIs)11 in the brain, they provide a modiﬁcation of a causal structure learning algorithm that helps address this related problem. In fMRI data, from individual to individual, both the strength of causal relations among regions of the brain may diﬀer as well as the underlying causal structure itself may diﬀer. These possibilities coupled with various sources of measurement error lead to diﬀerent distributions (and data sets) for each individual of a study. The individuals of fMRI studies correspond to instructors in our online education setting; we face similar issues of diﬀering strength of causal relations and potentially diﬀering causal structure from instructor-to-instructor. Just as everyone has unique brain physiology, every classroom is unique. Ramsey et al. [21] propose IMaGES (Independent Multiple-sample Greedy Equivalence Search) as a modiﬁed version of Meek’s GES ([16], [17]) algorithm. This algorithm searches over equivalence classes of graphs, represented by a particular type of graphical structure called a pattern. Beginning with an empty graph, GES proceeds in two waves. In the ﬁrst, single edges are added until a Bayesian score assigned to each graph can no longer be improved. In the second wave, individual edges are deleted until the score can no longer be improved. Brieﬂy, IMaGES follows the same score-based procedure as GES, but at each stage the graph under consideration is scored based on its average score over each individual data set. This allows us to discover better models of the data we have despite the possibility of instructor-to-instructor variation in causal strength and causal structure. 
 (11) The construction of ROIs is itself another instance of the problem of variable construction that must be dealt with to reliably discover causal relations. 
 However, no work has deployed this technique in cases in which we also search for variable constructions or in applications outside of cognitive neuroscience. 
 7.2 Measurement of Target Variables for “Learning Outcomes”.
 In some cases, we are faced with a relatively simple (and relatively “noise-free”) target variable, for example, student retention rate or a variable representing whether (or when) a student dropped out of a particular course. At other times, our interest is establishing the predictors and causes of learning. To survey the broad scope of the problem of measuring student learning outcomes goes well beyond the scope of this paper. Sometimes we may have a well-devised assessment of learning gain in a particular course, perhaps from pre-test and post-test instruments. We then would seek out the predictors and causes of learning gain as advocated above. Frequently, however, we seek to measure “amount of learning,” treating it as a hidden construct with several possible indicators or measures. The design of instruments to measure such latent phenomena falls within the realm of the ﬁeld of psychometrics. Successful instruments to assess constructs like “amount of learning” must not only have established validity, so that they measure the intended phenomenon, but must also be reliable. Reliability is cashed out in social science research in a variety of ways. Dealing with diﬀerent instructors, who may grade by diﬀerent standards, leads us to seek an instrument with which we can achieve high “inter-rater” reliability. “Test-retest” reliability is another standard sought for learning assessment instruments. Further, we might be concerned that multiple items that make up an instrument are internally consistent, as assessed commonly by a measure like Cronbach’s alpha [22]. Some work has been done (e.g., [23]) connecting such traditional notions of reliability in the social sciences with reliable causal discovery, but further investigation is necessary. While the ﬁnal exam grade we consider in our pilot study has convenient properties for causal and statistical analysis as it is standardized for this course, designed by the course textbook manufacturer, and graded independently of the instructor, it is not ideal. We should seek out better, more robust indicators to assess learning outcomes, which are likely to vary from course-to-course and from program-to-program. Data-driven methods, augmented by expert domain knowledge, can inform the speciﬁcation of an outcome variable. We provide one potential methodological approach to this problem after we consider a third challenge we face. 
 7.3 Inferring the Presence of Unmeasured Common Causes.
 As is realistic in most social science applications, including those in the education domain, we must relax an assumption posited in our pilot study: namely, that our model includes all common causes of any variables therein. This assumption is called “causal suﬃciency” in causal discovery parlance (cf. [7]). If we relax this assumption and allow that there may be unmeasured or “latent” confounders of the variables we consider, we can deploy causal discovery algorithms that detect where such confounding may be present. The FCI algorithm [7] is intended to do just this, providing graphical output that indicates if variables potentially share an unmeasured common cause. The output of FCI for the data from our pilot study is provided as Fig. 7. Edges that appear as X ↔ Y indicate the presence of an unmeasured common cause of X and Y; edges that appear as X o→ Y indicate that either X is a cause of Y or they share an unmeasured common cause; edges that appear as X o-o Y indicate that either: (1) X causes Y, (2) Y causes X, (3) X and Y share a common cause, or (4) a combination of (1) and (3) or (2) and (3). 
 Fig. 7. Result of FCI search over data from our pilot study.
 The results align with intuitions in several ways, suggesting that measures of message counts share a common cause as well as suggesting that both GPA and average MBA exam scores share a common cause, perhaps something like background knowledge or ability. Other possible interpretations remain. The inference that we have possibly omitted common causes of variables provides great insight such that we might iteratively construct new models and measure new phenomena. Returning to the problem discussed in the previous section, we can deploy FCI search over a set of ﬁxed variables to inspect whether the result suggests that intended learning outcome measures share an unmeasured common cause; such a result provides evidence that latent variable model may be appropriate. We might construct a scale to serve as a proxy for such a latent variable reiﬁed as a learning outcome. The Build Pure Clusters algorithm (BPC [24]) might be deployed for a similar purpose to determine the suitable components of such a proxy constructed variable. Under suitable conditions (namely that indicators are linearly related to the underlying latent), the BPC algorithm discovers sets of variables that are measured indicators of only one underlying phenomenon and that satisfy other important assumptions for causal discovery. This returns to the problem of discovery and estimation illustrated in the large rectangle of Fig. 5. Such an algorithm can be fruitfully deployed, for example, to provide background knowledge to variable construction search procedures in cases in which we seek to construct a scale as a proxy for a latent phenomenon. 
 8 Conclusion.
 We have outlined several problems for causal discovery from observational, online education data, focusing especially on the complexity of data collected from online courseware. A pilot study suggests that ad hoc variable constructions are less successful than we would like for prediction and inferring causes of learning outcome variables. This contributes to the need for principled means of search over potential variable constructions from such complex data sets. Further, other important problems for causal discovery from data in this domain are illuminated, and we introduce techniques that might be deployed to (partially) solve these problems. Open avenues for research are plentiful. Setting up the appropriate space of search for variable constructions will require the input of domain experts and the implementation of appropriate search procedures. Data for other courses and from other academic programs will have to be considered. New “raw” variables can be included from the large amount of historical data available for the course used in our pilot study. Finally, the speciﬁcation of “learning outcomes” is a problem faced throughout education. Working from expert domain knowledge, we might seek ways in which data-driven methods can fruitfully address this problem, providing speciﬁcations useful for causal discovery and for the learning analytics and educational research community at large. Acknowledgments. The author wishes to thank David Danks, Satish Menon, Partha Saha, and Richard Scheines for helpful discussions and comments on early drafts of this work. Further, the author thanks Apollo Group, Inc. for a summer internship to conduct the “Simple Pilot Study,” and Partha Saha for extensive support and advice on data access, understanding, and manipulation during that time.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Variable Construction for Predictive and Causal Modeling of Online Education Data</rdfs:label>
		<dc:subject>online education</dc:subject>
		<dc:subject>causal discovery</dc:subject>
		<dc:subject>graphical models</dc:subject>
		<dc:subject>bayesian networks</dc:subject>
		<dc:subject>dimensionality reduction</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/stephen-e-fancsali"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/stephen-e-fancsali"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/45/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/stephen-e-fancsali"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Dataset-driven Research for Improving Recommender Systems for Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46/authorlist"/>
		<swrc:abstract>In the world of recommender systems, it is a common practice to use public available datasets from diﬀerent application environments (e.g. MovieLens, Book-Crossing, or EachMovie) in order to evaluate recommendation algorithms. These datasets are used as benchmarks to develop new recommendation algorithms and to compare them to other algorithms in given settings. In this paper, we explore datasets that capture learner interactions with tools and resources. We use the datasets to evaluate and compare the performance of diﬀerent recommendation algorithms for Technology Enhanced Learning (TEL). We present an experimental comparison of the accuracy of several collaborative ﬁltering algorithms applied to these TEL datasets and elaborate on implicit relevance data, such as downloads and tags, that can be used to augment explicit relevance evidence in order to improve the performance of recommendation algorithms.</swrc:abstract>
		<led:body><![CDATA[ 1. First, we present an analysis of datasets that have been (or will soon be) made publicly available and that capture learner interactions with tools and resources in TEL settings. These datasets can be used for a wide variety of research on learning analytics. 2. Second, the paper presents an experimental comparison of the accuracy of several collaborative ﬁltering algorithms applied to TEL datasets. 3. Third, we research the extent to which implicit feedback of learners, such as reading information, downloads and tags, can be used to augment explicit relevance evidence in order to improve the performance of recommender systems for TEL. The paper is organized as follows: Section 2 presents an analysis of datasets that capture learner interactions and that can be used for learning analytics. Section 3 presents an overview of existing recommendation algorithms, and in particular collaborative ﬁltering algorithms, that can be applied to these datasets to suggest relevant resources to learners or teachers. Section 4 presents an overview of evaluation metrics that are commonly used to evaluate recommendation algorithms. Then, we present our evaluation results of the application of these algorithms to TEL datasets. We evaluate algorithms based on both explicit rating data and implicit relevance data such as tags and downloads that are available in some datasets. Results and opportunities for future research in this area are discussed in Section 6. Conclusions are drawn in Section 7. 
 (6) http://adenu.ia.uned.es/workshops/recsystel2010/datatel.htm.
 2 DataTEL Challenge.
 In this section, we present the objectives and results of the ﬁrst dataTEL challenge that was targeted to collect TEL datasets. These datasets capture user interactions with tools and resources in learning settings and can be used for various purposes in the learning analytics research area. In this paper, we focus on the application of these datasets to validate recommendation algorithms and to tackle challenges to support recommendation for learning. 
 2.1 Objectives.
 In the world of recommender systems, it is a common practice to use public available datasets from diﬀerent application environments (e.g. MovieLens, Book-Crossing, or EachMovie) in order to evaluate recommendation algorithms. These datasets are used as benchmarks to develop new recommendation algorithms and to compare them to other algorithms in given settings [7]. In such datasets, a representation of implicit or explicit feedback from the users regarding the candidate items is stored, in order to allow the recommender system to produce a recommendation. This feedback can be in several forms. For example, in the case of collaborative ﬁltering systems, it can be ratings or votes (i.e. if an item has been viewed or bookmarked). In the case of content-based recommenders, it can be product reviews or simple tags (keywords) that users provide for items. Additional information is also required, such as a unique way to identify who provides this feedback (user ID) and upon which item (item ID). The user-rating matrix used in collaborative ﬁltering is a well-known example. Although recommender systems are increasingly applied in TEL, it is still an application area that lacks such publicly available and interoperable datasets. Although there is a lot of research conducted on recommender systems in TEL, they lack datasets that would allow the experimental evaluation of the performance of diﬀerent recommendation algorithms using comparable, interoperable, and reusable datasets. This leads to awkward experimentation and testing such as using datasets from movies in order to evaluate educational recommendation algorithms. This practice seems to lack the necessary validity for proving recommendation algorithms for TEL [18]. To this end, the dataTEL Theme Team of the STELLAR Network of Excellence7 launched the ﬁrst dataTEL Challenge that invited research groups to submit existing datasets from TEL applications that can be used as input for TEL recommender systems. A special dataTEL Cafe event took place during the RecSysTEL 2010 workshop in Barcelona to discuss the submitted datasets and to facilitate dataset sharing in the TEL community. 
 (7) http://www.teleurope.eu/pg/groups/9405/datatel/.
 2.2 Collected Datasets.
 Seven datasets have been collected as a result of the ﬁrst dataTEL challenge. In this paper, we use datasets that include usage related data (such as ratings, tags, reads or downloads) as a basis to demonstrate and evaluate social recommendation for learning. We present an overview of datasets that include such usage data, including information on the data elements that are available and basic statistics of the number of resources, users and activities that are stored. Some of these datasets are already publicly available, whereas others are still under preparation and not yet publicly accessible. An up-to-date overview of datasets is available at http://www.teleurope.eu/pg/pages/view/50630/. We expect an increasing amount of learning related datasets in the upcoming year. Mendeley dataset. The ﬁrst dataset was submitted by Mendeley [13] and includes usage data of papers that are available through the Mendeley scientiﬁc portal8 . Mendeley is a research platform that helps users to organize research papers and collaborate with colleagues. In the context of learning, such a dataset provides useful data for recommender systems that are targeted to recommend papers to learners or teachers or to suggest suitable peer learners on the basis of common research or learning interests. Examples of paper recommenders that have been evaluated in TEL settings are InLinx (Intelligent Links) [2], Papyres [26] and pioneering work on the application of recommender systems in TEL conducted by Tang and McCalla [31]. Although research on paper recommenders has been elaborated more extensively in the Research2.0 domain that emerged in recent years, the dataset is currently one of the few available datasets that captures a very large set of user activities. This dataset can be used meaningfully for research on TEL recommender systems in contexts where papers are considered as learning resources. Five ﬁles are included in the Mendeley dataset that capture data since 2009: – Online catalog. The online catalog ﬁle contains metadata for 1.857.912 articles. Articles have a title, year, number of readers and abstract. – Online article view log. The online article view set include a random sampling of 200.000 users that are extracted from usage logs. Time at which each view occurred is provided. – Library readership. The library readership set includes 41.220 user libraries that contain more than 20 articles. From the 13.313.548 library entries, 2.655.578 (19.95%) have been read by users. – Library stars. The library stars set provides data on articles that have been starred by users. 186.976 (1.40%) of the 13.313.548 library entries have been starred by users. – Article tags. This collection contains 254.681 tags that were applied to 27.652 articles by 4.099 users. 
 (8) http://www.mendeley.com/.
 Among others, this dataset is useful for research on (1) extraction of users interests, on the basis of articles that have been tagged, starred, read or added to libraries by users, and evolutions in these interests on the basis of time recordings, (2) identiﬁcation of users who share common interests, on the basis of their usage behavior, and (3) identiﬁcation of implicit quality/relevance indications of individual articles by analyzing their usage data. APOSDLE-DS dataset. The APOSDLE-DS dataset [1] originates from the APOSDLE9 project, which ran from March 2006 to February 2010. APOSDLE is an adaptive work-integrated learning system that aims to support learning within everyday work tasks. It recommends resources (documents, videos, links) and colleagues who can help a user with a task. The dataset captures 1500 user activities of 6 users during an evaluation period of 3 months. The activities captured are perform task, view resource, edit annotation, perform topic, selected learning goal, adapting experience level, adding resource to collection, being contacted, contacting person, browse data and creating new learning path. The dataset also includes 163 descriptions of documents and document fragments on which these activities were performed. From the collected data, the adding resource to collection action can provide direct information about the relevance of a resource. This action occurred 581 times within the evaluation period. Creating a new learning path is considered as an attempt to plan learning activities over a longer time period and can provide a solid basis for research on the recommendation of sequences of resources. Unfortunately, this action occurred only a few times (< 25). Also direct collaboration activities are rare: being contacted occurred 11 times and contacting person 69 times. Implicit data to cluster users who share similar interests or goals are available more extensively (149 perform task, 861 perform topic and 414 select learning goal activities). Whereas the current collection contains data of only a few users and may be too small for statistical analysis, the dataset provides a good example of relevant learning activities to be captured in learning settings. ReMashed dataset. The ReMashed dataset [10] was collected within the ReMashed environment10 that focuses on community knowledge sharing. The main objective of ReMashed is to oﬀer personalized recommendations from the emerging information space of a community. The ReMashed dataset is based on aggregating contributions of the users in the ReMashed portal [9]. 
 (9) http://www.aposdle.tugraz.at/ (10) http://remashed.ou.nl.
 This portal aggregates Web 2.0 contributions from a range of remote services (delicious, Youtube, Flickr, Slideshare, blogs, and twitter) of the users. The data collection started in February 2009 and is still ongoing. It includes information about interests (learning goals), bookmarks, tags, ratings and contents. Until now, 140 users are registered. In total, 23.000 tags and 264 ratings are given to 96.000 items. The ReMashed dataset includes only publicly available contributions from users. Although, the data is publicly available, the dataset is not prepared yet for public access as it requires anonymization and the commitment of the users. Organic.Edunet dataset. The Organic.Edunet dataset [21] was collected on the Organic.Edunet Web portal11 , a learning portal for organic agriculture educators that provides access to more than 10.500 learning resources from a federation of 11 institutional repositories. The portal mostly focuses on serving school teachers and university tutors and has attracted almost 12.000 unique visitors from more than 120 countries, out of which about 1.000 are registered users. This dataset contains data from the initial operational phase of the portal that took place in the context of the EC-funded Organic.Edunet project12 . The dataset was collected from January 2010 until September 2010 and includes information about 345 tags, 250 ratings and 325 textual reviews that these users have provided. The particularity of this dataset is the fact that ratings are collected upon three diﬀerent dimensions/criteria: the usefulness of a resource as a learning tool, the relevance to the organic thematic, and the quality of its metadata. This allows for the deployment of an elaborate multi-criteria recommendation service within the portal. MACE dataset. The MACE dataset [36] originates from the MACE13 project, which ran from September 2006 to September 2009. The MACE portal14 provides advanced graphical metadata-based access to learning resources in architecture that are stored in diﬀerent repositories all over Europe. Therefore, MACE enables architecture students to search through and ﬁnd learning resources that are appropriate for their context. From 2007 until now, 1.148 users registered at the portal. The portal oﬀers access to about 150.000 learning resources, from which 12.000 have been accessed by registered users. These objects hold together about 47.000 tags, 12.000 classiﬁcation terms and 19.000 competency values. Tags were assigned by logged in users and the classiﬁcation and competency terms by domain experts. Most user actions with the MACE portal were logged, including search activities, using facetted search, social tags, geographical locations, classiﬁcations and/or competencies, access of learning resources, download of resources, social tagging, including add tag, add comment and add rating, and access of user pages. 
 (11) http://www.organic-edunet.eu (12) http://project.organic-edunet.eu (13) http://www.mace-project.eu/ (14) http://portal.mace-project.eu/ 
 The time of each user activity is recorded. The dataset provides useful and rich data for various research purposes. In addition to explicit rating feedback, access time, downloads, tags and comments can provide useful implicit indications that can be used to gain knowledge about user interests. The availability of a relatively large set of both explicit and implicit relevance data makes this dataset a potentially useful candidate for recommender research. Travel well dataset. The Travel well dataset [35] was collected on the Learning Resource Exchange portal15 that makes open educational resources available from 20 content providers in Europe and elsewhere. Most registered users are primary and secondary teachers who come from a variety of European countries. The dataset contains data from the pilot phase which was conducted during the EC-funded MELT-project16 . These data were collected from August 2008 until February 2009 on 98 users. The dataset includes explicit interest indicators that can be used to infer the relevance of a resource for the user. Users can rate resources on a scale of 1 to 5 for usefulness and add tags to resources. In total, 16.353 user activities were recorded on 1.923 resources. The particularity of the dataset is that it contains information of the home country, mother tongue and spoken languages of users. Additionally, it has metadata on the origin of the educational resource and its language. The dataset thus allows tracking the interests of users on travel well resources, indicating that the user and resource come from diﬀerent countries and that the language of the resource is diﬀerent from that of the users mother tongue [34]. Additionally, this dataset is useful for research on extraction of teacher interests and identication of teachers who share common interests, on the basis of their tags and ratings. The availability of a relatively large set of such explicit relevance indicators makes this dataset a potentially useful candidate for recommender research in TEL. 
 2.3 Summary.
 Table 1 summarizes the details of the collected datasets, including information on the number of users, items and activities that are captured and details on the data elements that are provided. The MACE, Organic.Edunet and Mendeley datasets are the largest datasets that collected user data of 1.148, 1.000 and 200.000 users. The Travel well and ReMashed datasets contain ratings and tags of 98 and 140 users, respectively. The current sample of APOSDLE captures data of relatively few users. Of interest in this discussion are the data elements that are provided by the datasets. Explicit relevance feedback, such as ratings by users, are provided in the MACE, ReMashed, Organic.Edunet and Travel well datasets. These datasets provide ratings on a ﬁve point likert scale and are interesting datasets for evaluating recommender algorithms. Mendeley provides information on articles that are starred by a user (1 if the article has been starred and 0 otherwise), but the semantics of such stars in user libraries may be diﬀerent for diﬀerent users (i.e. a star can indicate relevance feedback, but may as well indicate that the user wants to read the article at a later stage). Therefore, the application of such data for recommendation is less straightforward. 
 (15) http://lreforschools.eun.org (16) http://info.meltproject.eu/.
 Table 1. Overview datasets. 
 In addition to ratings/stars, most datasets include additional user interactions, such as tags, downloads or the inclusion of a resource in a user library. In Section 5.2, we research the extent to which such activities can be used to improve the performance of recommendation algorithms. The APOSDLE dataset includes a wide variety of additional learner related activities, including tasks that are performed by a user, her learning goals and learning paths that she constructed. Whereas the dataset may be too sparse to draw conclusions at this point, the capturing of such activities has a big potential for building recommender systems for learning. The application of this dataset for recommendation for learning is further discussed in Section 6. At the time of writing, the Mendeley, MACE, APOSDLE and Travel well datasets are already publicly available. The Organic.Edunet and ReMashed datasets will be made publicly available soon, after clearing the remaining privacy issues. In the remainder of this paper, we report on experimental results with the datasets that are currently available. 
 3 Recommender Systems.
 Recommender systems apply data analysis techniques to help users ﬁnd items that are likely of relevance. Recommender algorithms are often categorized into three areas: collaborative ﬁltering, content-based ﬁltering and hybrid ﬁltering. Collaborative ﬁltering is the most widely implemented and most mature technology [4]. Collaborative recommender systems recognize commonalities between users on the basis of their ratings or implicit relevance indications and generate new recommendations based on inter-user comparisons. Content-based ﬁltering matches content resources to user characteristics [29]. These algorithms base their predictions on individual information and ignore contributions from other users. Hybrid recommender systems combine two or more recommendation techniques to gain better performance with fewer drawbacks [4]. In this paper, we evaluate the performance of collaborative ﬁltering (CF) on TEL datasets. Similar experiments on TEL settings have been reviewed in Manouselis et al. [20]. The basic idea of CF-based algorithms is to provide recommendations based on the opinions of other like-minded users. The opinions of users can be obtained explicitly from the users or by using implicit measures. Two approaches are distinguished for recommending relevant items to a user: – User-based collaborative ﬁltering computes similarities between users to ﬁnd the most similar users and predicts a rating based on how similar users rated the item. In a ﬁrst step, a user-based collaborative ﬁltering algorithm searches users who share similar rating patterns with the active user. In a second step, ratings from these similar users are used to calculate a prediction for the active user. – Item-based collaborative ﬁltering applies the same idea, but uses similarity between items instead of users. The approach was popularized by Amazon.com - i.e. users who bought x also bought y. In a ﬁrst step, an item-item matrix is built that determines relationships between pairs of items. In a second step, this matrix and the data on the active user are used to make a prediction. Once the most similar items are found, the prediction is then, for instance, computed by taking a weighted average of the target user ratings on similar items. To enable empirical comparison of diﬀerent approaches, we implemented different metrics to compute similarities between users and between items and diﬀerent algorithms for computing predictions, including the standard weighted sum algorithm and simpliﬁed Slope One scheme [17]. The diﬀerent approaches are presented brieﬂy in this section. A more thorough review of various design options that can be considered for collaborative ﬁltering algorithms can be found in [18]. We report on experimental results in Section 5. 
 3.1 User-based Collaborative Filtering.
 User-based collaborative ﬁltering assigns weights to users based on similarities of their ratings with that of the target user [6]. For calculating the similarity between a target user u and another user v, diﬀerent similarity metrics can be used. We ﬁrst brieﬂy present commonly used metrics. Then, we present the standard weighted sum algorithm for generating predictions based on these similarity computations. 
 Cosine similarity. In this case, two users are thought of as two vectors in the m-dimensional item-space. First, the set of items (Iuv ) that both user u and user v have rated is selected. Then, similarity weights are calculated using the following formula 
 FORMULA_(1).
 where rui is the rating of user u on item i and rvi is the rating of user v on item i. Basically, the cosine similarity between user u and user v is the angle between the ratings vector of user u and the ratings vector of user v. 
 Pearson correlation. In this case, similarity between two users u and v is measured by computing the pearson correlation between them using the following formula 
 FORMULA_(2).
 where rv and ru denote the average ratings for users u and v, respectively. In essence, this similarity measure takes into account how much the ratings of other users for an item deviate from their average rating value. 
 Tanimoto-Jaccard. The Jaccard or Tanimoto Coeﬃcient [32] measures the overlap degree between two sets by dividing the numbers of items observed by both users (intersection) and the number of diﬀerent items from both sets of rated items (union). The similarity between two users u and v is deﬁned as: 
 FORMULA_(3).
 where |Iu | and |Iv | represent the number of items that have been rated by user u and user v, respectively. This similarity metric considers only the number of items that have been rated in common and ignores rating values. The metric can be applied on binary datasets that do not contain rating values. In addition, studies have shown that the metric is advantageous in the case of extremely asymmetric distributed or sparse datasets [24]. 
 Prediction Computation. After computing similarity weights, top-K users with maximum weights are selected as experts. Suppose u is a test user and i is a corresponding test item. Let τu be the set of experts who have rated i. The predicted rating rui is computed as: 
 FORMULA_(4).
 Basically, the approach tries to capture how similar users rate the item in comparison to their average ratings. If τu is empty, i.e. no expert has rated the test item i, then the average rating of the user is outputted as the prediction. 
 3.2 Item-based Collaborative Filtering.
 Item-based collaborative ﬁltering applies the same idea, but uses similarity between items instead of users. Once similar items are found, predictions are computed by taking a weighted average of the target user ratings on these similar items. We brieﬂy describe the similarity computation and the prediction generation. The description is based on [30]. Item similarity computation. The computation of similarities between items proceeds in a similar way than computing similarities between users in userbased CF. The basic idea in similarity computation between two items i and j is to ﬁrst isolate the users who have rated both items and then to apply a similarity computation technique to determine the similarity wij . We illustrate the approach using the cosine similarity metric. Alternative similarity measures such as pearson correlation and tanimoto or jaccard coeﬃcients (see previous section) are also commonly applied to calculate similarity between items. To compute the cosine similarity, we ﬁrst isolate the co-rated cases (i.e., cases where the users rated both i and j). Let the set of users who both rated i and j be denoted by U, then the cosine similarity is given by 
 FORMULA_(5).
 where rui is the rating of user u on item i and ruj is the rating of user u on item j. Thus, this formulation views two items and their ratings as vectors, and deﬁnes the similarity between them as the angle between these vectors. Prediction computation. In the case of item-based predictions, a weighted sum technique computes the prediction of an item i for a user u by computing the sum of the ratings given by the user on items similar to i. Each rating is weighted by the corresponding similarity wij between items i and j. Formally, we can denote the prediction of item i for user u as 
 FORMULA_(6).
 Basically, this approach tries to capture how the active user rates the similar items. The weighted sum is scaled by the sum of the similarity weights to make sure the prediction is within the predeﬁned range. Slope One scheme. The Slope One scheme [17] is an alternative scheme to compute item-based CF predictions that simpliﬁes the implementation of standard item-based collaborative ﬁltering algorithms. The scheme is based on a simple ”popularity diﬀerential”. Let the set of users who both rated i and j be denoted by U. Given a training set c, and any two items j and i with ratings ruj and rui respectively by some user u in U, then the average deviation of item i with respect to item j is considered as: 
 FORMULA_(7).
 The slope one scheme then simpliﬁes the prediction formula to.
 FORMULA_(8).
 Details are presented in [17]. The advantage is that this implementation of Slope One does not depend on how the user rated individual items, but only on the user average rating and on which items the user has rated. Experimental results are presented in Section 5. 
 4 Evaluation Metrics.
 In this paper, we focus on the measurement of accuracy and coverage of recommendation algorithms, which can be measured by oﬄine analysis of data: – Accuracy measures how well the system generates a list of recommendations. Measures typically used are precision, recall and F1. Precision indicates how many recommendations were useful to the user, whereas recall measures how many desired items appeared among the recommendations. F1 is the harmonic mean of precision and recall - that is, (2 ∗ precision ∗ recall)/(precision + recall). – Predictive accuracy evaluates the accuracy of a system by comparing the numerical recommendation scores against the actual user ratings for the user-item pairs in the test dataset. Mean Absolute Error (MAE) between ratings and predictions is a widely used metric. MAE is a measure of the deviation of recommendations from their true user-speciﬁed values. The MAE is computed by ﬁrst summing absolute errors of the N corresponding ratings-prediction pairs and then computing the average. The lower the MAE, the more accurately the recommendation engine predicts user ratings. Root Mean Squared Error (RMSE) and Correlation are also used as statistical accuracy metric. – Coverage is a measure of the percentage of items and users for which a recommendation system can provide predictions. A prediction is impossible to be computed in case that no or very few people rated an item or in case that the active user has zero correlations with other users. A more comprehensive review of evaluation metrics for collaborative ﬁltering algorithms can be found in Herlocker et al. [12]. 
 5 Experimental Results.
 In this section, we present our experimental results of applying collaborative ﬁltering techniques to TEL datasets. We used the Apache Mahout17 framework for comparing the performance of diﬀerent collaborative ﬁltering algorithms on datasets. Apache Mahout is an open source framework that provides implementations of standard item-based and user-based collaborative ﬁltering algorithms and implementations of diﬀerent metrics to compute similarities between users and between items, including pearson, cosine and tanimoto measures. First, we present results of collaborative ﬁltering algorithms and the inﬂuence of diﬀerent similarity metrics on datasets that contain ratings, including the MACE and Travel well datasets. We also compare these results with accuracy results of algorithms on the MovieLens dataset [6], that is often used by the recommender system community to evaluate algorithms. Then, we present results of collaborative ﬁltering algorithms applied to binary data without ratings, such as data of Mendeley. In this set of experiments, we used implicit relevance indications such as tags and downloads as a basis to generate recommendations. 
 5.1 Collaborative ﬁltering based on ratings.
 In a ﬁrst set of experiments, we applied collaborative ﬁltering algorithms to datasets that contain rating data. First, we compare the inﬂuence of diﬀerent similarity metrics on collaborative ﬁltering. For this ﬁrst set of experiments, we selected all users from the MACE and the Travel well collection who provided at least 5 ratings. User ratings were randomly split into two sets - observed items (80%) and held-out items (20%). Ratings for the held-out items were to be predicted. We used the Mean Absolute Error (MAE) as the evaluation metric for predictive accuracy in this experiment. Results are presented in Figure 1. These results indicate that item-based CF based on tanimoto similarity outperforms item-based CF based on pearson and cosine similarity measures for both the MACE and Travel well datasets. 
 (17) http://mahout.apache.org/.
 In contrast, the use of cosine and pearson measures on the MovieLens dataset improves predictive accuracy of item-based collaborative ﬁltering. These results are consistent with previous experiments that demonstrate that the use of the tanimoto similarity measure on datasets that are very sparse, such as the MACE and Travel well datasets, is beneﬁcial [24]. 
 Fig. 1. MAE of item-based collaborative ﬁltering based on diﬀerent similarity metrics. 
 In a second experiment, we compared results of item-based, user-based and slope-one collaborative ﬁltering schemes. For each dataset, we used the best performing similarity measure. Results are presented in Figure 2 and indicate that also the best choice of algorithm is dataset dependent. In the case of MACE, standard item-based collaborative ﬁltering outperforms user-based and slope-one collaborative ﬁltering. For Travel well data, user-based collaborative ﬁltering outperforms the other schemes. The simpliﬁed Slope One scheme gives the most accurate results for the MovieLens dataset - which is consistent with ﬁndings reported in [16]. Whereas predictive accuracy results of the best performing algorithms on MACE and Travel well data are comparable to reported results of collaborative ﬁltering schemes applied to the MovieLens dataset, the major bottleneck of applying these collaborative ﬁltering schemes to the collected TEL data is the limited coverage of the approach. In MACE, only 113 of 1.148 users provided explicit relevance feedback in the form of ratings. In addition, only 1.706 of 12.000 accessed resources were rated. In the Travel well dataset, more users have provided ratings (56 out of 98), but the number of resources that have been rated by multiple users is very small. In order to address these sparsity issues, we elaborate on the use of implicit relevance indicators and the use of binary data for collaborative ﬁltering in the next section. 
 5.2 Collaborative ﬁltering on implicit relevance data. 
 Fig. 2. MAE of user-based, item-based and slope-one collaborative filtering. 
 Implicit feedback techniques appear to be attractive candidates to improve recommender performance in the TEL domain, where explicit feedback ratings are often sparse. Behaviors most extensively investigated as sources for implicit feedback in other areas have been reading, saving and printing [14]. Morita and Shinoda [25] show that there is a strong tendency for users to spend a greater length of time reading those articles rated as interesting, as opposed to those rated as not interesting. This ﬁnding has been replicated by others in similar environments [15]. Other behaviors that have been explored include printing, saving, tagging and bookmarking [28]. We explore the use of implicit relevance data in the Travel well, MACE and Mendeley datasets. In addition to explicit rating data, the Travel well dataset includes 11.943 tags that are provided by 76 users on 1.791 resources. In the MACE dataset, 48.004 tags are provided by 283 users on 6.673 resources. In addition, MACE includes: (1) information about the access of resources (resultViewed event), including the date and time when the user viewed the resource, (2) search terms that were used by the user, (3) information about downloaded resources (save event) and (4) comments that were added by the user (addComment event). The Mendeley dataset provides data about library readership, library stars and article tags. In a second set of experiments, we used these data as implicit relevance indications. In this set of experiments, we predict a ﬁxed number of top-N recommendations and not the ratings. In this case, implicit relevance data are used to rank items to the user in order of decreasing relevance. Suitable evaluation metrics are Precision, Recall and F1. Similar to Sarwar et al. [30], our evaluations consider any item in the recommendation set that matches any item in the test set as a hit. The number of top-N items to be predicted was set to 10. The tanimoto similarity measure was used to compute similarities between users. Performance results of user-based collaborative ﬁltering on the F1 measure are presented in Figure 3. As can be seen in this ﬁgure, the size of the neighborhood aﬀects the quality of the top-10 recommendations. In general, the quality increases as we increase the number of neighbors. However, after a certain point, the improvement gains diminish. Results indicate that implicit relevance indications can be used in a successful way. 
 Fig. 3. F1 of user-based collaborative ﬁltering with increasing number of neighbors. 
 For Mendeley, we used library readership and starred articles as implicit relevance indications. Based on these data, a standard user-based collaborative ﬁltering algorithm that predicts the top 10 most relevant items for a user has an F1 score of almost 30% - which is comparable to the application of user-based collaborative ﬁltering on the MovieLens dataset (±25%). Reasonable results were also obtained for the Travel well dataset. Similar to the low accuracy results of user-based collaborative ﬁltering on MACE data that were presented in the previous section, accuracy results remain low (< 5%) when additional data about tags and downloads is incorporated. These results are consistent with previous studies of user-based collaborative ﬁltering on extremely sparse datasets. To tackle this issue, part of our ongoing work is based on improving the performance based on alternative similarity measures [27]. We elaborate on useful extensions and future research directions for recommendation for learning in the next section. 
 6 Discussion.
 The goal of this kind of dataset driven research on recommender systems is to gain deeper insights into both relevant similarity measures between users and between items and relevant data that can be taken into account to support recommendation for learning. Results of our study show that the tanimoto similarity measure gives most accurate results on the current TEL datasets that are very sparse. The best choice of algorithm (i.e. user-based, item-based or slope-one) is dataset dependant. These results are consistent with previous ﬁndings that have been reported in [23]. The results indicate that the successful operation of collaborative ﬁltering in the context of real-life learning applications requires careful testing before their actual deployment. It is important to note that the presented experiments serve only as a ﬁrst step towards the understanding and appropriate specialization for recommendation for learning. This study has to be further complemented with experiments that will study the needs and expectations of the users, their information seeking tasks, and how recommended resources may be used in the context of their learning activities [22]. In this study only very generic collaborative ﬁltering algorithms have been tested. In the learning domain, researchers have proposed the use of additional learner or teacher attributes in recommendation processes [3]. Examples include knowledge or experience levels indicators, learning interests, learning goals, learning and cognitive styles, aﬀects and background information. In addition to interests and preferences that are available in most datasets, the learning goal or competencies of a learner are often incorporated as a basis for generating learning recommendations [5]. Data on competencies or experience levels is available in the MACE and APOSDLE datasets. In addition, APOSDLE provides data on the learning goal of the learner when she is performing a task. Such data is useful to improve similarity measures between users and to ﬁnd users who share similar goals, both as a basis to improve recommendation of relevant learning resources and to support recommendation of peer learners. We aim to experimentally test the performance of variation against several attributes of learners or teachers that are proposed in the literature. In order to create evidence driven knowledge about the eﬀect of recommender systems on learners and personalized learning, more experiments like the presented one are needed. The continuation of additional small-scale experiments with a limited amount of learners that rate the relevance of suggested resources only adds little contributions to an evidence driven knowledge base on recommender systems in TEL. The key research question remains how generic algorithms need to be modiﬁed in order to support learners or teachers. To give an example, from a pure learning perspective, the most valuable resources for a learner could be the recommendation of diﬀerent opinions or facts that challenge the learners to disagree, agree and redeﬁne their point of view. In order to enable such experiments, the capturing of learner or teacher data is a key requirement. Our ongoing research is focused on the development of a standardized data model that enables the uniform representation of both explicit and implicit relevance data of learners and teachers [8]. This data model will be standardized in collaboration with the CEN WS-LT Working Group on Social Data18 . 
 7 Conclusion.
 In this study, we presented datasets that capture learner interactions with tools and resources and that can be used for learning analytics research. We successfully applied several variations of user-based and item-based collaborative ﬁltering algorithms to these datasets. Challenges to be tackled include sparsity of data and require further research on both implicit relevance indicators as well as similarity measures to ﬁnd relevant items and/or users. To tackle these challenges, the further collection of suﬃciently large datasets that capture learner interactions in diﬀerent real-life learning settings is a key requirement. 
 (18) https://sites.google.com/site/censocialdata/home.
 Acknowledgements.
 This work is supported by the dataTEL Theme Team and the STELLAR Network of Excellence. Katrien Verbert is a Postdoctoral Fellow of the Research Foundation - Flanders (FWO). The work of Nikos Manouselis has been funded with support by the EU project VOA3R - 250525 of the CIP PSP Programme (http://voa3r.eu). This research used data operated by European Schoolnet (EUN). EUN does not guarantee the accuracy of any data and cannot be held liable for any errors or omissions. The research leading to these results has also received funding from the European Community Seventh Framework Programme (FP7/2007-2013) under grant agreement no 231396 (ROLE).]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Dataset-driven Research for Improving Recommender Systems for Learning</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/riina-vuorikari"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/riina-vuorikari"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/riina-vuorikari"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/47">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Usage Contexts for Object Similarity: Exploratory Investigations</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/47/authorlist"/>
		<swrc:abstract>We present new ways of detecting semantic relations between learning resources, e.g. for recommendations, by only taking their usage but not their content into account. We take concepts used in linguistic lexicology and transfer them from their original field of application, i.e. sequences of words, to the analysis of sequences of resources extracted from user activities. In this paper we describe three initial experiments, their evaluation and further work.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 Analyzing the behaviour of learners and deducing something meaningful from it is a major obstacle for teachers. In recent times, more and more educational institutions have therefore started to use electronic means to manage learning as well as teaching activities. A tool frequently used for this purpose is a Learning Management System (LMS). LMSs can handle activities from learners and teachers as well as administrative processes, ranging from learning resource provision to course related management tasks and grading. Many recent LMSs also include communication tools, such as chat, group discussion and email. In many cases a LMS collects data about the users’ interaction with information and services by the system, thereby generating large amounts of data. So far, these data are not made use of to their full extend, e.g. individual analysis of a student’s learning behaviour or performance. Both analysis results, however, could be used to improve the learning efficiency and effectiveness. For example, a teacher being notified about a student’s continuous search activities could address that student and help her personally. Furthermore, the system might let the teacher know that many students within one course follow the same search pattern which could be an indication for the teacher to provide additional learning resources. An approach to extract representative activity patterns from usage data is described in [1]. With the learner’s activities identified, recommendations of suitable resources, i.e. learning resources that are thematically related to a learner’s current usage context, could be triggered. Yet, recommendation approaches based on the resources’ topics are often difficult to apply or even fail: while the manual creation of semantic metadata is costly, automatic extraction of semantic metadata only works well for text-based resources. LMSs however often also have to deal with media types such as images, audio and video files. Adding social metadata to resources is another way to classify objects based on their content but it tends to be ambiguous or even faulty. In this paper, we present new ways of extracting semantic relations between learning resources by analyzing data of the users’ interaction with the system and thus the usage of learning resources. In contrast to collaborative filtering approaches [2] our approaches do not rely on the relations between users and objects but on the relations between the objects themselves, i.e. they are used together or used in similar contexts. We describe our first exploratory analyses of applying semantic relation detection techniques from corpus-driven lexicology to semantic object relation detection in learning contexts. The datasets of usage activities collected within the MACE portal [3] (a thematic web-based portal for learning resources in architecture) serve as our test bed. The rest of the paper is structured as follows: In section 2 we report on the background of similarity calculation in recommender systems. In section 3 we describe three ideas of how to adopt lexicological techniques to identify related learning resources. We then present the corresponding experimental investigations using the MACE-LOR [3] as a test-bed in order to illustrate our ideas in section 4 and give a summary and an outlook in section 5. 
 2 Background.
 Recommender systems deal with the delivery of items selected from a collection that the user is likely to find interesting or useful. The most common approaches of object similarity calculation used in recommender systems are user- or item-based collaborative filtering, content-based filtering or hybrid approaches which combine aspects of more than one filtering technique [4]. Commonly, hybrid systems combine content-based and collaborative-based techniques, but knowledge-based or demographic-based techniques can for example be integrated as well [5, 6]. Recommender systems in TEL need to satisfy other requirements than for example recommenders in e-commerce. The recommended items need to fit into the current context of the user, considering e.g. her competences and learning goals to ideally support her while learning. Approaches that deal with those requirements are comprehensively described in [7]. Ruiz-Iniesta et al. [8] introduce a domain ontology holding interrelated concepts to index learning objects and establish learning paths among them. The approach of Bozo et al. [9] relies on teacher profiles, e.g. information about educational level, subject, area, region and school type, and on learning object profiles, e.g. information about educational level and topic. Those systems are well adapted to the domain of technology enhanced learning. However, they have in common that they need explicit information about the items and/or the users, which is, in practice, usually sparse or not available at all. We adapt the item-based collaborative filtering approach but use a different way of calculating item similarities. For two objects to be deemed similar, their context of usage needs to be similar. We presuppose that users in TEL scenarios work predominantly on only one task within one session. That is, if two objects are used within the same context then they are most probably related via the session task, and this relatedness is a semantic/content-induced one. 
 3 Word Contexts and Semantic Relatedness.
 If you want to know the meaning of a word, it is a good strategy to look it up in a dictionary where you might find a helpful definition. In many cases, however, a definition (if there is one at all) might not be sufficient to fully understand and thus correctly use the word. The words “strong” and “powerful”, for example, have highly related meanings. Yet, we can say “strong tea” while we cannot say “powerful tea”. “Powerful drug” though is acceptable [10]. Definitions of the word’s meaning will most probably not cover such differences. Therefore, dictionaries usually give contexts in which a word typically occurs to illustrate the actual word usage. Context is considered to be significant for the meaning of a word : “You shall know a word by the company it keeps.” 11]. The company a words keeps – its cooccurring words – contributes to the meaning. Two words might just co-occur by accident. However, the co-occurrence might also be relatively frequent and thus statistically significant. Statistically significant co-occurrences reveal close relationships between the co-occurring words or their meanings respectively: they are used to detect multi-word expressions (‘New York’), idioms (‘kick the bucket’ [12]) or constructions with a milder idiomatic character (‘international best practice’ [10]). Subsequently the question arises whether we can apply that insight to learning resources and their usage contexts: do significant co-occurrences of learning resources in learning contexts reveal close semantic relationships between the resources? If two words occur in very different contexts, they can be assumed to be semantically non-related. If one word occurs in various contexts, i.e. contexts of different types, we can assume the word to be polysemous, that is different contexts correspond to different readings of the word. If two words, however, occur in very similar or even identical contexts, then we can assume that these words are semantically strongly related. ‘Relatedness’ can be specified as a kind of similarity: if, for example, the two words are co-hyponyms (that is, they have a common superordinate concept, which is very often true for words with highly similar contexts), they are similar regarding their superordinate concept [13, 14]. (Note that context-similarity is different from co-occurrence. Words with similar contexts do not need to co-occur in the same contexts.) Thus, by comparing the usage contexts of words we can detect semantic similarity. It is now very interesting to find out whether this is true also for objects other than words: can we detect semantic similarities between learning resources by comparing the usage contexts of these objects? This seems to be intuitively plausible but has to be proven. 
 Words co-occur with other words, with some of them a statistically significant number of times. The significant co-occurrences form the co-occurrence class of the respective word. It can now be examined whether words significantly co-occur in cooccurrence classes. These words again form another co-occurrence class, namely a higher order co-occurrence class. After several iterations higher order co-occurrence classes become semantically homogenous. Heyer et al. [14] show this for the cooccurrences of ‘IBM’, among other words. Their investigations are based on text corpora collected for the portal wortschatz.uni-leipzig.de (concerning the German treasury of words). The first co-occurrence class is rather heterogeneous, containing words like ‘computer manufacturer’, ‘stock exchange’, ‘global’ and so on. After some iterations of computing higher order co-occurrences classes, however, the classes become more homogenous and stable. The co-occurrence class of tenth order only contains names of other computer-related companies like ‘Microsoft’, ‘Sony’ etc. We can easily compute higher order co-occurrence classes for learning resources by deriving the first co-occurrences from usage contexts. The question is: do these classes become semantically homogenous, like the analogue classes of words? Let us take stock: current lexicology heavily relies on the investigation of word contexts. By comparing entire contexts and examining co-occurrences and higher order co-occurrences semantic relations can be detected. We adopt the notions from lexicology and test whether they can be fruitfully applied in information retrieval for the analysis of learning activities. 
 4 Experiments.
 We performed three initial experiments for answering the questions asked above: 
 - Can we detect semantic similarities of learning resources by comparing the usage contexts of these resources? - Do significant co-occurrences of data objects in usage contexts reveal close semantic relationships between the learning resources? - Do higher order co-occurrence classes of data objects become semantically homogenous, like the analogue classes of words? 
 We tentatively answer the three questions with ‘yes’ and take our answers to be hypotheses that are to be validated. The MACE-portal serves as a test bed for our experiments. MACE (Metadata for Architectural Contents in Europe, [3]) relates architectural learning resources stored in various repositories with each other to support students in finding relevant information. To enable connections between these learning resources, metadata representations of the resources are stored in the central MACE repository. The representations base on the MACE application profile which extends the Learning Object Metadata (LOM) standard [15] for architecture-specific needs. Our approach only uses the metadata representations of the learning resources: The attributes stored in the LOM instances are used to build document vectors for all learning resources. To this end, the English titles and descriptions of the learning resources, their repositories, their learning resource types as well as their free text tags, classifications and assigned competences are taken into account. The semantic similarity between two learning resources is then calculated using the cosine similarity measure [16] between their respective document vectors. The cosine similarity measure forms the baseline for evaluating similarities derived from the analysis of usage contexts. Within MACE, usage data are collected and stored using the CAM (Contextualized Attention Metadata [17, 12]) schema. A CAM instance comprises amongst others the user, the accessed resource, and the action performed on that resource. All CAM instances can be assigned to user sessions. For our experiments, we assume that a session constitutes the usage context for data resources and comprises all resources accessed by a user without a break longer than an hour. Altogether we consider 3130 sessions in which 11429 resources were accessed1. 
 4.1 Experiment 1: Context Similarities.
 Our first hypothesis is that learning resources with similar usage have similar content. Before we can test the hypothesis, we have to define ‘usage’ and ‘similar usage’. We do so via the concept of a Usage Context Profile (UCP): two resources have similar usage iff they have similar UCPs. The UCP of a resource is the set of its usage contexts. Usage contexts are related to user sessions: we define a usage context of a resource R as a pair <pre,post>, where pre is the bag of resources that a particular user accessed before R in the same session, and post is the bag of resources that the user accessed after R in the same session. Pre- and post-contexts can be represented as vectors, with the resources as dimensions and the numbers of accesses as coordinates. Fig. 1 shows the UCP of a resource E. E occurred in two sessions: in the first session it was accessed after A (pre-context) and before D and C (postcontext); in the second session it was accessed after C and F and before C and G. That is E’s UCP, consisting of two usage contexts which in turn both consist of a pre- and a post-context. 
 Fig.

 1. Usage Context Profile for resource E. 
 Next we define a measure of similarity for UCPs. The similarity of two UCPs is defined as the arithmetic mean of the pair-wise similarities of their respective usage contexts. 
 (1) If you are interested in the data set, please contact the authors 
 The similarity of two usage contexts is defined as the arithmetic mean of the similarities of the associated pre- and post-contexts. Finally, the similarity of two preor post-contexts is defined as the cosine similarity of the vectors representing these contexts. Note that usage similarity as it is defined here does not require conjoint usage – two learning resources are similar regarding their usages iff they have similar usage contexts. This does not require that they are ever used in the same session. So far, we do not take the order of learning resources in pre- and post contexts into account. By the concept of UCP-similarity we define a means for calculating usage similarity values for resource pairs. With respect to the two kinds of similarity values we can now refine our initial hypothesis and make it testable. The new hypothesis is: resources with similar usages have similar content, therefore the usage similarity values of resource pairs are positively correlated with their semantic similarity values. To test the hypothesis we generated two arrays of similarity values, one for the usage similarities of MACE learning resources and one for the semantic similarities of these resources. Thereafter, we computed the Pearson Correlation Coefficient [19] for these arrays. The result was a correlation coefficient of 0.35. That is, we detected a medium positive correlation between the different kinds of similarities. As our sample of resources was sufficiently large, the correlation coefficient can be regarded as representative. However, we expected a strong positive correlation and therefore our hypothesis is not strongly supported. We know that our semantic metadata from which the semantic similarity values were computed suffer from a scarcity problem. Most probably, this affected the result of the evaluation. We therefore manually compared the 100 resource pairs with the highest usage context similarity values. 92% of these resource pairs showed content similarities. We found text documents on the same topic, e.g. “risk factor analysis”, or resource pairs with one of them being an exercise and the other one a text on the same topic, among other kinds of similarities. We also discovered that many of the detected content similarities are not entailed in the semantic metadata and are thus not accounted for in our evaluation. We therefore expect that a better baseline derived from a richer set of semantic metadata will lead to a stronger correlation of usage and content similarities. For a detailed description of the approach and its evaluation see [20]. 
 4.2 Experiment 2: Conjoint Usage, First Order Co-occurrence.
 Our second hypothesis is that resources that are significantly often used together are semantically related. Semantically related objects can for example be about the same topic, they can be complementary regarding learning goals, etc. Since we access the semantics of learning resources via their metadata, we presume that all forms of semantic relatedness are somehow reflected as similarities of the metadata. We test our hypothesis as follows: We relate resources regarding their conjoint usage in sessions, thereby taking not only the number of conjoint sessions but also the number of accesses within these sessions into account. We normalize the values of conjoint usage by referring to relative usage frequencies. Based on these values we construct a graph with resources as nodes and normalize conjoint usages as weights of edges. We then apply a graph clustering algorithm. A common approach for graphbased clustering is Markov Clustering (MCL, [21]). However, since there is no guarantee that the MCL algorithm terminates, we apply Iterative Conductance Cutting (ICC) for clustering the graph which is similar to MCL regarding scalability [22]. The ICC algorithm starts with only one cluster and iteratively splits a cluster into two new clusters until the performance measure (conductance) is below a specific threshold. The conductance represents the relation of cross-cluster edges to cluster internal edges. In other words, this approach gives greater importance to vertices which have many similar neighbours and less importance to vertices which have few similar neighbours. The iteration ends when no more clusters can be split without violating the threshold. Since we claim that conjoint usage gives rise to semantic relatedness and that semantic relatedness is reflected in the similarity of semantic metadata, we expect that the resulting clusters are semantically dense. In other words: we expect the members of each cluster to be semantically similar, while members of different clusters are semantically distinct. We compute semantic similarity as metadata similarity; the semantic density of a cluster is the average semantic similarity of its members. We refine our initial hypothesis as follows: the mean semantic similarities (metadata similarities) of the individual clusters systematically differ from the mean semantic similarity of the whole population. We expect the mean similarities of the clusters to be significantly higher than the corresponding mean similarity of the entire set of objects. For the mean similarity value of each cluster we tested whether it significantly deviates from the mean of the entire population by applying a t-test. The results showed that 78% of the 106 generated conjoint-usage clusters have a significantly over-average semantic relatedness (p < 0.05) compared to the entire population. Only about 7% of the clusters had a significantly under-average mean similarity. This result is very promising. It clearly supports the hypothesis that conjoint usage gives rise to semantic relatedness. 
 4.3 Experiment 3: Higher Order Co-occurrence.
 Our third hypothesis is that usage-based higher order co-occurrence classes of learning resources are semantically homogenous. We presume that semantic homogeneity correlates with semantic similarity. That is, we expect that the members of a semantically homogenous class are more similar than the members of a class of randomly chosen resources. The concept of higher order co-occurrences is illustrated in Fig. 2. The objects A, B and E directly co-occur with object C in at least one session, while the objects B, D and F directly co-occur with object A and the objects D and G directly co-occur with object E. Therefore, objects B, D, F and G are second order co-occurrences of object C and form a second order co-occurrence class. We recursively compute significant co-occurrences of resources and thus generate higher order co-occurrence classes of resources. We do so by taking the sessions as input to generate significant (first order) co-occurrences and use these co-occurrence classes as input for the calculation of the second order co-occurrence classes which then form the input for the calculation of the significant third order co-occurrence classes and so forth. 
 Fig. 2. Co-occurrence tree of resource C. 
 For simplicity reasons, we consider all co-occurrences of a resource in the example in Fig. 2. In practice, we are only interested in significant co-occurrences. Therefore, we need to calculate a significance value for each resource in each cluster and to define a threshold. Resources with a significance value lower than the threshold are deleted from the classes and are not considered in the next calculation step. The significance value of a resource O in a co-occurrence class for a reference resource R is calculated by relating the number of contexts (here: sessions) containing O and R respectively, the number of contexts holding both resources and the total number of contexts. For further details see [14]. Currently, there is no generally accepted approach to define the significance threshold. Thus we choose an exploratory procedure for this problem as well and experiment with different thresholds. The computation of the co-occurrence classes stops when the classes stabilise. In our experiment this happened after six iterations. Given that this approach is not a hard clustering an object can be contained in more than one class. We evaluated the semantic density of the generated clusters as we did in experiment 2 using a t-test to compare the mean similarity of each cluster with the mean similarity of the population. Thereby, we gathered a significant over-average similarity of 82% of the 184 generated clusters (p < 0.05) compared to the mean similarity of all resources. In a manual inspection of the higher order co-occurrence classes we found some interesting relations between the resources, e.g. a cluster containing only Spanish documents describing building material, mainly cement, concrete and bricks; or another cluster containing only English documents describing modern, public buildings, mainly museums and police stations. 
 5 Outlook.
 We presented approaches for the identification of related learning resources from usage behaviour exploring techniques borrowed from lexicology to relate learning resources by only focussing on their usage contexts. If these techniques are successful, they can be fruitfully applied for recommender systems. We started with exploratory, experimental investigations. The results clearly indicate that the chosen approach is promising. We will investigate the appropriate parameters and thresholds that are used by the presented approaches. To this end, we will conduct additional experiments in other test beds in order to explore domain-dependent distinctions that need to be addressed. Moreover, we will adopt further natural language processing and information retrieval technologies and apply them to the objects’ usage contexts. The usage contexts of an object O can be subsumed under one vector holding all objects the object O was ever accessed with. Such representations can be used as input for itembased collaborative filtering approaches, where an object is normally described by a vector holding all persons that, for instance, viewed or rated the object. Assuming objects can be handled as words, we can also conduct techniques like Latent Semantic Analysis for dimension reduction on these vectors before using it for similarity calculations. Furthermore, we will explore if the presented approach can be transferred from relations between documents to relations between humans. For example, pre- and post contexts can be based on one’s own activities or that of others, etc. This way, we would be able to identify relations among humans that are expressed and quantified by their behavior. In order to test the approaches in real world settings, we will embed the techniques in recommender systems in the area of TEL. For example, within the MACE portal, a recommender could provide resources thematically related to the ones used in the current session. Evaluations of this recommender will show if the chosen approach yields acceptable results. 
 6 Acknowledgement.
 The research leading to these results has received funding from the European Community's Seventh Framework Program (FP7/2007-2013) under grant agreement no 231396 (ROLE project).]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Usage Contexts for Object Similarity: Exploratory Investigations</rdfs:label>
		<dc:subject>text mining</dc:subject>
		<dc:subject>clustering</dc:subject>
		<dc:subject>usage data analysis</dc:subject>
		<dc:subject>usage contexts</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/katja-niemann"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/katja-niemann"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/maren-scheffel"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/maren-scheffel"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/47/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/katja-niemann"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/maren-scheffel"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/48">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Applying Analytics for a Learning Portal: the Organic.Edunet Case Study</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/48/authorlist"/>
		<swrc:abstract>Learning portals are education-oriented Web portals, which provide access to a variety of educational material, usually coming from various sources. In order to explore how they can support their users during an educational activity (e.g. preparation of teaching a course), it would be interesting to study the behavior of their visitors, focusing on the particular context in which specific actions are taking place. For example, user activities may be analyzed during specific learning events, when activities are more focused. This paper discusses the case study of the Organic.Edunet Web portal (www.organic-edunet.eu), a learning portal for organic agriculture educators that provides access to more than 10,500 learning resources from a federation of 11 institutional repositories. The portal mostly focuses on serving school teachers and university tutors and has attracted until today almost 12,500 unique visitors from more than 130 countries, out of which about 1,000 have registered to the portal. An effort is made to study the users’ behavior, focusing in tutors and educators in both schools and universities, in relation to specific training events in which we know that they have been involved. Therefore, we analyze logs of user activities that took place on specific dates and geographical locations, in order to potentially identify notable changes in their normal visiting behavior.</swrc:abstract>
		<led:body><![CDATA[

 1. Introduction.
 Initially the term web portal was used to refer to well-known Internet search and navigation sites that provided a starting point for users to explore and access information on the World Wide Web (Winkler, 2001). The term “Internet portal” or “Web portal” began to be used to describe mega-sites (such as Yahoo!, Excite, AOL, MSN, Netscape Netcenter, and others) that many Web visitors used as a ‘starting point’ for their web surfing. Since that time, Web portal have significantly expanded and matured, and a diverse range of portal types have been developed and used in different contexts (Portals Community, 2001). Nowadays, Web portals are generally defined as gateways to information and services from multiple sources (Tatnall, 2005). One important component is the organization, navigation, labeling and indexing of their content in order to facilitate searching of information and services, so that users can search, identify and access the most appropriate resources for their needs. Learning portals, Web portals that offer learners or educators with a large selection of learning resources, are essential to the further integration of information technologies and learning (Holden, 2003). The purpose of a learning portal is not simply indexing and delivery but to facilitate actual reuse and sharing (Duncan, 2002). Thus, the expected usage of services and resources found in learning portals could be considered as different compared to the usage of other types of portals (such as entertainment, information or commercial ones). In order to explore more about the way that users of learning portals interact with the portal services and the indexed content, it is often useful to engage analytics - usually by studying the log files of the portal. Such a log analysis can take place in a systematic, repeatable but also practical way, allowing portal owners to explore the actual usage of their learning portal and to identify potentially interesting patterns of use (Piearrakos et al., 2003; Machado et al., 2003). In this paper, we examine the case of a Web portal that supports users in finding digital learning resources to support and enrich their teaching activities. It is the case of the Organic.Edunet portal (www.organic-edunet.eu) that aggregates resources on organic agriculture and agro-ecology, allowing educators to find and retrieve this content from a single point of access. We particularly examine how the portal has been used before, during and after the organization of training events for potential users (i.e. educators) in several European countries. This analysis focuses both on the general usage statistics of the portal, but it also looks at the user level, trying to identify changes on the typical user profile of the portal due to the training events that were organized. 
 2. Background.
 The Organic.Edunet Web portal aims to facilitate access, usage and exploitation of digital educational content related to Organic Agriculture (OA) and Agroecology (AE). Organic.Edunet aims to support stakeholders producing content about OA & AE in order to publish it in an online federation of learning repositories and describe it according to multilingual, standard-complying metadata. The portal front-end features a multilingual user interface translated into fourteen (14) languages, providing access to more than 10,500 learning resources from a federation of eleven (11) institutional repositories. The targeted audiences of Organic.Edunet portal are mainly educators: school teachers looking for resources to help them prepare their relevant teaching activities (e.g. how to set up a school garden to support hands-on environmental education); and academics (professors, teaching staff or researchers) in agriculture and life science topics, looking for resources to help them with their teaching, learning and research activities (e.g. writing a case study report on a particular crop or farm type). Potential users also include the stakeholders producing content, such as agricultural libraries and academic publishers. The portal has attracted, until the end of November 2010, 22.100 visits and 120.000 page views from 14.300 unique visitors from approximately 130 countries, out of which more than 1.000 have registered to the portal. These numbers assure that the portal has been used from a wide audience, thus allowing an analysis of the users’ behavior to reach safe conclusions as far as the portal usage is concerned. In order to introduce the potential users to the portal, a series of Open Days have been organized in various countries, featuring validation events where educators had the chance to work with the Organic.Edunet portal. These events were organized based on specific guidelines and also provided clear instructions to all the participants (http://virtuelleschule.bmukk.gv.at/projekte-international/euprojekte/organicedunet/open-days/). From its launch, the Organic.Edunet Portal has been linked to a Google Analytics (http://www.google.com/analytics) account in order to track the visits and be able to document its usage. Another envisaged use for this account was to try and evaluate the portal through analyzing focused statistics having to do with a range of parameters, like the day or time of visit, the geographic areas of the visitors, but also the origins of their visits. The visitors’ log files have been collected and analyzed using the Google Analytics tool in order for observations to be made in relation to the way the behavior of users was affected by the organization of such training events. 
 3. Methodology.
 A brief outlook on existing literature on evaluating learning portals has showed that there are different ways in which a portal can be evaluated. One option would be to provide users with questionnaires or web-based evaluation tools and analyze the feedback provided (Silius & Tervakari, 2003; van der Heijden, 2002). This evaluation has already been carried out in Organic.Edunet with an online questionnaire (http://www.ieru.org/organicsurvey/). A second option would be to apply an analytics approach, by analyzing the log files of the users in order to study their visiting behavior (Buckley et al., 2006; Carr et al., 2008; Hasan et al., 2009). Hybrid methods using both approaches can also been found in the literature (Stacey & Rice, 2002). 
 3.1. Context of study.
 The analysis carried out in the context of this paper, particularly focuses on studying how the usage of the Organic.Edunet portal was affected by the organization of the focused training events (i.e. Open Days). Overall, thirteen (13) of the Open Days that have been organized in five (5) different countries are examined. These Open Days had 160 participants comprising mainly from school teachers and academic staff. Brief information on these Open Days is presented in Table 1: 
 Table 1. Overview of the Open Days organized in Schools & Universities 
 3.2. Steps Taken and Data Used.
 In order to study the portal statistics before, during and after the Open Days, some variables have been defined and some assumptions were made. More specifically, when looking at the data on Table 1, it’s apparent that the Open Days took place on two distinct periods and not throughout the whole period of reference. The first group of Open Days generally took place during March & May 2010 (the fact that one Open Day took place during early February it is not considered to affect results so much) whereas the second group of Open Days took place on August-September 2010. Our analysis focuses on the period before the two groups of Open Days (Pre-OD), the actual Open Days (OD) and finally on the period after the Open Days (Post-OD), more specifically: 
 a) The time before the first group of Open Days is symbolized as “Pre-OD”, whereas the first period of Open Days is symbolized in all graphs as “OD1”. This time following the first group of Open Days is symbolized as “Post-OD1”. Respectively, the second Open Days’ period is called “OD2” and the period following the second group of events took place is symbolized as “Post-OD2”, 
 b) The portal-related variables selected to be examined were: Visits for the portal, Page Views, Unique Visitors, Traffic Sources and Most Popular Pages. These statistics are calculated for all the periods defined in (a), 
 c) The user profile that is also examined in this study, is comprised by a set of statistics that try to focus on the user-level behavior that can possibly lead to new insights on the use of a thematic portal with a focused community around it. More specifically, the variables measured here include: Average Time on Portal, Average Time on Page, Pages Accessed per Visit, Visitor Loyalty and Depth of Visit 
 Definitions.
 Deep visitor: Deep visitor would be the visitor that in one visit would open more pages than the average pages per visit of all the visitors put together for the same period of time. Statistics for this metric were calculated using the “Depth of Visit” available in Google Analytics, showing the percentage of users that visited one page per visit, two pages per visit, three pages, etc. up to the class of 51-100 pages. 
 Bounces: Any visit during which the user views only one page and then “exits” the portal, is considered as a bounce 
 Visitor Loyalty: Loyalty of each visitor shows the times per time period (i.e. day, week, and month) that a user visits a website. 
 Data post-processing and analysis was carried out using Microsoft Excel where exported CSV files from Google Analytics where processed. The results are presented in the form of tables and figures that focus on the variables that are examined. Column charts were chosen to present focused quantitative data whereas tables were mainly used to provide overviews of the data per region. The main research question explored in this initial analysis is: How do the usage statistics both on the portal level but also on the individual user level, change, after a series of training events on the portal is organized? E.g. do educators start using more the Organic.Edunet Web portal? How is that reflected on the user-level statistics? 
 4. Initial Results.
 4.1. Portal-level Statistics.
 In this section, we present the statistics related to the portal in general, trying to identify the way in which they change, if so, after the organization of an Open Day. 
 Table 2. Overview of portal statistics before, during & after the Open Day periods defined 
 Looking at Table 2, it would have been expected to get higher values for the periods of the Open Days in all metrics, when compared to the respective “Pre” and “Post” Open Day periods. So, for example, from 32.6 visits per day before the second Open Day period, we moved to a 54.2 visits per day during the Open Day period, which dropped to a 44.9 after the Open Days ended. It’s interesting to note that even though the numbers drop after the Open Days, they do not decrease to the same extent, showing that the Open Day actually brought upon some kind of change to the portal usage. One exception to this rule is the case of Page Views per day for the second period of Open Days, which from a 159.9 went up to 250.6 but after the Open Days, it dropped to 153.7 page views per day. Overall, comparing January 2010, when the portal started operating with November 2010, long after the end of the last Open Day, visits per day were tripled, page views per day rose by almost 60 views, whereas visitors per day were five to six times more than in the beginning. Examining the sources of visit, one could argue that time given, the users of the portal are using it more and more, and therefore it could be safe to assume that they would also visit the portal directly and not through search engines or other referencing websites. This assumption is contradicted by findings, as direct visits drop from a 65% before all Open Days, gradually to a 23.9% after their end. Visits generated by search engines do not seem to follow and clear pattern (ranging from 18.5% to 32%) whereas visits from referencing sites gradually increased from a mere 14% to almost 60%! The explanation to this comes from a factor outside the portal itself. The Organic.Edunet portal was mostly referenced in websites of consortium members in the beginning of the period examined but was also promoted through a network of affiliated partners which was developed during the course of the project, and mainly during the last quarter of 2010. In total, 83 affiliated partners put links to Organic.Edunet on their websites, which can explain the rise of visits coming from referencing websites. 
 The interpretation of bounces is also interesting. Overall, bounces seem not to be affected by the Open Days organized, in the sense that they do not fluctuate at all before and after the Open Days, but they show a steady increase throughout this period. Looking at the bounces in relation to the visits to the portal per day, it can be seen that the majority of visits per day, actually include bounces also, since bounces are visits with only one page visited at a time. This shows that the few visits that did not bounce off actually went into great depth. For example, for “Post-OD2” period, only 18.2 visits per day did not bounce off the portal, but these visits generated 127 page views per day, which are almost seven pages per visit on an average. Table 3, shows the most famous pages of the portal measuring the visits per day that each page attracted. As it was expected, most of the visits to the portal originate from the homepage, so this is the highest ranking page of all. Stats for the homepage do not show anything out of the ordinary, as visits per day rise for the periods of Open Days and they fall during periods in between. Simple search (textbased search) is also a very popular destination within the portal, but in this case, the visitation rates fluctuate less than the ones for the homepage. Despite that, comparing all the search functionalities offered, text-based search ranks on the top. Help on how someone can use these functionalities is also visited pretty often, with high numbers during the Open Day periods, also. Browsing through the resources based on specific criteria (educational level, language, difficulty, etc.) is also a widely used search method in the portal which follows a steady but decreasing usage. Browsing is followed by the Semantic Search which involves searching through the terms of a domain specific ontology deployed as a tree of terms and concepts. Semantic searching seemed to gain some ground before and during the second Open Day period, but again it fell to the levels of visitation before the Open Days. 
 Table 3. Most popular pages (visits per day) before, during & after the Open Day periods defined 
 “Educational Scenarios” is a portal page that offers content specifically designed for use in the classroom of either schools or universities, through elaborated scenarios on various topics related to Organic Agriculture and Agroecology. Overall, this page shows that despite a promising start (before the Open Days it ranked 3rd), in the end, it represents the least popular page, close to the tag-based search. Finally, tag-based search remained largely unused throughout the period examined. All these statistics, related to ways in which users search for content can greatly influence the interface design of the portal. 
 4.2. User-level Statistics.
 In this section, we present the statistics related to the users in specific, trying to identify the way in which their behavior changes, if so, after the organization of an Open Day. Looking at Table 4, it seems that the average time spent on the Organic.Edunet portal by each user is dropping throughout the period, starting from approximately six minutes, down to almost three minutes. So, overall, more visitors came to the portal as it was depicted in the portal level statistics, spending all and all, less time on the portal as it is shown here. Someone may have expected that this would also be reflected in the average time spent on each page, this is not the case. Although this time is also decreased, the decrease is both not that big and it also fluctuates from 59 to 73 seconds. Pages viewed per visit are dropping gradually, which was also reflected on the most popular pages. Looking at the depth of visit, which is closely related to the pages per visit figure, it seems that the depth in which the visitors use the portal, is more or less steady. This shows that each time, almost a quarter of the total visitors actually spend time with the portal, opening more pages, looking for content and using the search functionalities. 
 Table 4. Overview of portal statistics before, during & after the Open Day periods defined 
 User loyalty was measured as the percentage of visitors that came back to the portal one or more times during a week. This limit was defined in the context of this study to measure the loyalty of the users of educational portals, as no such information could be retrieved in other studies. Starting from the preOpen Day period, only about 5% of the users would return to the portal more than once a week. As expected, during the first period of Open Days, this number of loyal visitors tripled to 15% which more or less was sustained during the period that followed the first set of Open Days. As expected, the second set of Open Days enhanced visitor loyalty, with one fifth of the visitors coming back to the portal at least once a week. As with most of the stats that were analyzed so far, user loyalty seems to have been “helped” by the Open Days, increasing during them and returning to a lower level after them, which is marginally above the initial loyalty, that was measured before any Open Day took place. 
 Figure 1: Visualization of the changes in the user profile through the course of different periods 
 As it can be seen in Figure 1 the typical user profile changes gradually through the Open Days. For example, it’s apparent that Depth of Visit remains the same whereas Visitor Loyalty greatly fluctuates. Similar visualizations of the user behavior may help in depicting different types of usage profiles for similar learning portals. 
 5. Conclusions.
 This paper presented an initial analysis of the log files of the Organic.Edunet portal, a Web portal that supports educators in finding digital learning resources to support and enrich their teaching activities. It particularly examined how the portal has been used before, during and after the organization of focused training events for potential users in several European countries. It tried to study any notable changes in both the portal statistics overall and also study any changes in the behavior of the users, before, during and after the training events. The present paper reached the following conclusions: 
 - The portal statistics retained some of the dynamics that were created during the Open Days, even two months after their completion. This was confirmed for visits per day, visitors per day and pages accessed per visit, 
 - Bounces continued to rise throughout the period showing no direct relation to the Open Days organized. This rise could be partially explained by the fact that the visits originated from referencing sites in a big percentage which could mean that visitors accidentally stumbled upon it and exited after viewing only one page, 
 - It seems that traditional search functionalities worked better for the users that largely did not use tag-based search. Semantic search and browsing per topic were used, but still not as much as expected, 
 - The typical user before the Open Days would not return to the portal more than once a week and would spend more than six minutes in the portal in total in each visit. This user would occasionally visit more pages than the portal’s average, taking a “deep visit”. After the end of the Open Days the same user rarely returns to the portal more than once a week and spends significantly less time on the portal. The frequency in which “deep visits” occur stays the same 
 - Many project-related, outside of the portal scope, parameters affected the statistics on the portal (i.e. affiliation strategy of the project, press releases circulated) by drawing visits from a wide scope of people that will influence the usage of the portal 
 As far as the analysis of the Open Days in groups is concerned, one can argue that the results must be compared before and after each Open Day, but this was not chosen for two reasons. First of all, a preliminary analysis showed that the results were more or less as expected showing no research interest when analyzing Open Days per country; i.e. traffic is heavier during Open Days, with more visits or that time spent on the portal increases due to the structured exercises given during the Open Days. The second reason had to do with selecting a bigger sample of users by grouping the Open Days together, from which safer conclusions could be drawn, whereas the limited sample of 15 participants per Open Day could not provide that. Another issue has to do with the fact that there is no standard threshold that can characterize a user as loyal or otherwise, because this number is more or less affected by the nature of the website examined, the services it offers, etc. In a recent survey by Chitika Inc. (http://insights.chitika.com/2009/digg-facebook-loyal-readers/), related to Facebook and other popular websites, a loyal visitor was the one that came back to the website four or more times per week. In the case of an Educational Portal, this number might be too high of an expectation, so for the purpose of this study, it was decided to set forth our own threshold in characterizing someone as “loyal”. Future work should focus on delving deeper, trying to elaborate the generic research question into a set of research hypotheses that will be statistically explored. Through such an analysis, it can be decided if a measurable and confident change in the users’ behavior is observed. A more detailed analysis can also take place focusing on data such as the sections of the portals that are used more during and after the training events (i.e. do the users spend more time on new features/services that fit their needs?). Finally, we are interested to view results in the light of the interaction of different audiences with the portal (e.g. academics/researchers, teachers, public, other). This could also indicate if separate portal interfaces are needed to better serve each community. 
 6. Acknowledgements.
 The work presented in this paper has been funded with support by the European Commission, and more specifically the project 250525 “Virtual Open Access Agriculture & Aquaculture Repository: Sharing Scientific and Scholarly Research related to Agriculture, Food, and Environment” of the ICT Policy Support Programme (ICT PSP), Theme 4 - Open access to scientific information.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Applying Analytics for a Learning Portal: the Organic.Edunet Case Study</rdfs:label>
		<dc:subject>web portal</dc:subject>
		<dc:subject>log analysis</dc:subject>
		<dc:subject>learning repositories</dc:subject>
		<dc:subject>agriculture</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-palavitsinis"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-palavitsinis"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/vassilios-protonotarios"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/vassilios-protonotarios"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/48/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-palavitsinis"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/vassilios-protonotarios"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/49">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Stepping out of the box. Towards analytics outside the Learning Management System</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/49/authorlist"/>
		<swrc:abstract>Most of the current learning analytic techniques have as starting point the data recorded by Learning Management Systems (LMS) about the interactions of the students with the platform and among themselves. But there is a tendency on students to rely less on the functionality oﬀered by the LMS and use more applications that are freely available on the net. This situation is magniﬁed in studies in which students need to interact with a set of tools that are easily installed on their personal computers. This paper shows an approach using Virtual Machines by which a set of events occurring outside of the LMS are recorded and sent to a central server in a scalable and unobtrusive manner.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 The ﬁeld of Learning Analytics is emerging as a combination of business intelligence, business logic, educational data mining and action analytics [8] where data is collected, analyzed and interpreted to derive so called actuators to optimize a learning experience. Much the same way in which a regular web site monitors the operations performed by the users to then infer patterns to suggest or modify the experience of future users, in the context of learning, students usually interact with a Learning Management System (LMS) that records all the operations. This wealth of data can also be analyzed and transformed into useful information to ultimately infer and apply changes to the current environment aiming at improving the process for the student, the instructor and/or the institution. There are numerous key developments that are behind the emergence of this new ﬁeld. LMSs, both commercial and open source, include modules that automatically register every event taking place in the platform. The higher the percentage of course activity that takes place in the LMS, the more detailed information is stored. For example, if a learning experience contains support for on-line quizzes, formative assessment, a chat room and its own email service, the platform may easily keep record of who did what, when and with whom. When combined with the well established techniques in the area of web analytics, having a detailed account of the interaction between students and instructors, or among students becomes a reality. Once the data is obtained from the tools used in a learning experience there are multiple objectives that can be tackled. For example, The Signals project a Purdue University [4] is an example of how learning analytics as described in [10] are applied at an institutional level to create an early detection system for student failure. The system uses the data already collected by the institutional LMS to detect in real time the type of events that, based on previous information, have a high probability of leading to student failure. Detecting these patterns translates then into a set of measures that are taking to anticipate the problem and thus reduce student failure rates. Interaction has been shown to be an important factor inﬂuencing student success [3]. The amount of interaction a student has with peers has a positive correlation with the academic performance [12]. As a consequence, having a detailed account of the interactions that occur in a learning experience will likely oﬀer a good predictor of academic performance, which itself is one of the most important aspects of an educational institution. With these new tools, a learning community can be “seen” in ways that were never considered before [7]. But in this scenario, there are several challenges that need to be overcome. Campbell and Oblinger [5] characterized the process of learning analytics as an engine with ﬁve stages: capture, report, predict, act, and reﬁne. The ﬁrst step already faces the challenge of having an adequate observation capability. A detailed account of any event that takes place in a learning scenario is the ﬁrst requirement to have a solid foundation upon which to build the reporting, predicting and acting mechanisms. After the data has been obtained, it should be reported in meaningful forms to all stakeholders. Several visualization techniques have been applied speciﬁcally to data gathered in courses (see for example [14, 15]). The next challenge is to determine which factors are truly signiﬁcant to achieve accurate predictions. In [13] a detailed analysis is performed considering initially a set of 22 variables recorded by the LMS (BlackBoard Vistat m (www.blackboard.com) . Out of these 22 factors, only 13 were found to have a positive signiﬁcant correlation with student ﬁnal grade. A ﬁnal multi-variable linear model is proposed with only three of these factors accounting for 33% of the variability. These factors are: total number of discussion messages posted, total number of mail messages sent, and total number of assessments completed. Once a model has been created, the following steps face the challenges of inferring relevant interventions, and ﬁnally, to design a feedback process to reﬁne the overall mechanism. There are already several approaches that close this cycle. In [11] a system is presented in which all the interactions of the students with the course material and among each other are recorded and made available to the instructors when modifying the course content. The system uses Semantic Web techniques to translate LMS logs into resource annotations that are then inserted into the editing tool used by the instructors to create content. 


 1.1 The challenge of recording the interaction.
 But recording the interactions that take place in a learning environment is becoming more diﬃcult. The initial model used during the early stages of LMS deployment in educational institutions could be called “LMS-centric”. There were numerous analogies between LMSs and conventional knowledge management tools. But with the advent of the Web 2.0, the “LMS-centric” model has failed [6]. Although the latest LMSs oﬀer an increasing set of features, students are beginning to reach the educational institutions with solid experience on how to interact with their peers in ways that are not covered by the LMS. The main consequence is that a signiﬁcant part of the interaction during a learning experience is beginning to take part outside of the LMS in what it could be called a “de-centralized approach”. Even the LMSs themselves have contributed to this de-centralization. For example, most LMSs oﬀer the option of receiving email notiﬁcations when new messages are posted in forums. The chances of students using an email client outside the LMS are increasingly large. Email support is another example. LMSs oﬀer internal an email account to each user, but they are no competition in terms of features other platforms available on the Net. This tendency is more exacerbated when a learning experience contains a signiﬁcant amount of activities that cannot be embedded by any means in an LMS. In experimental sciences, students typically require the use of special resources for procedural activities. The extreme case of this tendency is in ICT education where most of these special resources are applications that can be installed in the student personal computer. Furthermore, some studies are beginning to conﬁrm that students use conventional ICT tools to access an increasing number of resources outside the institution LMS [21]. The main consequence of this tendency is that in order to maintain the eﬀectiveness of learning analytics, new techniques are required to extend beyond the LMS-centric approach and adapt to the Web 2.0 style. 
 1.2 Observation to support assessment.
 Another factor that is changing the educational landscape is the transition from a purely expository instructional method to a “learner-centered” approach [17] where the tutor adopts a more supportive role, and the learner explores, participates and is more active during the learning process. This change of philosophy is having numerous ramiﬁcations within the academic world. Entire degree programs are re-organized in order to accommodate the new role of the student. Teaching staﬀ needs to adapt their pedagogical techniques to a, sometimes, totally new approach. Together with these changes, numerous accreditation institutions have emerged with the objective of assuring that educational institutions embrace quality assurance and sustained innovation techniques. For example, ABET (Accreditation Board for Engineering and Technology) is an institution that provides accreditation for degrees in the area of applied science, computing and engineering education. The focus of the accreditation process is on “what is learned rather than what is taught” (www.abet.org). 
 The approach described in this document is being deployed in an engineering degree that currently pursuing accreditation by ABET. The institution describes what students are expected to know and be able to do by the time they graduate. Again, in the speciﬁc case of engineering education, some of these outcomes have a strong procedural nature. For example: “(k) an ability to use the techniques, skills, and modern engineering tools necessary for engineering practice.” [1]. In order to assure that on graduations students are capable of using modern engineering tools, they need to practice with them through activities. This is an example of the type of new outcomes that are being requested from applied science degrees that are diﬃcult to accommodate by conventional LMS. At most, LMSs may cover this aspect of the learning process by supporting on-line quizzes, but as an indirect measuring tool. A second example of the limitations of LMSs is highlighted by another program outcome: “(d) an ability to function on multi-disciplinary teams”. Teamwork requires a high degree of student-student interaction. There are studies that rely on interaction through forums hosted in an LMS to gain insight on the level of collaboration within teams [2]. But if students are already used to communicate using a variety of Web 2.0 type of tools, it is highly unlikely that when immersed in a collaborative setting, they would use an LMS for these tasks. From the previous observations, there are several questions that lay ahead in the area of learning analytics: – How much do they rely on interaction taking place in the LMS? – How can they cope with new forms of interaction? – How are they aﬀected when analyzing interaction in collaborative environments? This document describes the approach to obtain learning analytics in a concrete scenario of collaborative activities within a course of an engineering degree. Although still in the preliminary stages, we believe there are several observations that can help shed some light on the previous questions. 
 2 Approach.
 The approach was deployed in the face-to-face course “Systems Architecture”, which is part of the degree in Telecommunication Engineering (www.it.uc3m.es/labas/syllabus_en.html). The total number of students that initially signed for the course was 248 and were divided into ﬁve sections groups. The course contained the following learning outcomes: 
 1. Design and development of applications in the C Programming Language. 2. Use proﬁciently the tools for application development. 3. Apply team working techniques to develop an application for a mobile device. 4. Use of self-learning techniques. 
 Outcomes 3 and 4 refer to generic methodological aspects. Team work was used during the second half of the course (six weeks) in which groups of four students were created by the instructors to work in a project. Several documents about team dynamics were requested as readings and a class session was devoted to discuss teamwork, agree on a team contract and discuss the diﬀerent type of conﬂicts that may arise. The measures to achieve outcome 4 were applied throughout the entire course. Each session had two sets of activities, previous and in-class. The set of previous activities required an objective that would be reviewed in the following class. Students found this methodology signiﬁcantly diﬀerent to those used in other courses. The course followed a continuous evaluation scheme. Five partial examinations spread along the semester were combined with small exercise submissions. The goal was to engage students to regularly work in the course. The ﬁnal course grade was simply the sum of all these partial scores; no ﬁnal exam was given. 
 2.1 Providing a fully conﬁgured development environment.
 The main complication from the point of view of analyzing the interaction derived from outcomes 2 and 3. In order assure that students use proﬁciently the tools for application development, they required a development environment fully conﬁgured and, most importantly, with high availability (to promote oﬀclass work and do not overload computer rooms). This type of environment was clearly beyond the reach of the institutional LMS, and therefore, the possibility of observing the interaction with these tools was initially non-existent. The adopted solution was based on the use of a virtual machine. Lately, virtualization has been considered in education in order to easily facilitate students fully conﬁgured machines that can execute with barely any conﬁguration steps in their personal computers [9]. The use of this this approach had several beneﬁts. First, all students had initially the same exact set of tools properly conﬁgured which greatly simpliﬁed the design of activities to use them. Second, the machine was conﬁgured so that students could access the ﬁles stored in their regular personal computers. Third, the virtual machine (although including a fully conﬁgured operating system) was portrayed to the students as the application to use when working on the course material. And last, but most importantly, the machine included a system to record the events occurring with respect to the installed tools. More precisely, the monitoring mechanism was capable of recording the following events: – Power-up and shutdown of the machine. – Invocation of a previously selected set of tools. – Internal commands used by the students in some of the development tools – Historic data about the sites visited with the included browser With this mechanism, a wide variety of interaction events that otherwise would be ignored, were recorded and stored in the virtual machine. 
 2.2 Support for a shared folder.
 The use of the virtual machine was combined with support for a web-based folder shared among the team members in which students could store any ﬁles they needed related to the course. More precisely, a ﬁrst folder was created for each pair of students during the ﬁrst half of the course, and a second shared folder was created for the teams formed in the second half of the course. Instructors had access to the shared folders of all the teams under their supervision. This addition turned to be a powerful communication channel not only among students, but also between students and instructors to solve problems, check errors, and generic consultations. During the conﬁguration phase of the virtual machine described in 2.1, the shared folder was conﬁgured as the repository where all the recorded events were stored. A non-intrusive procedure would be in charge of sending the recorded events whenever the students submitted a new version of the ﬁles in the webbased folder. In order to comply with the current legislation, the virtual machine was downloaded only by those students that agreed with the terms of use described in a document. Furthermore, the machine was conﬁgured to boot up with the browser open and showing a page explaining the recording mechanism, the steps to disable it, and the contact person to exercise the rights over the collected data (delete, query and modify). Figure 1 shows a screen capture of the initial desktop of the virtual machine. 
 2.3 Support for actuators.
 With the conﬁguration described in the previous session, the virtual machine can be thought of as the application that students need to use when working on the course material. But being outside of the LMS not only poses a challenge to record and collect data, but also to the step of acting. Although the experience is only at the ﬁrst stages, and as such, only the data collection aspect has been deployed, a solution has been considered and conﬁgured to be able to act on this environment. A widget displaying the content of a pre-deﬁned folder has been installed in the desktop of the machine. Initially, the folder contains the terms of use for the machine (that the student agreed to). The widget is shown in the upper left corner of the desktop illustrated in Figure 1. Using a technique similar to the one to send the recorded events, a new set of ﬁles can be uploaded to this special folder such that their corresponding icons appear in the desktop widget. 
 Fig. 1. Initial screen of the virtual machine.
 With the described conﬁguration, students would see how the folder shown in their desktop keeps changing its content. The type of resources that can be added range from ﬁles (documents, audio, video) to URLs to access remote resources. 
 2.4 Encoding the events.
 The events recorded in the student machines oﬀer a very detailed account of the procedures followed as well as the tools that were invoked. The captured information has been encoded using the CAM (Contextualized Attention Metadata) format [19, 22, 18]. CAM provides a data model for representing user activity together with contextual information. Educational application of the CAM framework are discussed in [20], where the tool CAMera for monitoring and reporting on learning behavior is described. CAMera collects usage metadata from diverse various applications, represent these metadata with CAM and reports them to the learner. More complex applications in the scope of adaptation and web-semantics have also been built based on this format [16, 16]. 
 3 Initial Results.
 The initial objective of this approach is to explore ways to extend the datagathering phase of learning analytics beyond the LMS and into environments in which a signiﬁcant amount of interaction is taking place. The initial conditions were also to deploy the data gathering in a scalable and non-intrusive way. The virtual machine was made available at the beginning of the course. Out of the 248 students that signed out for the course, a total of 220 downloaded the machine (88.71%). Out of the remaining 28 students (11.29%), most of them opted to use their own conﬁgured environment. The large percentage of students that decided to use the machine shows its acceptance as the course tool. The number of downloads, though turned out to be not a good estimation of the true activity carried out by the students in those machines where the recording mechanism was not disabled. The events received in the ﬁrst half of the course (in which students worked in all the activities in pairs) were 48, 342 for a total of 115 students (an average of 420 events per student). 
 3.1 Activity outside the LMS.
 An important side-eﬀect of placing the data-gathering phase outside of the LMS and into a fully conﬁgured environment was to be able to measure the percentage of URLs that were related to the LMS. In other words, by exploring the events encoding a visit to a URL we can have a ﬁrst look at the percentage of traﬃc that goes to other sites. Out of the almost 49, 000 events, 15, 507 (32.07%) were events in which a URL was opened with the browser. When counting the number of unique URLs, this number falls down to 8, 669. Out of these, only 2, 471 (28.51%) pointed to the LMS. An initial interpretation (pending a more thorough analysis) seems to suggest that students interact with a large number of resources that are outside of the LMS. 
 4 Discussion and Future Work.
 In this paper a context has been described in which in order to assess the degree of interaction that students are having with a previously detected set of tools and among themselves, the LMS oﬀers a very poor coverage. The context is derived from the adoption of learning outcomes that require procedural activities with tools and functionality beyond the scope of a conventional LMS¿ The described approach proposes extending the scope of the data-gathering techniques to include a fully conﬁgured virtual machine containing all the required tools as well as a mechanism to record a subset of the most representative events. A detailed description of the terms of use of the machine with instructions on how to disable the recording mechanism, as well as how to check, modify or delete the information, was included with the machine. The machine is also conﬁgured to establish a bidirectional communication channel with a central server to send the recorded events and receive new resources that are shown in a desktop widget as actuators on the learning environment. The received data has been encoded using the CAM format and is being prepared to perform a more sophisticated algorithm to detect special patterns to detect early which students are not using properly the given tools. The work is still in its preliminary stage in the sense that only the datagathering stage has been successfully deployed. Still, the approach has been shown to be scalable (more than 200 students) and in-obtrusive (students do not sense that the events are being recorded). The obvious line for future work is to identify those variables of the recorded events are more suitable to make predictions of those students that are not using properly the tools included in the machine. A second line of work has also been conceived to combine the recorded events and the information extracted from the LMS to detect potential anomalies in the collaborative part of the course. The challenges in this context are bigger because teams are suppose to meet regularly on face-to-face meetings in which there is no type of event recording. 
 Acknowledgment.
 Work partially funded by the Learn3 project, “Plan Nacional de I+D+I TIN200805163/TSI”, the Best Practice Network ICOPER (Grant No. ECP-2007-EDU417007), the Accion Integrada Ref. DE2009-0051, and the “Emadrid: Investigacion y desarrollo de tecnologias para el e-learning en la Comunidad de Madrid” project (S2009/TIC-1650).]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Stepping out of the box. Towards analytics outside the Learning Management System</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>learning management system</dc:subject>
		<dc:subject>virtual machines</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/abelardo-pardo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/abelardo-pardo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/49/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/abelardo-pardo"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Discourse-Centric Learning Analytics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50/authorlist"/>
		<swrc:abstract>Drawing on sociocultural discourse analysis and argumentation theory, we motivate a focus on learners’ discourse as a promising site for identifying patterns of activity which correspond to meaningful learning and knowledge construction. However, software platforms must gain access to qualitative information about the rhetorical dimensions to discourse contributions to enable such analytics. This is difficult to extract from naturally occurring text, but the emergence of more-structured annotation and deliberation platforms for learning makes such information available. Using the Cohere web application as a research vehicle, we present examples of analytics at the level of individual learners and groups, showing conceptual and social network patterns, which we propose as indicators of meaningful learning. Keywords: Learning Analytics, Discourse Analytics, Discourse Analysis, Argumentation, Sensemaking, Social Network Analysis, Web Semantics</swrc:abstract>
		<led:body><![CDATA[ 1. Extract of a real Facebook Dialogue. 
 Subject-line threading, tagging, or anchoring commentary to a document structure do provide mechanisms, albeit weak, to navigate thematically. Such environments minimize the effort required by the user to add a contribution (they just click “comment” or “reply”), but the trade-off is that important phenomena that can signify more reflective learning conversations are hidden in the free-text content, making it harder for both participants, and the software platform, to understand questions such as: What are the key issues raised in the conversation? What are the emerging questions? How much support is there for this idea? Who disagrees, and what evidence do they use? What kind of argument is made to support this? One must simply read the entire online conversation, along with all the “noise” which cannot be filtered. In order to address these limitations, we are now seeing the emergence of robust tools for more structured deliberation and argument mapping [9-11], prefigured by extensive CSCL research [12-13]. These tools are now finding application in many forms of knowledge work which require clear thinking and debate, including learning, scenario planning, and policy formulation. As part of our broader conception of Hypermedia Discourse [14], we have developed a web application called Cohere [15], providing a medium for engaging in structured online discourse, or for summarising/analysing it, e.g. as a moderator, educator or researcher. Following the approach of structured deliberation/argument mapping, Cohere renders annotations on the web, or a discussion, as a network of rhetorical moves: users must reflect on, and make explicit, the nature of their contribution to a discussion. A simple example illustrates how a Cohere discussion is different from typical social network interaction, such as in Facebook (Figure1). In the example dialogue (Figure 1), Van has proposed a dinner for the 10th of October which Miriam cannot attend, while Anna and Aurelie can make it. From how the dialogue is represented, it is not evident at first that the central dinner-invitation post is the second one, nor what the positions of other participants are. The only way to make sense of the conversation is by reading it whole. While this is feasible with just three participants and 5 posts, this does not scale for complex debates with many participants. Cohere aims to address this problem with two key extensions: • Adding icon types to the posts • Making semantic connections between posts (see Figure 2). 
 Figure 2: Shows how Cohere augment common online dialogue text: the icons at the side of each post shows rhetorical role of the post and the semantic connections the rhetorical move between posts. 
 With Cohere, users can pick an icon to associate to their post, which explain the rhetorical role of that post in the wider conversation (e.g. Van is raising and idea/option to make a dinner on Friday, Miriam presents a con to the proposal: since she has guests that w-e, Anna present a pro in favor of Van proposal: there will be no Lab dinner in that w-e, etc) (Figure 2). Moreover with Cohere users can explicitly connect their post to the post which is relevant to what they want to say. They can do so by making a connection between posts, which explain the rhetorical move they want to make in the conversation (i.e. Anna and Aurelie agree with Van’s idea while Miriam disagrees) (Figure 2). Cohere augments the online conversation by making explicit information on the rhetorical function and relationship between posts. Moreover users can browse the online dialogue not as a linear text but as a semantic network of posts (Figure 3). 
 Fig.3. Cohere’s environment in which online dialogue is represented as semantic network of posts. 
 By structuring and representing online discourse as semantic network of posts Cohere enables a whole new way to browse, make sense of, and analyze the online discourse. In this paper we discuss what it mean to use Cohere’s online dialogue environment to monitor online learning activities and develop useful learning analytics, by starting on the analysis of the online discourse which learners are involved in. In particular in the next section we describe how discourse analytics can enable a deeper understanding of the online discourse, of the participants to the discourse and the social and learning dynamics. 
 3 Learning Analytics on Discourse Elements.
 In the previous sections we have described why discourse is a key indicator for learning (section 1), and we have described the specific type and technology for online discourse we will be focusing on: Cohere (section 2). In the following we present several examples of simple learning analytics based on discourse elements. These examples are intended to work as a proof of concept for the potential of discourse-centered learning analytics that is to say a focus on learners' discourse as promising site to identify patterns of meaningful learning. Cohere introduces two main discourse elements in an online conversation: 1. The post type: which is represented with an icon and label, and expresses the rhetorical role played by the post in the wider online conversation (Figure 2 and 3); 2. The semantic connection: that is represented by a link and label, and expresses the rhetorical move the author of the post wanted to make in the conversation, and toward a specific post or participant (Figure 2 and 3). In the following we describe what kind of learning analytics can be done on those two discourse elements and give concrete examples of how Cohere can provide learning analytics per learner and per group, to identify: • learners’ attention: what do learners focus on? What problems and questions they raised, what comments they made, what viewpoints they expressed etc. • learners’ rhetorical attitude to discourse contributions: With what and who do a learner agrees/disagrees? What ideas he supports? What data he questioned? • learning topics distribution: What are the hottest learning topics, by who they have been proposed and discussed? • learners’ social interactions: How do learners act within a discussion group? What are the relationships between learners? 
 4 Analytics per Learner.
 Cohere provides two main types of learning analytics: analytics per learner and analytics per group. In the following we discuss the main analytics per learner which consist of two tables (Node Type table and Link type table); and two lists of connections (comparing thinking and information brokering connection list). We will use as example data the statistic for Rebecca, a semi-experienced Cohere user, which has been also involved in one of the use cases described in the following section (section 5). 
 4.1 Analytics on Post Types: Analyzing Rhetorical Roles.
 The table on post types (called node types) counts, shows the variety of types of posts that the learner has added to the conversation, and with how many posts of each type he has contributed. Statistics on node types measure the rhetorical role of the comment that the learners are making, (e.g. Does she ask many Questions? Does she contribute Data, or just Ideas?) and therefore provide an indicator of learners’ attention and performance. For instance higher the number of created posts, higher the learner’s engagement in the discussion: for instance a higher number of theory type posts may indicate a learner’s interest in theoretical issues. Statistics on the node type can be also interpreted as a way to classify the role of that learner in the group. For instance, does he work as the person providing answer? He could be imagined as a point of reference-tutor. If we look at a concrete example, in Figure 5, Rebecca has contributed mainly with ideas, general opinions, and she has offered several international perspectives on the conversation. We may also notice that in just three posts she has raised questions. By looking at the node type table it is possible to evaluate learner’s performance connecting the discourse outcomes with the specific learning goal. For instance, there may be learning scenarios in which the learning goal is to share online resources, therefore in those cases the scores on “Data” posts type would offer a useful figure on how the learner performed in this task. 
 Figure 5: Node types table: shows the different typologies in which the learner classified his posts, and the counts of how many times each type had been used in the conversation. 
 With the post types table, Cohere draws a picture on the kind and quantity of contributions to the conversation that the learner has given and therefore, in different learning scenarios, it may enable inferences on how the learner has performed in the specific learning task. 
 4.2 Analytics on Link Types: Analyzing Rhetorical Moves.
 The table on link types (Figure 6) describes the rhetorical moves that the learner has made in the conversation. It gives the list of all link types, used by that learner, to express his ideas and connect them with other people’s ideas. The list will be ordered as descending on the number of time the semantic link has been used. Three main elements can be observed by looking at this table: the language the learner uses to describe his thinking, his attitude toward the discussed topic, and how this language and attitude are similar or different from other learners in the same group. In order to evaluate learners’ attitude Cohere classifies semantic links type into three categories: Positive, Neutral and Negative (Figure 6). Positive link types are represented with a green link and label, and express positive rhetorical moves such as i.e. supports, agrees with, improves on, is consistent with, predicts, proves, solves the problem etc. These categories provide indicators of the attitude a learner had toward the learning task and within the conversation. In example, in the figure shown (Figure 6), we can see that the example learner has maintained a positive stance within the conversation. 
 Figure 6: Link types usage statistics. Arrows show three connections categories - Neutral, Positive and Negative. 
 In fact, in Figure 6 green links score quite high into the table and the most of the learner’s contributions to the conversation have been devoted to identify: consistencies, coherences, answers to questions raised by herself or other learners etc. More precise statistics can be easily obtained from this table. For example, the percentage of positive contribution can be calculated as: 
 FORMULA_(1).
 %P is equal to the summation of the number of times that the learner used a green link ( N gl ), extended to all the green links (g=1,..G; where G is the number of different green links type), divided for the total number of links he has created (T), and multiplied by 100. 
 This percentage can be used as an indicator of the learner’s positive attitude toward the learning task, and toward other learners, within the online discourse. The same calculation can be repeated for neutral and negative link types. This would give the following analytics for our example learner (Table 1). These statistics can be extended to all learners in a class or in a group of inquiry and therefore compare their attitude within the online conversation. 
 Table 1: Example of Link Type Analytics for a learner. 
 The second observation, that can be made analyzing the link types table (Figure 7), concerns the learners’ language used to describe their rhetorical moves. Different learners may give different meaning, or nuances of meaning, to the same concept and this may mirror in using different terms to express the same concept, or the same term while referring to quite different meaning. If we look at Rebecca’s list of rhetorical moves, we can notice that she classified as gray (neutral) rhetorical moves which refer to descriptive turns in the conversation, aiming to: i.e. identify similarities, illustrate, give examples and discover relationships. And the classification is quite coherent, in the sense that all the link types she has chosen convey the same descriptive meaning. In the same way, the green links seem to represent positive moves such as: improvement, support, problem solving etc. We may notice though that she classified as positive the relationship “causes”, which has a quite ambiguous interpretation. In fact other learners may have classified “causes” as neutral link type since causal moves may imply both positive and negative consequences and therefore the term does not bring per se a positive interpretation. Presented with link analytics of this sort, an educator might ask questions such as: Why did Rebecca classify this term as green? How did other users interpret this rhetorical move? Comparison between different learners link types table, and different analytics on the data can support the understanding of those deeper reflections on learners’ use of language. For example, Figure 7 shows how four users who all participated in the same investigation, used the five link types which were calculated to have highest usage within the group. 
 Figure 7: Comparing four users’ usage of link types. 
 Usage is a relative metric, which may be constrained to specific users group, or to specific users type (I.e experts, or non-expert). For instance, if we want to calculate the usage of a semantic link type by the most expert Cohere users, we can calculate the weighted summation of the number of time the link type has been used from the top 20 users, following the formula: 
 FORMULA_(2).
 
 where: - i is the semantic link type, and varies from i=1,..n; where n is the number of all the different semantic connections type used by the top 20 Cohere users. - u is the user and vary from 1,..20 for the top 20 Cohere users - N ui with u=1,..20 is the number of times the user u has used the connection i; - and the weights w i are obtained by calculating the topological matrix of semantic-type/user and summing the row elements to obtain the number of users which used that same link type. 
 In other words, for each link type i, the weight w i is proportional to the number of users that used! link type. The weight is a measure of popularity of each link type that within the top 20 expert Cohere users, and the more popular is a link type the more it will score in the Usage calculation. By applying this formula we choose popularity as the main factor to determine if a semantic link type is used, in other words we make sure that if just one user has used a semantic link type many times, but nobody else has, this link type will not score high in the link type usage. 
 4.3 Learners’ Attitude to Compare Thinking.
 An important aspect of learning is the capability of the learner to think critically and reflect on his personal point of view by comparing it with others. When a learner decides to connect his idea with another person’s idea this could serve as an indicator of reflective thinking, insofar as the learner is assumed to have read the target node, and reasoned about the relationship with his own idea, in order to select a link type. Cohere counts and lists for each learner all the links in which she has connected his opinion to another person’s contribution to the online conversation (Figure 8). In the picture we can see that for each connection there are three user icons, which represent: the link’s author (at the center under the semantic connection label) and the authors of the two posts that are being connected. Compared thinking statistics counts the connections in which the link author (at the center) is also author of one of the two connected posts. 
 Figure 8: List of semantic connections in which learners have compared their thinking with other learners. 
 4.4 Learners as Information Brokers.
 Another analytic afforded by semantic discourse of this sort concerns the degree to which users’ act as information brokers between others. Since connecting is an explicit, reflective act in Cohere, it is straightforward to count how many times learners create semantic connections between nodes authored by others (Figure 9). 
 Figure 9: List of semantic connections in which a learner acted as information brokers. 
 5 Discourse Network Analyses and Visualization: Analytics Per Online Discussion Group 
 Cohere calculates different kinds of statistics on online data generated and shared by group members. The group statistics summary shows two main factors: • Discourse element statistics, such as the most popular link type and the most popular node type • Discourse network statistics, such as i.e. the node type, the post and the learner with highest degree centrality. Let us elaborate the properties of the discourse network. Its structure consists of two superimposed networks that are assumed to be strongly connected: • Concept network – which relates the nodes that learners created. • Social network – which relates learners that participate to Cohere discussions posting Ideas, Questions and Arguments etc; For data analysis we considered that, in the concept network, the posts are the nodes, and the semantic relations among posts indicate the edges; whereas the social network maps the pattern of relationships among actors. In particular, we considered the users as nodes and we measured the edge between two users by counting the times that a user created a semantic connection that targeted a post authored by another user. In the following tables, we show the different meaning that each Network Analysis metric has in the social and concept network adapted to our context. 
 Table 2. Ego-network measurements. 
 Table 3. Network measurements. 
 In order to provide some concrete examples of how the above network metrics can be used to analyze learners activities in online discussion groups we present two use cases. In the first use case (OLnet team discussion) Cohere has been used by a group of researchers to annotate the document of a project proposal, and to reflect on which areas of the proposal they were making a contribution. With Cohere’s Firefox sidebar users can annotate the document and share their annotation in the group discussion environments. These annotations are initially presented as list of posts presented in reverse chronological order within the discussion group. After this initial phase of reading and annotating the document, participants were asked to have a group discussion on the main research questions addressed by the team, the main project achievements and how they related to the project goal. In order to do this, they had to create new posts in which they described more general reflections on research questions, goals and activities and then they had to start creating semantic connections between the document annotations and the posts (for more info on the Cohere’s user interface and how to build semantic connections and discourse networks through Cohere please refer to [16,17]). 
 This resulted into a discourse network in which document notes, open questions, ideas and other posts’ type are connected, and node’s icons and links express the rhetorical role and move played by each post into the online discourse. In the second use case (COP15 discussion) four researchers have used Cohere to collaborative annotate web news, documents, blog posts etc about the United Nation Climate Change Conference COP15. Results of the web annotations have then been used to inform an online dialogue on the main issues tackled during COP15, as reported by the press or as micro and macro blogged by participants to the conference. In order to have a specific focus for the discussion participants choose to discuss one of the public's top questions that have been suggested on a Open University Platform (see page: http://www.open.ac.uk/platform/join-in/your-votes/question-bypopular/Climate%20Change); that is: • How do we know that climate change is real and we're not just experiencing a weather cycle? Participants were asked to explore and annotate key Open Educational Resources (OER) and Social Media pages (such as Blogs, Wikis, Twitter streams, and web pages in general) with ideas to help answering the tackled question. Moreover they were asked to make connections between their ideas and other participants’ ideas. In this process the main driving question and the identified relevant OERs have been used as evidences to base claims/ideas. This resulted in a web of ideas and annotated resources on the issues at stake, meaningfully connected into a discourse network. In the next paragraph we will describe how statistic on discourse network can provide insights on the contents of the group discussion and on the conceptual and social interaction between group members. To analyze both the online group discussions and to compute some of above mentioned Network Analytics, we used UCINET tool [18]; instead we used NodeXL tool [19] for both concept network and social network visualization. 5.1 Concept Network Analysis and Visualization 5.1.1 Link distribution: Is the network topology hub and spoke or random? The first analysis that we conducted on the datasets of the two use cases looks at links distribution to assess the presence of hub users and hub topics. The existence of hubs indicates the presence of hot topics/posts or key/most-active users. Network’s hubs are nodes with the highest degree centrality. From the analysis of link distribution, emerges that both OLnet and Cop15 discussion groups are characterized by a power law distribution. The power law tail indicates that the probability of funding posts with a large number of links is rather significant; this means that the network connectivity is dominated by few highly connected posts [20]. As illustrated in the two histograms below (Figure 10), in both the network it is possible to identify a hub with a highest degree followed by smaller ones. From the analysis emerges that the hub is a post labeled #COP15 and classified as “idea type”. The hub post has been connected to many other posts, which present annotations of various web resources. The learner who created the post was in fact using the hub to cluster those resources under the # tag “COP15”. This highlights a use of Cohere in which the learner, more than dialoguing is rather mapping out his notes on web resources and then sharing them with the group within the online discourse. 
 Figure 10: Link distribution histograms for COP15 and OLnet Team discussion groups. 
 A different case is the OLnet discussion group, which highlights a use of Cohere as tool for collective inquiry. In fact the discussion presents two hubs, both with degree equal to 8: • “What motivates registered users to learn in socio-collaborative ways on OpenLearn?” which was classifies by the author as “idea type” post; and • “How can we build a robust evidence base to support and enhance the design, evaluation and use of OERs?” which has been classified as a “question type” post. The first thing that we can notice is that the two hubs are both posts which present an open question to the group. This seems to suggest that within all posts’ types, questions have a higher discourse power, in that they trigger learners’ participation and interactions. Of course more systematic observations on wider and different online discussion groups are needed to appropriately test this hypothesis. Other considerations can be also done by looking at the hub posts’ type. The learner who authored the first hub did not correctly classify the rhetorical role of his post within the wider conversation. The post clearly states a question but it has been classified as “idea”. This may be due to misunderstanding of the learning task or to less confidence in the use of the technology; in any case this observation would alert a tutor on the learner performance. This example, in the same line with the analysis done in section 4.1, highlight the value of using posts type to classify the rhetorical role of the posts within the online conversation. 5.1.2 Components analysis to compare the struceture of OLnet and Cop 15: Are the networks connected? The second analysis which has been conducted on the two datasets consists in assessing the presence of components. A component is a connected subset that composes a disconnected network. Within networks’ components there are no links/paths between the nodes belonged to different components. Therefore network components identify isolated subsets of people or topics within the discourse network. If we look at the social network this analysis assesses the degree to which a network is disconnected: for instance: a social network which is fully connected has only one component. While if we look at the concept network the analysis assesses the number of different subtopics discussed in a group and at the same time. 
 From the analysis emerges that both networks present several components and this implies that the networks are weakly connected. In details, COP15 group presents 9 components but the bulk of nodes belong to two components. OLnet group presents 10 components but the bulk of nodes belong to one component. The presence of components in each group can be interpreted as the emergence of different sub-discussions independent among them. Analyzing the size of each component, (number of node in each component) emerges that not all the subdiscussions are developed by learners in the same way. Bigger components can be interpreted as hot sub-topics which attracts a greater interest than others. We can also notice that the number of posts in the group discussion may have an influence on how hub topics distribute. For instance if we compare the two groups we can notice that in more developed discussion groups, such as COP15 group, two components absorb the bulk of nodes (161 out of 178). While in a group with less posts, as OLnet group, the bigger component absorb less than 45% of the total nodes. This could indicate that at the beginning of the discussion, learners try to explore a wider deliberation space talking about different aspects of the same topic; then gradually, they start to focus on few sub-topics and to deepen them. This hypothesis would need to be proof/disproved by more in depth analysis, but consideration on the line of these give an example of how analyzing network metrics can inform the understanding of group dynamics. Finally specific network visualizations can be drawn to focus on the main network analysis metrics. The next figure (Figure 11) shows results of the concept network visualization for the OLnet Team discussion group, done with NodeXL [19]. 
 Figure 11: Concept Network Visualization of OLnet team discussion).
 In particular in figure: • Edge shape depends on link type (Positive=solid line; Neutral=dotted line, Negative=dashed lines) • Edge width depends on the frequency of the relationship • Vertex size depends on the degree centrality 
 5.2 Social Network Analysis and Visualization.
 One approach to studying collaborative environment, as well as collaborative network, has been the application of Social Network Analysis. The phrase “social network” refers to the set of actors and the ties among them. The network analyst would seek to model these relationships to depict the structure of a group. One could then study the impact of this structure on behaviour of the group and/or the influence of this structure on individuals within the group [21]. Focusing on depicting the structure of the network, the paper applies the main structural measures of SNA to Cohere’ discourse network in order to analyze the typology of network which emerges from the online discussions. In the following we present the Social Network Analysis (SNA) for the OLnet discussion group. The SNA measurements that we consider in our analysis are: out degree and in degree centrality. We adapted the meaning of these two measures to our case, indeed: • Out degree measures the users’ activity level; • In degree is a sort of indirect measure of quality and relevance of a user’s posts. In the table (Table 5), we show the results that emerge from the analysis of OLnet group social network. The node more active is Learner 1. Her outdegree is equal to 11. It means that she creates 11 links among different posts. While, Learner 6 is the learner with the higher indegree value. Her indegree is equal to 11. It means that L6’s posts are considered more interesting and/or relevant by the group. 
 Table 5. Outdegree and Indegree values for the Olnet Team discussion (11 participants). 
 By using the network metrics detailed in Table 2 the SN of the OLnet Team discussion group can be represented as follows (Figure 12, NodeXL tool’s visualization [19]). 
 Figure 12: Social network representation of the OLnet Team discussion group. 
 In particular in figure: • Link width indicates the frequency of relationship (reply). • Edge shape indicates the link type (positive: solid line, neutral: dotted line, negative: dashed line). The final shape depends on the prevalence of one of two link type. • Vertex size depends on the in-degree centrality of each users (bigger node have higher out-degree centrality). • Vertex colour depends on the out-degree centrality (more black sphere have a higher in-degree value instead grey node have lower indegree value 
 6 Conclusions.
 Drawing on Mercer’s socio-cultural discourse analysis and argumentation theory, we have motivated a focus on learners’ discourse as a promising site for identifying patterns of activity which correspond to meaningful learning and knowledge construction. However, in order for online discourse to deliver on the promise of learning analytics, software platforms must gain access to qualitative information about the pragmatic dimensions of conversational contributions, that is, the rhetorical dimensions to a contribution. We identified the emergence in recent years of more-structured deliberation platforms on the Web, descended from hypertext research systems in the 1980s. Users of such tools make explicit certain classes of information which is very difficult to extract from naturally occurring text, although we are now investigating computational linguistics tools for detecting rhetorical gestures [22] In particular, we are interested in the rhetorical role that a user’s contribution is making to a document or conversation (e.g. identifying a problem, responding to a query, challenging or supporting a viewpoint, contributing new data), and the nature of the connection to other contributions using semantic relationships. 
 Using the Cohere system as an experimental vehicle, we have presented examples of learning analytics at the level of individual learners and groups to better understand: • learners’ attention: by analyzing the specific type and quantity of contributions to the conversation that a learner gives it is possible to measure several aspects of the learner performance (section 4); • learners’ rhetorical moves within the online discussion: By analyzing the semantic connections between posts we can enhance our understanding on the different ways in which learners participate to the conversation. Moreover we can make consideration on what attitude they have toward the discussed topic and the role they play within the group (section 4); • learning topics distribution: by applying concept network analysis we can identify what are the hottest learning topics and by who they have been proposed and discussed. Moreover we can see how topic and subtopic distribute within the online conversation (section 5); • learners’ social interactions: by applying SNA we can map how learners act within a discussion group, what are the relationships between learners, where and how much they interacted with each other, and who is the key learner and why (section 5). These examples don’t aim to present in depth analysis of the collected use case data, they are rather meant to give a proof of concept of the potential impact of discourse-centric learning analytics in the study of CSCL. Based on those examples we argue that discourse-centric learning analytics such as these enable more in-depth reflections on learners’ activities than would be possible to achieve with quantitative analysis of lower level actions (such as mining of logs on how many resources are downloaded, how many times they have logged into a system, how many comments they have made, how long they have spent on a task, etc). This is partially due to the nature of the analyzed data. Discourse-centric learning analytics are based on data that makes explicit the learner’s cognitive context (e.g. what kind of rhetorical move the learner wanted to make with a comment, what meaning he gave to a connection, what contrasting viewpoints he detected etc). By analyzing more richly expressed and structured data, discourse-centered analytics can augment the level of accuracy and the cognitive depth of the inferences that can be made on where and how learning happens. Acknowledgments We gratefully acknowledge the support of the William & Flora Hewlett Foundation, through their funding of the Open Learning Network Project (www.OLnet.org).]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Discourse-Centric Learning Analytics</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>discourse analytics</dc:subject>
		<dc:subject>discourse analysis</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-de-liddo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-de-liddo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ivana-quinto"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ivana-quinto"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/michelle-bachler"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/michelle-bachler"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/lorella-cannavacciuolo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/lorella-cannavacciuolo"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-de-liddo"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/ivana-quinto"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/michelle-bachler"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/lorella-cannavacciuolo"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/51">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>SNAPP: A Bird's-Eye View of Temporal Participant Interaction</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/51/authorlist"/>
		<swrc:abstract>The Social Networks Adapting Pedagogical Practice (SNAPP) tool was developed to provide instructors with the capacity to visualise the evolution of participant relationships within discussions forums. Providing forum facilitators with access to these forms of data visualisations and social network metrics in ‘real-time’, allows emergent interaction patterns to be analysed and interventions to be undertaken as required. SNAPP essentially serves as an interaction diagnostic tool that assists in bringing the affordances of ‘real-time’ social network analysis to fruition. This paper details the functional features included in SNAPP 2.0 and how they relate to learning activity intent and participant monitoring. SNAPP 2.0 includes the ability to view the evolution of participant interaction over time and annotate key events that occur along this timeline. This feature is useful in terms of monitoring network evolution and evaluating the impact of intervention strategies on student engagement and connectivity. SNAPP currently supports discussion forums found in popular commercial and open source Learning Management Systems (LMS) such as Blackboard, Desire2Learn and Moodle and works in both Internet Explorer and Firefox.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 Socio-constructivist theorists advocate for the use of collaborative learning activities as a process for promoting student understanding [1]. Although traditionally, collaborative learning activities were conducted in on-campus settings, there has been an increasing shift for these practices to be facilitated through the online context. This has been driven, in part, by increasing student diversity and a demand for greater course flexibility. As such, the adoption of online learning activities is becoming commonplace within contemporary education practice. At many universities the Learning Management System (LMS) is seen as the primary vehicle for enabling online collaborative learning activities. Commercial and open source LMS such as Blackboard, Desire2Learn, Sakai and Moodle are generally centrally run systems that are made available to faculty. Despite the vast array of pedagogical benefits these technologies bring, teachers frequently note that the online environment lacks the student learning cues that are readily obtained in more traditional modes of education delivery (face to face) [2]. For instance, the classroom cues that assist teachers in identifying which students require support, are actively engaged or have cognitively “checked out” of the class activities. These types of formative feedback mechanisms are critical for instructors to better adapt the flow, language, and structure of the lesson in ‘real-time’ to maintain engagement and better promote understanding. While there exists a vast array of learning cues in the LMS and other integrated student online systems, there is a disconnect between the student tracking and reporting processes and subsequent instructor pedagogical interpretations [3]. Instructors and tutors require the ability to gauge student activity and interaction so that they are able to better optimize and adapt the learning activities that take place within the online collaborative learning environment. However, current versions of LMS tend to only contain basic interaction statistics such as the number of times a page was viewed, or the number of messages posted or read. These forms of basic content access statistics do not provide the necessary evidence of learner participation and engagement within the learning network. Thus, there are minimal informed opportunities for instructors to identify students requiring learning support, at risk of attrition, or dis-engaged from the learning network. The development of rich student participation and interaction data married with effective and easily interpretable visualization processes, are critical to aid online instructors in their teaching tasks. In this context, Social Network Analysis (SNA) provides a framework for merging both complex group and individual data sets with easily interpretable visual representations. Numerous case studies have demonstrated the value of SNA as a means to assess participant interaction within online learning environments [4-7]. For instance, case studies conducted by De Laat [4] and Aviv et al., [7] used SNA to address research questions related to the level of participant activity, identification of central participants and network density. SNA has also been used to assist instructors in identifying isolated students [8], provide evidence of group cohesion [7] and creativity [9]. While there has been increasing applications of SNA for educational purposes, the full potential of SNA for providing ‘‘real-time’’ data remain largely unrealised. This is especially pertinent for online collaborative learning environments [10]. The importance for providing instructors with ‘‘real-time’’ data related to relationship establishment is critical to inform instructors of any necessary modifications to activities to better scaffold learning content [11] and promote diversity of engagement [12]. There is strong evidence to suggest that within well structured activities, knowledge construction processes reach higher levels of critical thinking and that students are able to establish and sustain cohesive groups [7]. This evidence serves to substantiate the need for automated SNA tools that are able to deliver ‘‘real-time’’ analytics for tutors and instructors. The Social Networks Adapting Pedagogical Practice (SNAPP) tool aims to deliver ‘real-time’ social network analysis and visualisation within LMS discussion forums. The discussion forum is one of the most frequently used collaborative tools within an LMS [13, 14]. The manner in which forum threads are displayed however, makes it difficult for instructors to perceive conversation dynamics, and determine whether participants are actively engaged or merely peripheral on-lookers [15]. The first version of SNAPP released in 2008 extracted post-reply data from forum threads, inferred relationships and allowed the SNA data to be exported for further analysis in tools such as NetDraw [16]. During this time, many instructors that used SNAPP were found to import the SNA data into NetDraw for the purpose of scaling nodes and edges according to post and interaction frequency for display on a sociogram. SNAPP proved useful but numerous technical steps were also involved in the export and import of data. In 2009, the Australian Learning and Teaching Council (ALTC) funded a research grant to further research and develop SNAPP for the higher education context. This lead to the development of SNAPP

 1.5 which embedded an interactive sociogram within an LMS based discussion forum [17]. SNAPP 1.5 however, only stored the aggregate of interactions between participants up to the date the analysis was performed. During trials of SNAPP 1.5, one of the key features that instructors requested was the ability to filter the social graph by date. SNAPP 2.0 includes the ability to display animations of network evolution. This is achieved by storing the date of each participant’s interaction. The interactive sociogram serves as an alternate representation of the threaded forum that provides insight into participant interaction, emerging patterns and network density. The SNAPP interface is illustrated in Figure 1. SNAPP includes controls that allow the user to interactively filter the sociogram visualisation by date and relationship strength. Node size and edge widths can also be scaled according to post frequency and relationship strength accordingly. The Java Universal Network Framework (Jung) library is used to render the sociograms [18]. 
 2 Implementing SNAPP as a Client-side Browser Extension.
 There are a diverse set of challenges confronting the development of any analytic solutions for learning and teaching practice. For instance, ensuring that any technical solution is cross-platform, cross browser and compatible with the myriad of LMS available. The LMS landscape comprises both commercial vendors such as Blackboard (including WebCT) and Desire2Learn and open source products such as Moodle and Sakai. Unfortunately, to date there is no common server-side development platform in existence that affords any extensions to target all systems uniformly. The introduction of the Learning Tools Interoperability (LTI) standards may begin to address this issue. However, LTI adoption among vendors and developers is still in its infancy. Presently, each LMS has their own Application Programmable Interface (API), extension framework and is developed in a specific programming language. As the initial version of SNAPP was required to extract social network data for comparative analyses across multiple universities, each using a different mandated LMS, cross-LMS integration was a critical feature to address. Although, the various LMS extension frameworks allow for new tools to be added, they do not necessarily provide for existing LMS tools to be enhanced. For instance, developing enhancements for LMS based discussion forums or synchronous chat. As a result, developers will often focus on providing an extension in lieu of a direct code modification. Although, this context may differ for the Open Source LMS, there are disadvantages to any code modification in terms of the ability to upgrade to later versions. Thus, a unique feature of SNAPP is the manner in which it seamlessly integrates with the discussion forum and provides the user with interactive sociograms as a visual representation of the student interactions and group dynamics emerging from the implemented learning activities. Without this direct discussion forum tool enhancement, SNAPP’s usefulness as an embedded ‘real-time’ diagnostic tool would be lost. 
 Fig. 1. The SNAPP Interface. 
 The SNAPP design team sought to address these development challenges by using client-side browser based techniques. The bookmarklet technique was chosen because it worked in multiple Web browsers, enabled forum data extraction from multiple LMS and allowed the sociogram visualisation to be embedded directly within a forum. Other client-side techniques include native browser extensions and GreaseMonkey userscripts. Both of these techniques however, require administrator access for installation – a privilege not afforded to all academics at universities that mandate the use of a Standard Operating Environment (SOE). Bookmarklet installation merely requires a link to be dragged on to a toolbar (Firefox) or added to a favourites list (Internet Explorer). Thus, the use of the bookmarklet for dissemination, and adoption is not limited or impeded by end-user IT related permissions. 
 3 Forum Data Extraction.
 SNAPP infers participant relationships from the post-reply data. Discussion forum threads are stored in a database table with each row containing all of the information related to a single participant post. The post title, description, author and date are stored in the database. If the post is not the commencement point of a thread, a reference back to the parent thread in the form of a unique identifier is also stored. Forum posts are essentially stored in this hierarchical manner so that the parent-child relationship between posts and replies can be captured and displayed visually as a threaded tree of messages. Retrieving this information directly from a database using a series of SQL queries is a routine task. However, at the time of SNAPP’s conceptualization, not all LMS vendors made available the captured student interaction data via an API or Web service. However, all LMS do display the required network interaction data when a forum is displayed as a threaded tree with indents inserted to structure the hierarchy of posts and replies. SNAPP uses the Javascript client-side scripting language to retrieve the post-reply data from the threaded tree view of the forum. SNAPP uses the attributes of each post, including the author and date, to produce sociogram and social network metrics within any specified timeframe. The sociogram produced is in fact an alternate visual representation of the activity in a forum. The threaded forum tree view, displays interaction in chronological order but it is difficult for instructors to rapidly gauge the strength and diversity of the relationships evolving between participants. Recent research involving SNAPP indicates that the use of visualisations such as sociograms provides an easily interpretable interface for instructors [19]. These additional pedagogical insights were previously, neither easily obtained nor obvious from the discussion forum view or student data tracking tables. The sociogram is not a replacement for the threaded tree view. SNAPP is a complementary tool that further aids the analysis and interpretation of the captured interactions and observed social patterns. 
 4 Mapping SNAPP Features to Pedagogical Intent.
 Although the current suite of LMS include student tracking data, business intelligence or learning analytics functionality, the captured and reported data are often presented to instructors in a complex format that is isolated from the specific learning context. The poor reporting and visualization techniques associated with current LMS has resulted in minimal uptake of the included tracking and reporting tools [14, 20]. Broadly speaking, the available analytical tools are most likely to be used by University Administrators seeking information related to adoption rates for return on investment analyses or institutional technology reviews. The translation of interaction data from analysis to informed pedagogical action is for the vast majority of teaching faculty, a complex and potentially labour intensive process [21]. The following sections illustrate how SNAPP attempts to reveal the underlying pedagogical response and action to observed patterns of behaviour evolving from the student interactions. 
 4.1 Identification of Isolated and Highly Interactive Participants.
 There is an observed correlation between an individual’s connectivity to peers and their overall academic success [12, 22, 23]. As numerous commentators have previously noted, in the world of online learning, attrition rates are frequently reported as higher than their on-campus counterparts [24-26]. This has in part, been attributed to a lack of connectivity that is both social and academic, with fellow learners and the institution. Thus, any aids that can be afforded to forum facilitators to more accurately identify students that have not connected or have dis-engaged with the learning network early in their academic study, may assist with addressing concerns related to online attrition. SNAPP has been developed to provide forum facilitators with rapid and easy identification of a participant’s overall level of engagement with the student learning network. In this instance, early identification can provide an opportunity for instructors to intervene before students become disenfranchised with the learning process. Isolated students in SNAPP appear as nodes with no connections. This indicates that the participant has submitted a post but no other participants have responded. It can be difficult to identify participants at either end of the interaction spectrum on sociograms especially if the forum contains a large number of users. SNAPP therefore, provides the capacity for users to filter nodes based upon the number of posts. Filtering removes nodes above or below a threshold value. Filtering nodes above a threshold value reveals the participants that are central to a discussion. SNAPP scales nodes based upon the number of posts made by a participant. Connections between nodes are also weighted according to the number posts and replies made between the participants. The reciprocal directionality of the interaction can also be interrogated by passing a mouse over a connection. 
 4.2 Identification of Patterns and Structural Holes.
 SNAPP includes various graph layout algorithms to help users discover and interpret emerging network structures. The Fruchterman-Reingold, Kamada-Kawai, Spring and Circle layout algorithms are included to assist with the detection of network patterns for alignment with the teaching intent. For instance, the facilitator-centric pattern [27] arises when there are direct interactions between individual students and facilitator with minimal student to student activity. This pattern has a distinctive star shape and would commonly occur in an FAQ style forum, where the facilitator directly responds to student queries. However, if the intent of the forum is to promote knowledge sharing and construction among participants, then the emergence of the facilitatorcentric pattern would be interpreted as undesirable. Identification of this pattern early in the course provides an opportunity for facilitators to introduce alternate learning interventions and then monitor any changes in network composition. The presence of structural holes within a network indicates the development of actor sub-groups or cliques. The development of sub-groups may indicate strong bonding among a core set of students. However, the formation of these strong cliques can be to the detriment of other actors attempting to engage. The formation of these groups can also limit the diversity of engagement with peers. For instance, Dawson [12] observed that in large class forums students will form cliques based on perceived academic potential. In essence, high performing students flocked to other high performers to the exclusion of all other potential participants. In these situations, an effective strategy may include assigning participants to new groups and establishing additional group specific discussion forums. Another strategy is to encourage participants to interact across multiple cliques (i.e. bridge structural holes) to foster intergroup idea sharing. 
 4.3 Ego Network Exploration.
 A sociogram provides a visual representation of the relational interconnectivity across the entire network and highlights where dense, reciprocal and transitive connections exist. Within large participant cohorts it becomes difficult to gauge the relationship strength and reciprocality at an individual level by looking the sociogram as a whole. As much as Instructors require the capacity to view the entire social network, they also require the ability to analyse the social structures that surround a participant. This is commonly referred to as ego network analysis. An ego network consists of the selected actor and includes all other actors directly linked and their associated relationships with other connected participants. SNAPP incorporates functionality to aid with the exploration of ego networks. Clicking on a participants’ node in SNAPP highlights all of the nodes that comprise the immediate ego network. Ego networks with several strong ties are often considered to be homophilous in nature. In these instances, actors with strong relationship ties frequently share common attributes or interests. While strong ties promote a sense of community they can also reduce the degree of diversity an individual is exposed to. In certain educational contexts (collaborative learning), the development of a heterogenous network is more in line with the overall pedagogical intent. Heterogeneous networks tend to promote a greater number of weak ties and therefore increased access to knowledge and resources. Weak ties are shown to introduce novel knowledge into the network while participants that share strong ties usually have access to the same information [28]. Thus, it can be viewed as advantageous to embed specific learning activities that directly foster interaction between participants from different discipline areas and groups e.g. medical, dental and nursing students enrolled in a mandatory clinical ethics course, to promote the development of weak ties. Ego network analysis also provides an effective means to evaluate the role an instructor or tutor plays in a network. A sociogram is able to reveal whether an instructor or tutor is central or peripheral to the network, but with ego analysis the types of students that an instructor or tutor interacts with is able to be evaluated. In a study conducted by Dawson, [12] tutors primarily interacted with high performing students despite isolated and low performing students making several unrequited posts. It is common for facilitators to be a necessarily central and dominate actor in the network at the start of a course to establish relationships and promote communication exchanges. However, as the course unfolds the facilitator will gradually move to the periphery of the network to play a more mediating role. 
 4.4 Monitoring Network Evolution and Discussion Continuity.
 In the first release of SNAPP, post-reply relationships between participants were aggregated with no date information stored. This limited the sociogram to be a representation of the network, at the last time participants were active in the forum. The temporal nature of interaction was therefore lost. In SNAPP 2.0, individual postreply interactions are stored with their relevant date and time stamps. In terms of functionality this means that users can filter activity by date and view the resulting visualisation and social network metrics at a specified point in time. An animated view of network evolution is also incorporated to allow for the identification of events leading to the emergence of an interaction pattern. All of the available graph layout algorithms can be applied during the playback of network evolution to aid with the discovery of patterns. The storage of temporal interaction data also enables the analysis of forum activity over time. Post frequency distribution over the duration of forum activity can be plotted to a graph. This assists facilitators in identifying periods of increased forum activity and in determining the events (e.g. examination period) and interventions responsible for triggering the activity. SNAPP has been developed to aid with the analysis and interpretation of interaction patterns as a course progresses and class relationships and interactions form. For example, SNAPP can be triggered at any time to display a visualisation of current participant interaction – based on these early analyses alternate interventions can be designed to engineer more pedagogically desirable user engagement activity. SNAPP promotes the use of SNA as a ‘real-time’ diagnostic tool providing instructors with the insight they require to moderate a forum effectively. Included in SNAPP 2.0 is the ability for instructors to document intervention strategies using the annotations functionality. Annotations are textual descriptions that are stored with a date and time stamp. These annotations are potentially useful for instructors when for reflecting on the impact of implemented moderation strategies. 
 4.4 Evaluating and Comparing Multiple Forums.
 Multiple forums are often used over the duration of a course to address a variety of learning objectives. Individual forums are set up to cover different topics and cater for diverse learning needs. Forums may also be setup to encourage online collaboration within groups where access to the forum is restricted to group members. SNAPP is able to conduct analyses across multiple forums as well as individual forums. Viewing the resulting social network visualisation of all activity over the duration of the course provides a high level indication of the network depth, relationship strength and also allows for the identification of central and peripheral participants. The position of instructors and tutors within the resulting network pattern is also crucial to the evaluation. These forms of analyses address questions relating to the interaction characteristics of the facilitators (e.g. a central or peripheral role). Additionally, the analysis of multiple forums provides an indication of which moderation techniques were successful and how these techniques can be improved. 
 The analysis of where and how students interact in various class forums can reveal much about an individual’s motivations and learning preferences. For example, Dawson, et al., [29] observed that students were predisposed to contribute to either learning or administrative focused forums depending on their individual learning disposition. Additionally, in terms of class, and small group work contributions, Marcos-Garcfa et al., [30] compared student interaction within generic whole of class forums and small group specific forums. The authors noted that while a sub-set of students were able to make significant contributions at the class level, they failed to initiate any interaction within the small group work forum. In these cases the students’ interactions in the small group work forums were limited as a result of personality conflicts and general dissent. The capacity to monitor both individual and multiple forums provides instructors with a more holistic picture of both the broader class structures and individual student learning. 
 5 Future Directions and Conclusion.
 Although, SNAPP provides a valuable analytical resource, there remain numerous areas for further development and expansion. SNAPP currently performs social network analysis and produces easily interpreted visualisations of discussion forum activity. However, learning activity design is not an isolated process. The online and offline learning design process involves the integration and coordination of multiple tools both collaborative and individual (for example, assessment, synchronous discussions, academic literacy). In recent years commercial and open source Learning Management Systems have begun to introduce additional tools such as blogs and wiki’s either as native applications or extensions. Future releases of SNAPP require the ability to perform analysis within these additional applications as well as across the broad range of collaborative tools that are used within an online learning activity. The future development of learning analytic applications broadly should be guided by an imperative to aggregate from diverse data sets. For instance SNAPP will commence the incorporation of algorithms that are able to infer social relationships originating from blog commenting as well as the knowledge co-construction that occurs within collaborative editing environments such as a wiki. In addition to social network analysis and visualisation, SNAPP also provides basic metrics of individual participation in the form of the number of posts submitted. Connection strength is currently an aggregate of the number of times participants have actively responded to each other. Passive participation also occurs within a discussion forum where participants read or browse messages but don’t respond. The incorporation of passive activity will allow lurkers to be identified and help instructors to compare active and passive participation. Passive participation within forums is not currently tracked within many LMS. There is however scope to implement such tracking within open source systems, Moodle being a prime candidate. SNAPP does not analyse message content and as a result neglects to incorporate references made to other participants within text. The use of computational linguistic techniques such as Named Entity and Anaphoric Resolution need to be incorporated to further improve the accuracy of the inferred social structure. Due to the complex nature of interactions that occur within collaborative learning environments there is a need for implementing multiple learning evaluation techniques [31]. In this context, there have been numerous frameworks proposed for evaluating computer supported collaborative learning [e.g. 32, 33-37]. In particular, Weinberger, et. al, (2006) described a multi-dimensional framework involving participation, epistemic, argumentative and social mode dimensions. While SNAPP produces visualisations and metrics to assist with the evaluation within the participation and social mode dimensions, it presently lacks analytics specific to the epistemic and argumentative dimensions. These later elements can only begin to be addressed through content analysis. In essence, evaluations of the perceived ‘quality’ of the discussion are frequently overlooked as a result of the labour intensive nature of the process. It is therefore important for automated content analysis techniques to be incorporated within learning analytic tools. The merging of SNA techniques with automated content analysis will provide instructors with a more complete assessment of the individual and group dynamics evolving from the implemented learning design.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>SNAPP: A Bird's-Eye View of Temporal Participant Interaction</rdfs:label>
		<dc:subject>social network analysis</dc:subject>
		<dc:subject>analytics</dc:subject>
		<dc:subject>network learning</dc:subject>
		<dc:subject>learning management system</dc:subject>
		<dc:subject>graph theory</dc:subject>
		<dc:subject>visualisation</dc:subject>
		<dc:subject>inferring social networks</dc:subject>
		<dc:subject>computer supported collaborative learning</dc:subject>
		<dc:subject>evaluation</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/aneesha-bakharia"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/aneesha-bakharia"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/51/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/aneesha-bakharia"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/52">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>The Who, What, When, and Why of Lecture Capture</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/52/authorlist"/>
		<swrc:abstract>Video lecture capture is rapidly being deploying in higher-education institutions as a means of increasing student learning, outreach, and experience. Understanding how learners use these systems and relating this use back to pedagogical and institutional goals is a hard issue that has largely been unexplored. This work describes a novel web-based lecture presentation system which contains fine-grained user tracking features. These features, along with student surveys, have been used to help analyse the behaviour of hundreds of students over an academic term, quantifying both the learning approaches of students and their perceptions on learning with lecture capture.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 Lecture video capture solutions (e.g., opencast [1], echo360 [2], epresence [3], virtproducer [4]) are rapidly being adopted by traditional higher education institutions to increase the levels of blended learning available to students. This adoption is driven in part by the dramatic reduction in the costs of technology to institutions and students, the high availability of broadband internet access, the proliferation of media rich devices such as smart phones and tablets, and an interest in repurposing the traditional and widespread, “sage on the stage” model of teaching for anytime anywhere learning. Despite this adoption, very few studies have been done on the ways students use lecture capture technology to assist in their learning. This paper furthers this area by considering explicitly how students use the underlying technology, not necessarily its effects on student marks or enthusiasm. Using a bottom-up approach, we examine student interactions in the environment and outline a model for user tracking. The most important result from this work is the demonstration that low-level tracking data collected from lecture capture systems can be used to describe students around pedagogical goals. We augment this investigation with a more traditional top-down student survey on perceptions and experiences. Using grounded theory, we distil a variety of high-level student opinions about why they used the lecture capture system the way that they did into a set of categories that describe their system use. 
 2 System & Study.
 2.1 Recollect Lecture Capture System.
 The Recollect system was developed in house at the University of Saskatchewan as an automated lecture capture, processing, and delivery system. It runs on commodity hardware inside of the classroom and can record the projector signal, one or more video cameras, and a single audio input. Students receive emails when lectures have been processed and published online, and they view lectures using an Adobe Flashbased web interface. Recordings from various video devices are merged into single streams based on declarative templates that are configured on a class by class basis. For instance, some classes may have a template that shows a single VGA feed with audio, while others may include two smaller NTSC-based camera feeds to the left of the VGA feed. Students cannot switch between feeds while viewing the lecture; they must watch the video using the template that the instructor has chosen for the class. Students cannot download videos, and must watch them through the web interface. Recollect allows instructors to create a template for how they would like the video to appear. All of the templates include a thumbnail menu on the left-hand side of the screen. The thumbnails in this menu are auto-detected segments of the lecture (see [5] for more information) that can be used for navigation. The traditional media “scrubber” widget is shown underneath all videos, and allows for navigation and adjustment of video parameters (e.g., sound levels). Instructors are given several options when creating their template. A typical classroom deployment can be seen in Fig. 1, where the template chosen by the instructor shows the lecturer next to his slides. 
 Fig. 1 The Recollect system showing a typical classroom deployment, likes, VGA of the presenter desktop on the right-hand, and a camera view of the instructor roughly in the centre. 
 Novel to this system is that it has been created with user modeling in mind. In short, the field of user modeling posits that by tracking individual accesses to software features and content, profiles of students and student cohorts can be created. These profiles can be used for automatic reasoning to provide content recommendation, environment adaptation, and just-in-time tutoring. While a full treatment of this area is well beyond this paper, we draw experience and motivation from two works in particular: the ecological approach, and attention metadata. The ecological approach is described succinctly by its creator, Gordon McCalla, who indicates that it “...involves attaching models of users to the information they interact with, and then mining these models for patterns that are useful for various purposes” [6]. Once patterns are found in the usage data, it informs the creation and operation of intelligent tools. These tools can then work in concert with the original system, creating new user modeling data for other tools in a sort of ecosystem of to support earning. 
 2.2 Situated Study.
 The Recollect lecture capture system has been deployed for several years, and a study investigating its use was conducted over one 15 week academic term in 2010. During this term, students from professional colleges and a number of different disciplines, including the sciences, social sciences, and humanities, were invited to use the tool to augment their in-class learning. The tool was made available to every student enrolled in the courses being recorded, as well as, depending on the instructor, to students in similar courses taught at different times. A total of 1,125 students participated in using the lecture capture system, out of roughly 2,000 eligible students1. Students were shown a brief five-minute introduction to the tool on the first day of class, and were not given external motivation to use the tool through payment or academic reward (e.g., marks for participation). At the end of the term, students were invited to fill out a 20 minute survey, on paper or online, describing their experiences with the lecture-capture system for a chance to win a gift certificate. This study had two goals: to create a low-level semantic logging framework that collected student interactions within the learning environment, and to analyse student interaction and perception data to form groups based on learning preferences. The stateless, distributed, and often disconnected nature of web systems makes user tracking difficult, and most content management systems which include video content do so only at the coarsest grain size. For instance, kaltura [8] and blackboard [9], only capture analytics at a high level, such as the number of times a student viewed a video or page. Unlike other forms of web content, video usage data at such a high level is difficult to use because the content changes rapidly and a single page impression does not characterize how long or the way in which a user interacted with the video. To address this, we built Recollect with an event model where every interaction the user has with the player (e.g., clicking on a button, seeing a popup thumbnail, seeking to a new time position) sends a tracking event to the server streaming the video. The player was also built with a “heartbeat” mechanism where a new event was created for every 30 seconds of continuous playback. Each event, regardless of its originating action, identified the user and aspects of the high-level video-player state, such as the current position in the video and whether the video was playing or paused. Many events also included specific details relevant to the action; for instance, a seek in the video would include both where (the time offset) in the video the user was when they started seeking and the position in the video they chose to seek to. Over the four month period of the study 3.4 million events were logged with the Recollect system,

 1.8 million of which were “heartbeat” events that we analyse in the following section. 
 (1) Exact numbers of eligible students are unable to be calculated as instructors or other students could share lectures with whomever they wished. 
 3 Quantitative Results.
 We hypothesised that students could be categorized into different groups based on their access patterns. In particular, we formed several sub-hypotheses for each kind of group that we predicted would occur: - H1: There will be a group of minimal activity learners. These students may have preferred methods of achieving their learning goals and will investigate the tool but not adopt it in any regimented fashion. Note that we only consider students who try the tool in our analysis, so this group will contain students who have viewed at least single lecture through the recording software. - H2: There will be a group of high activity learners. These students may not watch all of each lecture, but will watch some content each week. The key element of this group is that they are embedding video lectures in their learning routine. - H3: There will be a group of disillusioned learners. These students will be keen enough to use the tool near the beginning of the course but will stop using it because they found it did not aid in their learning. - H4: There will be a group of deferred learners. These students will not use the tool at the beginning of the course but began to use the tool closer to the end of the course will exist. This could be because students are leaving learning to the end of the course, or find latter course content builds on early content thus requiring more/deeper review. 
 To test these hypotheses, we inspected heartbeat data for each student who used the tool for each week in the course. We discarded the sixth week from each student’s data as an outlier because the university was closed for holidays and accesses to the lecture playback tool were minimal. Using k-means clustering [10] with the Weka toolkit [11], we aggregated data for a large class participating in our study2. We limited our investigations to a single class to control for class-specific effects such as the timing of assignments and exams, or the cancelation of class due to holiday or instructor illness. We changed student access data into nominal values of “y” indicating that the student watched at least 10 minutes of lecture video that week or “n” to represent that they didn’t watch 10 minutes or more video. Only students who had accessed the system were included. The question of the number of clusters to choose when using k-means is always an issue, with fewer clusters generally seen as better since the introduction of each new cluster can lead to over-fitting the model to the data. We chose a number of clusters equal to our hypotheses plus one as an initial metric. The addition of the fifth cluster was to account for a group that we believed would exist but would be hard to classify; those students who used the tool intermittently to “catch up” on classes they may have missed. The results of k-means clustering with five categories are shown in Table 1. The Weka-based k-means clustering showed strong support for three of our four hypotheses. In particular, c0 corresponds well with H1, c4 corresponds with H2, and c3 corresponds with H4. Notably, H3, the hypothesis that students would start using the tool at the beginning of the term but drop off as the term progressed, was not verified. 
 (2) The class we picked to analyse, a second year Chemistry class, was chosen because it had a high number of participants. Data from other classes was omitted due to space constraints. 
 Table 1. Results of k-means clustering with five categories versus the 15 weeks of the academic term. The vast majority of students fall into the first cluster representing minimal or no accesses to the video playback tools. 
 The most surprising result was the formation of two clusters around watching the video only in week eight or the combination of week seven and week eight (clusters c1 and c2 respectively), the former being extremely large. Referencing the course syllabus, the end of week eight corresponded to the placement of the midterm exam. Thus we present a new hypothesis backed by this data: - H5: There are a group of just-in-time learners. These students use the tool only for midterm exam review, though midterm review may stretch over several weeks of academic lecture time. Despite a good fit to our initial hypotheses, we experimented with both more and fewer clusters. The most interesting result was running k-means with a cluster size of 6 (shown in Table 2). This data shows students can be clustered well into all of the hypothesis previously given, including moderate support for H3 through cluster c5’. Drop off in engagement could be related to the tool, the content, or other factors. As the cluster includes an increase of viewership before the midterm examination it’s unlikely that students’ dropping the class is the main characteristic of this cohort. Further investigations into why students quit using the tool are being planned, including an active monitoring of course registrations. It should be noted that the increase in number of clusters results in a greater potential for data over fitting, especially as some clusters become quite small. Students who miss a few lectures and review them strategically are not well represented by these clusters. Evidence from student surveys and data analysis suggests that, while this group is small, it does exist. This may be because the clustering algorithm considers all attributes weighted evenly; as a result, it over specifies when more generic higher-level attributes might be a better indicator of the presence of this group. Further, the effects that other assessment mechanisms in the course have on viewing is not clear; the course had several assignments which are not represented in this data, but the sheer number of students identified as just-in-time learners suggests that this warrants deeper investigation. It was surprising to see that during the time between the last week of classes (week 13) and the final exam (end of week 15) no clusters included viewership of lecture material. This indicates that student learning strategies with respect to lecture capture vary between when class is in session and when formal instruction has ended. This begs for further investigation; our initial expectation was that students would have more time and thus be more likely to use recorded content right before the final examinations. 
 Table 2. Results of k-means clustering with six categories. Very similar to the results with five categories shown in Table 2, this clustering now includes moderate support for H3 where we expected students to use the tool initially but gradually reduce their usage over time. 
 4. Qualitative results.
 At the end of the study period we conducted a survey to collect both qualitative and quantitative data. The qualitative data collection was intertwined with the quantitative by inserting open-ended questions into the appropriate sections of the survey. We then used grounded theory [12] to extract themes by coding each participant's responses to open-ended questions about system features and use. As a result of the open-ended nature of the questions, it was possible for students to express multiple ideas, which could result in a response being coded as belonging to multiple themes. Even though we collected student opinions about specific system features, we focus this paper on participant responses to system use questions since their responses regarding features were limited to technical details of those features rather than the motivation behind their overall system use or lack thereof. There were several students who did not use Recollect. When students were asked about why they chose to forego using Recollect they were able to select a reason from a predefined list or explain their reasons. Of the students who responded to this question, 81 selected a response from the list. They selected "I thought the recoded lectures were not valuable" most frequently (52), but other reasons were also chosen (19 were unaware that Recollect was available and 10 had limited computer and internet access). The explanations given by 176 students who expanded on their behaviour helped us to understand their approach to how a video capture system, such as Recollect, should fit into a class at the university level. We identified several themes from their responses that we combined into the following categories: - Logistical & Technical Problems: Scheduling or technical limitations that prevented students from using the system. This includes system avoidance because of previous bad experiences and the negative reports of others. - Unknown Resource: Students were unaware of Recollect's features or existence. - Don't Want/Need it: Students who thought that their current resources and learning efforts were sufficient or better than those provided by Recollect. - Anti e-Learning & e-Support: Students who were uncomfortable with or against the idea of online learning materials as well as those who did not want to help students who had been absent. - Will Only Use When Needed: Students who only used or would only use Recollect when they missed class or needed to review a concept. - Haven't Studied Yet: Students who claimed that they would use Recollect to study for their final exam. 
 In Fig. 2 (a), we can see that the primary reason for students abstaining from Recollect use was that they did not feel that they needed the provided support. Many students "...only used it when [they] missed class..." or "...as a back-up reference...". The second most common response involved their lack of use of Recollect because of scheduling or technical constraints, such as the inability to read what was written on the chalkboard or hear student questions. While many students did not use Recollect, many others did; we asked these students what they liked best about Recollect. We analyzed their 207 open-ended responses and identified themes that were grouped into the following categories: - Logistics & Scheduling: Recollect allowed for anytime anywhere access to lecture material, which meant that students could maintain the schedules that fit their lifestyles and did not need to interrupt class or bother others to obtain course material. This also includes positive changes to the classroom environment that resulted from Recollect's use, e.g., only having interested students in class. - Class Attendance Not Required: Students were happy that they could miss class when ill or because of unforeseeable events. - Review & Notetaking: Students could review materials that they had missed or failed to understand, including verifying and completing personal lecture notes. - Having It: Students expressed a desire or appreciation for having the video, which includes positive statements about specific system features and feature requests. - Helps Understanding: Recollect in some way facilitated student understanding of course material. This includes accommodating learning styles and allowing students to focus on the message being delivered in class rather than on taking notes. 
 As Fig. 2 (b) shows, students appreciated being able to miss class when they needed to. Recollect enabled them to stay at home when they were sick without sacrificing their ability to receive a similar educational experience as they would have had in class. Students also felt that Recollect "...helped when [they] missed class or couldn't hear..." what their instructor was saying. One nursing student said that she "...was able to get a missed lecture and take better notes..." by using Recollect's pause and play functionality. The second most appreciated functionality provided by Recollect relates to this latter student comment since the system allows students to review materials. The flexibility that a lecture capture system provides by enabling anywhere, anytime learning is phenomenal: "...[Recollect] reduced stress if [I was] forced to miss a class for whatever reason...”. Students appreciated that they "...didn't have to worry about contacting [their] classmates to know what [they] missed...". 
 Fig. 2 a) An analysis of why students, across all courses, did not use the video lecture capture system. While most felt it was unnecessary, the majority were not against its use. b) What students, across all courses, liked most about having video lectures available to them. 
 One student who felt particularly challenged by his workload said that “...[he] enjoyed using it for the particularly difficult concepts in the course, and it was also really handy when [he] was being destroyed by a multitude of assignments and midterms within the span of a week...[He] was able to use the time [he] would be in lecture to attempt to pass all of those things, and [he] got caught up in class using recollect without being one of those mass-email jerks that ask for notes in every class...”. Students also liked having recorded lectures because it allowed them to listen to their instructor's explanation, which allowed them to focus on understanding so that they could ask questions in class and fill in the details in their notes later. Some students even went as far as never taking notes during class: "I'd listen to lecture and not take notes, then watch video later and then take notes". The argument that attendance is reduced when using lecture capture technologies is one of the principal issues facing adoption. While we did not collect metrics to measure attendance, one computer science student’s comment indicated that a lack of attendance is not always a negative when supporting classroom learning: “... [Recollect] also played a part in lowering the attendance of students who were largely disruptive in class, improves the classroom environment.” Unlike other lecture capture solutions, Recollect allowed users to take notes, within the system, while watching the video. Notes are indexed by the relative slide position using automated indexing. Students can convert these notes into printable PDF’s which include copies of the slides, and notes are automatically shared with other students in the class through the note-taking interface. Despite the student demand for using lecture capture for note-taking, very little activity was observed in the provided note-taking tools included in Recollect. Most students used the videos to take their own notes on paper. Student responses to lecture capture were far from uniformly positive; some students were ambivalent to the learning benefits it might bring or even directly hostile to the opportunities it might present to others. One chemistry student stated that “I go to the lectures, so I don't bother watching videos to relearn what I already did that day”. One of the more hostile responses came from a biosciences student who resented people who did not attend class. He said: “I showed up for class. Student's who don't show up for class should not be rewarded with lecture videos. Lecture podcasts should only be used for off campus education. Showing up for work is a reality of life, and students should get used to showing up for commitments they have made.” 
 5 Conclusions & Future Research.
 The adoption of lecture capture is increasing, with many institutions making selected lectures available on the web for free viewing through initiatives like the OpenCourseWare (OCW) consortium [13], portals like iTunes U [14], or YouTube Edu [15]. Lecture capture is particularly well suited for traditional higher education institutions that want to leverage their faculty and classroom experiences in increasingly connected online learning environments. This paper has made contributions to the understanding of what kinds of students use lecture capture systems, when those students engage in reviewing content online, and why they are motivated to use this technology. Analytics in learning systems can be used to provide both auditing and interventions in student learning. While we intend neither of these explicitly with this work, we aim to scaffold support for them by demonstrating how a low-level video logging tool can use automated clustering techniques to group students into pedagogically motivated cohorts. This form of analytics has been largely unexplored when it comes to lecture capture, and fits well with the heritage of intelligent and adaptive learning systems described in the user modelling and artificial intelligence in education communities. Building intervention tools to take advantage of these clusters, such as intelligent content recommendation or help seeking tools is a natural next step. What motivates students to use lecture capture is a broad question. We explored this by asking students a mixture of closed and open-ended questions. Students provided candid feedback and, while the diversity of opinion on how and even whether lecture video should be used varies, the opportunity to review a lecture and make notes seem central to the learning process. Our analysis limits itself to a single cohort for quantitative measures and to a single semester of data for qualitative measures. Tying these two data sets together into a single model is difficult; collecting qualitative data is expensive, and the diversity of teaching approaches in different courses makes collapsing usage data into one coherent set non-trivial. Nonetheless, even a surface analysis as we have done results in interesting and pedagogically useful results. Armed with the knowledge of how students use lecture recordings, we can begin to build intervention tools and strategies to increase student learning and satisfaction in rich media education environments.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>The Who, What, When, and Why of Lecture Capture</rdfs:label>
		<dc:subject>lecture capture</dc:subject>
		<dc:subject>clustering</dc:subject>
		<dc:subject>analytics</dc:subject>
		<dc:subject>student experience</dc:subject>
		<dc:subject>participation</dc:subject>
		<dc:subject>recollect</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carrie-demmans-epp"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carrie-demmans-epp"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/greg-logan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/greg-logan"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-greer"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-greer"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/52/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/carrie-demmans-epp"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/greg-logan"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-greer"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/53">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>A Unified Framework for Multi-Level Analysis of Distributed Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/53/authorlist"/>
		<swrc:abstract>Learning and knowledge creation is often distributed across multiple media and sites in networked environments. Traces of such activity may be fragmented across multiple logs and may not match analytic needs. As a result, the coherence of distributed interaction and emergent phenomena are analytically cloaked. Understanding distributed learning and knowledge creation requires multi-level analysis of the situated accomplishments of individuals and small groups and of how this local activity gives rise to larger phenomena in a network. We have developed an abstract transcript representation that provides a unified analytic artifact of distributed activity, and an analytic hierarchy that supports multiple levels of analysis. Log files are abstracted to directed graphs that record observed relationships (contingencies) between events, which may be interpreted as evidence of interaction and other influences between actors. Contingency graphs are further abstracted to twomode directed graphs that record how associations between actors are mediated by digital artifacts and summarize sequential patterns of interaction. Transitive closure of these associograms yields sociograms, to which existing network analytic techniques may be applied, yielding aggregate results that can then be interpreted by reference to the other levels of analysis. We discuss how the analytic hierarchy bridges between levels of analysis and theory.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 The rapid adoption of information and communication technologies (ICT) in support of “online,” “distributed,” and “networked” learning and knowledge creation activities [1], and their blending with face-to-face venues [14] is well known to the research community to which this paper is addressed. In this paper we use learning as shorthand to include any enhancements of individual or collective knowledge or skills, whether or not it occurs in formal educational settings. We include in our scope of interest learning in (for example) online university settings, professional communities, and virtual organizations [2, 4, 8, 29]. We will refer to these collectively as socio-technical networks [19]. A related trend is towards open learning communities. Courses in formal educational settings need no longer isolate participants from others in different courses, but can embed courses in online communities of learners, for example supporting transdisciplinary graduate education [10]. In corporate or other work settings, professional learning communities similarly may cross team contexts rather than being isolated in work teams [42]. The fundamental question of interest in all of these settings is how learning takes place through the interplay between individual and collective agency. All learning activity requires that individuals take actions, but these individual actions are contingent on the actions of others in their socio-technical network contexts, actions that reflexively construct those contexts. The first analytic challenge addressed by this paper is that learning and knowledge creation activities in these networked environments are often distributed across multiple media and sites. As a result, traces of such activity may be fragmented across multiple logs. For example, the networked learning environments we study offer mixtures of threaded discussion, synchronous chats, wikis, whiteboards, profiles, and resource sharing. Events in these media may be logged in different formats and recorded in databases and text files, disassociating actions that for participants were part of a single unified activity. This disassociation is exacerbated when activity is distributed across multiple virtual sites or spread over time. Also, the granularity at which events are recorded may not match analytic needs, and media-level events may be the wrong ontology for analyses that begin with relationships rather than individual acts. Translation from log file representations to other levels of description may be required to begin the primary analysis. As a result of these various issues, the coherence of distributed interaction and phenomena that emerge from this interaction are analytically cloaked. Furthermore, understanding distributed learning and knowledge creation requires multi-level analysis of the situated accomplishments of individuals and small groups and of how these local accomplishments give rise to larger phenomena in networks such as the dissemination and transformation of ideas, implicit coordination of the activities of many participants, and the accrual of collective knowledge. Consider the question of how the design of the virtual environment influences emergent phenomena. Everything builds on the existence of multiple successive moments in which an individual is experiencing some presentation of the virtual environment, cares enough to act, and is able to choose an appropriate action. Whether and how this action has implications for network or community level phenomena requires that some trace of the action be given persistent form that other participants might later encounter in their experience of the virtual environment [18]. Appropriate aggregation and availability of such traces can drive dissemination of ideas, align participants, and lead to accrual of collective resources of value. Critically, an empirically grounded understanding of this emergence requires analysis at both fine-grained and aggregate levels. The same can be said for understanding the relationship between small group interactions and larger scale phenomena. In summary, since interaction is distributed across space, time, and media, and the data comes in a variety of formats, there is no single transcript to inspect and share, and the available data representations may not make interaction and its consequences apparent. To address these concerns (and to support the diverse research in our laboratory), we have developed a framework consisting of an abstract transcript representation that collects relevant events into a single analytic artifact, and an analytic hierarchy that supports multiple levels of analysis. This paper describes the framework and discusses its potential roles in unifying multiple sources of data and bridging between levels of analysis and theory. We discuss how the framework addresses several specific analytic needs, including: (a) scaling up microanalysis of interaction to large data sets, (b) enabling the translation of event logs into tie data appropriate for social network analysis, and (c) interpreting results at one level in terms of another (e.g., relating social network analytic results back to their interactional settings). Throughout the paper, a simple example drawn from our prior research illustrates many of the features of the framework. 
 2 Preview.
 The analytic hierarchy consists of several abstraction layers of analytic representations that we have found to be useful, summarized in Table

 1. Process traces such as log files are abstracted to domain models describing the actors, actions and media objects involved in event models, which are collections of temporally tagged events. These event models can be further elaborated by installing directed graphs of empirical relationships between events called contingencies. Contingencies can be any observed relationship between events (e.g., two events are by the same actor, involve the same object, are temporally contiguous or proximal, or overlap in content). Contingencies situate participants’ acts in relation to other events—hence the name contextualized action model. The analytic utility of contingency graphs is enhanced if focused on those contingencies that may be interpreted as evidence of uptake: interaction and other influences between actors. When such interpretations are made, contingency graphs are abstracted into uptake graphs, representing interaction models. 
 Table 1. The Analytic Hierarchy. 
 Interaction can be further abstracted to two-mode directed graphs, called associograms, which record how associations between actors are mediated by their creation and modification of and access to digital artifacts: hence the name mediation models. Associograms also summarize sequential patterns of interaction, making it easier to localize certain patterns. Reduction of associograms by transitive closure into direct ties between actors yields sociograms, representing tie models. Existing network analytic techniques may be applied to sociograms. The results of network analysis can then be interpreted by reference to the other levels of analysis. Thus associograms bridge between interaction analysis and network analysis. The analytic concepts (e.g., contingencies, uptake, mediated associations, and ties) in this paper are not new. Rather, the value of the framework relies on the fact that they are abstractions of concepts commonly applied in existing analytic practice (e.g., adjacency, edits, replies, etc.) as will be detailed later. Thus the framework is offered to coordinate and augment rather than replace existing analytic practices. The layers are explained in more detail in the following subsections. The process trace, domain, and event models and transformations between them are likely to be familiar to readers: brief sections on these layers are included for completeness and to provide the foundation and examples for describing subsequent layers. Contingency and uptake graphs and associograms are more unique contributions, so are described in some detail here—see also [37] for extensive discussion of motivations for contingency graphs and examples of their use for uptake analysis. The most abstract layer is covered substantially in the social network analysis literature [e.g., 41], so is described here only in relation to how it is derived from the layer below, and what that vertical relationship enables that would not be possible with direct measurement of ties. Throughout this presentation, applications to the study of learning analytics are discussed. The methods described in this paper have been applied in numerous analyses of data from an online learning environment and from laboratory studies of ICT mediated collaboration. At present we are using these techniques in analyses of SRI’s Tapped-In teacher professional community [12, 32], a virtual organization that hosts many thousands of education professionals annually in more than 8,000 usercreated spaces that include IRC, threaded discussions, shared files and URLs, and other tools to support collaborative work. 
 3 Process Traces.
 Any analysis of interaction begins with a process trace, or record of activity left in the environment and accessible to the researcher. Examples include software log data (software application or server logs), audio and video recordings, and textual transcripts. The analytic hierarchy described herein was originally designed to support analysis of both software logs and video recordings, sometimes in conjunction (e.g., we have analyzed application logs and screen capture of the same application [25, 26]). For learning analytic applications and to emphasize the potential for automated analysis, this paper focuses on software logs, and does not touch on issues of video analysis; see [15, 17] for discussion of such issues. The analytic hierarchy is illustrated throughout this paper by building on a simplified example taken from one of our online learning community applications, disCourse. 
 Figure 1. From Process Trace to Domain and Event Models. 
 The disCourse environment provides threaded discussions, wiki pages, resource sharing with searchable metadata, and user profiles, organized in a workspace metaphor that collects together tools and resources relevant to a given group, such as a class [36]. The lower portion of Figure 1 shows excerpts (edited for anonymity and simplicity of presentation) of an http server log1 from disCourse. The example in this paper builds on these logs. See [37] for the full text of the example. 
 4 Entity-Relations: Domain Model.
 Prior to or concurrently with the construction of the event model (next section), it is necessary to construct an ontology of the kinds of entities involved in the application domain of interest. Classes of entities and potential structural relationships between them are defined (e.g., actors, discussions, and messages, related by containment, threading and authoring relations). As the trace or log file is processed, new instances of entities and their structural relations are added to the domain model when they are encountered, along with relevant attributes that are expected to be needed for analysis. This is undertaken in conjunction with construction of the event model. For example, the right hand side of Figure 1 illustrates a domain model fragment representing how messages m1, ... m4 are created by participants P1, P2, P3 (shown by shading), related to each other by a threading relation, and contained in a discussion forum. The content of messages are also recorded in the domain model. Temporal information is recorded in the event model, discussed next. 
 (1) disCourse logs events in a database. HTTP server logs of the same events are shown in this example to illustrate the method using log formats familiar to readers. 
 5 Events: Event Model.
 The process trace is transformed into a set of events that constitute an analysis’ first commitments concerning the relevant units for analyzing processes. This transformation involves the Exploratory Sequential Data Analysis (ESDA) operations of chunking and coding [31]. For example, the first three lines of the log of Figure 1 all are part of the process of posting a message in a system in which each message is previewed before posting. These three traces are chunked together and represented as the single event w1 in the event model, along with information about the actor (P2, indicated by grey), action taken (w for writing), object (message m1), contents and location (recorded in the domain model), and temporal scope of the action. We call this layer the event model because the focus is on individual actions and other events by nonhuman actants such as software display events. (Actant is Latour’s [22] term for non-human entities that yet have agency in networks of associations.) The events have not yet been put in relation to each other, other than ordering along a timeline. Events may be derived from distinct process traces that come from different media, tools or sites, and are recorded in different formats. For example, chat contributions, wiki edits, whiteboard edits, file uploads, etc. can be merged into a single event stream. (To remain faithful to the case example and avoid complicating the figures, this capability is not illustrated in the figures, but it is a simple extension.) A key concern is persistence of identity across tools and sites: some work may be required to ensure that each given actor is represented by the same identifier in the event model, and likewise for the identity of digital objects shared across tools (ideally persistence of identity should be addressed in mash-ups for the learners’ sake [20]). Once this has been accomplished, the event and domain models taken together provide an abstract transcript of the data that re-assembles in one analytic artifact the diverse events that were for their actors a single activity. If the transcription is complete with respect to the needs of a given analysis, then it is not necessary to retain the original process traces. However, we retain pointers to the original process traces because it may not be possible to identify all needs in advance. We may need to recover other information from the process trace. Also, any transcript includes initial theoretical commitments [11, 28], which may turn out to be faulty, necessitating a return to the original process traces. A number of analyses can be undertaken on the event and domain models without further analysis. In our research, this is the level at which we answer basic questions about the distribution of activity in the environment: who is participating with whom, in what virtual sites or contexts, and involving what literal content. But to analyze interaction and uncover ties between actors we must relate events to each other. 
 6 Contingency Graph: Contextualized Action Model.
 Contingency graphs are an empirically grounded elaboration of the abstract transcript to make analytically relevant relationships between events explicit. We originally called these relationships dependencies, but have renamed them contingencies because they capture relationships between events that may be merely contingent or incidental to the situation, rather than being causal or deterministic. The graph simply makes relationships that are latent in the data more explicit, and does not constitute a commitment concerning actors’ intentions. Human action can be embedded in its context in many ways, including accidental relationships, or opportunistic leveraging of contextual and historical features as well as necessary antecedents for action [6, 22]. Thus, a contingency graph represents how action is embedded in the context of other events. Examples of contingency types we have used are listed in Table 2 (not intended to be a complete taxonomy). A detailed presentation of the motivations and theory behind contingency graphs and their application to interaction analysis may be found in [37]. Construction of a complete graph of the contingencies between events in a process trace is not practical, as it would result in a graph with a high “signal” to “noise” ratio that is too complex for processing. (Imagine a graph in which each event is linked to every one involving the same actor, or the same object, or that has overlap in lexical content, or occurred nearby in time, and so on.) An analyst chooses those contingencies that are relevant for specific analytic purposes as guided by explicit or implicit theory. Therefore a contingency graph reflects further commitments on the part of the analyst. However, even though a contingency graph is theoretically selective, we always base contingencies on empirically observable relationships between events found in the event and domain models, preferably those relationships that are unambiguous and can be detected automatically. If this standard of evidence is followed, a contingency graph can be treated as an abstract transcript that makes the evidence for interaction or other phenomena of interest manifest. 
 Table 2. Examples of contingency types. 
 Figure 2. Installing Contingencies. 
 Contingency graphs can be constructed automatically from the layers below it [see, for example, 26]. For example, for each event in which an actor accessed an object we might scan back to find the last event in which the object's contents were modified, and install a media dependency. Contingencies can also be installed from a given event to the most recent prior event involving the actor, to prior events in which the actor accessed a media object with similar inscriptions (e.g., lexical phrases or graphical devices), or to temporally recent events in the same spatial site. A challenge with algorithmic installation of contingencies is limiting their number. Temporal or sequential proximity are useful (and computable) heuristics for selecting relevant contingent events, as they follow the local continuity of human attention and goal directed behavior: what actors do at any given moment is likely to be contingent upon their immediately prior act. For example, Figure 2 shows the events of Figure 1 with contingencies installed. The single arcs represent media dependencies, and the double arcs represent multiple contingencies, such as temporal proximity combined with same actor and possibly inscriptional similarity. The act of reading a message (r1, r2, etc.) is media-dependent on the act of creating the message (w1, w2, etc.). The act of writing a message (e.g., w2) may be media-dependent on the act of creating the message to which it is a threaded reply (e.g., w1) and is contingent on the messages that the author has recently read (e.g., r1). In this example, the message created by w2 contained a nounphrase in common with that created by w1. 
 Once constructed, various kinds of analytic actions are possible on contingency graphs. For example, suppose a particularly productive session was identified in which participants made significant ideational progress. One option is to examine the interaction of the session participants more closely to identify the relationship between group processes and their accomplishments, and how participants appropriated the interactional affordances of the available media for these purposes. We have used the contingency graphs in several studies to support this kind of microanalysis of interaction [24, 25, 26, 38]. Recurring patterns of interaction so identified could be searched for in the overall contingency graph to find other sessions that have similar patterns of activity, to see whether they display similar productivity. Such pattern matching techniques are similar to structural equivalence metrics in social network analysis, which can be employed once contingency graphs are converted into sociograms, as discussed in section 9. Another option is to look outside the session to find influences from or to other sessions. One can trace same-actor and media-dependency contingencies, following the actors and actants respectively. Tracing proceeds forward in time to see whether the new ideas of the session were disseminated elsewhere, or backward in time to identify possible predecessors of the ideational advance. Such an analysis grounds the concept of brokers in actual accomplishments, not relying solely on structural relationships that do not guarantee such accomplishments. At this writing we are constructing a contingency graph of several years of data from Tapped-In in preparation for application of methods such as those just described. 
 7 Uptake Graph: Interaction Model.
 As discussed above, contingencies are so named because they can include circumstantial relationships between acts with varying degrees of relevance to interaction. Analytic interpretation is required to identify relationships between events that are not merely circumstantial, but reflect intentional acts. An act of uptake is one in which an actor takes traces of one or more prior events as having certain significance for an ongoing activity [37]. For example, a speaker takes up some aspect of the prior speaker’s utterance, or a message poster in a discussion forum can take up some aspect of the message being replied to. Uptake is a generalization of all interactional relationships used in analysis, such as comment, reply, elaboration. It includes these relationships, but also applies to spatio-temporally distributed associations between actors in which they may not even be aware of each other, let alone be directing their actions towards each other, such as tagging, downloading, etc. Therefore, uptake is more general than transactivity [5], which requires otherdirectedness. Uptake is an appropriate generalized unit of interaction in networked learning environments, where individuals may benefit from each others’ presence without conversing directly. The essential idea is that the trace an actor's actions have left in the environment (e.g., chat contribution, discussion posting, uploaded file, profile, recommendation) is taken up by another actor in some manner. Uptake of traces can result in stigmergic effects, i.e., implicit distributed coordination of collective action [30]. 
 Figure 3. From Contingency to Uptake Graphs. 
 Illuminating these stigmergic effects reveals the contingencies by which an individual’s actions are connected to information, actions, and resources from sources that may otherwise not be known to that individual, even if embedded within one’s known social network. An uptake graph is an interaction model, as it describes the interaction that the analysis claims is taking place. Although all analytic artifacts from process traces on up involve theoretical decisions, the move from contingency graphs to uptake graphs is a move from primarily empirically accountable representations to those more strongly determined by analytic interpretations. Representationally, an uptake relation is a subgraph of contingencies, as illustrated in Figure 3. An analyst collects contingencies that are considered to be analytically meaningful: a number of contingencies between two or more acts may corroborate the interpretation that the final act is an intentional taking up of traces of the prior ones. For example, w2, in which P1 posts a reply to the message posted by P2 in w1, is contingent on w1 in these ways: there is a media dependency (m2 is linked by threading to m1); lexical overlap (m2 contains phrases also found in m1); and a chain of temporal proximity (w2 took place shortly after read event r1 by the same actor, and r1 is mediadependent on w1 by virtue of reading m1). All of these contingencies are taken as evidence for an intentional relationship of w2 to w1, and collapsed into one uptake arc. Because of this relationship between contingencies and uptake, an uptake graph may be seen as an abstraction of a contingency graph, and many of the same analytic moves (such as pattern matching and tracing actions) apply to both. 
 Contingency and uptake graphs are described more fully in [37]. We have used contingency and uptake graphs to provide interactional accounts of specific accomplishments of participants [24, 25], to trace out information sharing [40], and to detect roles of participants not visible in the final media trace [38]. For example, examining only reply structure (the threading relationship between messages in Figure 1) we might miss the fact that m4 played an integrative role in this discussion. The uptake graph of Figure 3 makes this integration explicit as a structure of uptake converging on w4. Integrative or convergent acts are important to group learning processes such as intersubjective meaning making [35] and community knowledge building [16]. Contingency and uptake graphs represent process models: they focus on how acts relate to each other and constitute a process of interaction. Their basic unit is acts and other events: the actors and entities through which interaction takes place are attributes of these events. Now we turn to an alternative derived representation that makes these actors and entities explicit, rather than the events. 
 8 Associograms: Mediation Model.
 In the study of socio-technical networks, we are interested in how the technological infrastructure enables and is utilized by the social actors to interact with each other. The next layer of the analytic hierarchy makes the objects of this technological infrastructure explicit and shows how they mediate interaction between participants. Analysis at this layer provides the mediation model, and is represented by multimodal bipartite graphs in which participants are related to each other via the objects through which they interact. We call these graphs associograms to distinguish them from sociograms in a manner that honors Latour's [22] concept of mediated associations that assemble a social system. Associograms are multimodal because there may be two or more types of nodes—actors and the various types of media through which they interact—and they are bipartite because they are divided into two partitions: actors in one partition and the various types of media objects in the other. Directed arcs represent state-influence (a weaker form of state-dependency): they extend from an object to an actor if the state of the object is influenced by some action of the actor (e.g., writing a message or editing a wiki), and from the actor to the object if the state of the actor has been influenced by accessing the object (e.g., reading a message or wiki) One can construct associograms from a set of events, whether taken directly from the event model, or events of interest that were selected from the contextualized action or interaction models (contingency graphs or uptake graphs, respectively). A node in any of these models represents an event, and actors and objects are attributes of the node. This is largely reversed in an associogram: actors and objects are nodes, and events are links between nodes. For example, in Figure 4, w1—the event of P2 writing m1—becomes a directed association from m1 to P2 (m1’s state depends on P1), and r1—the event of P1 reading m1—becomes a directed association from P1 to m1. 
 Figure 4. From Events to Associogram. 
 An associogram can be constructed at different granularities. Object nodes could be created for each individual object (e.g., one node for each message, wiki page, chat, etc., as in Figure 4), or they could be aggregated for object types (e.g., all associations via messages aggregated into a single node, those via wikis in another, etc.) in order to characterize how interaction is distributed across types of media. Some information is lost in either case: all the events involving an actor and an object will fall into the same two nodes and links between them. For example, if P1 reads m1 multiple times there is still only one link from P1 to m1, and if P2 edits a wiki multiple times, there is still one link from the wiki to P2. Some of this information can be preserved by weighting the links with number of occurrences, or by putting backpointers to the originating event nodes. Temporal sequencing is mostly lost, though it can be recovered by following these backpointers to the contingency graph. This information reduction is actually an advantage of associograms: they reduce the clutter of interaction models to expose recurring patterns of mediation. An example is given next. 
 8.1 Finding Interaction Patterns.
 Associograms can help expose patterns of interest in contingency or uptake graphs. For example, consider the question of finding which participants are in dialogue with each other. Dialogue is clearly a prerequisite for learning through argumentation, intersubjective meaning-making and group cognition [3, 33, 35]. A key indicator of the presence of dialogue is what we call a round trip: one participant makes a contribution that is accessed by another participant who then makes a contingent contribution (evidencing uptake) that the first participant then accesses [40]. In a contingency graph one would need to trace out many paths from each participant to find paths that go to another participant via a read and then a write and then back to the first participant. In an associogram one need only find cycles in the graph. If the links are weighted with frequency counts, the minimum weight of the path is taken as a measure of extent of dialogue. For example, in Figure 4 there is a cycle (following the arrows in reverse to trace chronology rather than dependency) P2←m1←P1←m2←P2. This corresponds to the round trip in which P2 posts m1, P1 reads it and posts m2 in reply and P2 reads m2, completing the round trip. Note that P2 need not post a reply to m2 to complete the round trip: an analysis that looks only at the threading structure of posted messages and does not include read events would miss this round trip. 
 8.2 Characterizing Mediation.
 Degree and path analysis of an associogram can reveal the roles different media play in a socio-technical network. Media objects or media types (in an aggregate associogram) that have high in-degree are accessed by many actors, and hence may be influential sites where an educational intervention can reach many participants in a socio-technical network. Those with high out-degree are modified by many actors, and hence may be sites where ideas are aggregated or consolidated (potential roles as community memory, or locus of knowledge building). In a weighted associogram, heavily weighted links indicate that actors visit the incident objects repeatedly. These measures may be compared between different media types to assess their relative roles. Additional roles can be identified, such as liaison roles, where the media object or type connects other objects or actors that would not otherwise be reachable. For example, we have used associograms constructed from bridging events to assess the roles of different media (discussions, wikis, resources and profiles) in mediating bridging in a socio-technical network [36]. 
 8.3 Characterizing Mediated Relationships.
 Associograms summarize how objects directionally mediate the interaction between any given two people. The subgraph of all paths of length two (direct mediation) between two persons can be used in at least two ways to characterize the relationships between those persons as mediated by the socio-technical network. First, we can recognize defined patterns, two of which are shown in Figure 5. Second, profiles of mediated interaction between any two people can be represented as vectors of the weights on paths of different types and directions (e.g., P1 to P2 via discussions, P2 to P1 via discussions, P1 to P2 via wikis, etc.). Cluster analysis of these vectors can reveal recurring types of relationships. These approaches are currently being investigated in a dissertation by Kar-Hai Chu, under the authors’ direction. 
 Figure 5. Pairwise Associations (Relationship Model). 
 9 Sociograms: Tie Model.
 Finally, we briefly note that associograms can be transformed to conventional sociograms by transitive closure of the paths between actors, or by other computations that interpret patterns of mediated associations as ties. As shown in Figure 6, this results in a directed graph or an asymmetric matrix representing the ties between actors. Well established methods of social network analysis (SNA) can then be applied [41], but with advantages that would not be realized if one had merely constructed sociograms directly from source data (e.g., surveys about ties). A tie in a sociogram or sociomatrix is really shorthand for a complex network of multi-mediated interactions that develop over time. If suitable back-pointers to prior representations (the associograms and, via them, the contingency graph) are maintained, then results obtained via network analysis of ties can then be interpreted and understood by expanding back to the mediation and interaction models underlying those ties. In fact, bidirectional construction and deconstruction of ties was one motivation for the development of this analytic hierarchy. We wanted to leverage the power of SNA for describing patterns of relationships between actors in terms of the structural positions they occupy in relational networks, but wanted to retain the complex and artifact-mediated interactions that ties summarize. In classic SNA research, ties are identified through manual techniques such as interviews or questionnaires, eliciting subjective perceptions of relatedness to others. Some computer-mediated environments allow for a more automated data collection, such as email networks, but those generally represent explicit, intentional communication. Online environments, through activity logging, offer the potential of automatically computing ties from traces of behavior, but they log actions rather than relationships. Therefore, methods to convert activity and interaction into ties are of value. These traces may not reflect subjective perceptions of relationships between persons, but have the advantage that this conversion can be automated. The analytic hierarchy supports such an automated translation from activity logs to mediated interaction and ultimately ties to which SNA methods can be applied. But the value of the analytic hierarchy does not only lie in automating the gathering of tie data for SNA. Analysis and interpretation can go in the other way as well. Once analytic results are reached using summary representations of ties—or using bipartite representations of artifact-mediated relationships—these results can be interpreted by unpacking the binary ties back into the interactional sequence that they summarize. 
 Figure 6. From Associogram to Sociogram. 
 Part of the unique power of social network analysis is in the ability to operationalize systemic processes, providing a window into social systems that is otherwise difficult to gain. Taking a systemic approach to learning frames the learning process as a group of individuals that, when interacting with each other, or artifacts of interactive activity, create a whole greater than any isolated individual [9]. If we are to analytically study learning processes it is helpful to have frameworks for analysis that can trace the patterns of interaction generated from learning processes, and harness the information available from the structure revealed by those patterns. The framework presented above provides a missing piece in the analytic ability to extract structural indicators (traces) of activity, both intentionally (e.g. discussion board posts), and unintentionally cast off (e.g. accessing a digital artifact), from individuals interacting and learning in mediated environments. 
 10 Discussion.
 10.1 Status.
 The framework discussed in this paper has been used extensively in our research through mixtures of manual and ad-hoc automated analysis, leading to implementation of a principled analytic software toolkit. Research in our own laboratory is diverse: we study technology-mediated interaction ranging from dyads to online communities, and our methods draw on experimental, conversation-analytic and network analytic traditions. We developed the framework to make distributed interaction visible, but also as a means of coordinating the various strands of our own work. Initially we developed contingency and uptake graphs through extensive use in manual analyses [24, 38]. Then we implemented software tools for constructing contingency graphs automatically from log files and developed rudimentary analytic tools that leverage these representations [25, 26]; and we have used associograms in ad-hoc computational analyses [36]. More recently we developed a comprehensive “Traces” analytic toolkit that is implemented in Java with Hibernate persistence and is applicable to a variety of socio-technical networks. The Traces design includes: (1) an Entity-Event-Contingency (EEC) core, supporting the fundamental classes of Entity, Event, and Contingency; and classes that are likely to be common to all analyses (e.g., specializations of Entity into ArtifactBase, ActorBase and IdeaBase abstract classes); (2) an Analytic Core layer, including classes for Uptake, various types of Contingencies, and Composites (CompositeEntity, e.g., discussion forums; CompositeEvent, e.g., chat sessions; and CompositeContingency, used to represent uptake); (3) a Domain Core, with common domain objects such as Chat and Contribution, Discussion and Message, etc. These cores are extended for application to specific data sources such as Tapped-In, using (4) data source specific extensions to the Domain Core such as Calendar Events (these may migrate to the core if they are found to be useful across systems, and (5) classes that map the data source databases and logs to the above. Presently the Traces toolkit does not include associograms: our analyses at the levels of mediation and tie models are handled by exporting to other tools available for the task at hand (e.g., Jung and UCINet). We have imported two years of data from Tapped-In, and are conducting analyses to be reported in future publications. 
 10.2 Multi-Level Analysis and Theoretical Multivocality.
 Although this paper outlines how the analytic hierarchy is constructed as one goes from log data to more abstract representations of action, interaction, mediation and tie, it should be emphasized that the analytic hierarchy is not just a data interpretation framework. It is also intended to be a structural framework for connecting theorizing at different levels. Developmentally, the framework arose out of our own need to reconcile our research on small group interaction in computer supported collaborative learning and online learning contexts with our emerging research on online communities. It was clear that studies of communication networks, and social network analysis in particular, had something to offer, but the “ties” of such analyses seemed to hide away the very processes we were interested in: the interaction and how it was influenced by and appropriated the media we were designing. Therefore we constructed the framework as an explicit bridge between analyses of local interaction and of larger social phenomena, with the expectation that it would also guide our bridging between theoretical explanations at these different levels. This work is sympathetic to calls by Contractor and colleagues [7, 27] for multitheoretical and multi-level (MTML) analyses and models of communication networks, but is based on a different conception of layering than MTML. The MTML approach calls for examining (1) the properties of individual nodes (incorporating attributed-based data); (2) properties of the network under consideration (including dyadic, triadic and global properties); and (3) relationships of this network to other relations over the same network constituents or the same relations as they change over time. These levels change the granularity or scope of analysis, but stay within an ontology of structural relations between a set of network constituents. Our hierarchical approach adds a more “vertical” dimension, changing the ontology between layers, from relationships between observed events, to mediated associations, to direct ties between actors. Marin & Wellman [23] contrast attribute based explanations, which explain behavior in terms of attributes of individuals, with the network analysis' position “that causation is not located in the individual, but in the social structure.” Similarity of attributes is explained by similarity of network positions, due to the similar “constraints, opportunities and perceptions created by these similar network positions.” We agree with their critique of attribute-based explanations but wish to avoid the opposite oversimplification: individuals’ similarities do not arise merely out of static structures piping influences into the individuals from without. A range of thinkers, including Garfinkel [13], Blumer [6] and Latour [22] have argued (each in their own way) that social regularities are constructed and sustained through interaction between actors (whether strictly local interaction, as for Garfinkel, or potentially mediated across time and space, as for Latour). To fully understand social systems we must examine interaction. Colleagues2 have offered the analogy that ethnomethodological interaction is the quantum mechanics of social science. We can ignore it when explaining social life at a Newtonian level, but to really understand the origins of the social world we must dive in and find how fluctuations in microphenomena can have an influence on larger scale change. Latour [22] has made a similar observation in claiming that the “sociology of the social” may seem adequate for explanation of stable states of affairs, but Actor-Network Theory’s “sociology of associations” is needed to understand rapidly changing networks. Our position is that network structures are relevant because of how they support interaction. The network structure is not enough: to explain the origins of social life we must understand the nature of the communication or interaction that takes place. In socio-technical networks, this includes understanding how that interaction is embedded in and exploits the resources of technological infrastructures; i.e., how it is mediated. The present work offers a conceptualization of how to map between these different levels of theory and analysis, viz., structure, mediation, and interaction; and also provides specific representations for supporting analytic work with computational tools. In addition to providing a unified analytic artifact and supporting multiple levels of analysis, a third concern has motivated the work reported here. Researchers from multiple theoretical and analytic traditions are studying distributed and networked learning, virtual organizations, and similar socio-technical networks. This diversity can mean balkanization, or it can be a strength. A single integrated discipline of Learning Analytics may not be possible or even desirable, but there must be some basis for dialogue and coordination between the traditions. Shared instruments and representations mediate the daily work of scientific discourse [21], and advances in scientific disciplines are sometimes accompanied with representational advances. Similarly, researchers studying socio-technical networks could benefit from shared ways of conceptualizing and representing distributed interaction, or at least from boundary objects [34] that make discourse between multiple analytic traditions possible. We offer this framework as a potential basis productive discourse among multiple analytic voices [39] in the study of socio-technical networks such as networked learning by enabling the development of shared conceptualizations, representations, and tools at a given level of analysis and supporting bridging between different levels of analysis. 
 (2) Ravi Vatrapu, personal communication, July 28, 2007; David Sallach , personal communication, May 23, 2010 
 
 Acknowledgements.
 This work was supported by NSF Award #0943147. The view expressed herein do not necessarily represent the views of NSF. The authors thank Nathan Dwyer, Richard Medina, Ravi Vatrapu, and Kar-Hai Chu for discussions and collaborations through which the present framework was refined.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>A Unified Framework for Multi-Level Analysis of Distributed Learning</rdfs:label>
		<dc:subject>socio-technical networks</dc:subject>
		<dc:subject>distributed learning</dc:subject>
		<dc:subject>networked learning</dc:subject>
		<dc:subject>interaction analysis</dc:subject>
		<dc:subject>social network analysis</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/devan-rosen"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/devan-rosen"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/53/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/devan-rosen"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/54">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Revisiting Formative Evaluation: Dynamic Monitoring for the Improvement of Learning Activity Design and Delivery</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/54/authorlist"/>
		<swrc:abstract>Distance education courses have a tradition of a formative evaluation cycle that takes place before a course is formally delivered. This paper discusses opportunities for improving online and blended learning by collecting formative data during course presentation. With a goal of overall improvement in instructional effectiveness and identification of promising practices for inclusion in a learning activities design library, we propose the immediate and on-going monitoring of the effectiveness of learning activities, tutor facilitation and learner satisfaction during the course presentation. This has implications for constructively involving the learners and facilitators in the course improvement process. While originally conceived to reduce the time for pilot evaluation of new courses and learning activities, the proposed system could also be extended to individualized and blended learning environments, and if implemented using semantic web technologies, for research into the effectiveness of learning activity patterns.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 Distance education has a long tradition of conducting formative evaluation of instructional materials and learning activities before the ongoing delivery of a course. The feedback from pilot testing and expert evaluation enables course designers to catch and correct any weaknesses detected. The lessons learned can be incorporated into the professional design heuristics of the course designers enabling promising design practices to be reused in new courses, and disappointing practices to be redesigned or rejected. In recent years there has been an influx of traditional “face to face delivery” institutions to the online environment. Sometimes, indeed often, they expect the instructor of a face to face course to convert their course (or certain activities of their “blended” courses) to on-line delivery with a minimum of instructional design support, and there is little provision for observing which learning activities work and which require improvement. Typically course evaluation takes place at the end of the course, after the final marks have been submitted, but before learners receive grades. This delayed process does not capture immediate responses and reflections in time to provide meaningful formative evaluation that might enhance the learning experience of a course in session. Thus for both blended and distance courses a case can be made for a system for improving formative evaluation. This paper looks at the potential for embedding formative evaluation tools in both online and blended course delivery. Our goal is to improve the quality of online and blended learning experiences, along with facilitation and instructional design practice, by stimulating ongoing reflective practices among course designers and course facilitators such as professors, instructors, mentors or tutors. We recognize that there may be pitfalls to openly soliciting feedback from learners during a course, and there may be governance and collective agreement issues arising from providing feedback on the effectiveness of learning activity facilitation. The proposals contained here are work in progress, and an opportunity to open dialogue and critical reflection on this topic. We are fully aware that every on-line cohort establishes informal back channels where the learners actively blog their opinions – possibly the only ones not in the conversation on instructional effectiveness are the faculty presenting the course. 
 2 Formative Evaluation.
 Scrivens [1] coined the terms “formative” and “summative” evaluation to distinguish between evaluation of educational materials during their development and at the end of the instructional cycle. Formative evaluation is intrinsic to instructional systems design models [2] [3] and it has been ingrained into the development cycle of most distance learning organizations that produce instructional media or course packages. During the “big media” phase [4] when distance education was dominated by centralized production facilities turning out television shows and print packages, formative evaluation was a key part of ensuring quality before printing hundreds of copies for the warehouse or broadcasting on television. Distance learning was in a sense asserting its rightful place, and the best way to counter criticism of traditional universities was to demonstrate the quality of the courses was as good as if not better than the traditional offerings. Indeed, what most distance learning courses lost in presence they more than made up for in a systematic approach to development, the alignment of course materials to instructional objectives, and the thoroughness of content delivery and student assessment [5]. Formative evaluation was also an essential part of multimedia development [6] and carried into web site development [7]. It was evident in early on-line course development, again in response to concerns about the quality of courses that simply shovelled content onto the web [8] [9]. Various formative evaluation approaches were suggested by Reigeluth and Frick [10] with the intent of improving instructional design theories that then would translate into improved theories and instructional design. However, as online delivery became mainstream and blended with classroom instruction, formative evaluation seemed to lose its earlier attention in the literature. Perhaps course designs and instructional activities became somewhat standardized, but probably the real reason was that increasingly in the 2000s, web delivery had become accepted as a credible indeed essential extension of the academy. With the volume of courses to be transferred to the web there were insufficient instructional design resources to conduct formative evaluations. This period of adjustment was characterized by the downsizing of resources for centralized distance learning departments as faculties set up their own distance programs, the growth of learning management systems making it easier for individual instructors to load content online, the rise of the dual mode university and in Canada the reduction in the number of single mode distance universities [11] [12]. Traditionally neither formative nor summative evaluation has seen a comfortable fit in the face-to-face classroom [13]. Courses were taught by faculty who were expected to “get the bugs out" in two or three terms. As this same expectation is creeping into the practice of online education, formative evaluation in online learning has not seen a high profile in practice over the past decade. Yet formative evaluation can strengthen both the implementation of a program and the knowledge gained within it [14]. 
 3 Challenges in Online Educational Practice.
 In addition to changes in instructional development models for online courses, the past twenty yedars of educational practice have seen a change from objectivist philosophies and paradigms to increasingly constructivist views [15][16]. The internet is increasingly seen less as a medium of delivery and more as a medium of communication in which interactions can take place among learners and instructors, and learners and the content [17]. Traditional models of distance education offered individual delivery of content-based instructional materials. Alongside the growth of channels for interaction, cohort paced courses have been implemented. These require learners to interact in many ways and to create new knowledge together. The resulting learner engagement can promote both achievement and retention. The need for formative evaluation of learning activities in online, paced cohort courses is important in view of this shifting role of the learner. Learners are active participants with rich and complex experiences. The learners’ engagement in the collaborative activities places them in the position of co-creators of knowledge within the learning environment, as well as self-organizers of their learning [18]. As described by Parrish, “While IDs [instructional designers] work to tame instruction into a manageable, replicable process that begins by predetermining outcomes to be measured through properly aligned assessments, engagement describes that wild aspect of the process in which the learner is as much or more in control of the activities as the ID” [19]. The situatedness of the learners and the contexts in which they find themselves become meaningful realities in the learning environment [20]. Development of community, shared practices and reflection are important parts of learning activities. The application of cooperative learning techniques to the design of learning activities for cohort-paced e-learning can produce engaging discussion, reflection and deeper processing of the content. With instructor-facilitated cohort/collaborative approaches providing such positive results, distance learning course providers are abandoning investments in comprehensively detailed content packages and elaborate instructional designs. Institutions notice that these changes make a difference, and cohort-paced distance learning courses have lower drop-out rates than their self-paced counterparts, about 85% retention versus 65% for individualized delivery [21] [22]. In a review of literature Means, Toyama, Murphy et al. [23] noted significant effect sizes for facilitated and collaborative online learning when compared with individualized delivery for the same content, although they were careful not to attribute this as a media effect noting that the cohort modes often involve different activities and increased time on task. At the same time, it is clear from the research that the many of the types of activities included may be of little value. For instance, they also noted that the provision of extra video clips and chapter quizzes contributed little to student achievement, while activities that provoke reflection and engage the learners’ metacognitive processes can yield improvements in learning. Richards [24] observed that trivial learning activities such as knowledge level multiple choice quizzes, or forum directions to “post your thoughts and reply to the thoughts of two other learners” led to a superficial understanding of the course content. It is therefore important to continue formative evaluation in these dynamic new learning environments, in order to determine which activities are both valued and valuable and those which are little more than "make work" projects. Feedback from learners in these environments is necessary in gaining a better understanding of these activities. In this paper we strongly advocate for careful design of such activities for cohort-paced elearning, and suggest that if formative evaluation is no longer conducted before the delivery of a course, then it should be embedded into the course delivery. This should be simple to implement. Finally, since the purpose of formative evaluation is to inform practice and improve delivery, the process should promote reflection on the part of learners, instructors and designers as all have a role to play within the learning experience, and all might benefit from an open discussion on improving the learning environment. 
 4 Other Benefits of Formative Evaluation.
 Eijkman raises a series of questions we as educators need to consider in our use of web-based learning and social environments. For instance, what “practices, habits, and patterns of use emerge?" and “What changes need to occur in institutional policies and technological practices in order to integrate the social Web effectively into the educational mainstream?” [26]. Documentation or other forms of visualization of learning activities and designs can help to capture emerging innovative and expert practices [26] [27], and to gain a deeper understanding of the user experience. While not a primary focus of this paper, research and development around reusability or adaptation of learning activities and designs along with educational policy and practice can also benefit from formative evaluation. If formative evaluation of activities leads to their improvement over time due to the use of this feedback in updating and maintaining courses, these activities can be added to design libraries for re-use and sharing. Further, analysis of the broader emerging patterns may be incorporated into strategic and operational planning. The goal in the end of improvement of learning activities and designs is improved quality of instruction [28]. 
 5 A Simple Micro Model of an Online Learning Activity.
 Fig. 1 diagrams three nested levels of the design and delivery of an online learning activity. Level 1 is the Instructional Design Level – the level at which instructional goals are aligned with learning activities that are appropriate for the learners. Level 2 is the Facilitation Level – and encompasses those roles, activities and resources that come together during the conduct of a learning activity. Level 3 represents the Learner Experience. Note that each level has been allocated three phases of preparation, enactment, and reflection. It is our belief that this is the simplest depiction possible for our purposes and we recognize that learning environments and learning activities may become extremely crowded with multiple roles, players and resources. We fully anticipate that other evaluators may want to expand this depiction to be more explicit or to compact the phases to be more specific. In some settings the design and facilitation roles may involve the same individual(s). In some settings the facilitator may also be consulted in the design process, while in others, the facilitator may become involved years after the initial design, after a course has run several times. 
 Fig.

 1. A Conceptual Model for Dynamic Evaluation of Learning Activities.
 Online learning activities evolve to meet the needs of content, audience and the constraints of the instructional system. The model looks at a single activity, whereas a “course” is a strategy of intentionally sequenced progression through a series of learning activities. Some activities such as a lecture are well-structured, and others like a reading assignment are loosely-structured. It is also possible that parallel learning activities such as study groups may be autonomously initiated and conducted by a learner or group of learners as they form a learning community. Whether these should be included in the scope of the Dynamic Evaluation Model is left to the discretion of those conducting the evaluation. Similarly, there may be others external to the instructional process having a bearing on the results. While Garrison and Anderson [29] only identify instructors, peers and content in their interaction model for online learning the actual educational environment may include professional faculty developers, mentors, peers, friends, family and others – anyone who influences the decisions and performance of any of the key roles. 
 6 Aligning the Model with Instructional Design Methodology.
 As discussed earlier, in a cohort-paced constructivist learning activity not all learner activity is predictable since learners bring their own experiences and contexts to the learning situation. While situated in the design and execution of intentional learning activities, the model also takes into account learners’ own experiences of the activity. We use the term “learning activity” to avoid confusion with the more technical terms “learning design” and “lesson plan” which are expressions of learning activities. The term “learning activity” encompasses any activity that brings learners into planned contact with content, other learners, and experiences that promote acquisition of skills, knowledge and attitudes. This broad definition is congruent with similar definitions [23]. While traditionally instructional design does not include accidental nor incidental learning activities, in more open ended learning environments learners might influence the learning environment in unpredictable ways, and in their search for alternate explanations may discover materials useful to others. To the extent that instructional design is an intentional and iterative process, we look at preparation (planning and alignment of goals with activities), the design itself, and reflection on the outcomes of the design. Preparation is included as part of facilitation because so much success depends on the facilitators’ skills and knowledge of facilitation techniques, their understanding of the activity and their role in and commitment to its success. Preparation is also important for learners in terms of both prerequisite skills and knowledge and in terms of adequate direction to participate the learning activity. We believe that reflection is a part of all processes; and in terms of improving the system, early reflection catches errors before they can become deeply embedded in the teaching-learning system. 
 7 Practical Issues.
 The goal of formative evaluation is to improve the learning experience. If evaluation of the learning activities is not conducted until the end of the course or beyond, then no remediation can take place if there is a problem. We propose the following guidelines: 
 1. Formative evaluation should take place during or at the end of each learning activity. 2. Formative evaluation should seek data and reflections from learners, facilitators and designers. 3. Formative evaluation to seek both quantitative and qualitative data. 4. The results of the formative evaluation should be open to all participants. 5. If error correction is required, it should be considered immediately 6. If activity re-design is required it should be embarked upon so that it can be revised for the next course offering. 7. If learning activities are to be evaluated for several courses, then investment in an evaluation system to gather and analyze the data should be considered. 8. The results of dynamic formative evaluations may have value in explaining the findings of end of course evaluations, and in the evaluation of generic learning activity designs, including the training of facilitators, and the directions provided to learners. 
 For purposes of brevity, we have not described the importance of linking such a system to descriptive ontologies such as Learning Object Context Ontology [30]. However, we believe that semantic tagging will enhance the ability of researchers and designers to better understand the patterns that may emerge from the data collected, and raise the importance of both instructional activity design and evaluation as part of organizational learning. 
 8 Proposed Implementation.
 Richards [24] embedded questions on the efficacy of cohort learning activities in a graduate distance course in Instructional Design at Athabasca University. For each activity, the learners were asked eight questions to rate their experience (along a five point Likert scale) and provided an opportunity to comment. This rating has been conducted a number of years and Fig. 2 shows a typical result. As the evaluation was constrained to a single course, a Moodle questionnaire was used to present the questions and collate the data. Unfortunately, the raw data are not available so neither is further analysis - even simple statistics such as the standard distribution or the maximum and minimum values are unavailable. For a more robust system capable of handling multiple courses we propose to implement the dynamic evaluation system external to the learning management system so that we can have greater control over the data, and the results would be then returned to the course participants through a web service. The course instructors and course designers would also have access to the participant comments. In a course with several class sections or perhaps teaching assistants, additional questions could be developed to link into each section and to pass back the appropriate identifiers to and from the LMS. If the function of the embedded questionnaires is to improve the learning activities, then the most important question is what suggestions the participant offers to improve the learning activity. For research purposes, it will be useful to ferret out other additional information for example on the role of the facilitator in animating a learning activity. While it would be appropriate to ask the learners if "the facilitator/ instructor/tutor contributed to the success of this activity", it could only be interpreted in light of facilitators’ own reflections about their preparation for the activity, the amount of time devoted to the activity, and other such factors. Similar questions might be asked of the course designers when they review the results of the activity. It is important to note that it may be very easy or very difficult to pin down why a cohort activity works or does not work. For example the questions used in Fig. 2 take for granted that many preparatory steps had already gone correctly: learners had the appropriate prerequisites, text books had arrived, individuals had read the prescribed materials, there were no untimely interruptions in internet services, and other such assumptions. These are extrinsic factors. Intrinsic factors are more within the realm of the course developers and facilitators – was the activity relevant, was the group size appropriate (what about the group make up?), was the time appropriate. The outcomes are the feeling of connectedness (which is a vector on group cohesion), that all members of the group contributed equally is in part a function of the balance between individual and group accountability – group projects generally do not work if there is no positive interdependence [311]. Finally, the achievement worthiness is important: was the activity worthwhile and did the activity help with learning? We can well imagine learning activities that are well-intentioned but involve superficial treatment of the content and thus provoke little or no deep learning and have little long term effect on understanding or retention. Dynamic formative evaluation seeks to gather data to ascertain the effectiveness of a learning activity, if required remediate with the current learners, and make adjustments as required in the activity before the next class. The adjustments may be with the content and materials, the directions to the facilitator role, or directions to learners. However, as noted earlier, a significant value of dynamic formative evaluation may be in generalizing the lessons learned and formalizing the expertise so that it can be shared with other course developers. This loftier purpose requires the design of a data base that is semantically enriched so that pattern description of the activities and the roles can be generically described with ontologies such as the Learning Object Context Ontology (LOCO) and extensions [30]. The semantic tags will enable pattern analysis across several courses, initially to allow course developers to locate and view how winning activities are embedded in existing courses, but also in the long run to identify and extract patterns into a library of successful practices. This then brings us close to the ideals of Koper [28] in documenting successful learning designs that can be reused in a pragmatic manner. Before closing it is important to note that a key implementation issue will be acceptance of the system by all users. In distance delivery student response to end of course questionnaires administered by administrative and marketing groups can be as low as 10 per cent, while Richards [24] found embedding the questions as part of the course brought a 100 per cent response rate. For dynamic formative evaluation to be effective it needs to be an active part of the learning experience – the questions should provide feedback to the learners on how their perceptions and experience compared with that of others, and there should be an active response to problem areas identified. Going further, the dynamic evaluation system could also solicit suggestions to update the learning resources that might be used to help others learn – moving from a prescriptive to a constructive learning environment has been a successful strategy in the corporate learning context of the IntelLEO Project [32]. Similar benefits should be obvious for facilitator and course designers in improving the quality and efficiency of the online learning experience. 
 Fig. 2. Typical results of a learning activity evaluation in MDE604. 
 9 Summary.
 In summary, the purpose of this paper was to provoke discussion about the need to revisit formative evaluation of elearning activities and course designs. If elearning and blended models are the new reality of distance education, then formative evaluation is more important than ever. Because of the proliferation of distance education, much of it developed without the assistance of an instructional design team, and the complexity of constructivist learning design in cohort-paced courses, in many cases formative evaluation needs to take place during early course delivery. A dynamic process for formative evaluation on the success of learning activities (whether designed or not) is important in the creation of an informed community of practice. Currently, because of back channel communications among the learners, the only ones out of the feedback loop are the instructors and course designers. A dynamic learning activity evaluation system will help to close that gap.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Revisiting Formative Evaluation: Dynamic Monitoring for the Improvement of Learning Activity Design and Delivery</rdfs:label>
		<dc:subject>formative evaluation</dc:subject>
		<dc:subject>learning activities</dc:subject>
		<dc:subject>cohort-paced</dc:subject>
		<dc:subject>online learning</dc:subject>
		<dc:subject>elearning</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/griff-richards"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/griff-richards"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/irwin-devries"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/irwin-devries"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/54/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/griff-richards"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/irwin-devries"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/55">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>iSpot Analysed: Participatory Learning and Reputation</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/55/authorlist"/>
		<swrc:abstract>We present an analysis of activity on iSpot, a website supporting participatory learning about wildlife through social networking. A sophisticated and novel reputation system provides feedback on the scientific expertise of users, allowing users to track their own learning and that of others, in an informal learning context. We find steeply unequal long-tail distributions of activity, characteristic of social networks, and evidence of the reputation system functioning to amplify the contribution of accredited experts. We argue that there is considerable potential to apply such a reputation system in other participatory learning contexts.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction to iSpot.
 Fig.

 1. The iSpot home page, showing images of latest observations (top left), observations from this time last year (bottom left), and the sign up/search/log in panel (right). 
 iSpot [1] is a website that enables learners to post observations of wildlife, and get help with identifying what they have seen from experts and other users. The aim is to encourage learners to engage with the natural world, to enable them to identify species and hence to learn about what they have seen. The main activity on iSpot is the posting of observations (see Fig. 2). Learners upload photos of wildlife they have seen, and log when they saw it and where. If they are able, they also add an identification: the name of what it is they have seen. To help with the quality of the identification, iSpot can look up scientific names from common names, and check spelling with the dictionary of UK species curated by the Natural History Museum. Other users of the site – whether experts or beginners – can then click ‘agree’ on the identification if they think it is correct, or post an alternative identification if they think it is not. A sophisticated but simple-to-use reputation system, described in more detail in section 2.2, gives users of the site feedback about the expertise of other users. The site is organised into eight broad taxonomic groups: Amphibians and Reptiles; Birds; Fungi & Lichens; Fish; Invertebrates; Mammals; Plants; and Other organisms. 
 Fig. 2. An observation on iSpot, showing the user who submitted it (top), the image captured (just below), and an identification, the number of agreements with that identification, and links to the Encyclopedia of Life and the NBN (shaded box at bottom). 
 The UN declared 2010 to be the International Year of Biodiversity, to celebrate the diversity of life on earth and of the value of biodiversity for human lives [2]. However, expertise in correctly identifying observations, for baseline recording of biodiversity, is in such short supply that automation has been proposed [3]. But social networking, as harnessed in iSpot, can make the process more efficient, and spread taxonomic knowledge [4]. The goal of iSpot is not to produce new data, but new data recorders. (1) 
 Fig. 3. The iSpot ecosystem, showing the iSpot website (centre) surrounded by users, experts, formal learning, broadcasts, iSpot Keys, and mentors (around). 
 The iSpot website does not stand alone. It is part of an ecosystem of activity specifically designed to support and encourage learning at scale (see Fig. 3). In the UK2 there are many (largely) voluntary expert organisations who are the repository of expertise in identification and recording of particular taxa of wildlife. There are also many millions of enthusiastic viewers of nature programmes. With strong links with BBC TV and radio broadcasts, iSpot helps motivate users to take their interest further, providing an informal learning space. Learning pathways that take users beyond the website were also designed in from the start. There are the expert organisations, who are generally very keen to welcome and nurture new members. There is also a carefully-designed route through to formal learning, via a short open-entry universitylevel course called Neighbourhood Nature, offered by the Open University in distance mode, which could form the first step on a journey through to a related degree. The ecosystem also includes a network of biodiversity mentors, who engage in outreach activities in their local regions, taking pro-active steps, particularly to reach under-represented and under-privileged groups. Finally, there are iSpot Keys, which are designed to provide a new, more learnerfriendly way to identify species than traditional dichotomous keys. A key strategy of the project was to aim to do one thing well (social networking to support learning about identification of wildlife), leaving other sites to do other tasks more effectively. So apart from the iSpot Keys, the site does not itself provide significant help with identification, but links instead to other excellent resources that are available online, such as field guides. Similarly, the site does not contain information about particular species, but provides direct links to the Encyclopedia of Life (EOL) [5], an online reference which aims to cover all 1.9 million species known to science. 
 (1) However, useful data has been generated: some expert organisations working with iSpot are very interested in transferring records to formal recording schemes. 
 (2) At the time of writing, iSpot is funded to operate only in the UK and Ireland. No geographic restriction is in place on use of the site, but as species – and expertise in identification – vary so considerably around the world, we are working to set up versions of iSpot tailored to other areas. 
 2 Design Rationale.
 2.1 General.
 iSpot is an example of what the MacArthur Foundation describes as participatory learning [6]. The learner is an active participant, engaging in authentic, open-ended creative activities, developing their interest and passion. This type of participatory learning is an excellent complement to traditional approaches to Open Educational Resources (OER), providing a motivating link between the worlds of informal and formal learning [7]. The design of iSpot was underpinned by the notion of shared social objects (see e.g. [8][9][10][11][12]): there was a focus on sociality, rather than pure functionality; and on social interactions mediated by a social object (the observation), rather than a pure social network. Both the object (the observation) and the social conversation related to it (the identifications, agreements and comments) are important. 
 Fig. 4. Fairy rings of participation, showing the visible layer of participation operating in four different modes, underpinned by an invisible network of interactions. 
 In a community of practice [13], members learn from legitimate peripheral participation [14]. Preece and Shneiderman [15] categorise successive levels of social participation in online communities as reading, contributing, collaborating and leading. An analysis of participatory activity on 50 sites [16] suggests that users connect, participate and collaborate around a shared object, transferring information and pooling knowledge within and between communities, in four different modes: 
 1. Browsing, gathering and sharing content 2. Giving and receiving feedback and expertise 3. Collaborating and jointly deciding about actions 4. Sharing control with other members over the community and content 
 A user’s participation is influenced by several hidden elements (e.g. reciprocity, identity, habits, real-world probes), which may not be immediately evident in their actions, but can be inferred by analysing user reactions. Engeström [17] compares such underground activities to mycorrhizae: symbiotic associations between the fungus and roots of a plant. We liken this process to a “fairy ring”, a circle or arc of mushrooms, where the mycelia of a fungus grows invisibly beneath the surface, but its presence can be inferred from occasionally wildly fruiting mushrooms, and also from careful examination of the surface. Figure 4 shows what we call a “fairy ring of participation”, which informed the design of iSpot and the analysis presented below. All activity on iSpot is visible in the sense that there are no private or restricted spaces. This is to maximise the potential for learning through legitimate peripheral participation, and to reduce the potential for unpoliced abuse. The development process of iSpot was underpinned by this theoretical understanding, and followed a simplified version of the socio-cognitive software design approach [18][19][20], drawing on aspects of Agile development [21]: extensive consultation and envisioning with stakeholders, followed by rapid iterative release informed by feedback from users. 2.2 The Reputation System and Learning Analytics In formal learning, a teacher (loosely defined) arranges learning activities for students, and motivates and tracks their progress through formative and summative assessment. Indeed, it is widely attested that assessment ‘defines the de facto curriculum’ [22][23]. The results from assessment can also be used to inform improvement of the learning activities. This feedback is very important for both the learners (to improve their own learning) and the teacher (to improve their teaching). In informal learning, there is no ‘teacher’ as such, although there may be someone who arranges learning activities for others. The participants do not usually have the same access to external feedback about their learning. Additionally, a teacher in formal learning can often track student progress through direct observation of learning situations, whether face to face or online. Learning analytics can be useful in making learning visible in ways it would not otherwise be. Learning analytics promise to be particularly useful for informal learning contexts, where direct observation is even more problematic. The iSpot reputation system was designed to support learning on the site by providing a form of external feedback, recognizing and rewarding the activities that the team wanted people to engage in on the site. There is a large literature on reputation systems (e.g. [24] [25] [26]), which has informed the design of the iSpot system. The purpose of any reputation system is to facilitate trust between users, by making actions and feedback transparent, and encouraging reciprocity. Particular attention was given to incentives for users to provide honest, positive feedback. Some users find negative feedback disproportionately demotivating; some might seek to escape it by creating a new, separate account. So no facilities for negative feedback are provided. 
 One concern is the potential problem of users colluding to rate each other positively, perhaps using multiple ‘sockpuppet’ identities. This is known to be very hard to defend against without a central, trusted authority [27] that can identify a trusted user or users to be the source of reputational score [28].3 iSpot uses just this approach, using the network of experts from expert organisations. The use of ‘experts’ as a source of reputation does not depend on a particular disciplinary epistemology: for subjects outside ‘hard’ Science, Technology, Engineering and Medicine, it might be more appropriate to describe them as what Northedge [29] would call fully functional users of the discourse within a particular specialist discourse community. There are two aspects to the iSpot reputation system, which are displayed together on the users’ profiles (Fig. 5): social points, and scientific scores. 
 Fig. 5. Part of an iSpot profile for a single user, showing social reputation (Social Points), and then, for each iSpot group, a ‘star’ rating of icons, with a count of observations added, identifications made, agreements received, and agreements given. Note a single icon for Fish (denoting score > 0), a gold icon for Fungi and Lichens (denoting an expert), and three icons for Plants (denoting score > 10) – see Table 1. 
 Social points are gained for engaging in activity on the site: the number of observations posted, the number of identifications made, and the number of agreements received and given. Even these ‘social’ measures are not entirely separate from technical/scientific approval: ‘agree’ appears superficially similar to a social network’s ‘like’ functionality, but here it carries the connotation ‘agree that this identification is correct’, an arguably less subjective judgement. 
 (3) Or for each user to calculate reputation scores for themselves – which is mathematically equivalent to them treating themselves as a trusted user. This is unwieldy in a social network, since different users would see different reputational scores for the same user. 
 Scientific scores are intended to be a rough measure of expertise in accurate identification. Because the skills needed for accurate identification vary between species, the scientific scores are recorded separately for each of the eight groups. The scores are accumulated as follows: experts (known to the iSpot team to be such) are given a starting score of 1000,4 and all other users start with 0. If user A ‘agrees’ with an identification of user B, user B’s score increases by user A’s current score divided by 1000. So, for instance, if an expert agrees with user B’s identification, user B’s score will increase by 1000/1000 = 1. If user B – with a score of 1 – then agrees with an identification, that will increase the score of that user by 1/1000. For any particular identification, a user can accumulate a maximum of 1 to their score. 
 Table 1. Relationship between score and icons shown. 
 The scientific scores are not shown directly, but indicated by a number of icons representing the score, calculated on an exponentially-increasing basis5 as shown in Table 1. These icons are displayed on a user’s profile page (Fig. 5). The relevant icons are shown next to a user’s name in any context where their reputation in that group is pertinent. So, for instance, it is shown next to their name on an observation (Fig. 2), and next to the list of users who have agreed with an identification (Fig. 6). 
 Fig. 6. Part the list of users who have agreed with a particular observation of a Bird on iSpot, showing the names of users who have agreed with the identification, icons indicating their reputation in the Bird group (blue bird icons, gold-circled bird icon), and relevant ‘badges’ for expert organisations (e.g. black butterfly on yellow square). 
 (4) The factor of 1000 was chosen based on many factors, including: providing a clear initial distinction between known experts and novices; providing scope for reputation to grow over time; and making it very tough but feasible to achieve the maximum score. 
 (5) An exponential was chosen over a linear approach in expectation of the highly non-linear pattern of activity and reputation that is observed – see p.11. 
 The ‘agree’ mechanism is designed to encourage and reward reciprocity on the site: nearly anyone can agree with an identification, regardless of their knowledge, and give it a virtual ‘thumbs up’. However, the reputation score calculated on the basis of these agreements is not so egalitarian. The site displays ‘badges’ for users who have been identified by the iSpot team as being members of expert organizations. This provides a reputational marker for those experts, and also provides a way for those expert organisations to connect with interested users (the ‘badge’ is a link to their organisation’s web page). The experts thereby have an incentive to contribute constructively to the community. In addition, a key source of expert contributors to the community is the iSpot project itself, through the expert team members and through the network of mentors. 
 3 General Learning Analytics.
 Having explained the design and functionality of iSpot, this paper now presents empirical data to illustrate how participatory learning and reputation unfold in a real setting. There are three main sources of data: Google Analytics, active on the site since before the launch on 27 June 2009; the database of the site itself; and qualitative analysis of the activity on the site. The dataset analysed covers the period from the earliest online availability of iSpot on 29 September 2008 up to 4 October 2010. There is very little activity before the public launch on 27 June 2009. The dataset contains 6,487 users, 27,493 observations, 47,355 images, 33,088 identifications, and 83,029 agreements. Users have posted an average (mean) of 4.2 observations; observations have an average of 1.7 images and 1.2 identifications; and identifications have an average of 2.5 agreements. However, some of these averages disguise very asymmetrical distributions, as will be shown. 
 Fig. 7. Visitors to iSpot over time, from Google Analytics. 
 The number of visitors to the site (Fig. 7) shows many interesting patterns. The number of visitors increases over time, but shows a series of very sharp spikes. There is also a clear weekly pattern of activity below the big spikes: the site is much more heavily used during the working week than at weekends. The very sharp spikes occur at the same time as iSpot received coverage in the mass media. The first is on 13 October 2009, when a news story about a six-year-old girl spotting (via iSpot) a rare species of moth for the first time in the UK received considerable coverage in the national press. The next spike occurs on 6 April 2010, when a national radio programme (Saving Species on BBC Radio 4, weekly audience over 1m [30]) mentioned iSpot at length, with a call to action. iSpot has an ongoing relationship with the programme. There are subsequent spikes in traffic, on 22 June 2010 and 29 June 2010, when iSpot was again featured with interviews with iSpot users and an explicit call to action. There is a further spike on 18 May 2010, when a popular television broadcast (Springwatch on BBC2, audience 2.1m [31]) mentioned iSpot on air. There is a clear difference in the pattern of activity around these spikes. The first spike, around the moth story, shows little ongoing effect after the initial interest. But the traffic spikes from the broadcasts show evidence that a significant proportion of those visitors became ongoing, long-term visitors and contributors. Indeed, Tuesdays (the day Saving Species is broadcast) are the busiest day of the week on the site. 
 Fig. 8. Observations posted to iSpot, by month posted, from database. 
 The data from Google Analytics are matched by activity apparent in the database. Fig. 8 shows the number of observations posted to iSpot per month. The broad shape is similar, but the pattern is smoothed out. There is some evidence of an overall seasonal peak of activity, matching the number of species active and the desirability of field work. (Only the hardiest insects and naturalists are active in mid-winter.) Table 2 shows the activity on the site, broken down by group. Invertebrates is by far the most active group, followed by Plants, Birds, and Fungi & Lichens. The other groups have very low activity by comparison. There are interesting patterns in the numbers of users who post observations, identifications, and agreements. For Invertebrates, Fungi & Lichens, and to a lesser extent Plants, these figures fall off fairly sharply. For other groups, including Birds, the decline is much less steep. This may be because identification of common species in Birds is easier for non-experts than in other groups. 
 Table 2. Activity on iSpot, broken down by group, from database. 
 The seasonality apparent in the overall numbers of observations (Fig. 8 above) is revealed in finer texture when the data is disaggregated by group as in Fig. 9, which shows the number of observations per month by the date of observation. (6) 
 Fig. 9. Observations posted by month observed (not date added to iSpot) for selected groups. 
 (6) NB The date of observation is different from the date the observation was added to iSpot. Users often upload observations going back some time – there are even two observations in the dataset with an observation date in the late 1980s. 
 The peak for Invertebrates appears to come in July/August, but for Plants slightly earlier. Fungi & Lichens peaks much later in the year. These peaks parallel the abundance and ease of identification of the species: plants are most easily identified when in flower, and it is almost impossible to identify fungi when they are not fruiting without specialist equipment. The Fungi and Lichens peak in November 2009 may be the result of a related survey on air quality, which encouraged observations of lichens. The Birds group appears to have two annual peaks: it may be that the spring peak is associated with an abundance of migratory species, and the winter one with ease of observation (lack of foliage) and lack of other observation opportunities. Thus, the analytics here reveal the texture of the learning experience, with strong echoes of real-world activity beyond the website itself. In this and subsequent sections, the paper will focus mainly on the Invertebrates group, the largest and most active group, for reasons of space. Other groups are largely similar in overall character. 
 Fig. 10. Invertebrates: Observations per user, ordered by number of observations. 
 The activity on the site is far from evenly distributed. Figure 10 shows clearly that the pattern of posting of observations in the Invertebrates group is very unequal. Twelve users have posted more than 200 observations; 194 users have posted fewer than ten. The pattern in other groups is very similar, even in groups with much lower activity. This type of ‘long tailed’ steep distribution has long been attested as a feature of social networks [32]. Anderson argues [33, p126] that it requires three factors: 1. Variety (there are many different sorts of things); 2. Inequality (some have more of some quality than others); 3. Network effects, such as word of mouth and reputation, which tend to amplify differences in quality. It seems highly likely that all three factors are present on iSpot: the reputation system is precisely designed to amplify differences in quality and reflect that in scores. However, the precise nature of these steep distributions is not immediately clear. Figure 11 shows the same data as in Figure 10, but presented on a log-log scale with a power law drawn for comparison. Even if the data were a perfect fit to this straight line, considerable care would need to be taken before one could claim with any certainty to have discovered a power law [34]. The distribution is steep, and has ‘fat tails’, as does a power law, but is not a power law, or at the very least it is not the same power law over all of the data. 
 Fig. 11. Invertebrates: Observations per user, ordered by number of observations, log-log plot, showing power law (dotted line) with exponent of -1.3. 
 4 Qualitative Analysis.
 This section presents, for context, comparison and triangulation, a brief qualitative analysis of the activity on iSpot in general, and in two of the groups. 
 4.1 General Activity.
 In iSpot, experts, mentors and citizen scientists form the core community, while novices, students from the Neighbourhood Nature course, and nature enthusiasts form the peripheral clusters. The core community helps create links between the different groups by helping out, encouraging and opening up social conversation in the forums and comments section. This conversation shows a transfer of knowledge within and between the different groups, and also with external communities (e.g. Wikipedia, BugLife, British Beetles etc). Initial motivations for commenting, adding an identification or an agreement include content quality of the observed species (e.g. “cool photograph!”), uniqueness (“very rare, I understand it only appears for 3 weeks each year in May and June.”) and relevance (“The hummingbird hawk has it's very own They Might Be Giants song”). 
 However, the level of activity and mode of participation of the different users is highly dependent on the ‘invisible layer’ of underlying elements and motivations (see section 2.2). These include reciprocity (e.g. a user adding content will stop participating if their activity is not reciprocated, while activity that is reciprocated encourages them to add more content; this is especially true for the new members and novices in the scientific field of the group); real-world probes (students asked to post observations as part of the Neighbourhood Nature course); and identity (experts knowing each other will discuss their findings and collaborate to find the right identification for the species). There is considerable evidence of interaction with layers of community beyond the site itself (e,g. “Tompot Blenny? Look at my flickr pictures”, “there's a BBC story about 10 million ladybirds descending on a farm in Somerset”, “all my colleagues now know what a nettle weevil looks like”). 
 4.2 Invertebrates.
 The Invertebrates group shows activity in all four modes of the “fairy rings of participation” model: browsing, gathering and sharing content; giving and receiving feedback and expertise; collaborating and jointly deciding about actions; and sharing control with other members over the community and content The rare moth story (the news story in section 3) illustrates this: an initial observation (“My daughter found this strange moth on our windowsill”) sparked increased interest and activity, growing spectacularly when it was stated that a new species had been discovered (“this is indeed the first British record of the Euonymus Leaf Notcher.”), and this knowledge was transferred into the wider global community (“a colleague in Taiwan, within the moth's native distribution, also confirms it”), and the discovery secured a place in the Natural History Museum collection – after the girl had taken it to Show And Tell at school. There are many examples of learning in a social yet scientific environment. One Neighbourhood Nature student and iSpot beginner starts with a shy observation of a micro-moth and request for help from the experts; this grows into several consecutive activities, with the learner making more accurate and scientific descriptions of the observed species and showing increased confidence about identifications of other users’ observations. 
 4.3 Fungi And Lichens.
 The Fungi and Lichens group contains one single key expert, plus nature enthusiasts and Neighbourhood Nature students. This is a small group with considerable activity, with users helping each other to identify species and providing many agreements – again, working in all four modes. One learner joined the group as a course requirement for Neighbourhood Nature, with little knowledge. Her activity and progress is extraordinary, uploading images and making identifications on her own observations but also on other users’ observations. She links information from external sites to support her arguments, receiving feedback and many agreements from experts and other users. She improves so much that when an expert makes an observation (“white fungus on log with teeth like outgrowths”), she makes a suggestion that it is something different (“Phlebia rufa – Small crust-like fungus growing on dead wood”) that makes the expert rethink and alter their identification (“after [the] suggestion of Phlebia … I was wondering if this one might be a Phlebia instead since it has the pores of a suitable shape …”). A key element in this social conversation is reciprocity in the community around the learner, including the expert’s support. 
 5 Reputation Learning Analytics.
 The reputation scores on iSpot are, we believe, unique as a measure of activity on a social network: they are designed to be a proxy measure of skill in a particular task, rather than a more social measure of acceptability. The analysis presented in this section is exploratory, rather than complete. 5.1 Reputation Received The reputation received by users on iSpot in terms of icons (as per Table 1 above) – is shown in Table 3, along with the number of accredited experts (‘gold’ icons).7 Across all groups, there is a clear tapering off, with large number of users having earning a single icon, and increasingly smaller numbers having earned more. 
 Table 3. Distribution of reputation icons earned by group. 
 The reputation score accumulated by individual users in the Invertebrates group is shown (logarithmically) in Fig. 12. As with the number of observations, it is clearly a sharply unequal distribution. There is also a sharp discontinuity in the curve. This occurs between a reputational score of 1.0 or higher, between N = 618 and N = 619 in the ranked data. The users with a score below 1.0 have not had any expert agree with their identifications, since a single expert’s agreement will add 1.0 to the score. It is perhaps unsurprising that there is a difference in the dynamics of score between those who have received some reputation from experts, and those who have only received it from other non-experts. 
 (7) Not all of these experts are actively engaged on the site in any given period of time. 
 Fig. 12. Invertebrates: log plot of reputation received, ranked by reputation received, showing clear discontinuity at reputation < 1.0. 
 Fig. 13. Invertebrates: reputation received, log-log plot, first 618 users (reputation score ≥ 1.0), showing power law (dotted line) with an exponent of -1.4. 
 The two sections of these data are shown separately in Figures 13 and 14. The shape of the graph for those with reputation ≥ 1.0 (Fig. 13) is not a power law (i.e. not a straight line), but it is very clearly long-tailed. For the population with reputation < 1.0, yet to come to the attention of experts, a logarithmic relationship appears to be a good fit (Fig. 14). These differently shaped distributions are consistent with the patterns of activity being different between the two subpopulations. The data for the other groups are strikingly similar: there is a distinct discontinuity between the population with scores influenced directly by experts, and those without. 
 Fig. 14. Invertebrates – reputation received for users with reputation < 1.0, showing logarithmic curve fit (solid line) giving y = -0.197ln(x) + 0.8317, R2 = 0.98. NB Not log plot. 
 5.2 Reputation given.
 Fig. 15. Invertebrates: Reputation given ordered by reputation given, log-log plot, showing power law (dotted line) with exponent of -3.6. 
 The reputation given by a user is the sum of the amount by which they have increased other users’ scores by clicking ‘agree’ on an identification. This quantity is not shown on the site in any way. The sum total of reputation given will exceed the sum of reputation gained, since an individual identification can only gain the user a maximum of 1.0 point, but many experts may agree with the same identification, which is counted as reputation given of 1.0 points times the number of agreements made. Figure 15 shows the distribution of reputation given for Invertebrates. Once again, the data do not fit a power law, but they are clearly very unequal. The sharpness of the fall-off is much higher than for the distribution of observations and reputation received. The data do not observe power laws, but the exponent of a fitted power law can give a (very) rough indication of the sharpness of the decay, yielding -1.3 for the observations (Fig. 11), -1.4 for the reputation received (Fig. 13), and -3.6 for reputation given (Fig. 15): a dramatically steeper curve. The data for other groups are very similar. In summary, the reputation given on the site follows a very, very steeply unequal distribution, even by comparison with observations posted and reputation gained. This is consistent with the intended purpose of the reputation system in magnifying the impact of known experts on the distribution of reputation. 5.3 Reputation Given vs Reputation Received 
 Fig. 16. Agreements received against agreements given for Invertebrates, log-log plot, showing fitted power law (dotted line) with exponent 0.57 and R2 = 0.47. 
 Is there a relationship between the amount of reputation given and received? Figure 16 shows a log-log plot of the number of agreements. The data suggest that if there is a relationship, it is steep and very widely scattered, particularly for lower numbers of agreements: a power law explains less than half of the variance. For reputation given and received (Fig. 17), the picture is similar: such relationship as there is is very non-linear, and the data are widely spread and not well explained by a power law. So there may be some correlation between agreements given and received, and between reputation given and received, but the relationships are different, not remotely linear, and very highly variable: most users give far more agreements/reputation than they receive, or vice versa. 
 Fig. 17. Reputation received against reputation given for Invertebrates, log-log plot, showing fitted power law (dotted line) with exponent 0.345 and R2 = 0.62. 
 6 Discussion.
 This exploration of analytics data on iSpot reveals both the gross, large-scale picture of participatory learning activity on the site, and some of the more fine-grained, nuanced texture of activity, illustrated by the qualitative sketches. Patterns of activity are influenced by the broader context of the site, as in the spikes of traffic from broadcasts, the baseline growth of activity over time, and seasonality. The distribution of activity on the site is highly unequal: that is, a small number of users account for a very large amount of activity, and a large number of users each account for only a very small amount of activity. This pattern is consistent across types of activity and across groups. It is strongly indicative of a functioning social network. The focus on the shared social object in iSpot enables social interaction in a scientific mode. The presence of experts has a noticeable and dramatic effect not just on the quantity of reputation received, but fundamentally on its distribution. The iSpot reputation system is amplifying the contribution of accredited experts on the reputational scores of the users of the site, without significantly inhibiting non-experts from participating and ‘agreeing’ with identifications. The iSpot reputation system allows learners to track their own abilities and that of other users, and appears to be extremely powerful, motivating and useful. There is considerable potential to apply a similar reputation system in any participatory learning situation or domain, providing: skilled, authentic judgements can meaningfully be made by users on the data available; it is easy to indicate agreement with the judgement; there is a degree of consensus among an ‘expert’ community about the correctness of judgements; and the network of users can be seeded with relatively uncontroversial ‘experts’ who have an appropriate incentive to help. The dataset presented here warrants considerable further study. In particular, we intend to probe the nature of the long-tailed distributions more closely, to compare these data with social network analysis, to explore the degree to which learning can be identified more explicitly in the data, and to carry out deeper qualitative research in to the activity on the site. 
 Acknowledgments. 
 We are grateful to the funders: iSpot is provided by The Open University as part of the OPAL project (www.opalexplorenature), which is funded by the UK National Lottery through the Big Lottery Fund. We are also grateful to Jonathan Silvertown, the inventor and leader of iSpot, and to the rest of the iSpot team, including Janice Ansine, Mike Dodd, Richard Greenwood, Martin Harvey, Richard Lovelock, Donal O’Donnell, Jonathan Silvertown, Jenny Worthington, and all others who have contributed to the success of the project.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>iSpot Analysed: Participatory Learning and Reputation</rdfs:label>
		<dc:subject>participatory learning</dc:subject>
		<dc:subject>social networks</dc:subject>
		<dc:subject>social objects</dc:subject>
		<dc:subject>analytics</dc:subject>
		<dc:subject>power laws</dc:subject>
		<dc:subject>wildlife</dc:subject>
		<dc:subject>reputation</dc:subject>
		<dc:subject>reputation systems</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/doug-clow"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/doug-clow"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/elpida-makriyannis"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/elpida-makriyannis"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/55/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/doug-clow"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/elpida-makriyannis"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/56">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Cultural Considerations in Learning Analytics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/56/authorlist"/>
		<swrc:abstract>This paper discusses empirical findings demonstrating cultural effects on social behavior, communication, cognition, online learning and draws implications to online learning. Implications for learning analytics are discussed.</swrc:abstract>
		<led:body><![CDATA[

 1. Introduction.
 According to George Siemens, learning analytics “is the use of intelligent data, learner-produced data, and analysis models to discover information and social connections, and to predict and advise on learning” (http://www.elearnspace.org/blog/2010/08/25/what-are-learning-analytics). The LAK 2011 conference call for papers defines learning analytics as “the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs.” In this paper, we present our vision of leveraging learning analytics tools and techniques to support teachers‟ dynamic diagnostic pedagogical decision-making in actual K-12 classroom settings. Our vision seeks to extend the current state-of-the-art in learning analytics in at least four directions, to apply learning analytics in the primary and secondary education formal classroom settings compared to tertiary education settings, focus on real-time use of learning analytics by teachers for technology enhanced formative assessment, apply an extended version of the pair analytics method in visual analytics, and finally, to review and build on current work in the learning sciences and the method of design-based research. The primary contribution of our paper is the presentation of the preliminary triadic model of teaching analytics (TMTA). The remainder of this paper is organized as follows. Section 2 presents prior empirical work documenting cultural influences in online learning settings. Section 3 first presents cultural differences in teaching and learning in formal classroom settings, communication styles and cognitive processes and then discusses the implications for online learning and learning analytics. Section 4 concludes the paper with the identification of several challenges and directions for future work. 
 2. Related Work.
 We present below a selective survey of prior empirical work that documents crosscultural differences in online learning. Vatrapu [1] found that despite differences between American and Chinese cultural group participants on (a) how they used the tools and resources of the learning environment (appropriation of affordances) and (b) how they related to each other during and after their collaborative learning interactions (technological Intersubjectivity), there were no significant individual learning outcomes differences. Kim and Bonk [2] report cross-cultural differences in online collaborative behaviors of the US, Finnish and Korean participants in their study. Daniels, Berglund and Petre [3] found cultural differences in international projects in undergraduate CS education. McLoughlin [4] based on her experiences with developing web-based instruction for Australian Indigenous education calls for a culturally responsive technology. Iivonen, Sonnenwald, Parma, and Poole-Kober [5] found culturally influenced differences in language and communication styles in a library and information studies course taught over the Internet in Finland and US. Walton and Vukovic„s [6] work with south African students from disadvantaged backgrounds found that cultural differences make it difficult for the students to make the transition to the web use. Crump [7] explored the effects of computing learning environment on the newly arriving international students at universities in New Zealand. The author reports that the cooperative and collaborative learning environment was an issue of concern to the students. The author says it is likely due to the oversimplification of social structure of groups, individual and group goals and the diverse nature of knowledge construction in the collaborative learning environments. Duncker [8] conducted an ethnography of the usability of a library metaphor used in digital libraries in the cultural context of the Maori who are the indigenous population of New Zealand. Duncker says that metaphors and metaphorical thinking are strongly rooted in culture. The Maori found the digital libraries interesting but difficult to use due to the breakdown of the library metaphor caused by a number of cultural misfits. Keller, Pérez-Quiñones and Vatrapu [9] outlined cultural issues and opportunities in computer science education. In the next three sub-sections, three separate lines of empirical findings from the fields of social behavior, communication and cognition are presented and implications for online learning are discussed. 
 3. Culture and Behavior.
 Cultural models can be used to identify the differences in cultures that affect the computer supported collaborative learning environments. There are two kinds of cultural models: models that use typologies and models that use dimensions. Typologies describe a number of ideal types each easy to imagine. Dimensional models group together a number of phenomena in a society that were empirically found to occur in combination into dimensions. Typologies are difficult to adopt in empirical research as real cases very rarely correspond to one single ideal type. 
 3.1 Hofstede’s Cultural Dimensions Model.
 Hofstede‟s seminal work on cultures in organizations formulated a framework of four dimensions of culture identified across nations. Each dimension groups together phenomena in a society that were empirically found to occur in combination. In this section, Hofstede‟s definitions for these original four cultural dimensions are listed followed by a discussion of each dimension with respect to online learning. Hofstede‟s cultural dimensions model indicates what reactions are likely and understandable given one‟s cultural background. 
 3.1.1 Low Power Distance vs. High Power Distance.
 Power distance is the “extent to which the less powerful members of institutions and organizations within a country expect and accept the power that is distributed unequally” [10, p.28]. People in large power distance cultures are much more comfortable with a larger power/status differential than small power distance cultures. Table 2.1, adapted from Hofstede [11, p. 313], outlines the effects of power dimension that have implications for online learning environments. It is important to note that Hofstede‟s conception of power distance is not a bi-directional one; it is conceived as a subordinate‟s expectation and acceptance of unequal distributions of power in a social setting. 
 Table 3.1: Power distance dimension in traditional classrooms.
 If online education is offered as an alternative to traditional schooling then it is important to investigate how students perceive the social affordances of the virtual learning institutions. In the context of collaborative problem solving, students coconstructing concept maps are provided information attributed to scientists who have authority by virtue of their expertise and experience. Arguments from authority are valued in the scientific enterprise if those authorities themselves adhere to the scientific method. The point here is not whether the issues of power distance will show up in online classrooms but rather how does this dimension help understand the interactional behavior in an online learning setting. 
 Power distance becomes an important dimension to consider in collaborative problem solving involving students from lower and higher ends of the relatively ordered dimensional scale. Collaborative learning does away with the traditional instructional role of a teacher. Collaborative learning replaces the teacher-student dyad with a student-student dyad, replacing a didactic pedagogical approach with a social constructivist one. In the case of high power distance cultures, this reconfiguration in learning results in a replacement of the more hierarchical power structures with flatter ones. Do students take advantage of this in an online learning setting? From a cognitive standpoint, it is interesting to investigate how “cultural schemas” adapt in this reconfigured learning setting. For example, what would be the role of confirmation bias in problem solving in intercultural collaborative learning environments? Do students from high power distance cultures conform to the expert opinion even in cases where it explicitly contradicts the available evidence? What role does the collaborative other play in these learning interactions? These are a few of the questions that become relevant when the power distance dimension is considered in intercultural online learning settings. 
 3.1.2 Individualism vs. Collectivism.
 “Individualism pertains to societies in which the ties between individuals are loose: every one is expected to look after himself or herself and his or her immediate family. Collectivism as its opposite pertains to societies in which people from birth onwards are integrated into strong, cohesive in-groups, which throughout people’s lifetime continue to protect them in exchange for unquestioning loyalty” [10, p.51]. This dimension describes the degree to which a culture emphasizes an individual‟s reliance on the self or the group. Table 2.2, adapted from Hofstede [11, p. 312], outlines the effects this dimension that have implications to online environments. This dimension is of particular interest to the social constructivist theories of learning given the small group size emphasis. In inter-cultural online learning groups, dynamics of in-group and out-group memberships might affect how certain technology affordances are appropriated as social affordances. They might also affect the perception of other students in the online learning environment. The notion of face-saving is of important when it comes to subjective perceptions and evaluation of the user interface, online interaction and instructional elements of an online course. 
 Table 3.2: Collectivism vs. individualism dimension in traditional classrooms 
 Based on socio–cognitive conflict theory [12], collaborative learning effectiveness is thought to be influenced by the extent that students jointly identify and discuss conflicts in their knowledge beliefs [13]. This works well in an individualist culture but in collectivist cultures consensual forms of intersubjective meaning making processes may be more prevalent. 
 3.1.3 Femininity vs. Masculinity.
 “Masculinity pertains to societies in which the gender roles are clearly distinct; femininity pertains to societies in which the gender roles overlap” [10, p. 82]. This dimension refers to expected gender based division of labor in a culture. The cultures that score towards what Hofstede refers to, in a confusing choice of category labels, as "masculine" tend to have very distinct expectations of male and female roles in society. The more "feminine" cultures have a greater ambiguity in what is expected of each gender. Table 2.3, adapted from Hofstede [11, p. 315], summarizes the implications of this dimension for online learning environments. 
 Table 3.2: Femininity vs. masculinity dimension in traditional classrooms 
 Collaborative learning is often distinguished from cooperative learning by the argument that collaboration involves joint activity or an effort to maintain a joint conception [14] whereas cooperation involves a mere joining of individual activities [15]. Collaboration is often conceived of as being beyond a basic division of labor and more of an enterprise involving parties with equal stakes. 
 3.1.4 High Uncertainty Avoidance vs. Low Uncertainty Avoidance.
 “The extent to which the members of the culture feel threatened by uncertain or unknown situations”[10, p. 113]. This dimension refers to how comfortable people feel towards ambiguity. Low uncertainty avoidance cultures feel much more comfortable with the unknown. High uncertainty avoidance cultures prefer formal rules and any uncertainty can express itself in higher anxiety. Table 2.4, adapted from Hofstede [11, p. 314], summarizes the effects this dimension that have implications to online learning environments. 
 Table 3.3: Uncertainty avoidance dimension in traditional classrooms 
 The dimension of uncertainty avoidance can affect how students perceive social affordances of the online learning environment. Also of importance are the effects of culture on the interpretation or an acknowledgement of the ambiguous data and judgment of the relevance of data in the unfolding interactional sequence. 
 3.2 Culture and Communication.
 3.2.1 E. Hall’s Low Context vs. High-Context Communication Dimension 
 Besides Hofstede‟s cultural dimensions model the dimension of “low-context” vs. “high-context” cultures introduced by Hall [16] are important in the contexts and situations of intercultural communication. According to Hall [16], in high-context cultures, usually the cultures with high power distance, a member needs to be explicitly asked to respond to elicit behavior that is a deviation from the norm. Table 2.5 lists patterns of Hall‟s cultural communication context dimension. Hall characterizes speaking as an art in high-context cultures, with an emphasis on the emotional aspect. High-context cultures privilege social motivation. In low-context cultures, by contrast, members expect to influence others to act by explicitly pointing out pertinent information. The information provided implicitly enables the communicating other to take the desirable decision. Low-context cultures privilege rational information. 
 Table 3.4: Low-context vs. high-context cultural communication styles 
 If the communicative context varies across cultures than it becomes a variable of interest in the learners‟ interactional accomplishment of problem solving. 
 3.3 Culture and Cognition.
 According to Nisbett and Norenzayan [17, p. 1], mainstream psychology in general had made four basic assumptions about cognition. Adapting from them, the four foundational psychological assumptions regarding human cognition are: 
 Universality: Basic cognitive processes of sensation, perception, attention and memory are universal. In other words, basic cognitive processes are invariant across cultures and communities. 
 Content Independence: Basic cognitive processes are invariant across contents. In other words, cultural differences in content do not affect the nature and structure of the basic cognitive processes. 
 Environmental-Sufficiency: Cognitive processes of general learning and interference operate upon environmental contents to equip the child for functional survival. The environment provides content to cognitive processes without the need for cultural or social interventions. In other words, cultural differences in cognitive processes are due to different environmental influences and not social influences. 
 Infinite Cultural Variance: Since the universal basic cognitive substrate is content independent and environmentally-sufficient, the range of cultures is a function of the variance in environmental conditions. In other words, cognition places no constraints on the possible evolutionary design space of cultures. 
 All in all, these four assumptions have led to a belief in a fundamental dissociation between cognition and culture. One consequence of this was that psychology and anthropology evolved into independent academic disciplines with mostly nonoverlapping research agendas. However there were some exceptions to this dualist view of cognition and culture. These exceptions include in psychology, Lev Vygotsky and colleagues [18, see 19 for a biographical and historical treatment of this influential research movement]. In cognitive anthropology, most notably D'Andrade [20]; and in cognitive sociology, Dimaggio [21]. Recent empirical results have shown that culture and cognition are mutually implicated and are not disassociated as traditional psychology has postulated. 
 3.3.1 Nisbett and Colleagues’ Cross-Cultural Psychology Findings.
 Richard Nisbett and colleagues have embarked upon an experimental cross-cultural psychology research program to systematically investigate cognitive differences across cultures. Table 2.6, compiled from Nisbett [22, pp.xix, 44-45] and Nisbett and Norenzayan [17, pp. 21-25], presents a concise summary of above discussion along with empirical evidence from the literature. The cultural difference in attention to field vs. object is highly relevant to collaborative “knowledge map” learning environments. East-Asian learners might pay attention to a meaningful group of interrelated knowledge map objects whereas Western learners might attend to individual objects and evidential relational links. The cultural difference in attention can vary the ways in which referencing and deixis are carried out in collaborative discourse. East-Asian learners might make more references to regions of the concept maps and groups of related concept map objects (i.e., to fields of interest), whereas Western learners might reference individual objects in their collaborative discourse. This translates into a socio-technical design hypothesis that given the choice of referencing regions of concept map areas and individual objects in the concept map, East Asian learners will appropriate the affordances for referencing fields. On the other hand, Western learners will appropriate the affordances for referencing individual objects. 
 Table 3.5: Cognitive differences between East-Asians and Westerners.
 The implications from the cultural difference in perception is that Western learners by virtue of being more susceptible to “primacy effect” might favor earlier perceptions of information related to a collaborative learning task. East-Asian learners might perceive more relationships between the information in concept map and instructional materials leading to a greater number of evidential relation links in the concept map. The cultural difference in causal inferencing processes implies that East Asian learners might be more inclined to reason-giving that prioritizes situational factors when compared to the dispositional attributions of Western learners. One particular implication would be the cultural effect on collaborative argumentation. Also, EastAsian learners‟ perception of their collaborative partners might follow this same trajectory. This might manifest as East-Asians‟ giving higher ratings for their collaborative peers due to situational attributions explaining any perceived unpleasant “performance.” Western learners might perceive their collaborative peers for their dispositional “competence.” This cultural difference in cognitive processes might manifest as East-Asian learners preferring a highly inclusive final conclusion in intercultural collaborative problem solving tasks. Western learners might argue for more differentiated analytical hypothesis that seems logically the most viable. 
 5. Discussion.
 Taking the existing body of research on cultural effects on social behavior, cognitive processes, online pedagogies and HCI, learning analytics should consider both appropriation of affordances (tool use) as well as technological intersubjectivity (how learners and teachers relate to, interact with, and form impressions of each other in technology enhanced teaching and learning settings). Given that both seminal networked learning research (Hiltz, 1994) and current online learning best practices prescriptions (Moore, 2006) emphasize student collaboration, and since these aspects vary across cultures in traditional classroom settings (Hofstede, 1986) as well as online learning settings (Edmundson, 2007), learning analytics should critically examine mono-cultural design assumptions. Learning environments and learning analytics solutions that do not incorporate diverse “alternates for action” might not achieve the best results in terms of student learning processes, outcomes and satisfaction. In order to exhaustively study the potential effects of culture on the appropriation of potentials for action and the negotiation of the meaning of those actions, one needs to analyze individual actions in the context of their interactional sequences [31]. Therefore, learning analytics tools should support micro-genetic analysis of learners‟ interactions apart from aggregating behavioral outcomes for statistical testing. Although the cognitive embeddedness of discourse and knowledge-building have been theorized and empirically evaluated [32, 33], social engagement and cultural embeddedness aspects of these design implementations have remained unexamined so far. Learning analytics designers need to consider ways of facilitating the varying degrees of social and cognitive embeddedness. Increasingly, issues are being identified in the cross-cultural implementation of online learning or e-learning systems which are primarily designed, developed, and evaluated in North America and/or Western Europe contexts [34]. To help remedy this situation, future work could investigate three models of cultural influence in online learning and learning analytics: (1) culture-specific, (2) culture-comparative, and (3) culture-interactional. Culture-specific work studies learning analytics in a specific cultural context where learners have a shared sense of identity. Culture-comparative studies investigate learning analytics processes and products across cultures. In culture-interactional studies, learning analytics in intercultural settings is the primary focus.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Cultural Considerations in Learning Analytics</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>online learning culture</dc:subject>
		<dc:subject>behavior</dc:subject>
		<dc:subject>communication</dc:subject>
		<dc:subject>cognition</dc:subject>
		<dc:subject>computers</dc:subject>
		<dc:subject>human-computer interaction (hci)</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/56/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Generating Predictive Models of Learner Community Dynamics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57/authorlist"/>
		<swrc:abstract>In this paper we present a framework for learner modelling that combines latent semantic analysis and social network analysis of online discourse. The framework is supported by newly developed software, known as the Knowledge, Interaction, and Social Student Modelling Explorer (KISSME), that employs highly interactive visualizations of content-aware interactions among learners. Our goal is to develop, use and refine KISSME to generate and test predictive models of learner interactions to optimise learning.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 The nascent field of Learning Analytics focuses on "the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs" (https://tekri.athabascau.ca/analytics/call-papers). One approach to learning analytics is social network analysis, which examines the patterns of interaction among learners. Social network analysis of, in particular, elearning is facilitated by the availability of digital data that are amenable to such analysis. Considerably less attention has been paid to the content of the artifacts around which the learners are interacting. Content analysis is time-consuming, painstaking, and detailed work. Without content analysis, however, claims about the nature of the dynamics among learners are left wanting. Understanding learning, it seems, requires digging deeply into the data that are available. In this paper we introduce a framework that interweaves social network analysis, semi-automated content analysis, information visualization, and applied economic theory to help us understand and optimise learning. We are interested in investigating research questions such as: Can we “predict” when particular interactions will result in learning? What are some characteristics of interactions of effective learning? 
 This paper begins with a brief introduction and survey of relevant literature using social network and latent semantic network analysis (LSA) to analyze online discourse. Next, a description of the prototypic software environment (the Knowledge Space Visualizer or KSV) on which the new software (the Knowledge, Interaction and Semantic Student Model Explorer, or KISSME) is being developed is presented. The use of LSA in the generation of student models suitable for studies of collaborative learning is then proposed. Finally, we present a theoretical framework for understanding the dynamics of collaborative learning in terms of examining the outcomes of social and semantic interactions among participants. 
 2 Background.
 Wasserman and Faust [1] describe social network analysis (SNA) as a methodology that focuses on relationships and patterns of relationships. As such it “requires a set of methods and analytic concepts that are distinct from the methods of traditional statistics and data analysis” (p. 3). They cast SNA in the broader list of topics that have been studied using network analytic methods, including community [2], group problem solving [3-5], diffusion and adoption of innovations [6-8], and cognition [9, 10]. No matter what the objective of the study, though, network analysis focuses on the relations between units. Studies have explored the application of SNA to explore learning and knowledge construction in Networked Learning/Computer-Supported Collaborative Learning (NL/CSCL) environments. However, researchers have yet to achieve consensus on what methods to use. For example, de Laat, Lally, and Lipponen, [11] used content analysis, critical event recall and SNA to study interaction patterns. They suggest that SNA can be used to complement content analysis [12, 13] to describe and understand patterns of interaction in NL/CSCL. Of the various network metrics that are available (see [1]), these researchers focus on density and centrality. In contrast, Reffay and Chanier [14] applied SNA to determine the cohesion of groups engaged in CSCL. They argue that embedding tools that perform such analyses in the design of the learning environment itself may be more effective than time-consuming content analysis to support teaching and learning. The importance of time-based analyses has also been noted [15][16] The study by de Laat et al [11] was the first application of using SNA to illustrate how patterns change over time and the relationship of those patterns to teaching and learning. An important generalization from the literature is that the essential features to conduct SNA are two or more units, usually learners and the elucidation of the relationship between them. But there is another equally important type of network analysis to be considered in learning analytics and knowledge work: the network of ideas. Ideas, unfortunately, are difficult to delineate. 
 2.1 Latent Semantic Analysis.
 Latent semantic analysis (LSA) represents both a statistical technique and a model of human knowledge acquisition. Landauer and Dumais [17] propose LSA as a model that could answer the question, how do individuals know so much given as little information as they get? This problem is variously known as Plato’s Problem, the “Problem of Induction", the “poverty of the stimulus", or “the problem of the expert". (Plato’s solution was that individuals possess innate knowledge and only need some stimulation to reveal it.) LSA provides a high-dimensional representation of the associations between words and the documents containing those words. The final output from LSA is a series of measures that describe the relationships between units such as words, documents, or words-and-documents. In LSA, each document or word is represented by a vector in high-dimensional latent semantic space. The vector is calculated by examining patterns of co-occurrence of words in a term-by-document matrix, which is subsequently simplified using Singular Value Decomposition (SVD). Thus, each document is represented by a vector of numbers, typically numbering between 100 and 300 elements. Whereas dimensions resulting from the application of SVD to data can typically be interpreted (e.g. the dimensions from Principal Components Analysis), the dimensions resulting from LSA are not typically interpretable. This limitation has made the interpretability of LSA-based analyses difficult in the past. Information visualization techniques seem to be a natural next step in interpreting LSA, and can be used to create meaningful representations of ongoing learning processes. Visualization of LSA-derived similarities may be problematic, though, due to an unacceptable reduction of dimensionality to two or three dimensions suitable for visualization from that which is optimal for LSA (typically around 300) [18]. 
 3 Software.
 In this section we describe software designed to support the visualization of learner models based on social and semantic networks. We present a description of the Knowledge Space Visualizer (KSV), a prototypic software system on which our new software, KISSME, is based. 
 3.1 The Knowledge Space Visualizer (KSV).
 KISSME extends the Knowledge Space Visualizer, which was developed by the first author for his doctoral dissertation. The KSV was designed to allow researchers to use computer-assisted two-dimensional visualization of learner-generated contributions to an online discourse space. In its simplest form this generates a graph in which nodes are contributions and links are relationships between those contributions such as "reply", "reference" and "annotate" (see Figure 1). These explicit relationships between contributions are based on the behaviours of the contributors. A learner, for example, can intentionally choose to make a contribution that is a reply to another learner's contribution. In the resulting graph the links are based on these behavioural relationships. Content is not considered. In addition to the explicit linkages defined by behaviours such as replying, referencing and annotating there exist implicit linkages between contributions to the discourse space. These implicit linkages concentrate on the similarity of the content of the contributions. Whereas human raters can evaluate the similarity between documents reliably and with good validity, it is very tedious and time-consuming work. There are a variety of automated and semi-automated techniques that can be used to determine the similarity of text-based contributions. One powerful technique is LSA, described above. 
 Fig.

 1. Structural relationships between contributions. Blue lines indicate "build-on" or "replyto" relationships. Magenta lines indicate "reference" links. 
 The preceding examples are based on the use of a force-directed layout algorithm to position the nodes in to respect the strength of the ties between them while minimizing the distortion of the network of the relationships between the nodes. Other types of layouts are also possible. For example, other researchers [19] have highlighted the importance of chronology when studying the dynamics of learning communities. The KSV supports this sort of inquiry by facilitating the positioning of notes chronologically. More generally, the KSV supports the use of any categorical, ordinal, or continuous variable from the data set to define either of the axes for the display. So in addition to the use of a continuous chronological scale to define the horizontal axis, authorship can be used to define the vertical axis. An example of the resulting learner-time display is shown in Figure 2. Once contributions are positioned on whatever set of operationally defined axes the analyst has chosen, links between nodes can be overlaid without affecting the positioning of the nodes. For example, the behavioural links can be overlaid on the learner-time display to show how patterns of interaction change over time. An example of this overlay is shown in Figure 3. In a similar way, links between contributions based on latent semantic analysis can be overlaid on the same learner-time display to show the degree to which contributions are similar over time and authorship. More computationally intensive measures can also be visualized. For example, one can determine which contributions were opened (and possibly read) by a learner within some specified time interval before that contributor added a new contribution to the discourse space. An example of this sort of "recency influence" diagram is shown in Figure 4. 
 Fig. 2. Chronological-authorial layout of contributions. 
 Fig. 3. Chronological-authorial layout of contributions overlaid with structural links. 
 Perhaps some of the most interesting diagrams that can be produced using the KSV are based on the superposition of different link types on the same layout. For example, one can overlay links of LSA-based semantic similarity atop those based on "recency influence" to investigate the degree to which the content of recently opened (read) contributions is reflected in new contributions. The KSV also allows the user to constrain the analysis by specifying beginning and end dates for the analysis. Rather than specifying the dates a priori, the user can manipulate the beginning and end dates with specially designed slider. In addition to being able to manipulate the beginning and end dates independently of one another, the user can manipulate both dates simultaneously, effectively providing time slices of the network graph. 
 Fig.4. Chronological-authorial layout with overlaid with structural and recency links. 
 One of the key innovations of the KSV was the use of flexible thresholds in the creation of network representations. This is what allowed us to create visualizations of LSA-based representations of texts. Rather than attempting to provide a twodimensional layout based on the first few dimensions resulting from the matrix decomposition used in LSA, our approach has been to determine the similarities between documents based on the cosines between the vectors representing documents. A graph is then created in which the nodes correspond to the documents and the edges correspond to the LSA-based similarities between them. A force-directed layout algorithm is then applied to the graph such that the positions of nodes in the twodimensional representation minimize the distortion of the (very low dimensional) representation. This representation of a maximally connected graph typically lacks clarity, and in typical cases where there are tens or hundreds of nodes the graph is essentially unintelligible due to the large number of edges. This problem of overly connected graphs also presents a conceptual problem: does it make sense to connect two document nodes if their LSA-based similarity is very low? Other researchers [20] have attempted to address the "threshold problem" but heir research suggests that no typical value of cosine threshold for determining document similarity exists. Our approach to tackle this problem is to provide the end user with control over the choice of threshold to use. We do so by providing a slider control in the software that allows the user to specify the cosine value below which edges are not drawn between document nodes. The dynamic nature of this control allows the user, for example, to examine patterns of cluster formation as the similarity threshold is varied. This provides an example of how visual approaches to learning analytics can provide solutions to previously intractable problems. The answer to the question of "when are two documents (or ideas) different" is typically "it depends on what you're looking for". Given a collection of documents generated by students on, for example, the physics of light. At the most permissive level of similarity threshold, all documents are related by virtue of being in the same language. This corresponds to a similarity threshold of zero. At a value slightly higher than zero, one could imagine the documents cluster into two groups: one about colours of light and one about reflection. As one raised the threshold higher yet one could imagine the colours cluster fragmenting into smaller clusters of related notes about topics such as rainbows, wavelength, and so on. The interactive nature of being able to manipulate the threshold supports this broad range of possibilities for determining the diversity of ideas that are present in discourse space. The Knowledge Space Visualizer, while providing powerful visualizations of multi-dimensional networks, has several limitations. First, it relies on the end user having an functional installation of a recent version of Java. Recent advances in browser-based technology -- specifically the widespread adoption of HTML5 -- has enabled the production of highly interactive browser-based visualizations. Perhaps more significantly, the KSV was limited by its focus on document-based networks. The KSV enables the visualization of relationships between documents, based on both explicit and implicit linkages, but other than examining patterns of authorship and coauthorship it was not particularly good at generating visualizations of author-based networks. We are working on creating next-generation software that will facilitate the examination of networks of authors. In its earliest versions, the KSV was highly tuned to data from Knowledge Forum. The KSV was recently enhanced to allow the importation of data from almost any data source that provided indications of authorship, chronology and content. The KSV was released as open source code and is maintained on Google Code at http://code.google.com/p/ksv. 
 3.2 Visualizing Student Models: The Knowledge, Interaction and Semantic Student Model Explorer (KISSME) 
 Recent work has led to the implementation of a learner model based on interactions with other learners. The functionality of the KSV, in terms of being able to manipulate the threshold at which two nodes are considered similar enough to be joined by visible edges, was extended from document nodes to learner nodes. Put another way, a learner model based on social network analysis was created in the KSV and the implementation of a flexible threshold (based on the intensity of the interaction between any two learners) allowed researchers to investigate patterns of interaction. The KSV allowed the analyst to exercise considerable control over various parameters such as the intensity of interaction necessary to establish a social link between participants, as well as the date at which the social network was analysed. The ability of the analyst to vary these parameters allowed the detection of patterns of interaction that were previously obscured [21]. However, the network between authors was based solely on their patterns of interaction. No information about the content of their contributions was used in the generation of the graphs. The ability to model students or other participants and then to visualize those models in an interactive visualization environment offers the potential to gain insights into the nature and outcomes of interactions between learners. In the work with the STEF lab we constrained our analyses to focus on the social networks that formed among learners. While this approach revealed interesting patterns of interaction, we felt the results were incomplete because no attention was paid to the content of the learners' contributions to the online discourse space. 
 Other researchers have conducted studies that meld automated interaction analysis with manual content analysis [11, 16]. However, manual content analysis represents the rate-limiting step in this sort of analysis. Because manual content analysis takes so long it is incommensurable with real-time analysis, which is one of our goals. Therefore, we are interested in using some sort of automated or semi-automated content analysis. For reasons specified earlier we have chosen to use latent semantic analysis to help us conduct automated content analysis. For our purposes, all that we are using LSA for is to generate mathematical representations of the participants' contributions to the discourse space. We can then use those mathematical representations in a variety of ways. LSA uses a vector representation of text. One characteristic of these vectors is that they are additive: the vectors of two documents can be added together to get the vector of the combined documents. We can extend this property to generate latent semantic models of participants by adding together the vector representations of all their contributions to the discourse space. This is not the first application of LSA to student modelling. Other researchers [22-24] have used LSA in student modelling but they have not focused on the collaborative nature of learning. Still others have extended techniques from earlier research on LSA to apply to e-learning contexts [25-27]. Zampa and Lamaire's recent work [23] builds on the notion of matching students to text based on the Vygotsky's Zone of Proximal Development. However, theirs is an individualistic model: the selection of "stimuli" is meant to effect individualized optimization of learning. Our approach is somewhat different: we are interested in combining information about patterns of interaction among participants with information about the content of those contributions. We too take a Vygotskian approach: that optimal learning will take place when interactions occur between individuals who are neither too similar nor too dissimilar from each other, based on the semantics of what they have written. This approach of combining social network analysis and latent semantic network analysis is an example of the sort of "multi-dimensional" network championed by Noshir Contractor [28]. Our current work includes the implementation of software that will allow us as researchers to examine the interplay of interactions between learners and the latent semantic models of those learners. We are interested in testing the Vygotskian hypothesis that uptake [29] is most likely to occur when the semantic relatedness of the corresponding contributor models is neither too high nor too low. We are also interested in simulations of learner interactions that take into consideration both interactions and semantic relatedness. This, we believe, would allow us to generate models of community dynamics in collaborative learning. Once we have simulation data that incorporates interaction and content we can make inferences about the characteristics result in the success (broadly defined) of some learning communities. 
 4 Game Theoretical Approaches to Understanding the Learner's Group Dynamics 
 Our approach to understanding community dynamics is based on understanding the nature of the interaction between members of that community. We are examining a variety of theoretical approaches but one that seems particularly promising is the application of game theory [30] to interactions between users. This approach requires us to consider the outcomes of interactions between users in terms of "payoffs" to each player. Of course, different players can employ different strategies. We consider this to be part and parcel of learning: our hypothesis is that as learners gain expertise, they enhance their repertoire of learning strategies, and through experience they learn when to employ particular strategies. 
 5 Summary.
 We have proposed a framework that combines social network analysis and latent semantic analysis of online discourse. The proposal is speculative: previous work with latent semantic analysis has yielded promising results that may help us understand the nature of interactions among learners. Examining those interactions using a framework such as game theory may allow us to gain insight into the nature of community dynamics.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Generating Predictive Models of Learner Community Dynamics</rdfs:label>
		<dc:subject>information visualization</dc:subject>
		<dc:subject>latent semantic analysis</dc:subject>
		<dc:subject>social network analysis</dc:subject>
		<dc:subject>learner models</dc:subject>
		<dc:subject>game theory</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-teplovs"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-teplovs"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nobuko-fujita"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nobuko-fujita"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-teplovs"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/nobuko-fujita"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/58">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Evolving a learning analytics platform</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/58/authorlist"/>
		<swrc:abstract>Web-based learning systems offer researchers the ability to collect and analyze ﬁne-grained educational data on the performance and activity of students, as a basis for better understanding and supporting learning among those students. The availability of this data enables stakeholders to pose a variety of interesting questions, often speciﬁcally focused on some subset of students. As a system matures, the number of stakeholders, the number of interesting questions, and the number of relevant sub-populations of students also grow, adding complexity to the data analysis task. In this work, we describe an internal analytics system designed and developed to address this challenge, adding ﬂexibility and scalability. Here we present several examples of typical examples of analysis, discuss a few uncommon but powerful use-cases, and share lessons learned from the ﬁrst two years of iteratively developing the platform.</swrc:abstract>
		<led:body><![CDATA[ 1. A Wright Map compares the distribution of IRT person parameters of all students to item parameters of all questions in the GMAT Verbal section in Grockit. This plot suggests that the student abilities are slightly above question diffculties, which informed our decision to author more challenging questions for this domain. 
 Similarly, we collect a list of all student unique identiﬁers, along with the ability estimate (IRT person parameter, θ) for the student in that subject. The resulting database tables are accessed through an R script, which is used to calculate and display backto-back histograms to create a Wright Map of the data [4]. This analysis is then automatically disseminated to the content authoring team and the student modeling team. When we ﬁrst ran this analysis last year, we found that of the ten domains for which we had developed IRT models, the quantitative questions that we had in place for the GMAT group could better reﬂect the ability level of the student population. Based on this, our content authoring team subsequently focused on developing additional content at the upper end of the diffculty scale. The value in creating a common analytics pipeline is that new analyses can be performed rapidly, past analyses can be archived for future access, and common usage patterns can be easily identiﬁed and codiﬁed. 
 3 Asking the same questions using different datasets.
 When Grockit expanded from a single student population (post-college learners studying for a business school entrance exam) to other groups of learners, including high school students, we found ourselves wanting to ask the same questions of different subsets of learners. Beyond simply segmenting students in our database, we started wanting to ask questions based on a variety of other criteria: speciﬁc time windows, a particular teacher’s class, all students in the treatment group of some controlled experiment, only the questions most recently added to the system, only students who have acted as peer tutors, only students who have worked with a speciﬁc instructor, excluding response data from teachers/tutors/administrators/authors, or any number of other data restrictions. In order to avoid constantly modifying (or duplicating) queries, we chose to add support for on-the-ﬂy redeﬁnitions of these sorts of constraints. As the data that we work with is highly relational, the goal was for these restrictions to cascade seamlessly through the model (e.g. if we exclude a particular game, we necessarily wish to exclude all questions answered in that game, any reviews of that game, all explanations read during that game, etc.) Online analytic processing (OLAP) was designed to address this sort of challenge, and the solution that we built shares several qualities with Relational OLAP (ROLAP) systems [3]. We chose to use a lightweight view-based solution, with our MySQL database. Data selection queries were modiﬁed to work with (non-materialized) views of the database tables rather than with the tables themselves. These view deﬁnitions form a directed acyclic graph (DAG), so most views are deﬁned based on other views. Based on this DAG of non-materialized views, a single view redeﬁnition (such as students who answered questions yesterday) effectively redeﬁnes all other views dependant on that one. Records in tables that are dependent on more than one other table are therefore only included in the associated view if all upstream records are considered.1 In addition to this cascading intersection, a question can be asked of a composition of multiple non-overlapping view deﬁnitions. This means that we can easily ask questions about item responses to Geometry questions among students who have been active during the past 30 days. As these analyses are run on replicas of the application’s production database rather than the database itself, long-running queries can process nearrealtime data without degrading the performance of the application server. One question we were interested to understand earlier this year: Do students generally spend more time or less time to answer a question that they ﬁnd easy? That they ﬁnd diffcult? Does this vary by subject matter? To understand the relationship between subjective item diffculty, accuracy, and time taken, we used our item diffculty parameters to estimate the probability that each item response would be correct. We view this probability as a subjective diffculty metric, based on the assumption that students who have a very low probability of answering a question correctly will ﬁnd that question diffcult, and students who have a very high probability of answering a question correctly will ﬁnd it to be easy. One notable beneﬁt of separating the question deﬁnition from the data deﬁnition is that stakeholders can often access the results of existing queries for new subsets of students without requiring any new code to be written. New entries just need to be added to the report-scheduling queue: 
 QUERY=probability_time_accuracy VIEW=GameGMATQuantitative+UserStudyingForGMAT 
 QUERY=probability_time_accuracy VIEW=GameACTEnglish+UserStudyingForACT 
 (1) For example, a database row recording a student having answering a question is only included in the view of that table if records for both that student and that question are included in views of their tables, respectively. 
 Fig. 2. Average time to answer a question, as a function of the IRT-based probability of response accuracy (used as a proxy for subjective diffculty). GMAT Quantitative data on left, ACT English data on right. 
 Fig. 2 displays the output of this query for different subsets of data. For GMAT Quantitative questions, students are, on average, spending less time answering questions that are subjectively very easy or very diffcult than they spend on questions with are in between, a ﬁnding that supports Koster’s theory of the connection between challenge appropriateness and learner engagement [6]. Interestingly, a different trend is seen in ACT English responses, where students generally spend less time the easier they ﬁnd the questions. With this ﬂexibility, a common use-case for the analytics platform, asking the same question of a different set of data, no longer requires any code changes, and could be made accessible to non-technical stake-holders. 
 4 Recording events to enable experience analysis.
 We found that some of the questions that we sought to answer required knowledge about events that were not already being recorded in the database. Rather than create new models and relational tables for each such event type, we opted to add support for a simple experience logging facility. These are currently written to the database to allow analytics queries to join them with other relational data, but they could theoretically be logged elsewhere initially and only merged into the analytics environment at analysis-time. 
 Questions that we have been very interested in become possible to answer by annotating adding a few such event experiences. Did a student ever see their study plan? Did they start viewing an available video explanation? Did a student ever access their performance analytics page? These event experiences played a role in a bigger-picture question that we’ve been exploring recently: Of all of the many opportunities for learning within Grockit – individual problem solving, small peer-group study, instructor-led lessons, skill-based video explanations, private tutoring sessions, and skill-customized practice, among others – which of these is most effective? Understanding this could inform decisions ranging from the study plans that we offer students, the speciﬁc activities that we encourage at various points in time, and even decisions about removing certain activities altogether. When we ﬁrst sought to ask this question, we found that we hadn’t recorded all of the data that we were interested in examining (including information on partial or entire video explanations watched). Instrumenting the system with a few logging records was suffcient to collect the additional information, and we look forward to running and sharing our ﬁndings on this analysis in an upcoming publication. A simple event-logging facility within a web application allows data analysts to start recording data necessary for analysis but otherwise not recorded, often in a single line of code. 
 5 Testing new hypotheses with controlled experiments.
 Where OLAP-style analysis allows us to understand and describe trends that we see in collected student data, this approach does not directly let us test out new hypotheses. As with other types of web applications, web-based educational software provides an ideal environment for running randomized controlled experiments [5]. Our goal with Grockit’s implementation of a framework for doing so was to (a.) minimize the amount of work necessary to introduce a new experiment, and (b.) minimize the amount of work necessary to analyze an experiment. We illustrate below how just a few lines of application code automatically generate an analysis of the differences among treatment and control groups over a ﬁxed set of outcome measurements (used for all such analyses). For a recent experiment, we sought to understand the affect that the diffculty of the ﬁrst few questions presented to a student had on subsequent participation and retention rates. This code randomly assigns students (with equal probabilities) to the treatment or control groups, and provide a different user experience based on that assignment: 
 experiment = { "InitialQuestions" => ["EasyFirst", "Normal"] } if (SplitTest.find_or_create_assignment_for_experiment(self, "InitialQuestions") == "EasyFirst") show_easy_first else show_regular end 
 The ﬁrst line deﬁnes a unique identiﬁer for the new experiment, which, combined with the unique identiﬁer for a particular student (based on self, in the second line), is the basis for a random assignment to one of the groups. 
 Fig. 3. A slightly reformatted portion of the automatically-produced output of a controlled experiment testing the effect of the diffculty of the ﬁrst few questions presented to a new student. “Difference” indicates a conﬁdence interval around the difference between the percentages from the two groups. A single * indicates a p-value that is signiﬁcant at the α = 0.05 level, and a double ** indicates signiﬁcance at the α = 0.01 level (two-tailed). 
 By adding this application code, the analysis in Fig. 3 is automatically generated and distributed daily. Here, students were assigned to the treatment group with a p = 0.5 probability. The null hypothesis is that the two populations of students have the same true proportions, and the alternative is that the proportion is different in one of the populations. In the case of the analysis in Fig. 3, we found that the EasyFirst question selection strategy resulting in an increase in the rate of subsequent study, group study, and participation in group discussions. A built-in infrastructure for introducing and evaluating randomized controlled experiments allows for a powerful and valuable new class of analysis: hypothesis testing. 
 6 Distribution: From push to pull.
 Originally, the completed reports were emailed as PDF ﬁles to speciﬁc recipients. As the number of reports and students increased, emailing many large ﬁles became infeasible as a way of distributing the results of the reports. In place of this, we now have a centralized, web-based reporting system. All past reports are archived, and the source data and intermediate data ﬁles are persisted to allow for later analysis, comparisons over time, or replications of past results. Furthermore, by using the same user authentication as the main Grockit site, we can also provide teachers with access to reports on their classes. Finally, by providing a common way of accessing the reports, it allows stakeholders to discuss individual reports simply by sharing a link to the report under discussion. Access control, search, and data archiving can all be simpliﬁed by means of a centralized, web-based repository for the output of an analytics reporting system. 
 7 Focusing on performance, scalability, and stability.
 As the number of students in the system has increased, the size of data to be analyzed has become much larger, forcing us to consider how to scale the workﬂow and analyses to deal with more reports on more data. The ﬁrst and simplest answer is increasing the power of the computers running the analyses, in our case by performing the analyses on Amazon EC2 instances with additional RAM and faster processors. One useful tool for making use of these powerful remote machines in developing analyses is the ability to run analyses as though they were local, by sending the request to the remote machine, having it perform the data selection, analysis, and visualization, and then retrieving the results for display as though they had been run locally. Additionally, since we do not want to slow down our production web-server or database with intensive analysis, we replicate the databases. Originally, this was done daily at the beginning of the set of analyses to be run; but as the data size increased, copying over all of the data became infeasible. Instead, we continuously keep the reporting database near-realtime by using MySQL replication against the production database, ensuring that our reports are run on up-to-date data. This automatic replication process also allowed us to begin to distribute reporting across multiple machines; instead of having a single machine run all reports and waiting for each report to ﬁnish before running the next one, we have multiple machines, each with an up-to-date copy of the database and workﬂow code, which can run through the entire reporting process in parallel. The next step in the process it to have a pool of ready reporting machines, along with a central job distribution system, such that any report request, whether run periodically or by an analyst trying to answer a new question, can be sent directly to the next available machine. Providing a central job distribution process information about priority and the state of running jobs would allow all job requests to be integrated, and also allow for any user viewing a report on the web-based system to initiate report jobs on-demand. As the analyses become valuable on a recurring basis, rather than simply being a one-time answer to a question, we need to ensure that they are stable in the face of continual changes to the data and application, so that the stakeholders relying on the analysis can rely on their being ready and available. While we have automated tests that ensure that reports are successfully generated, we wish to also have a simple capacity to ensure that the results of the analysis are still reliable and accurately reﬂect the answer to the question. This additional capacity can be provided by having a single test ﬁle for each report, that veriﬁes correct results from a pre-speciﬁed testing dataset. This can be done without slowing down the process for rapid one-off analyses, which can be performed without creating or providing automated tests. As the dataset grows and results become mission-critical, issues of performance, scalability, and reliability become increasingly important. 
 Fig. 4. For each student, we provide access to a simple report showing their accuracy in each of several skill areas. This allows them to visualize their strengths and focus on the areas they are weak in. The student can select various diffculty ranges and time ranges to view their analytics over. 
 8 Providing analytics directly to learner and teachers.
 As teachers and students become more data-oriented about their learning process, several analyses initially prepared using the analytics pipeline have since been moved to a user-facing location within the application itself. Analytics reports that were distributed weekly to teachers in classroom pilots were subsequently incorporated into the teacher’s dashboard in Grockit. Skill-grained student performance reporting, illustrated in Fig. 4, was also ﬁrst created within the analytics pipeline, which allowed for rapid iteration and reﬁnement before it was transitioned into the production application. Once this transition is made, computation time becomes critical. In order to minimize page load speeds, we proceed by precomputing and caching, leveraging a Hadoop infrastructure. However, as the number of games and questions in the system increases, directly computing this when the student requests it began to take several seconds, especially for more active students. This is particularly true given that the student can select not just to view their all-time analytics results, but also select from 35 different combinations of problem diffculty and response recency to analyze their performance. To solve this, we began precomputing portions of the students’ analytics information. For each day the student was active, for each skill to be analyzed, we compute their total number of questions answered, number correct, and total time taken. This can be done using Hadoop for all users in just a few minutes, allowing for frequent updates of the precomputed data. Generating the percentage correct and average time for each individual skill or track becomes a simple matter from this data. This can then quickly be retrieved as a single row from an indexed table, and sent to the client browser for rendering. Overall, we saw between a 10-25x improvement in response time. As we provide more analytics within the application, increasing the amount of precomputing and cloud-based parallelization will allow us to provide feedback without sacriﬁcing performance. Students and teachers may beneﬁt from more direct access to the results of a learning analytics system. Doing so may require aggressive performance optimizations to provide immediate access to near real-time analysis. 
 9 Conclusion.
 To take full advantage of the rich data available from computer-based learning systems, creating a pipeline for processing and presenting advanced analysis can be a signiﬁcant boon for learning about students’ behavior and performance. We have described many of the advantages to be gained by doing so, as well as methods which we have used to achieve them. We hope that our system may serve as an example of what is possible by automating this pipeline. In making the analytics more readily available, scalability and stability must be addressed; we have described these challenges as well as approaches we are developing to address them. Finally, the future of analytics is one where the results are available not only to researchers and system designers, but also directly to students and teachers. In order to do this, we must not only make the process of developing new analytics as easy as possible, but also reliable and accessible to the teachers and students that is meant to inform and help.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Evolving a learning analytics platform</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ari-bader-natal"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ari-bader-natal"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-lotze"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-lotze"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/58/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ari-bader-natal"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-lotze"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/59">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Learning Analytics as Interpretive Practice: Applying Westerman to Educational Intervention</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/59/authorlist"/>
		<swrc:abstract>In Westerman’s [1] disruptive article, Quantitative research as an interpretive enterprise: The mostly unacknowledged role of interpretation in research efforts and suggestions for explicitly interpretive quantitative investigations., he invited qualitative researchers in psychology to adopt quantitative methods into interpretive inquiry, given that they were as capable as qualitative measures in producing meaning-laden results. The objective of this article is to identify Westerman’s [1] key arguments and apply them to the practice of learning analytics in educational interventions. The primary implication for learning analytics practitioners is the need to interpret quantitative analysis procedures at every phase from philosophy to conclusions. Furthermore, Westerman’s [1] argument indicates that learning analytics practitioners and consumers must critically examine any assumption that suggests quantitative methodologies in learning analytics are inherently objective or that learning analytics algorithms may replace judgment rather than aid it.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 In traditional cognitive science inquiry, measurement almost always involves significant levels of abstraction away from the phenomena of interest. Usually, observable behavior is of interest because it is assumed to indicate cognitive phenomena. For example, in learning measurement, the factors of interest are unobservable, so behavior observation is used as a proxy. In online learning environments, however, even greater abstraction is required in order to conduct inquiry. Behavior, in most online learning scenarios, is not directly observable. So, second-order proxies that represent directly observable behavior must also be constructed. Furthermore, these second-order proxies are typically encumbered by a severely impoverished vocabulary—a language of actions consisting almost exclusively of mouse clicks and keystrokes. Hence, online and blended learning environments present situations for inquiry that, in most cases, require even more examination of assumptions and methodology than required in traditional cognitive science inquiry. Despite the potential advantages of scale in learning analytics to investigate learning phenomena, great care must be taken in order to account for the philosophy and human judgment behind the measures and constructs employed in a study in order to interpret results in a reasonable manner. 


 1.1 Westerman’s Interpretive Inquiry and Learning Analytics.
 The arguments of Michael Westerman [1], though directed towards critics of traditional psychology who eschewed positivism and quantitative methodology, have particular relevance to learning analytics. Westerman invited qualitative researchers in psychology to adopt quantitative methods into their practice, which is no small invitation. Most qualitative researchers avoid quantitative research methods because of their traditional association with positivist philosophy, which purports that control, prediction, objectivity, and universal models are the end goal of science [1]. Qualitative researchers in social science are usually interested in questions that address the meaning of psychological phenomena more than how to replicate them. Westerman argues that quantitative methods, however, are not tied to positivism, and in fact are fundamentally interpretive and meaning-laden. Consequently, researchers interested in questions that address meaning should adopt quantitative methods into their repertoire of inquiry tools. The implications of quantitative methods lacking default objectivity, requiring interpretation, and addressing questions of meaning are a watershed for the practice of learning analytics. Given the multiple levels of abstraction involved in identifying and interpreting behavior in online settings, we contented that Westerman’s arguments regarding interpretive quantitative inquiry have particular relevance to learning analytics practice. 
 2 The Mismatch Between Positivism and Scientific Inquiry.
 Mainstream social and physical science are usually associated with positivism [1]. Positivism, as referred to in this article, is a philosophy of science in the tradition of Aguste Comte that assumes that determinate, value-free, causal accounts of phenomena can be made through objective methodology, hypothesis testing, and operational definitions[1-3]. A good example of the type of scientific inquiry that a positivist philosophy is likely to produce comes in the words of the well-known physicist Stephen Hawking, who wrote, “If one takes the positivist position, as I do, one cannot say what time actually is. All one can do is describe what has been found to be a very good mathematical model for time and say what predictions it makes” [4]. In one of Westerman’s [1] central arguments, he questioned the appropriateness of employing positivism in the inquiry of psychological phenomena, given its assumptions of objective methodology and value-free ontology. In the light of learning analytics, positivist philosophy also poses a formidable contradiction between its assumptions and the science that educational interventionists attempt to conduct. 
 2.1 Positivism and Operational Definitions.
 So what is the trouble with a philosophy that provides the rationale for objectivity, causality, and prediction in scientific inquiry? The challenge rests on the central assumption that objectivity exists in the practice of science. In social science, operational definitions play a prominent mediating role that define how phenomena are observed, measured and analyzed, which can hardly be called objective. Westerman explained that “Notwithstanding nearly ubiquitous references to ‘‘operationalizing’’ variables and hypotheses about relations between variables, quantitative research procedures as they are actually employed do not objectively translate theoretical ideas about constructs and processes into meaning-free language about procedures” [1]. The challenge that operational definitions pose in research can also be illustrated by taking a closer look at how they create abstractions—caricatures of actual behaviors or psychological phenomena. Similar to many definitions of the term, the Center for Teaching and Learning at the University of Texas defines operational definition as, “a specific statement about how an event or behavior will be measured to represent the concept under study” [5]. The language “to represent” is key here. Operational definitions do not actually define concepts or observable behavior, but act as abstracted mediators of how behaviors and concepts are measured and interpreted. Westerman goes on to say, “In fact, what instruments of this kind provide by way of so-called ‘‘operational definitions’’ are natural language explanations of each category and examples. Such definitions are very useful, but they are anything but exhaustive. Indeed, they are useless if not employed by a coder with a wealth of background knowledge about the concepts, interpersonal behavior in our culture, and family life as we are familiar with it” [1]. A good example of how operational definitions provide a challenge in online environments occurred during Fast Company’s Influence Project [6] in the summer of 2010. The magazine asked its readers to participate by creating a profile on the project’s website. Participants gained “influence” by how many people clicked on their profiles. As the project came to a close, it became apparent that defining influence by the number of clicks a profile received was not the best measure. A lot of people tried to game the system, so judgment was required in order to define what constituted a valid click. The project organizers felt in the end that influence would have been better defined by how many participants were able to, not only persuade individuals to click on their profiles, but also to convert people who clicked on profiles into creators of their own profiles. Even with the latter definition of influence, however, many nuanced variations of online influence could not have been discovered if such a reduced meaning of influence were used as the sole definition and data point. From the report, we find that Fast Company used other means besides click counts in order to distinguish among six types of online influence: large existing networks, static advertisements, commoditized celebrity appeal, overt ideology, grass roots activism, and ability to convince others to participate, which is a much richer account of profile relationships than either number of clicks or number of converts. The point here is not to avoid systematic inquiry in scientific observation and analysis of data, but rather to appreciate that, unlike positivist assumptions of objectivity, interpretation is required at the most fundamental level of scientific inquiry, given the inseparable part that human judgment plays in defining the constructs that researchers examine. Learning analytics practitioners, particularly should avoid placing confidence in the idea that observational data collected through web-analytics measurement tools objectively map on to the constructs they are investigating through the lens of operational definitions. 
 2.1.1 Positivism, Operational Definitions, and Learning Analytics.
 Even seemingly simple constructs of potential interest to learning analytics researchers like “time on task” must first be constructed (hence the name) before they can be recognized, recorded, and analyzed. When a student sits silently in a comfortable chair looking intently at the page of a book, this behavior indicates that he might be attending to the book’s contents. However, he might also be daydreaming about his girlfriend, or worrying about his father’s illness, or thinking about an essentially infinite list of other things. “Looking intently at the page” is only a proxy for reading. In online settings, second-order proxies are further abstracted away from the true subject of a researcher’s interest. As an example, take a researcher who is interested in the amount of time an online student spends “on task.” Rather than observing a student sitting silently in a comfortable chair looking intently at the page of a book, the researcher may have access to a “page load” event and a “page unload” event in a webserver log. These events outline a rough window of time. But was the browser window containing the text the researcher hoped the student would read even in focus between the two events? Was the student even in front of the computer while the browser window was in focus? Was the student looking at the browser window, or texting, or reading a magazine? Technological tricks may be able to help us answer these questions. And when we overcome these many obstacles, we have only arrived back at the original level of uncertainty present in direct observation. Could reasonable people create meaningfully different operationalizations of the construct “time on task” in online settings? Could different operationalizations of the construct applied to the same data produce different answers to research questions? If the answers to both these questions is yes, as we believe they are, the purportedly objective process of conducting learning analytics research is built on a foundation of subjectivity. 
 2.2 Positivism and Methodology.
 Another way in which positivist philosophy diverges from the practice of science is in the assumption that methods and instruments objectively display what is being measured; “structured observations do not provide a way to examine hypothesized associations in a transparent manner” [1]. As Westerman went on to explain, “Interpretation plays a role when it comes to measurement, which lies at the heart of quantitative research. This point is obvious regarding research based on clinical judgments about such global constructs as ‘‘irritable’’ or ‘‘submissive.’’ Research of this sort may employ the technical machinery of Likert-type scales or Q-sort procedures, but it clearly is based on rich appreciation of the meanings of human behavior” [1]. Just because a concept is associated with a number does not mean that the association is objective. Judgment, and hence subjectivity, is always involved when assigning a metric to phenomena in the real world. Failing to recognize the nature of human judgments in scientific methodology has significant consequences regarding the validity of research conclusions. Joseph Rychlak illustrated these consequences well in his treatment of Philipp Frank’s Philosophy of science: the link between science and philosophy. “The obvious lesson is that science is not only a methodological endeavor. Constant attention must be given to theoretical considerations—or, as they might be called, metaphorical or philosophical considerations…[In] Newtonian science, the uncritical acceptance of empirical data without sophisticated study of assumptions lead to a “theorization” of scientific method—that is, the assumptions of the method were projected onto the world as a necessary characteristic and then “proved so” by the results of these very same method [7]… they constantly fall into the errors of…confusing what is their methodological commentary with their theory of explanation” [8]. In other words, not accounting for the method effects will make it impossible to evaluate whether your conclusions are accurate or valid, given an inability to distinguish between the results that represent the psychological phenomena and those that represent error. 
 2.2.1 Positivism, Methodology, and Learning Analytics.
 Frank’s warning about “the uncritical acceptance of empirical data without sophisticated study of assumptions” is even more important in the context of learning analytics research. Because learning analytics data is so inexpensively and easily captured, large collections of data are becoming available for study. Faced with access to large collections of data and powerful open source analysis software, researches will be subject to a variety of temptations to poke about in this data in thoroughly unprincipled ways. While these fishing expeditions may uncover seemingly interesting relationships between constructs, without an interpretive framework grounded in specific theoretical commitments, the data tail may come to wag the theory dog. 
 2.3 Positivism and Ontology.
 Another problem with positivism is how its ontology eschews meaning. As Stephen Hawking explained, positivism provides no framework to examine the meaning of phenomena, which is a necessary consideration when applying research results to the situations of individuals. Hawking’s belief that prediction is the preponderance of science is a prime example of the type of philosophy that Philipp Frank [9] rebuked when saying that science is more than objective methodology. The philosopher of science noted that, “scientific findings (validated predictions or observations) outstrip the common sense understanding of them, taking us back to that condition earlier in history where we could control and predict without knowing why, what, or how such regularities in events were really brought about. Man predicted his course of travel under the stars, controlled the crops through practical knowhow, and cured himself of certain diseases centuries before there was anything like a scientific account of these beneficial outcomes” [8]. In other words, mathematical models, control, and prediction are not sufficient to answer questions about why something happens or what it means. Furthermore, given the irreducibly interpretive nature of inquiry, not attempting to answer questions of meaning and purpose may easily lead to the wrong conclusion, even if one is able to replicate observed behavior. 
 2.3.1 Positivism, Ontology, and Learning Analytics.
 Even if the atheoretical application of learning analytics techniques can uncover stable relationships between an impoverished lexicon of online behaviors and (equally subjective measures of) academic performance, we have only arrived back at “that condition earlier in history where we could control and predict without knowing why.” We will never be able to understand why certain behavioral patterns relate to academic success or struggle without engaging in explicitly interpretive work—interpretive work that draws on a wealth of background, theoretical, and empirical knowledge about the learning process. Inasmuch as a primary goal of learning analytics is to support and improve learning in human beings, the findings of learning analytics research must be translated in concrete interventions with human beings before the value of learning analytics is realized. And if the learning analytics path leads inexorably to interventions with human begins, we must consider learning analytics to be an inherently ethical activity. In this context, asking explicitly interpretive questions of “why?” and “how?” with regard to the findings of learning analytics research gains importance beyond the requirements of responsible science and crosses firmly into the realm of ethics, requiring us to proactively work to protect the interests of the people with whom learning analytics may suggest we intervene by understanding issues of why and how. 
 3 Learning Analytics, Hermeneutics, and Interpretive Inquiry.
 For a number of reasons outlined above, a positivist view of learning analytics appears to be a combination that leaves out key components such as tractable, desirable or ethical. If positivism’s goal of defining universal, perfect models of phenomena is not achievable because of the subjectivity inherent in use and interpretation of inquiry conventions, constructs, and tools, then what philosophy is reasonable in which to approach science? Westerman proposed hermeneutics as an ideal philosophy of inquiry. It brings meaning and interpretation to the forefront of its explicit assumptions, “Our accounts must always refer to what people are doing, that is, to meaningful practices, rather than attempt to fully explain the meaning involved in what they are doing in some other terms” [1]. But how does one arrive at meaningful explanations of phenomena? Westerman identified multiple vehicles for approaching meaning in scientific inquiry, including metaphors and reductionism. As he pointed out, however, “Note that these...positions about meaning share something in common. They represent different ways of maintaining that the nature of objects is such (whether that nature is characterized by abstract meanings or the absence of meaning) that a subject reflecting on those objects from a removed vantage point could arrive at what Wittgenstein [10] called crystalline understanding, that is, a complete, determinate account” [1]. Approaches to meaning that assume an objective, removed observer do not fit within the hermeneutic framework of using “Practical activity [as] bedrock” [1]. Hermeneutics differs from crystalline understanding accounts of meaning by assuming that behavior is concrete and part of practical, meaning-laden activities. Behavior is concrete because the people “behaving” act and live in the world and within a social context. As we compare Westerman’s non-examples of meaning with his proposed interpretation of practical activity, the case for hermeneutics in learning analytics will begin to emerge. 
 3.1 Losing Meaning through Metaphor.
 One way of prescribing meaning to phenomena is by calling out similarities among new and the known phenomena. Westerman identified abstractions and metaphors as a path that many researchers take to achieve this type of meaning, though he felt it was misleading. “According to the tradition—rationalism, in particular—meaning refers to abstract structures that lie behind the diversity of events. Philosophers proceeding along the lines of the tradition locate the capacity to appreciate such meanings in the subject’s mind, and psychologists follow suit with ideas about how there are such abstract structures as scripts or rules inside the mind” [1]. For example, the information processing (IP) metaphor in cognitive science stands out as a primary instance of making the metaphor more real than the observed behavior. IP is a metaphor that cognitive science uses to describe mental processes. It was derived from computer science [11] and views the world as information to be inputted into the mind in order to be encoded for long-term memory [12]. Because computers have inputs, outputs, processors, and memory, so must humans. Right? Unfortunately for cognitive science, there is no rationale beyond preference alone to use the IP metaphor in order explain the mind. Nevertheless, cognitivism has made IP the vernacular for describing its inquiry, just as the steam engine was the preferred mind metaphor before the advent of computers (a metaphor which we ‘enlightened, 21st century scholars’ now scoff at as unbelievably puerile). The problem with metaphors and abstractions is that they all, by design, illustrate only some properties of the situation or object, while obfuscating others. Consequently, a metaphor cannot be a theory of all things. Whichever learning metaphor is employed, some type of learning will not find place to be adequately described. In the case of the IP metaphor, it can explain psychological phenomena as long as they superficially appear similar to how a computer processes data. As will all abstractions, IP loses its explanatory power as the psychological phenomena we attempt to explain diverge from the affordances [13] of the metaphor. IP reaches its limits in accounting for play, creativity, exploration, etc.—things a computer cannot do. Hence employing abstractions gives meaning, but the meaning portrayed may not be sufficiently representative of the phenomena under study. 
 3.2 Losing Meaning through Reductionism.
 Another way of assigning meaning to phenomena is by reducing it to supposed fundamental components, which is to say a definition of not what something is, but what something is made of. As previously discussed, linguistic operationalism is a prime of example reductionism. But behavior can be reduced in other ways; as Howard Gardiner described, “It seems to some observers that an account of the classical psychological phenomenon of habituation in terms of neurochemical reactions is an important step on the road to the absorption of cognition by the neurosciences. Once the basic mechanisms of learning have been described in this way, no additional level of explanation will be needed; in a way that would please such behaviorally oriented philosophers as Richard Rorty, these reductionists believe there is really nothing more to be said when neurophysiology has had its say” [11]. If the meaning of behavior can be reduced to no meaning by cutting it down to neurophysiology, the same could be said of behavior on the electronic networks through which students in online and blended learning environments interact. The implication would be that behavior is no more than the sum of its frequency counts, which sheds no light on the meaning of psychological or learning phenomena. This by no means is a call to avoid inquiry involving quantitative analysis. As Westerman explained, “The key point here is that even though mathematics enters into the picture via data analysis, our examination of phenomena is not mathematical in nature. Although this statement may seem strange, it is accurate because the mathematical aspects of the research procedures are embedded in the larger context of the meaningful, interpretive procedures” [1]. Aspects of behavior may be measured, counted and analyzed, but the philosophy behind the inquiry determines whether one interprets an ability as the sum of the observational data or as a meaning-laden practice in the lives of individuals. 
 3.3 Finding Meaning through Concreteness.
 Addressing meaning through interpretive inquiry requires looking at practical behaviors. “Practical” points to how the behavior is concretely embedded in a social practice. As Westerman eloquently stated, “We appreciate the meaning of objects of inquiry from the ‘‘inside.’’ Their significance always refers to the roles they play in the world of practices in which we are already engaged. This locates meaning in the world (not the dead world of brute events, but the living world of practices), not in the mind. As a result, meanings are concrete, not abstract—an impossibility from the point of view of the philosophical tradition, but a central tenet of a perspective that takes practical activity as the starting point” [1]. But how does viewing the significance of behavior as concrete change how inquiry is performed? Westerman noted that, “Recognizing that behaviors are parts of practices, however, leads to advocating the use of meaning-laden measurement procedures, because the significance of behaviors depends on the role they play in practices” [1]. So, interpretive inquiry must include a way to address the meaning that results from behavior in context. Westerman gave the example of codifying a heated conversation through either relational codes such as ‘‘A yells at B,’’ or by decibel-level. He suggested that the “objective measure” can be misleading, if, for example, “A says something endearing to B while the two stand on a corner with noisy traffic going by.” [1]. The question of meaning in inquiry brings into focus the relevance to how humans carryout their lives. In learning analytics, could meaning be assessed just by correlating grades to behavior counts? Could confirmatory factor analysis attribute the indicator variables to factors reasonably without looking at the meaning of the items used in the datagathering instrument? The concrete human experience weaves the interpretation of results into the fabric of conclusions, not at the loss of objectivity (inquiry is not objective in the first place), but in the acquisition of meaning, plausibility, and authenticity. 
 4 Conclusion.
 Learning analytics is an exciting new field of research with the potential to drastically improve learning. Online learning systems have become the educational equivalents of physics’ Large Hadron Collider, generating massive amounts of quantitative data that can be subjected to a wide variety of mathematical analyses. Armed with these huge data sets and powerful computational tools, there is a temptation for educational researchers to regress toward positivism in their approach to inquiry. In this article we have pointed out problems with the positivist approach in social science generally and in the context of learning analytics specifically. As Westerman presented the foil, “The empiricist wing of the tradition offers…the idea that the apparent meaningfulness of events can be reduced to chains of brute occurrences, behaviors, and sense data. Psychologists turn to this idea when they argue that they can operationalize constructs and hypotheses” [1]. Nevertheless, meanings are concrete, not abstract, because they are located in the act of participating in the practical activity of everyday life. Furthermore, science does not equal methodology. Methods cannot answer our questions. Instead, researchers must combine philosophy and science in a practice of meaning-laden, interpretive inquiry in order to provide answers to difficult questions, such as how observed patterns in the population apply to the cases of individuals. Consequently, we have argued that hermeneutics provides a more appropriate philosophical framework in which to conduct learning analytics research. We hope this article will catalyze a critical discussion in the learning analytics field, enabling researchers and practitioners alike to more effectively support learning in all its forms.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Learning Analytics as Interpretive Practice: Applying Westerman to Educational Intervention</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>interpretive inquiry</dc:subject>
		<dc:subject>quantitative inquiry</dc:subject>
		<dc:subject>educational intervention</dc:subject>
		<dc:subject>operationalism</dc:subject>
		<dc:subject>positivism</dc:subject>
		<dc:subject>hermeneutics</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-atkisson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-atkisson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/david-wiley"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/david-wiley"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/59/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-atkisson"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/david-wiley"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Towards Visual Analytics for Teachers' Dynamic Diagnostic Pedagogical DecisionMaking</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60/authorlist"/>
		<swrc:abstract>The focus of this paper is to delineate and discuss design considerations for supporting teachers’ dynamic diagnostic decision-making in classrooms of the 21st century. Based on the Next Generation Teaching Education and Learning for Life (NEXT-TELL) European Commission integrated project, we envision classrooms of the 21st century to (a) incorporate 1:1 computing, (b) provide computational as well as methodological support for teachers to design, deploy and assess learning activities and (c) immerse students in rich, personalized and varied learning activities in information ecologies resulting in high-performance, high-density, high-bandwidth, and data-rich classrooms. In contrast to existing research in educational data mining and learning analytics, our vision is to employ visual analytics techniques and tools to support teachers dynamic diagnostic pedagogical decision-making in real-time and in actual classrooms. The primary benefits of our vision is that learning analytics becomes an integral part of the teaching profession so that teachers can provide timely, meaningful, and actionable formative assessments to on-going learning activities in-situ. Integrating emerging developments in visual analytics and the established methodological approach of design-based research (DBR) in the learning sciences, we introduce a new method called “Teaching Analytics” and explore a triadic model of teaching analytics (TMTA). TMTA adapts and extends the Pair Analytics method in visual analytics which in turn was inspired by the pair programming model of the extreme programming paradigm. Our preliminary vision of TMTA consists of a collocated collaborative triad of a Teaching Expert (TE), a Visual Analytics Expert (VAE), and a Design-Based Research Expert (DBRE) analyzing, interpreting and acting upon real-time data being generated by students’ learning activities by using a range of visual analytics tools. We propose an implementation of TMTA using open learner models (OLM) and conclude with an outline of future work.</swrc:abstract>
		<led:body><![CDATA[

 1. Introduction.
 According to George Siemens, learning analytics “is the use of intelligent data, learner-produced data, and analysis models to discover information and social connections, and to predict and advise on learning (1).” 
 (1) http://www.elearnspace.org/blog/2010/08/25/what-are-learning-analytics 
 The LAK 2011 conference call for papers defines learning analytics as “the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs.” In this paper, we present our vision of leveraging learning analytics tools and techniques to support teachers’ dynamic diagnostic pedagogical decision-making in actual K-12 classroom settings. Our vision seeks to extend the current state-of-the-art in learning analytics in at least four directions, to apply learning analytics in the primary and secondary education formal classroom settings compared to tertiary education settings, focus on real-time use of learning analytics by teachers for technology enhanced formative assessment, apply an extended version of the pair analytics method in visual analytics, and finally, to review and build on current work in the learning sciences and the method of design-based research. The primary contribution of our paper is the presentation of the preliminary triadic model of teaching analytics (TMTA). The remainder of this paper is organized as follows. In section 2, we briefly review two strands of research on analyzing learning data from computer supported collaborative learning (CSCL) and higher education. In section 3, based on the Next Generation Teaching Education and Learning for Life (NEXT-TELL) European Union integrating project proposal, we present the new demands faced by teachers in classrooms of the 21st century. Section 4 introduces the concept of teaching analytics and presents the preliminary triadic model of teaching analytics (TMTA). In section 5, we conclude the paper with the identification of several challenges and directions for future work. 
 2. Related Work.
 We present below two selective reviews of recent empirical work in learning analytics from computer supported collaborative learning (CSCL) and the Learning Analytics in higher education. 
 2.1 Computer Supported Collaborative Learning (CSCL).
 Various researchers [33, 34] in CSCL have considered the role of "productive multivocality" in the analysis of collaborative learning2. “Multivocality” refers to fact that CSCL researchers take diverse theoretical, methodological, and analytical approaches to the empirical study of how technology enhanced interaction supports learning processes and leads to learning outcomes. Multivocality can either be a source of strength (in the diversity of perspectives on a complex phenomenon such as learning) or a symptom of weakness (incoherency, divergence of empirical findings, incommensurability of perspectives and so on). “Productive multivocality” can be achieved only “if the “voices” share sufficient objects to reach some degree of coherence in the discourse of the field3.” Through a series of workshops, a group of CSCL scholars have sought to bring together different researchers, who brought with them a variety of data sets and analytic tools and approaches. Part of the motivation in doing so was to determine the degree to which there was commonality to support dialog between the various players and reach some degree of coherence in their discourse. 
 (2) http://engaged.hnlc.org/story_comments/list/13 (3) CSCL 2009 Workshop: Common Objects for Productive Multivocality in Analysis http://engaged.hnlc.org/story_comments/list/13 
 The workshop participants were asked to consider analytic efforts across five dimensions: purpose of analysis, unit of interaction, data and analytic representations used, analytic manipulations, and theoretical orientation. Suthers et al. [32] have extended some of these ideas further by developing what they call an "uptake analysis framework" to help conceptualize, represent, visualize, analyze and interpret distributed interactions. However, in CSCL, one side effect of the symmetrical socio-technical configurations of students, equitable division of labor, shared conception of the problem, and shared task goals is the displacement of the teacher from the analytical center and a delimitation of the teacher’s role to that of a facilitator at worst and a curriculum designer/learning architect at best. Moreover, there exists a gulf of relevance between the emerging results of learning analytics work in CSCL and the professional practice of teachers. Creating solutions and generating implications for the professional practice of teachers has been a topic of interest and importance within CSCL [e.g., 21, 22, 26] and we seek to re-engage with that. 
 2.2 Learning Analytics in Higher Education.
 Networked learning analytics were first proposed for Learning Management Systems (LMS) such as Blackboard and Moodle with the objective of collecting data from learners in a non-intrusive, unobtrusive and automatic ways in order to trace the trajectory of the learning process and for appraisal and assessment of the effectiveness of online and blended courses [27]. Emerging empirical results indicate that Learning Analytics can help predict student performances with respect to learning across a variety of courses and academic programs in higher education. The use of academic analytics generated actionable intelligence for designing early interventions for freshman students at-risk of not returning for the sophomore year at the University of Alabama from 1999-2001 [7]. Another example is the Signals program [2] at the Purdue university mined institutional data from campus IT systems, analyzed the collected data, identified atrisk students and generated actionable information for designing educational interventions. Results show a significant improvement of student learning performance and subjective satisfaction [2]. Prior findings also show that monitoring and predicting the key performance indicators (KPI) of students with the help of learning analytics can help in designing, tailoring and targeting highly effective student interventions [10, 14]. Further, current results show the benefits of using learning analytics for performance monitoring and outcomes prediction for student populations in general at higher education institutes beyond the at-risk student segment [11, 23, 30, 42, 44] 
 2.3 Summary and Critique.
 An overarching observation is that the voice of the researchers and administrators in many of these approaches and studies comes through loud and clear. What is less prominent is the voice of the teacher or practitioner. We have evidence that the voice of the teacher can be very powerful when it comes to learning analytics. Some studies [35] have suggested that the sorts of detailed information that have typified analytic feedback have been useful to researchers, a more intuitive, user-friendly, and visually sophisticated representation is more powerful for use by teachers for just-in-time assessment. 
 Knowledge building systems, with formative assessment, can be conceptualized as a cybernetic system with feedback loops serving to drive the system in new directions [28]. To optimize performance, feedback must be relevant and timely. Analysis of discourse from computer-supported collaborative learning environments is common but, as noted in [20], relatively little attention has been paid to the “formative, embedded, and transformative aspects of assessment in collaborative inquiry.” We offer two scenarios based on real anecdotes suggesting new ways in which teachers, researchers, and analysts can interact to support rapid feedback. 
 2.3.1 Scenario #1.
 Students engaged in online knowledge building often appear to be collaborating but the extent to which they are doing so is not often apparent. Are students really working together to build knowledge? What evidence can we garner that that is happening? One fourth-grade teacher was facing exactly those questions, and she was able to use a graphical social network analysis tool to show the sociograms that resulted from looking at who was interacting with whom in the online database. She used this tool to help her understand the extent to which students were interacting. At one point, a group of teachers from another school district visited her classroom and posed similar questions. She immediately started the social network analysis tool, and showed the visitors what she thought were unimpressive results: the data showed that all students were interacting. Of course, the visitors were anything but unimpressed. They were stunned by four things: that the students were interacting to such an extent, that the data to support such a claim were readily available; that the tools existed to provide simple representations of complex phenomena, and that she was able to use and demonstrate the tool so effectively. 
 2.3.2 Scenario #2.
 An experienced teacher was working with her 10-12 year old students on a module about electricity. The students were very engaged and had spent considerable time working through interesting problems. They had contributed a considerable number of notes to the online database that they used to track their inquiries and the unit had already gone on for several weeks. But were they covering the mandated curriculum topics? How could she obtain objective verification that her students had covered the curriculum even if she believed they had? A visual analytics expert had devised a tool that allowed a user to visualize the degree to which the curriculum had been covered. By literally lining up the curricular expectations on one side of the screen and the students' traces on the other side and examining the links between them the visual analytics expert was able not only to reassure her that her students were well on track, but to also allow her to see the few remaining curricular expectations that needed to be covered. The teachers' feedback on the visualization led the visual analytics expert to improve the visualization tool to make the same sorts of comparisons easier in the future. Though these may seem far-fetched or perhaps, unique scenarios, we argue that they are both representative of learning and teaching situations encountered in formal learning settings. Particularly, when we consider the new demands being made on teachers in the 21st century classroom. 
 3. NEXT-TELL: New Demands on Teachers in the 21st Century Classrooms 
 According to Peter Reimann and colleagues of the Next Generation Education, Teaching and Learning for Life (NEXT-TELL)4 integrating project recently funded under the European Commission’s Seventh Framework Programme, the following are the new demands that teachers face in the 21st century classrooms (NEXT-TELL Consortium, 2010). Develop 21st Century competencies in addition to subject-matter specific Knowledge, Skills and Attitudes (KSAs)5 Personalize learning by planning lessons and learning activities for the individual student6 Teach adaptively in the classroom, making good use of ICT [9, 25] Provide evidence-based accounts for selected learning activities and assessments Be accountable towards stakeholders (students, parents, policy makers). As the NEXT-TELL project consortium says: In order to deal with these demands, teachers need to rapidly capture an ever-increasing amount of information about students’ learning, interpret this diverse body of information in the light of students’ development, appraise it in light of curricular goals, and make reasoned decisions about next learning steps. However, in comparison with most other professionals from whom clients expect rapid decisions in a dynamically changing environment, presently teachers often do not get the information they need for decision making in a timely fashion and in an 'actionable' format. This is particularly a challenge in technology-rich settings (the school computer lab, the laptop classroom) with high content and communicative density, where students engage with learning software and tools that teachers can only partially follow at any point in time. However, as technology increasingly is permeating all schools and all classrooms, the challenge is there for all to face. (Peter Reimann et al., 2010) Drawing on this, we propose that learning analytics research should focus on providing both computational and methodological support for teachers in real-time and in-situ classroom settings. Towards this end, we sought to integrate emerging developments in visual analytics and the established methodological approach of design-based research (DBR) in the learning sciences. The results of this integrative exercise are the approach called “Teaching Analytics” and a model of teaching analytics, termed “triadic model of teaching analytics (TMTA)”, discussed next. 
 (4) Peter Reimann et.al, www.next-tell.eu (5) European Reference Framework: Key competences for lifelong learning. (6) Harnessing Technology for Next Generation Learning: Children, schools and families Implementation Plan 2009-2012. Downloadable from BECTA: http://publications.becta.org.uk/display.cfm?resID=39547 
 4. Triadic Model of Teaching Analytics (TMTA).
 Our model of teaching analytics seeks to adopt and extend the model of pair programming from the software engineering paradigm of Extreme programming. We propose an extensible triadic model. More specifically, teaching analytics adapts the Pair Analytics method [1] in visual analytics [36]. The Pair Analytics method was inspired by the Pair Programming7 model in the Extreme Programming8 software engineering approach. In pair programming, “all code to be sent into production is created by two people working together at a single computer8.” Our vision can be outlined as below: To empirically explore the effectiveness, efficiency and satisfaction in fundamentally transforming the teaching profession from a “lone ranger” model to the collaborative model where teachers, analysts and researchers with complementary expertise collaboratively leverage their knowledge, skills and aptitudes towards enhancing learning in high-performance/high-bandwidth/highdensity classrooms of the 21st century. However, the dyadic configuration of “driver” and “navigator” in pair programming and pair analytics creates a bootstrapping problem for learning settings: can we really throw a Visual Analytics Expert (VAE) and Teaching Expert (TE) together into a classroom setting and expect them to work productively without explicit facilitation, intelligent scaffolding, and guided design? Facilitating interaction is a role that can be fulfilled by a Design-Based Research Expert (DBRE). As such, we adapt and extend the dyadic model of pair analytics in visual analytics to a Triadic Model of Teaching Analytics (TMTA) as shown in Figure 1: 
 Figure 1: Triadic Model of Teaching Analytics (TMTA).
 At its core, our model sees collaborative knowledge building between teachers, analysts and researchers. Each has a complementary role in the teaching analytics setting. 
 (7) http://www.extremeprogramming.org/rules/pair.html (8) http://www.extremeprogramming.org/ 
 Eliciting criteria for Teaching Analytics involves a collocated collaborative triad of a Teaching Expert (TE), a Visual Analytics Expert (VAE), and a Design-Based Research Expert (DBRE) analyzing, interpreting and acting upon real-time data being generated by students’ learning activities by using a range of visual analytics tools. We think of the relationships between the TE, VAE and DBRE as a dynamic socio-technical system. The design considerations are about creating feedback loops between the three individuals, such that each one drives the other two to higher levels of performance on the positive side (with the cost of anxiety in the negative case). That is, feedback from the teacher inspires the VAE to create new, better visualizations and for the researcher to better understand the ongoing teaching and learning processes while feedback from the VAE – perhaps in the form of visualization artifacts – allows the teachers to better understand what is going on in the classroom from a learning activity design perspective and the research to hypothesize, test and predict student learning trajectories and performance outcomes. All in all, these feedback loops should culminate in the teacher providing timely, meaningful actionable, customized and personalized feedback to students. The key point here is that each member of the triumvirate of TE, VAE, and DBRE can gain from the other two, not that each partner's role is to highlight deficiencies of the other two. Therefore, TMTA involves a close collaboration between the TE, VAE, and the DBRE. It includes teaching practitioners in the design process and invites them to contribute significantly to the innovation of the visual analytics tools. This allows these learning analytics tools to address pedagogical issues as they arise and evolve in real classrooms. In the next section, we outline an approach to TMTA based on open learner models (OLM). 
 5. TMTA and Open Learner Models.
 An obvious starting point for developing the TMTA approach is to base it around the existing work in Artificial Intelligence in Education, on open learner models. A learner model holds information (usually) about an individual learner, and the model is automatically and dynamically updated during the user's interaction with a computer-based/online educational environment. The learner model typically includes data about the learner's knowledge state, which may include specific difficulties and misconceptions; and it can also have data on other aspects of the learning process (e.g. representation, content, teaching style preferences; motivational, social, affective attributes). The learner model is then used by the educational environment to adapt its teaching to the specific needs of the individual learner (the environment 'understands' the user's understanding). An "open learner model" is a learner model that can also be externalised to the user [4]. This externalised (open) learner model may be simple or complex in format using, for example: text, skill meters, concept maps, hierarchical structures, animations [3]. Normally the user who accesses the learner model is the learner. Common purposes of externalising the learner model to learners are to promote metacognitive activity such as awareness-raising, reflection, self-assessment and planning [5]. Some learner models have, however, also been made available to teachers [6, 13, 43]. Teacher access to the learner models of their students can help them to better understand learners' needs as individuals and as a group, and can therefore enable teachers to adapt their teaching. Of particular interest in NEXT-TELL is the possibility of open learner models to support the routine but dynamic decision-making that teachers need to perform in the classroom. While the above describes the typical situation of open learner models, it is easy to envisage this being extended for use in TMTA. A range of visualisations or externalisations of the learner model have been explored (e.g. Bull et al., 2010), and these could be further extended to support the synthesis of work between teaching experts, visual analytics experts and design-based research experts, as required for the proposed TMTA approach. 
 6. Discussion.
 As mentioned in the prior section, we conceive of the Triadic Model of Teaching Analytics (TMTA) as a socio-technical system. Such systems are characterized by socio-technical interactions. The design considerations are to develop, deploy and evaluate the use and impact of the perception and appropriation of socio-technical affordances in the TMTA socio-technical system. Affordances are action-taking possibilities and meaning-making opportunities in an actor-environment system relative to the competencies of the actor and the capabilities of the system [39]. Based on the theory of socio-technical interactions in technology enhanced learning environments developed in [38, 39], we propose that design dimensions based on affordance classes [39] can help inform realize the idea of TMTA. Future work will consist of a systematic exploration and exploitation of the affordance classes in different socio-technical configurations of TMTA. In conclusion, we would like to highlight the similarity between the TMTA and the productive multivocality framework mentioned in the introduction. Whereas the productive multivocality framework focuses on relationships between researchers, the TMTA extends that multivocality to include teachers, design-based researchers, and visual analytics experts. Each voice in the system shares the goal for sustained innovation in leveraging the design of affordances of visual analytic tools to support teachers' dynamic diagnostic pedagogical decision making. 
 Acknowledgement.
 This work is supported by the NEXT-TELL - Next Generation Teaching, Education and Learning for Life integrated project co-funded by the European Union under the ICT theme of the 7th Framework Programme for R&D (FP7).]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Towards Visual Analytics for Teachers' Dynamic Diagnostic Pedagogical DecisionMaking</rdfs:label>
		<dc:subject>visual analytics</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>teaching analytics</dc:subject>
		<dc:subject>learning sciences</dc:subject>
		<dc:subject>computer supported collaborative learning (cscl)</dc:subject>
		<dc:subject>multivocality</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-teplovs"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-teplovs"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nobuko-fujita"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nobuko-fujita"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/susan-bull"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/susan-bull"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-teplovs"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/nobuko-fujita"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/susan-bull"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/61">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Redefining dropping out in online higher education: a case study from the UOC</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/61/authorlist"/>
		<swrc:abstract>In recent years, studies into the reasons for dropping out of online higher education have been undertaken with greater regularity, parallel to the rise in the relative weight of this type of education, compared with brick-andmortar education. However, the work invested in characterising the students who drop out of education, compared with those who do not, appears not to have had the same relevance as that invested in the analysis of the causes. The definition of dropping out is very sensitive to the context. In this article, we reach a purely empirical definition of student dropping out, based on the probability of not continuing a specific academic programme following several consecutive semesters of “theoretical break”. Dropping out should be properly defined before analysing its causes, as well as comparing the drop-out rates between the different online programmes, or between online and on-campus ones. Our results show that there are significant differences among programmes, depending on their theoretical extension, but not their domain of knowledge.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 From an institutional perspective, university dropping out is very important, as dropping out needs to be seen as a failure of the university system to generate “product” (graduates) with an important quantity of public resources invested. In the analysis made in this paper – and based on the specific definition of dropping out presented in it – we see that during the first 26 semesters, the Universitat Oberta de Catalunya (Open University of Catalonia, UOC) received 62,450 new students enrolled in officially recognised degrees in Catalan; 13.3% of them finished a degree and 57.6% dropped out of their studies. These figures only include students who have been enrolled a certain number of semesters large enough to establish a criterion for dropping out, the main goal of this paper. In fact, establishing the exact moment when a student can be considered a drop-out is part of this goal. Dropping out is a highly relevant phenomenon that deserves to be analysed in detail. Furthermore, it is difficult to establish comparisons with other centres given the specificities of the UOC, a purely online distance university. One of the challenges posed to universities, be they brick-and-mortar or distance, is to define the concept of dropping out. The main difficulty lies in the fact that, faced with several successive semesters of non-enrolment by a student, it cannot be said with 100% assurance that this student has definitively dropped out of their studies, as it may happen that he or she is taking a longer or shorter break; this difficulty is even greater in distance studies, where the profile of the majority of students has more work and family commitments than that of brick-and-mortar type university students and where, therefore, the existence of breaks seems much more likely. Additionally, it should be considered that the official definition of dropping out does not reflect the particularities of online higher education. The main aim of this paper is to define dropping out in online higher education following an inductive process based on an objective analysis of enrolment and nonenrolment (i.e. breaks) data of all students in officially recognised degrees in Catalan at the UOC between 1996 and 2008. The definition reached using this methodology will be valid for all teaching institutions that offer studies of a certain duration and with non-obligatory enrolment. In particular, it is highly adaptable, due to the possibility of the existence of breaks, to institutions offering distance university education. Dropping out analysis requires a certain historical database in order to be accurate. In this sense, the UOC, despite being a very young and pioneering university in the field of virtual education, has already been in existence for fifteen years and has, at present, almost 35,000 active students in officially recognised degrees in Catalan, which allows it to undertake a quantitative analysis on the basis of a strong statistical representation. To conclude, we should stress that this definition of dropping out will be established from an institutional perspective, i.e., without considering the perspective of the student; in this way of thinking, a student may “drop out” (not have achieved the aim of the qualification) from the point of view of the university, but he or she may be fully satisfied with the teaching experience, having achieved his or her personal learning objectives. Therefore, from an institutional point of view, the definition of dropping out will always be harder or more negative than reality. The rest of this paper is organized as follows: Section 2 describes some approximations to the higher education dropping out phenomenon in brick-and-mortar universities and, especially, in distance learning institutions. Section 3 explains the data used and the methodology followed for analyzing it. Section 4 presents the main results and, finally, Section 5 deals with the conclusions and future research in this topic. 
 2 Prior analysis of university dropping out.
 Lassibille and Navarro [1] offer a recent view of the subject through the Spanish brick-and-mortar university system. They analysed 7,000 students enrolled at the University of Malaga starting in September 1996 and running to June 2004 (8 academic years). The authors ruled out analysis of students who took a break during a certain academic year, as they only accounted for 2% of the total (in the case of students analysed at the UOC – online students – this figure is around 14.6%). The definition of dropping out they give is very clear: a student is considered to have dropped out of a particular degree course even if they move to another one (we see later that this criterion is also used in the analysis of UOC students). On the basis of this definition, overall dropping out by this cohort (i.e. the group of students who began at the start of 1996) during the period under consideration is a high 46.1%. The authors’ work immediately concentrates on analysing the causes of dropping out. Other authors [2] put this figure at 40%-45% during the last 100 years. In 2004, Berge and Huang [3] conducted a synthesis of the bibliography available at the time on the problem of student drop-out rates in e-learning. They recorded the existence of a higher level of dropping out in virtual environments than in brick-andmortar environments, as can also be seen in Frankola [4]; although they did point out that the problem of dropping out is complex and multidimensional. The authors established the definition of a holistic model of university dropping out, which concentrates its causes – and therefore the possible actions for reducing this dropping out – on institutional factors (including actions by lecturers and administrative personnel) and on the socio-demographic and academic characteristics of the students. This model provides a general framework that each institution should adapt to its specific characteristics. In their definition of the complementary term to dropping out (which we could call “continuity”), Berge and Huang state that the ultimate aim of continuity needs not always to be seen as the qualification obtained through study (completion). For some students, successful learning experiences have more to do with “participating” and not with “completing” or “obtaining a degree”, as stated in Kerka [5]. In this sense, Pappas and Loring [6] give the name “degree seekers” to students, who, from an institutional point of view, may be considered “drop-outs” when they stop their studies. However, we should remember that this paper adopts a totally institutional perspective on dropping out, which does not take into account, at least in this initial stage of analysis, the aims of students. It is supposed, therefore, that all students who enrol in a degree course intend to complete it and obtain the accreditations. 
 2.1 Dropping out in distance higher education.
 It is interesting to note how the UNED, the main Spanish distance university founded in 1972, analyzes the subject of dropping out by its students. In an extensive article, Callejo [7] takes dropping out analysis from a more long-term perspective than in a brick-and mortar university1, in the same way as this article. However, in his definition of dropping out, he introduces a more qualitative element, namely the intention to continue or not to continue by students who are taking a break from their studies (which he asks them about in a survey). Callejo negates that the congestion of distance universities is one of the main causes of dropping out (it can be seen that degree courses with large numbers of students – e.g. law and psychology – have proportionally lower rates of dropping out than others with fewer students – such as engineering). The author cites the intrinsic difficulty of the contents of each programme as one of the main causes of dropping out. Lastly, it is interesting to see how a benchmark university in the field of distance education on a world scale, the Open University (UK), has analysed its drop-out rates. Tresman [8] stresses the specificity of adult distance students and calls for an analysis that matches this profile to be undertaken. He states that, “the vast majority who withdraw – 94 per cent – still aspire to earn credit for the course/award upon which they embarked”. He immediately collects their reasons for withdrawing from their studies during the various stages in their academic lives and, finally, proposes a strategy to improve student continuity. Tresman starts the work that Ashby [9] would undertake two years later, taking the Open University as a reference. The author calls the classic definition of continuity the “institutional dimension”, understood to be “passing the course” and presents two new dimensions or definitions that relativise the importance of the final qualification: that of the student and that of the employer. The author gives the total drop-out rate (during the first academic period) as 40% and says that, if this figure needs comparing, it should be with other part-time educational institutions and not with other traditional university education (full-time brick-andmortar) institutions. 
 3 Data and methodology.
 The data used in this paper are taken from UOC academic databases. For this initial study, only student enrolments are analyzed. The following variables are available: IDP (Person Identifier Code), unique to each student, it allows individual and at the same time anonymous monitoring; gender; student’s date of birth; semester of the student’s enrolment; codes of the subjects enrolled in by the student; final grades obtained in the subjects; number of credits that the subjects carry and, finally, the academic programme, e.g. Law or Computer Engineering. Specifically, there is a record for each subject enrolled in the officially recognised degrees in Catalan from the start of the university until the end of the 2008-2009 academic year (in all, 1,169,262 records). 
 (1) Where taking a break during a single academic period is considered dropping out. 
 Observe that enrolment at UOC is opened each semester, which means bi-annually. A total of 19 degrees were offered during this period. Only valid enrolments have been included, i.e. ones that have been formalised and paid for, thus excluding enrolments that were subsequently cancelled. A total enrolment history was provided for 84,230 students, although only 62,450 (those of the 16 programmes with more available information) were analysed. This study ignores the pilot cohorts of the programmes that began at the start of the university, which limited student access during the first semester to a closed number and, for administration purposes; there was no access for new students during the second semester. To analyse the data described in this article, only the “IDP”, “semester of enrolment” and “academic programme” fields were considered. The information from these fields was used to generate 20 files, 19 for each programme and a general file for all programmes, each of which contains a record for each student. These students are those who enrolled in one or more semesters of the programme during the period in question. The records generated have the following coding: 
 10104;1;1;1;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0.
 where the first number is the IDP and then a binary string for the semester record (“1” = student enrolled at least in one subject, “0” = student not enrolled in any subject). The specific nature of this string is that, for analysis purposes, all enrolment sequences have been put in the “same starting position”, that is, the first semester when each IDP is enrolled in each degree is considered to be the same for all students. Obviously, the first element after IDP is always “1” (the first enrolment of each student). Finally, notice that the sequences “113000;1;0;0;0;0;0;0” and “10104;1;0;0” are different as more enrolment history about the first student is available for analysis (specifically, 7 semesters as opposed to 3). Once the enrolment sequences file of each programme is generated, the frequency of break sequences (that is, of sequences of one or more “0”) can then be analysed. This is performed via a process that detects the longest break sequence (with “1;0;...;0;1” format) within each enrolment sequence of each individual, with the particularity that if, for example, a student has taken a break once over 5 semesters and over 2 during another semester, he or she will only be calculated as having taken a break over 5 semesters. Notice that this process does not take graduates into consideration, as they may be considered as taking a break or abandoning their studies, when they have in fact obtained their degree. For exemplification purposes, Table 1 shows the probability of having a break of N semesters for the Law degree (with 7,938 students and a history of 24 semesters) and the Market Research and Techniques (MR&T) degree (with 1,718 students and a history of 14 semesters). 
 Table.

 1. Analysis of the break sequences from Law (left) and MR&T Studies (right). 
 The column description of Table 1 is as follows: the first column is the number of semesters of consecutive breaks (namely N); the second is the number of students enrolled in the Law degree that take a break of length N; the third and fourth columns are the percentage of such students with respect to the total of students in the degree and the accumulated percentage, respectively. Columns 5-7 reproduce the same for the MR&T degree. In the light of these results, there are two students in the Law degree that take a break of 19 consecutive semesters, which may be surprising but shows the wide diversity of online students' interests and behaviour. Nevertheless, in order to define dropping out, we are interested in establishing a threshold for what we consider a reasonable period of break time. As shown in bold in Table 1, only 3.77% of Law students take a break of 5 or more semesters. In the case of MR&T students, a similar percentage (3.14%) is found but only with 3 semesters or more, showing a relevant difference among academic programmes. In short, if we define dropping out as taking a break of 5 or more semesters for the Law degree, we are assuming an error smaller than 5%, which can be considered reasonable. On the contrary, dropping out is defined in the MR&T degree as having a break of only 3 semesters with the same error assumption. Notice that the fact that a Law student has the “1;0;0;0;0” string in his or her enrolment sequence is not sufficient information to see whether he or she will drop out, as we need an additional semester as mentioned above. Following this criterion we are able to label each student with a sequence of N or more “0” as dropping out. Therefore, a definition of the drop-out rate for a specific programme would be reached inductively as being the proportion of students who have taken a break for N or more semesters out of the total number of students enrolled in the programme during the period in question. N is determined using the maximum probability of the 5% error rate in classifying the student as a drop-out once they have taken a break of N or more semesters in that specific programme; in other words, a maximum of 5% of students on that specific programme return to their studies after taking a break of N or more consecutive semesters, so the maximum error is upper-bounded. 
 4 Results.
 On the basis of the work set out in the section above, the definition of dropping out for each programme of officially recognised degrees in Catalan is reached. The specificity of the programme in question is highly relevant. Although, logically, the definition of dropping out in qualitative terms is the same for all courses; repetition of the probability analysis carried out for all programmes gives as the result different “quantitative definitions” depending on the values of the “parameter” of this definition, i.e. different N values for consecutive break semesters. 
 4.1 Differences among programmes.
 Table 2 provides a summary of the values associated with the graphs of the 16 programmes analysed (the three more recent programmes have not been included because more historical information was needed). The explanation of the fields of Table 2 is as follows. For each programme, the minimum number of consecutive break semesters needed to be considered dropping out is N, with the maximum error that is smaller than 5%, as well as the number of semesters defined in the curriculum of each programme2, the number of semesters since the programme began and the number of students with at least N+1 semesters. Finally, the last two columns make reference to the percentage of students achieving the degree or dropping out, respectively. 
 (2) Normally, the real duration is longer, as students enrol in fewer subjects than those theoretically defined in the curriculum, especially in distance universities. 
 Table 2: Results summary by programme and total of programmes. 
 As shown in Table 2, the number of semesters that define dropping out in each programme has particularly relevant variability: this value varies between 3 and 5 semesters. Notice that these figures are very conservative, since using an upper bound of 10% would have reduced the number of consecutive break semesters. An initial analysis of these results shows that there appears to be no relationship between the type of programme content, i.e. technical or humanistic, and the number of semesters that determines dropping out. For example, in the case of Computer Engineering, the value is high (5 semesters), but it is the same in the case of Humanities. On the other hand, it does seem that in programmes where students have prior higher education experience related to the curriculum they are studying (in Spain they are known as “Second cycle” degrees, the ones with a duration of 4 semesters at the bottom of Table 2), dropping out is decided with fewer semesters than on programmes where this experience is not required (known as “First cycle” or “First and Second cycle”). This way, the average number of semesters that define dropping out of all the students who have dropped out from second cycle programmes is 3.69 semesters, while it is 4.61 in students of the other programmes. To validate this statement statistically, an independent-samples T-test3 is carried out with SPSS package. Since the significance value of the test is less than 0.05, it can be safely concluded that the difference of 0.92 is not due to chance alone. It can also be seen that students in shorter duration programmes (second cycle) also take shorter breaks during their academic record, which seems logical. 
 (3) Equal variances not assumed. 
 4.2 Enrolment behaviour graphs.
 Armed with these parameters, graphs for each programme, as well as a general graph, can be drawn up to visually display the behaviour of enrolment, dropping out and accreditations of each programme. Figure 1 shows an example of the calculated graphs, in particular for the Law programme. For simplification purposes, only arcs with a minimum proportion of students (1%) are shown. The graph shows in some way the “survival history” of students of each programme. For example, in the first node of the graph shown, 6,149 students of Law (who enrolled at least 5+1 semesters ago) appear in the starting position (at the first semester). This number is greatly reduced in the second semester, in which only 4,048 students (65.83%) continue from the original 6,149. Noticeably, the main cause of this loss of 2,101 students is dropping out after the first semester (1,643 students). That is, one out of four students does not continue after the first semester. On the other hand, except one student obtaining the degree after his or her first semester, the rest (7.4%) take a break, which is also remarkable. 
 Fig. 1. Enrolment behaviour for the first 8 semesters of Law students. 
 5 Conclusions. 
 This paper deals with the formulation of a definition of dropping out that is suitable for the students of a distance higher education institution such as is the UOC. This objective is quite important if it is considered that most UOC students are adult students with work and family commitments additional to that of continuing education and, therefore, with a natural tendency to take academic breaks. On the other hand, this objective is somewhat difficult to reach, due to the fact that these break periods could be interpreted as only that (periods of rest) or, at some point, as indicators of having definitely dropped out of their studies. At the end of the analysis carried out in this work, it seems that the effort invested in reaching an inductive definition of dropping out, achieved using the enrolment and non-enrolment (break) behaviour of students in each programme or academic discipline, has led to particularly relevant results. This definition is highly sensitive to the reality of each programme. Consequently, it has enabled us to classify students as drop-outs when they take a break of just 3 consecutive semesters, for example in the case of Psychology or Tourism, and up to as long a period as 5 semesters, i.e. two years and a half, in the case, for instance, of Business Sciences or Humanities. The definition was sustained in an analysis of the Universitat Oberta de Catalunya (UOC) enrolment data, running from 1996-1997 to the 2008-2009 academic years for official degrees in Catalan, with at least 14 semesters of record. This includes 16 different programmes. In particular, an analysis in terms of programmes was carried out, concerning the frequency of break sequences, that is, the consecutive periods of non-enrolment that culminate in a reincorporation of students into the same degree. The result of this analysis is the minimum break sequence (of N or more semesters) that has a very low associated probability of students returning (fewer than 5%, the assumed error). N, the number of consecutive semesters in blank, enables a student to be classified as a drop-out for a specific programme. We should stress, however, that this N value is different for each programme, and that herein lies the potential for the definition of dropping out that has been reached in this paper. Therefore, a single definition for dropping out cannot be established at the university level. In an initial approach to the analysis of differences between programmes in terms of the number of semesters that trigger dropping out, it appears that in programmes where students have prior higher education experience, the decision to drop out is made more quickly (almost one semester before) than in programmes where such experience is not required. This may be due to students having clearer objectives in these types of programmes, which are based on completed studies (in a previous degree). It could also be related to the shorter theoretical duration of such degrees. We should also point out that total (accumulated) dropping out percentages for these kinds of programmes are significantly smaller than those for the other programmes. The analysis conducted in this paper, which allows us to establish whether a student can be considered a drop-out or not, is the starting point towards undertaking a close study of the characteristics of students who drop out. Such a study will be based on data already collected, but not yet incorporated into the analysis, such as age and sex of the students or other variables related to number and kinds of subjects taken every semester. We should also consider the addition of new variables which are the result of a more qualitative analysis than the one undertaken in this paper. Once the causes of dropping out have been detected, the establishing of corrective actions that have a positive effect on reducing dropping out should report benefits both at institutional and personal levels, especially for those students who have given up the fight with a sense of not having achieved their learning objectives.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Redefining dropping out in online higher education: a case study from the UOC</rdfs:label>
		<dc:subject>dropping out</dc:subject>
		<dc:subject>higher education</dc:subject>
		<dc:subject>online university</dc:subject>
		<dc:subject>cohorts</dc:subject>
		<dc:subject>distance education</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/josep-grau-valldosera"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/josep-grau-valldosera"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/julia-minguillon"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/julia-minguillon"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/61/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/josep-grau-valldosera"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/julia-minguillon"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/62">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Learning Designs and Learning Analytics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/62/authorlist"/>
		<swrc:abstract>The teaching demands and complexities evolving from recent government and university reforms, linked to quality assurance and accountability, emphasise the importance of developing replicable, scalable design and evaluative approaches. In this context learning design and learning analytics have emerged as two fields of research that can begin to address ongoing educational issues related to professional development, student and teacher evaluations and the development of personalized learning experiences. Learning designs are ways of describing an educational experience such that it can be applied across a range of disciplinary contexts. Learning analytics offers new approaches to investigating the data associated with a learner’s experience. This ‘ideas and innovations’ paper explores the relationship between learning designs and learning analytics.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 Across the Higher Education sector internationally, there is mounting interest in developing replicable and scalable exemplars and templates of effective teaching practice. This in part has arisen from increased accountability and quality assurance measures associated with the administration of education practice and its associated impact on the demonstration of teaching quality. Regardless of the drivers, the field of learning design has received much attention in the education sphere, largely due to its perceived capacity for addressing these concerns. Learning designs, or pedagogical models or patterns, are captured representations of teaching and learning practice that can act as a model or template replicable across alternate educational contexts. In essence, these learning design articulations begin to initiate and frame the intent of and process for the pedagogical experience. Learning design research and its up-take in educational practice has come in many forms. At the basis are ways of describing an educational activity (regardless of scope) in such a way that the activity can be applied across a range of disciplinary contexts. Essentially, the focus here is on the design, planning and, to some extent, implementation aspects of the educational experience. In practice, learning designs are predominately directed at the designer/teacher of the educational experience in order to provide accessible templates for developing learning actions. As much as learning design has tended to focus on the teacher for the initial design and implementation process, learning analytics largely centers on the users (learners) interactions with the developed learning artifacts and activities. In essence learning analytics seeks to use the data associated with a learner’s interactions to make pedagogically informed decisions and evaluations. This emerging interdisciplinary field draws on and integrates research related to, for example: data mining, social networks, data visualisation, machine learning, semantics, e-learning and educational theory and practice. Seemingly, learning analytics takes up where learning design finishes in the educational experience continuum – implementation and outcomes. This ‘ideas and innovations’ paper explores the relationship between learning design and learning analytic approaches. The paper considers the context in which learning design research emerged, and highlights how it has developed - with a focus on the work in Australia. It also considers work in social network analysis as an example subset of learning analytics to demonstrate how this field complements learning design. 
 2 Background.
 Although the terminology and the scope of research and application of both learning design and learning analytics approaches have evolved in recent years, their underlying basis is not new. Essentially these research fields are concerned with documenting and reporting on educational practice and experience. The interest in both learning designs and learning analytics has gained momentum within the context of two compatible worldwide trends within the higher education environment – calls for improved teaching and increasing use of technology. In the early 90s, commentators and policymakers began to argue for increased expectations of the quality of the teaching that was delivered in universities, and accountability for that quality[1]. This was met, in many countries around the world, with a wave of professional development programs aimed to improve the quality of teaching and funding for ‘innovation’ in teaching (such as the Australian Learning and Teaching Council, Higher Education Funding Council for England). Soon after this focus on quality teaching began, the increasingly accessible and user-friendly Web provided universities with an opportunity to expand their student markets and offer flexible forms of programs. There was increasing expectations for, and interest among, university teachers to integrate technology within their teaching practice. From those early days of the Web, research and teaching scholarship explored the opportunities that technology provided education – particularly in terms of improving pedagogy [2]. The increasing emphasis on and expectations towards technology integration placed added complexity to the teaching environment – its design, implementation and evaluation process. As a result, professional development and teaching innovation funding became, in the main, directed toward improving teaching using technology. Underlying the support for these projects was the premise that the outcomes, resources and products would be disseminated amongst the higher education community and adopted at the institutional and individual teacher levels. Developments in learning designs exemplify these principles of resource development, sharing and reuse. 
 Research in Learning Designs.
 In Australia, learning designs first emerged through an Australian Universities Teaching Committee (a predecessor to the current the Australian Learning and Teaching Council) project, funded in 2000, entitled Information and Communication Technologies and Their Role in Flexible Learning. The project established a framework for evaluating high quality learning in higher education [3], identified and evaluated educational examples against the framework, documented these examples and generic versions as resources, and disseminated these resources through the project website (http://www.learningdesigns.uow.edu.au/). These initial resources were termed “learning designs” and comprised a visual representation with supporting textual descriptions (see Figure 1 for example) [4]. 
 Figure 1: Example of a representation of a learning design from http://learningdesigns.uow.edu.au/exemplars/info/LD1/more/03Context.html 
 Concurrently, around the world, researchers have been interested in the educational and technical issues associated with how teachers and instructional designers could create document, share, adapt and implement, educational design ideas. This involved developing a technical language (IMS LD [5]), developing tools in which learning designs could be created and/or used [6] [7], and investigating how teachers interpret and use different learning design representations [8, 9]. The generic and contextual learning designs developed through the Australian project have been reported to be used by staff in educational development units and university teachers as resources to support the process for designing courses as well reflecting on the implementation once a course has been delivered [10]. Studies exploring the utility of the learning design approach have demonstrated positive results for a framework to integrate learning objects [11], usefulness in interpretation and application of representations by teachers [12] and as a tool that that allows extraction of teaching and learning processes for analysis [13]. 
 3 The Potential of Learning Analytics.
 The underlying driver associated with teaching quality assurances and processes is that it will lead to a better learning experience for students and thus, improved learning outcomes. While learning designs may provide theoretical, practice-based, and/or evidence-based examples of sound educational design, learning analytics allow us to test those assumptions with actual student interaction data in lieu of self-report measures such as post hoc surveys. In particular, learning analytics provides us with the necessary data and tools to support the accountability that has been called for in higher education. While learning designs facilitate the documentation of the original pedagogical intent learning analytics assists in measuring the effectiveness of the design approach in achieving the desired outcomes As with learning designs, learning analytics takes a range of forms and foci. One such sub-set of the learning analytics domain is social network analysis (SNA). SNA provides a way to understand the educational experience and outcomes for learners engaged in online communication activities. Much of the aforementioned take-up of technology in higher education has been the associated with the the discussion forum – particularly within learning management system course sites [14, 15] (. There are countless examples in the literature that describe the design and implementation of educational programs that use discussion forums. Where these cases are aligned with a theoretical premise, it is of the social cognitive aspect of learning and the benefits of collaboration (e.g., [16]). Thus it follows that, social network analysis provides a methodology for teachers to begin to understand the impact of their implemented learning design on the student learning experience and outcomes. However, this analysis needs to be readily accessible and interpretable for all educators regardless of expertise in network analyses; information and communication technologies or educational theory. To address this issue of user uptake and interpretation, Dawson [17] and colleagues developed an analytical tool named Social Networks Adapting Pedagogical Practice (SNAPP – available at http://research.uow.edu.au/learningnetworks/seeing/snapp/index.html). 
 SNAPP seamlessly integrates with learning management system discussion forums to extract data and provide teachers with real-time visualizations of the discussion forum activity (see figure 2). Recent evaluations of the use of SNAPP by university teachers demonstrated that the tool was seen to be particularly effective for retrospective analysis of an education implementation after a course was completed [18]. However, despite the ease in generating ongoing visualizations of student discussion interactions and relationship development, teachers did not take the opportunity to analyze data during actual course implementation. This process would have allowed for modification of the learning design ‘on the fly’ if the intended experience or outcomes were not being realized. While this learning analytics tool theoretically supports the implementation stage of that educational continuum - to date it has been largely used as a reflective tool. Further research is required to investigate how educators can be better prompted when student interactions indicate a deviation from the intended learning outcomes. For example, an educator observes minimal studentto-student interactions arising despite the implementation of learning activities designed to establish a learning community. 
 Figure 2: SNAPP generated sociogram of the student network relationships evolving from an online discussion forum (student names have been removed). 
 4 The Challenge.
 Essentially, the learning design approach is founded on a case-based reasoning premise. A documented learning design provides a case that is abstract enough such that teachers can imagine how it might be applied to their own context. However, investigation of the use of learning designs suggests that teachers also benefit from understanding the context of others and the environments in which they teach [12] and thus cases should not be so abstract as to lose a connection with the original context [19]. Regardless of the level of abstraction, the notion is that teachers learn or refine their ideas about design from the examples of other teachers. The dissemination and adoption of innovative teaching ideas in higher education – whether those ideas are expressed through learning design representations or other types of tools or resources, recognizes the social processes of being a teacher. University teachers report that their ideas about teaching come from interacting with colleagues and other teachers [20, 21]. In this context, teachers are less likely to seek information from external resources or online sources [20]. This research (and that of others investigating educational change) frequently highlights time as a barrier to the adoption of new ideas into ones teaching practice. Academics in higher education institutions are stretched across their teaching and research responsibilities. They are more likely to invest time to try new ideas if they are convinced, through evidence, that the innovation will have an impact for them and their learners that is both time and workload efficient. Learning analytics has the potential to provide this evidence. The research challenge is identifying effective and efficient ways to make learning design and learning analytics useful and relevant for teachers. The integration of research related to both learning design and learning analytics provides the necessary contextual overlay to better understand observed student behaviour and provide the necessary pedagogical recommendations where learning behavior deviates from pedagogical intention. 
 Acknowledgments. 
 The authors wishes to acknowledge collaborators in Learning Design research, Dr Shirley Agostinho, Associate Professor Sue Bennett, and Emeritus Professor Barry Harper.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Learning Designs and Learning Analytics</rdfs:label>
		<dc:subject>learning design</dc:subject>
		<dc:subject>pedagogical models</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>social network analysis</dc:subject>
		<dc:subject>university teaching</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/lori-lockyer"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/lori-lockyer"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/62/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/lori-lockyer"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/63">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>AAT – A Tool for Accessing and Analysing Students' Behaviour Data in Learning Systems*</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/63/authorlist"/>
		<swrc:abstract>In online learning environments, teachers and course designers often get little feedback about how students actually interact with and learn in online courses. Most of the learning systems used by educational institutions store comprehensive log data associated with students’ behaviours and actions. However, these systems typically reveal or report on very general and limited information based on this data. In order to provide teachers and course designers with more detailed and meaningful information about students’ behaviour and their use of learning resources within online courses, an analytics tool has been developed. The tool incorporates functionality to access and analyse data related to students’ behaviours in learning systems. This tool can provide valuable information about students’ learning processes allowing the identification of difficult or inappropriate learning material, and can therefore significantly contribute to the design of improved student support activities and resources.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction.
 Educational institutions need ways of responding to internal and external pressures for accountability. Designers of instruction need feedback about how successfully the teaching materials and learning activities support student success. Academic analytics is a relatively recent response to both these requirements in higher education. According to EDUCAUSE, “analytics marries large data sets, statistical techniques, and predictive modeling” [1] to better understand the wealth of data produced by interactions and transactions in organizational systems with a view to informing action. Campbell et al. [1] describe how the techniques and tools of early institutionallevel analytics efforts in administrative areas such as enrolment management and fundraising have evolved to include analyses of factors that support student learning and success. Administrative systems, registration systems and learning management systems (LMS) together provide large amounts of data that can be combined to provide understanding of student engagement and performance. 
 * The authors acknowledge the support of the Knowledge Infrastructure Program through the Open Knowledge Environment Project funding. 
 Advanced data analysis skills, integrated information systems and multifunctional collaborative teams are necessary to extract and interpret the evidence for decision making in various academic areas. For example, LMS data reveal student effort measures through participation in discussions, time on task, quiz results, and log files that register click patterns. These and other data can be combined to inform the development of a variety of interventions – from simple early alert systems to customized learning environments to personal learning plans. Some of the factors for success of academic analytic projects include careful attention to ethical and privacy issues, stewardship for systemic implementation plans, attention to faculty perspectives, broad-based collaboration and information sharing, and adequate resources and skills. Universities that have reported on their academic analytic projects include McGill University in Canada [2], University of Fairfax in the United States [3], and the University of Cordoba in Spain [4]. In 2003, a cross-functional team reporting to McGill’s Chief Information Officer used the data from one LMS to understand student and faculty preferences in order to establish criteria for the choice of a replacement LMS [2]. In 2009, a team in the Office of the Dean of Academic Affairs at Fairfax used the community of inquiry model [5] to interpret patterns of faculty interactions with students that were extracted from the enterprise course management system with goals of assessing the relationships between student satisfaction and instructor involvement, and of preparing for a long-term trend analysis project. In 2008, researchers at Cordoba reported on their data mining project in Moodle [6], describing the “emerging discipline” as a way of combining complex student usage data and applying the results to elearning problems such as assessing students’ performance, adapting courses based on learning behaviours, evaluating learning materials and courses, providing feedback to instructors, administrators and students, and identifying at-risk students [4]. They described the use of statistics, visualization, clustering, classification, and association rule mining in an iterative “continuous empirical evaluation approach” to course development for online learning. Further, Morris and Finegan [7] suggested using tools available to track student behaviour, collecting aggregate data over time to understand “course norms” for success and early indicators for failure to persist. They described efforts to predict student retention through analysis of relationships among academic and demographic data types (achievement, locus of control, financial assistance, attitude, motivation), and identified other relevant success factors such as instructor presence and multiple roles, feedback, explicit learning support, and technical assistance available. These projects and others are also informed by the notion of formative evaluation of courses, which is central to instructional design theories and models [8]. Formative evaluation, a methodology for improving the effectiveness and efficiency of instructional resources [9, 10], is a key phase in most instructional systems design models, which are typically used in the design of online learning environments [9]. Reigeluth and Carr-Chellman [11] proposed that in the information age, design of learner-centred instructional strategies for student success depends on information about learning behaviours and activities; the gathering of this information is facilitated by the technology of the LMS. Baker and Herman [12] provided distance learning guidelines that emphasize adaptation of materials based on students’ needs. 
 Activities that enhance motivation and social support lead to student engagement and help them sustain their commitment and persistence. Thus, formative evaluation leads to data-driven design decisions about suggested improvements, whether directly from students to the developer or through system data. They recommended focussing on specific elements of the learning environment to identify weaknesses and recommend revisions. Building on both lines of thinking above, Athabasca University (AU) – Canada’s open, distance university – intends to use data from its enterprise LMS (Moodle) to enhance its online courses through learning interventions and other support mechanisms. By tracking student activities in courses and then linking those data to other data sources we will better understand the effectiveness of the learning environment. This understanding will lead to revisions to course materials and structure that should enhance student learning, motivation and/or persistence. The project is building on past work on student persistence and completion rates at AU, and on studies from other universities on the success of interventions to decrease course dropout, student engagement analysis, and learning design improvements. This is one of several institutional projects designed to support AU's capacity for teaching and learning using digital technologies. Called Moodle Analytics, it involves crossfunctional teams of faculty and staff with special expertise in learning design, analytical approaches, institutional research, information technology applications and programming. In this paper, the Academic Analytics Tool (AAT) developed within the Moodle Analytics project is introduced. First, its objectives and the design decisions in developing the tool are explained. Subsequently, the architecture of the tool and its functionalities are presented. The last section summarises the paper and discusses the potential contributions of the tool to improve academic course design. 
 2 Objectives and Design Decisions for the Academic Analytics Tool.
 The Academic Analytics Tool (AAT) is a software application that allows users to access and analyse student behaviour data in learning systems; it enables users to extract detailed information about how students interact with and learn from online courses in a learning system, to analyse the extracted data, and to store the results in a database and/or CSV/HTML files. AAT is primarily developed for learning designers who want to get feedback about how students use and learn in courses, but it can also be used by teachers. In order to use AAT, users need to know the courses they aim at investigating well in order to interpret the results correctly. While several prototype tools exist that extract particular data from a learning system’s database and analyse these data in different contexts [e.g., 13, 14], AAT allows users to decide and specify what data they are interested in and what analysis they want to perform with this data. Furthermore, the data and information that can be extracted and analysed through AAT go far beyond the statistics and activity reports provided in some LMSs, which show limited information predefined by their developers (e.g., information about when a student logged in the last time or accessed a certain activity). Instead, AAT provides comprehensive and customized information to its users, allowing them not only to select from predefined types of information but also to specify what information they are interested in. Furthermore, most LMS statistics and activity reports are only based on the data from individual courses rather than from a set of courses hosted in a learning system. Similarly, most prototype tools aim at analysing data from one particular course. AAT is designed for academic analytics in educational institutions and therefore aims at flexibility with respect to the choice of courses, allowing, for example, the capture and analysis of data from all courses offered by the educational institution, courses of one or more departments/centres, a single course or a purposefully chosen combination of courses. Furthermore, distinctions can be made between the level of courses (i.e., undergraduate courses, graduate courses, 200-level courses, etc.). Another objective and design decision was to develop the tool in a generic way so that it could be applicable for different learning systems. Therefore, AAT can be used independently of the learning system used by the educational institution. Furthermore, most educational institutions use LMSs such as Moodle [6], Sakai [15], and Desire2Learn [16], systems which are frequently updated with new versions released regularly. By making the tool applicable for different learning systems, updates to newer versions of the same learning system can be handled easily. In addition, the tool aims at being easily extendable, for example, with respect to adding sophisticated analysis techniques such as artificial intelligence algorithms, different data sources such as data about students’ demographics, marks, etc., and any other kind of functionality that users require to conduct effective academic analyses. From a technical point of view, the tool is implemented as a web application using PHP as programming language. 
 3 Architecture of AAT.
 The architecture of the tool is based on the architecture of DeLeS [14], a tool for identifying learning styles from the behaviour of students in online courses. While DeLeS also aims at being applicable for different learning systems, several extensions in the architecture have been made for AAT in order to fulfil all the objectives described in the previous section. Figure 1 shows the architecture of AAT. AAT uses input data from one or several databases of a learning system, extracts and analyses the data that are specified by users, and stores these data within the Academic Analytics database or outputs csv/html files with the results. In order to fulfil the objectives described in the previous section, four design elements have been used: a framework of types of learning objects, patterns, templates, and profiles. In the following paragraphs, these elements are described. AAT is based on the assumption that each course consists of learning objects, which are digital resources that students interact with and learn from. Learning objects can be, for example, learning material, forum postings, questions of a quiz, the outline of the course as well as video and audio files. Since AAT mainly focuses on analysing the behaviour of students in relation to such learning objects, the consideration of these learning objects is of particular importance. 
 Fig.

 1. Architecture of AAT. 
 Learning objects have an inherent pedagogical purpose. However, learning objects of the same type can be used for different pedagogical purposes. For example, quizzes can be used for training or testing, and forums can be used for discussions or announcements. An analytics investigation on two learning objects of the same type used for different pedagogical purposes could lead to erroneous interpretations of results. For example, when analysing students’ participation in discussion forums, including forums for announcements would lead to aberrant results. Therefore, a framework of types of learning objects has been introduced that distinguishes between general types of learning objects and pedagogical types of learning objects. General types of learning objects refer to types of learning objects without regard to their pedagogical use (e.g., quiz, forum, resource). Each general type of learning object can be related to one or more pedagogical types of learning objects, which refer to a type of learning object associated with its pedagogical use or educational purpose (e.g., a quiz that is graded and a quiz that can be performed as self-assessment; a forum for announcements and a forum for discussions). By distinguishing between general types and pedagogical types of learning objects, mixing data that are based on learning objects with different pedagogical purposes can be avoided and misinterpretations due to such a mix of data can be prevented. Patterns are based on types of learning objects and specify what data the user is interested in and therefore, what data should be extracted from the database(s). A pattern can be a query that extracts specific data, or a formula supported by a query where the tool performs calculations on extracted data. Patterns can be, for example, the average amount of time each student spent on quizzes, the number of times a discussion forum has been visited by students, etc. Templates aim at making the tool applicable for different learning systems and can be seen as the interface between the tool and the databases. While patterns specify what data should be extracted from a database, templates specify where (i.e. what tables and columns) the respective data resides within the database of a particular learning system, considering the version of the system (e.g., Moodle 2.0). Different templates are developed for different learning systems (and different versions) and are then used for extracting respective data from the database of these learning systems. A profile can be seen as an experiment for extracting and analysing particular information. In a profile, a user specifies which learning system is used (through selecting a template), how to connect to the data (through selecting and setting up database connections), which courses, learning objects and time spans should be investigated (through selecting the data set), and which data the user is interested in (through selecting patterns). AAT guides the user through this specification process. Once the profile is created, it can be used to extract and analyse the specified information. 
 4 Functionalities of AAT. 
 AAT is an easy-to-use and powerful tool that allows users to study student behaviour in online courses. It allows users to execute predefined and customized queries against any learning system that stores its data in an SQL accessible database. Users can also chain together queries to make more sophisticated compound queries. More importantly it allows users to progressively improve the analytical capabilities of the tool with a simple to use graphical user interface (GUI). Figure 2 shows a screenshot of AAT, demonstrating the first step in creating such queries. During the installation of AAT, the administrator specifies database connectivity information and selects a suitable template for the LMS. Based on this information, AAT automatically finds courses and learning objects and makes predefined patterns available to the users. After the installation process, the users can change the selected settings, such as changing the template and adding/removing databases using the GUI. In the following sections, the main functionalities of AAT are explained. 
 Fig. 2. Creating Patterns/Queries in AAT. 
 4.1 Profiles. 
 Users of AAT perform analytical investigations by creating and executing profiles. To create a profile, the user needs to choose a data set (courses and learning objects) and a set of analytics operations to be performed on the data set. Analytics results are generated when a profile is executed. These results can be stored in the Academic Analytics database, displayed on screen, and saved as HTML and/or CSV file. 
 4.2 Choosing a Data Set. 
 Before users run an analytics query, they need to be able to precisely define the data set they wish to analyse. Using a GUI, users can select the data set they are interested in analysing from the identified pool of courses and their associated learning objects. Functionality for selecting groups of courses is also provided. Since online courses are not restricted by time constraints, some universities, such as AU, use a continuous enrolment model. In order to make AAT applicable for courses with semester-based enrolment as well as courses with continuous enrolment, AAT allows users to specify the exact periods of time they wish to analyse. 
 4.3 Choosing Analytics Operations. 
 Once a data set has been specified, users need to define the analytical operations they wish to perform on the data. They can choose from an extensive set of predefined patterns (e.g., overall activities of students in a course, the number of visits of particular types of learning objects, the amount of time spent on particular types of learning objects). Furthermore, users have the option of creating their own custom patterns. The ability to create custom patterns allows users to get answers to questions they need to ask. Multiple patterns can be applied to a data set. Patterns can be chained (i.e., the output of one pattern can be used as input into another pattern). Thus, powerful and complex queries can be constructed incrementally and progressively. The entire process of creating and chaining patterns is performed using a simple GUI, where users can either use an SQL editor that guides them step by step through the process or users can directly input SQL queries. For example, if a user wishes to identify quiz questions that are difficult to answer for students, he/she can build a pattern that extracts data about the average performance of students on questions within quizzes. On top of this pattern, the user can create another pattern that outputs all quiz questions where the average performance of students is lower than, for example, 70%. Using the results of this pattern, the user can create another pattern that investigates the question types (e.g., multiple choice, true/false, matching, etc.) of the questions that were difficult to answer for students and output a distribution of these types. Furthermore, a user can investigate the learning material that is associated with the questions that were difficult to answer and can, for example, create a pattern that looks into the time students spent on this learning material and compare this time with the average time students spent on all learning materials. 
 4.4 Pedagogical Types of Learning Objects. 
 While general types of learning objects (e.g., forum, quiz) are identified automatically by AAT when database connection and learning system information is available, specifying the pedagogical purpose of learning objects requires the user’s intervention. To address the issue of pedagogical purpose, AAT allows users to define pedagogical purposes for each general type of learning object, using user-defined controlled vocabulary. Controlled vocabulary schemes mandate the use of preselected terms which have predefined definitions. Subsequently, users are supported by AAT to annotate learning objects through a semi-automatic approach, using the defined pedagogical purposes. It is up to the user to interpret the meaning of the pedagogical purpose and thus, it is important for the user to consider the meaning of the pedagogical purpose when defining and/or using a pattern in order to perform data extraction and statistical analysis in alignment with that interpretation. Using the example given in the previous section, a user can define two pedagogical purposes for quizzes to distinguish between marked quizzes and self-assessment quizzes. Using only marked quizzes to analyse distributions of question types that are difficult to answer for students and learning material that is associated with these difficult questions, will result in more accurate understandings, since some students might not take self-assessment quizzes as seriously as marked quizzes and may choose to take the quiz before reading the learning material. 
 4.5 Working with Databases. 
 AAT requires read-only access to the database(s) of the learning system it is to analyse. Therefore, the user or administrator must provide database connectivity information. AAT can connect to multiple instances of a learning system’s database. In addition, AAT allows users to perform analytics on data from several different instances of a database (of the same learning system) simultaneously. Since it is not uncommon for universities to distribute course data across several databases, AAT is capable of working with such complex database configurations. 
 4.6 Working with Different Learning Systems. 
 There are many different learning systems available and new versions of learning systems are introduced frequently over time. To make AAT applicable for different learning systems as well as to allow upward compatibility with future versions of learning systems, AAT uses templates to define how to find specific pieces of information from a specific version of a learning system. AAT comes pre-packaged with templates for several learning systems / versions of learning systems. Therefore, a user simply needs to select the right template in order to specify the learning system in AAT. If a template is not available, for example, for a newly released version of a learning system, administrators of the AAT instance, who know the database of the new learning system well, can create new templates. These new templates can then be shared within the community and made available to administrators of AAT systems. 
 4.7 Extending the Tool. 
 AAT has been created in a modular fashion and many design features have been inspired from the content management system Drupal [17]. Administrators have the option of coding new modules to extend functionality of AAT. Writing AAT modules is as easy as writing Drupal modules. 
 4.8 Other Features. 
 In addition to the above-mentioned features, AAT provides strong data security and access control features, support for Smarty templates, embedded help files, a SQL editor, a GUI SQL query generator, history, and backup and recovery features. Furthermore, the designers of the tool have made every effort to make it user-friendly and user-centred without compromising the design principles or its functionality. 
 5 Conclusions. 
 In this paper, the Academic Analytics tool (AAT) is introduced. AAT is a powerful and easy-to-use tool designed to allow users to perform simple and complex analytical queries on students’ behaviour in online courses. In the following paragraphs, the possible benefits this tool can bring to educational institutions are described, discussing the plans of using the tool at Athabasca University (AU). The data that can be retrieved through the use of AAT on how students are currently using the learning objects in their AU courses will be evidence for the formative evaluation of those courses. The data will be analysed as part of our regular course revision process. Combined with students’ evaluations of the courses and professor and tutor recommendations for changes, these data will inform the work of our learning designers, who with subject matter experts will adapt and extend resources that are generating successful learning and revise materials that are of less direct value to students. Engaging learning objects will be shared across disciplines as appropriate, generating interest in new pedagogical approaches within the academy. Once it is possible to integrate data from administrative systems with data from the learning system, we will be well positioned to identify factors affecting student success. The infrastructure to be established will facilitate the extraction and transformation of data required for improved operational reporting across a number of different aspects of the teaching and learning environment at AU. Eventually it will be possible to generate automated interventions to enhance student retention, motivation and/or learning, and to generate customized dashboards for sharing progress information with tutors and students, thereby meeting institutional goals of quality and access. The importance of analysing student activities in Moodle has increased over the last couple of years as we have moved from simple course conversion to complete course re-design for the online environment. The direct value of the results of our analyses can be understood in two ways. We will have data about which learning activities students are completing and which ones they are not. This will build on course evaluation data and help inform improvements to individual courses. It will also allow us to evaluate and revise the standards we are setting for excellence in AU online courses. Future work will deal with conducting a study where learning designers will test AAT with respect to its usability and usefulness. Furthermore, we plan to release AAT as an open source product in order to allow other educational institutions to benefit from AAT as well. In addition, the indirect value of the project will be realized in a methodology for further activities in academic analytics.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>AAT – A Tool for Accessing and Analysing Students' Behaviour Data in Learning Systems*</rdfs:label>
		<dc:subject>academic analytics</dc:subject>
		<dc:subject>learning systems</dc:subject>
		<dc:subject>data extraction and analysis</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sabine-graf"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sabine-graf"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/cindy-ives"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/cindy-ives"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nazim-rahman"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nazim-rahman"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/arnold-ferri"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/arnold-ferri"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/63/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/sabine-graf"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/cindy-ives"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/nazim-rahman"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/arnold-ferri"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/64">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Academic Analytics Landscape at the University of Phoenix</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/64/authorlist"/>
		<swrc:abstract>The University of Phoenix understands that in order to serve its large population of non-traditional students, it needs to rely on data. We have created a strong foundation with an integrated data repository that connects data from all parts of the organization. With this repository in place, we can now undertake a variety of analytics projects. One such project is an attempt to predict a student’s persistence in their program using available data indicators such as schedule, grades, content usage, and demographics.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction to the University of Phoenix.
 The University of Phoenix is a regionally accredited degree-granting institution founded in 1976 by Dr. John Sperling. Based in Phoenix, Arizona and with over 200 locations throughout the United States and a strong online campus, the University of Phoenix is the largest private university in North America. As of May 2010, over 470,000 students were enrolled at the University of Phoenix. 


 1.1 History.
 In 1976, Dr. John Sperling, a Cambridge-educated economic historian and professor, founded University of Phoenix on an innovative idea: making higher education accessible for working adults. In the early 1970s, while a tenured professor at San Jose State University in California, Dr. Sperling and several associates conducted field-based research on new teaching and learning systems for working adult students. From this research, Dr. Sperling realized that the convergence of technological, economic, and demographic forces would herald the return of working adults to higher education. He saw a growing need for institutions that are sensitive to the learning requirements, life situations, and responsibilities of working adults. These beliefs resulted in the creation of University of Phoenix.1 
 1 Taken verbatim from our institution’s website at: http://www.phoenix.edu/about_us/media_relations/just-the-facts.html. For a detailed history of Dr. Sperling and the University of Phoenix, refer to the book Rebel With a Cause (John Wiley & Sons, 2000). 
 1.2 Students Served.
 In the past, the University of Phoenix focused on degree completion for nontraditional adult learners. Over the years, changing demographics have seen students with little to no credits starting at the university with the goal of completing their entire programs. The university also expanded in 2006 when it started offering associate’s degrees in addition to bachelor’s, master’s, and doctoral degrees. The demographics of the non-traditional learners at the University of Phoenix differ from students at traditional post-secondary institutions. Non-traditional students tend to be older, largely female, and tend to come from more diverse socioeconomic and racial backgrounds. Many non-traditional learners also have jobs and family obligations as opposed to 18-22 year-old residential students at a traditional institution. This different demographic changes the motivators and drivers behind the students’ actions. 
 1.3 Structure.
 Key aspects of the university’s organizational structure differentiate it from traditional degree-granting institutions and community colleges. These aspects have a significant effect on determining the direction of technological initiatives. Academically, the university has a central administrative unit. All programmatic and curricular decisions are made by the central Academic Affairs unit and then distributed throughout each of the campus locations. This process holds true for the creation of new programs, the modification of existing programs, and the development of course curricula. Courses are taught by practitioner faculty; the faculty members have experience working in the field that is related to their course. Curriculum is centrally designed by faculty members, content experts, and instructional designers. University of Phoenix faculty members have the academic freedom to enhance the standard curriculum with their expertise, content and theoretical knowledge, and the practical experience they gain as a result of working in the fields in which they teach. Each campus is an independent entity and receives operational support from the centralized organization (curriculum, marketing guidelines, operational requirements). Over two thirds of the students take courses online while the remaining students attend in a face-to-face modality. Because many students take one hundred percent of their courses online, there is a campus dedicated solely to online students. This is the largest of the University of Phoenix campuses and it is also based in Phoenix, Arizona. Students enrolled at physical campuses can choose to take their courses in local classrooms or they can attend online classes as long as the programmatic requirements are met. One last point is that the university’s academic calendar does not follow a semester, trimester, or quarter model. For the most part, courses are taken serially throughout a student’s entire program. At the associate level, students take two complementary nine-week courses concurrently. At the undergraduate and graduate levels, students take one five-week or six-week course in serial (respectively). A student may continue to take one course after another until the program is complete, or a student might choose to take time off between courses if there are scheduling conflicts due to personal or work issues. 
 1.4 Unique Needs of the Institution.
 Before diving into the details of how data are utilized by the University of Phoenix, it is important to emphasize how the structure of the organization lends itself toward a set of needs that differ from traditional universities. One common thread throughout the institution is scale. While the university maintains a class size in the mid-teens, there are hundreds of courses in progress affecting thousands of students on any given day. Students have scheduling flexibility because there is a good chance that the desired course is starting on any week. We need to think in terms of tens of thousands for course design, content, facilitation, tutoring, licensing, data collection, or any other variable function (or terabytes when it comes to data). Another aspect of our institution is operational efficiency. The sheer size of the business combined with the number of campuses and the centralized administration means services must be simple and as efficient as possible. This is common when it comes to areas such as applications and registrar functions. However, it must also be viewed from an academic perspective. A good example is in the licensing of software. Some academic software products require an instructor to upload a course roster to allow students to log into the product. Due to the flexibility of University of Phoenix scheduling, course rosters might, and frequently do, change right up to the time class starts. Instructors do not usually have the roster information in advance. In this case, administrators work with the content partners to provide a single sign-on feature that allows students to automatically login from our learning management system to the vendor system. 
 2 Our Approach to Data.
 As an institution, we are very aware of the fact that data will make or break our ability to educate our students effectively in the future. Although we have the advantage of a proprietary online learning system, we realize that we have not come close to tapping the potential of the data stored in our systems. To that end, we have spent a significant amount of time and effort over the past few years to ensure that data has its place in the foundation of the organization. 
 2.1 The Move Toward a Data Driven Culture.
 While it might sound trite, it is vital that any change starts with the people who make up the organization. We started a concerted effort to stress the importance of data at every rung of the organizational ladder. 
 One basic step we took involved messaging. After a restructuring of the product and engineering groups in 2009, our new management focused on three areas of performance: • Site Up • Data • User Experience The fact that data was one of the three focal areas of the group was a testament to our commitment to a data-driven culture. We followed up on the messaging with key hires in the data arena. This included bolstering our technical capacity and bringing on board staff with experience in analytics, cognitive science, and data-driven consumer products businesses. The investment in staff who can move, align, and interpret data is something that will pay dividends in the future. 
 2.2 Applications of the Data.
 Data are different from information. Data are atomic units; they set the foundation for capabilities that can have a deep impact on learning and business. We ask ourselves, “What can we do with the data once this foundation is set?” The following diagram illustrates our answer to that question: 
 Fig. 1. Pyramid showing the different applications of our data foundation 
 Reporting and business intelligence is the base of the pyramid. Although commonplace, we do not want to underestimate the impact of basic business intelligence. A good, integrated data structure can provide simple answers to many questions. Predictive analytics goes to the next level. It allows us to answer the tougher business questions and use data to look ahead. Data-driven learning is where we apply the data not only to business/operational questions but to the core activity of our institution - learning. These areas will be discussed in more detail in Section 4. 
 2.3 Data as a Strategic Advantage.
 The University of Phoenix is in a unique position as compared to traditional universities and colleges. Because we are a for-profit entity, we need to address business and financial implications in addition to the implications of learning. One of the largest advantages we have in the higher education space is our size. With over 400,000 students, we have the ability to use data and analytics that would produce accurate and reproducible results. We are not limited to testing a new learning tool on a class of 25 students. We can test with hundreds or thousands of students, so long as the trials do not negatively impact the students’ ability to learn. From a data standpoint, that means that we have more than enough data points to support the efficacy of the tool. 
 3 Topology.
 It should not be surprising that we have data strewn in different databases across the entire student lifecycle. This scattering of data reduces the efficacy of our analytic capabilities. To combat that, we set up a replication system where all data flows into a single integrated repository (see Figure 2). 
 Fig. 2. The flow of data to our integrated data repository.
 3.1 Source Systems.
 The first step in the workflow is to replicate all source systems. We use a commercial replication tool called Golden Gate to accomplish this (Golden Gate was acquired by Oracle in 2009). Golden Gate is used on any of the source systems we want to move to the integrated repository – these include both Oracle and SQL databases. One of the benefits of the replication is that we alleviate the problem of destructive data. Normally, if a field in one of the source systems is overwritten, we lose the older data forever. With replication, we effective-date the tables so that any overwrites are saved. This helps with older systems that inadvertently destroy data due to a poor/outdated design. The table below is a listing of some of the source applications that we replicate to our staging systems. It is not an exhaustive list. 
 Table 1. A partial listing of source data systems replicated to a single staging area. 
 3.2 Integrated Data Repository.
 The key to our data foundation is the integrated data repository (IDR). The IDR is a unified, normalized data structure of all data elements across the entire company. Replication copies each source table to a staging area. The next step is to travel from staging to the IDR. To do this, we needed to rationalize every field that we moved over. As an example, we looked at the student’s home zip code. We may have collected that zip code when the student first contacted the university, we may have the collected it on an application, and the campus may also have collected it in the course registration process. We now have three instances of the zip code in our systems and regardless of whether they are the same or not, a student should only have one current zip code on file. This is where the data modeling comes into play 
 Modeling. We have a team of data modelers who work to create a normalized physical model of all of the data elements. In our zip code example, the first thing the modelers do is create the proper data schema. Instead of having a Marketing_Person, Application_Person, and Registrar_Person table, we only have a single Person table. The next step is to determine which source table has the right zip code. We may determine that the zip code stored in the registrar’s database is the proper one to use, so we designate that field for transport. 
 Extract, Transform, Load (ETL). After the entire integrated schema is mapped out, the next step is to populate it. The ETL team writes jobs to move the data from staging into the proper place in the IDR. A significant part of the ETL process is quality control. As data are moved over, we check for inconsistencies and errors and do what we can to address them. The IDR is known as the single source of truth, so consistency and quality are vital characteristics. 
 Data Marts. One other facet of the architecture is a data mart. The IDR is large and normalized; this is not a good combination for efficient querying. In order to have a data structure that is built for fast querying of complex data, we need to de-normalize and index the data. There may be multiple data marts in existence. One may be a series of tables dealing with students’ progression throughout their programs, while another may focus on financials and accounting. 
 3.3 Hadoop.
 Due to the size of our institution, we know that we would be running into issues with the sheer volume of data in some of the tables. For example, the discussion forums databases have a record for each post for every student and faculty in every class. If you multiply the posts by students (and faculty) by week and by course, we can see millions of records in a week or even a day. Mining tables of this size in an efficient manner calls for a different solution. The solution we have adopted is an open source product called Hadoop. Hadoop was inspired by work at Google and extended through usage at Yahoo!, Facebook, and other prominent Internet companies. Hadoop addresses the problem of large datasets by using distributed parallel processing. A Hadoop cluster is made up of many commodity server nodes - the benefit is to use a large number of cheap servers instead of a small number of expensive ones. The University of Phoenix product group built a 40-node cluster in 2009, and we are continuing to develop its capabilities. Hadoop is used to solve specific problems with our data. It is most applicable in two cases. First, it helps to digest large datasets. Whether it is the discussion forum tables or raw web usage logs, Hadoop can process the data, create summarized tables, and send the summaries back to the IDR. Second, Hadoop can help with detailed data analysis on non-fielded data. The actual discussion forum posting is a block of text. In a traditional database, that block of text is lumped into one field and that makes it hard to mine the text beyond the use of simple query statements or regular expressions. With Hadoop, we can run cycles of queries or code against the nonfielded data, continually reducing the problem into smaller chunks. When we have derived the information we are looking for, we can write that summarized information back to the IDR for use with traditional queries. 
 3.4 Analytics Tools.
 The ‘last mile’ of analytics includes any data reporting, analysis, or visualization tool used to turn data into information. Following the mantra of ‘using the right tool for the right job’, there are a number of tools being used within our institution. A tool like Microsoft Excel is always available as a failsafe, but we rely on other products for more specialized needs. 
 Tableau. Tableau is a commercial data visualization tool whose strength lies in its ability to figuratively paint many different kinds of pictures. Unlike traditional visualization tools where one might start by selecting the desired type of visualization (e.g. scatter plot, bar graph), Tableau lets the user start by just adding measures and dimensions to a palette. As the user adds fields, one can either try different visualizations to see how it looks or use suggested types from Tableau. The product helps the user paint the picture that will tell the desired story in the best way possible. 
 R. If the need is to perform statistical calculations or correlations, R is the right tool for the job. It is a powerful open source software product that can complete a myriad of statistical functions. 
 PL/SQL. Many times, the need is to simply explore the data in order to zero in on whatever answer, question, or anomaly one is looking for. Our IDR is an Oracle database and a simple SQL querying tool such as PL/SQL Developer will often be the right tool for the job. 
 4 Analytics Applications.
 The data foundation described in this overview is just that – a foundation. In and of itself, it has no value. One must apply the data towards an end goal such as answering a business question. The pyramid in Figure 1 shows three levels by which we can categorize the application of data across our organization. 
 4.1 Business Intelligence (BI).
 The BI team works like many traditional reporting teams. The goal is to provide reporting services to the areas of the company where it is needed. The kinds of services provided depend on the needs and capabilities of the requestor. At its most basic level, we have the reporting tool. We use the commercial Business Objects tool to provide reporting to all areas of the business. The departments might author reports on their own or they may put in a request and have a central reporting team develop the report on their behalf. Another variation on reporting is dashboards. Our development group can create simplified visual dashboards that answer a few key business questions in an easy-to- understand manner. If reports are good for departments that need to make operational decisions, dashboards might be better for high-level overviews of a business process. 
 4.2 Predictive Analytics.
 The analytics team at the University of Phoenix is set up to focus in on the more difficult questions that cannot be easily answered with a single report. We just changed the curriculum in a certain course…is it a change for the better? Is one campus location doing a better/worse job than another in its ability to deliver instruction? How many weeks does it take for an MBA student to get to their fifth course? These are complex questions that require complex analysis. There is more than just answering the question, though. We want to be able to use data to predict future outcomes so we can stay ahead of the impending trends. Predictive analytics can be used to predict different outcomes including student learning, student success, marketing channel efficacy, or financial outcomes. 
 Student Persistence. The University of Phoenix is currently looking at one specific predictive channel focused on persistence in a program. The approach is similar to an actuarial table, but instead of predicting how long an insured person will live, we want to look at how long a student will progress through their program. As an institution serving non-traditional learners with competing factors like a job or a family, we know that external factors can hinder a student’s ability to stay in the program. We may not be able to avoid these factors, but if we see signs of them coming, we can help the student handle the change in a more productive manner. The goal with student persistence is to include as many factors as possible in a correlation model. By analyzing past data, we might be able to determine what factors indicate a high probability that the student is preparing to withdraw from the program. The IDR contains static information such as demographics and active information such as schedules and grades. It is our belief that some of these factors will have a high correlation with a student’s intention to withdraw. Therefore, we will be able to rate the withdrawal risk and give some sort of a persistence score. The obvious next question is, “So what do we do with this information?” If we are able to predict persistence/withdrawal with some level of accuracy, then we can proactively help the students with their decision-making. All students have an academic advisor who has the job of assisting the student throughout their program. It is our intent to provide the advisors with up-to-date persistence scores so that the advisors can intervene and help the student find the best course of action. We do not know if the best course of action is remediation, taking a break in scheduling, or some other solution. To that end, our intent is to focus on human intervention (with the advisors) instead of some automated remediation path. The student persistence analytics project started in October 2010 and we hope to share results as the project matures. 
 4.3 Adaptive Learning Engine.
 The top level of the data application pyramid is an adaptive learning engine. This is a longer-term project aimed at the heart of our institution. Our goal as a university is to help students achieve the learning outcomes as specified by the program. A project such as student persistence might help to keep the student enrolled in the program, but it does not address the learning. We have the desire to leverage all of the student data we have to help students traverse that optimal learning path. Through a combination of data analysis, learner profiling, and a learning platform that supports multiple paths to achieving the same outcome, we believe we can guide students down the path that best fits their individual needs as a learner. 
 5 Conclusion – the Current State of Analytics.
 It has taken the University of Phoenix many months to establish and populate the IDR, our foundation for analytics. As of this writing, the IDR is still not complete and new tables are continually being added. However, we are not waiting for it to be one hundred percent complete. There is enough data to initiate reporting and analytics projects that can both provide value to the company and set the stage for future research. We are fortunate to be able to dedicate multiple teams to different aspects of the analytics and we will continue to share outcomes with communities such as the Learning Analytics and Knowledge group as results become available.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Academic Analytics Landscape at the University of Phoenix</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>academic data</dc:subject>
		<dc:subject>integrated data</dc:subject>
		<dc:subject>data modeling</dc:subject>
		<dc:subject>predictive analytics</dc:subject>
		<dc:subject>hadoop</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mike-sharkey"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mike-sharkey"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/64/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/mike-sharkey"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/65">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>The Value of Learning Analytics to Networked Learning on a Personal Learning Environment</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/65/authorlist"/>
		<swrc:abstract>Some might argue that the analytics tools at our disposal are currently mainly used for boring purposes, such as improving processes and making money. In this paper we will try to define learning analytics and their purpose for learning and education. We will ponder on the best possible fit of particular types of research methods and their analysis. Methodological concerns related to the analysis of Big Data collected on online networks as well as ethical and privacy concerns will also be highlighted and a case study of the use of learning analytics in a Massive Open Online Course explored.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 The Internet and its recent tools and Web developments have added new research and evaluation tools to the arsenal of the educational researcher [2][3]. As educational practice and the settings in which learning takes place have changed with the proliferation of the Internet and its available tools, careful thought about these tools and considerations of the processes and means with which data is being collected and analyzed is once again required [4]. Lazer et al. [4] stress that social scientists have lagged behind researchers in other fields, for instance in fields such as biology and physics, and that it is unavoidable for analytics to become part of social science research. Moreover, they emphasise the urgency for a data-driven computational social science to develop "based in an open academic environment", rather than in the domain of private companies such as Google and Yahoo, and government agencies who are currently the main players in the analytics field. They answer the question: “What value might a computational social science – based in an open academic environment – offer society, by enhancing understanding of individuals and collectives?” [4, p.721]. We would like to add to this question one in the context of this paper: what would it offer stakeholders in the evaluation and improvement of the learning process: educators, researchers, administrators and learners themselves? In this paper we will illustrate the research methods used in exploring networked learning on a Massive Open Online Course. 
 Defining Learning Analytics.
 Analytics of web environments have been around for a while. The first reports we could find were from the mid 90s, and relate to the analysis of market trends using web logs and browser tags online [5]. Boyd highlights the "Big Data" development on the Internet, which “has created unprecedented opportunities for people to produce and share data, interact with and remix data, aggregate and organize data . . .” [3, p1]. Educause [6] highlights features of analytics tools: "provide statistical evaluation of rich data sources to discern patterns that can help individuals at companies, educational institutions, or governments make more informed decisions." There is a clear contradiction in this sentence: statistical evaluation of rich data sources. It seems that statistical evaluation is the perfect tool not for "rich" data sources, but for a "multitude" of data sources. Some researchers in the qualitative tradition might argue that "rich" data sources would be better analyzed through qualitative methods as these would be better at capturing the depth and richness than statistical analysis could do. Norris et al. [7] have a slightly different emphasis on the use of analytics; they would like analytics to be used to measure, compare and improve the performance of individuals, not just to better the experience but also to facilitate better outcomes to the activity. In the more specific context of education and learning some interesting distinctions in ideas and definitions on analytics have been proposed. Most analytics are related to the introduction of Learning Management Systems (LMSs) and are sometimes called Academic Analytics[8][9][10]. With the introduction of LMS came the back office functionality that would provide traces of participant" activities on the system and this data was then used to aid the management and effectiveness of institutional teaching and learning. Dawson et al. [9] added that the analysis of this data might be used to improve the student learning experience, which would not only require a quantitative analysis, but also a qualitative one, or at least a qualitative interpretation of findings. The interpretation would have to include a value judgment on people"s use of the environment: not only counting who uses the environment for what, but also judging what might be a good and what might be a bad experience, and offering suggestions for moving on the continuum from one to the other. The conference organizers provided us with their definition of learning analytics, which seems to express most of the above: “Learning analytics is the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs.” 
 Different Types of Analysis for Different Purposes.
 It follows that not only quantitative data should be used, but also qualitative as data collection would not only relate to the increase of the effectiveness of learning, for instance by showing trends in use, but also with possible changes in the learning process. As highlighted by Downes [10]: “There are different tools for measuring learning engagement, and most of them are quantificational. The obvious ones [measure] page access, time-on-task, successful submission of question results – things like that. Those are suitable for a basic level of assessment. You can tell whether students are actually doing something. That’s important in certain circumstances. But to think that constitutes analytics in any meaningful sense would be a gross oversimplification. There is a whole set of approaches having to do with content analysis. The idea is to look at contributions in discussion forums, and to analyze the kind of contribution. Was it descriptive? Was it on-topic? Was it evaluative? Did it pose a question?” [10] Parry [11] and Kop [12] highlight possibilities to take this one step further, and suggest that analytics could be used not only to provide managers of learning and possibly educators and learners with information that they can use to improve learning, but also to provide learners with recommendations in their learning based on earlier learning activity. This would, however, require that data is used in a different way than in academic analytics. It would entail that data is not just analyzed with steps being taken by people to improve the performance in formal education but rather that technological means are being used to link data and use it to improve learning. This would change the realm of analytics and move it outside academic management and perhaps back onto the open wider Web, as people don"t only learn in a formal academic environment but also outside it. We are most interested in analytics on online networks as our research interest is in Personal Learning Environments (PLEs). 
 Methods Used in Learning Analytics of Online Networks.
 Research in the intricacies of learning taking place on online networks is one of the means for our research into the design and development of a PLE. If people are encouraged to move away from the institution for their learning, it is important to find out if the informal (online) networks in which people do find their information and where they might develop understandings, are valuable to their learning experience. A network in the context of this paper would be an open online "space" where people meet, as nodes on networks, while communicating with others and while using blogs, wikis, audio-visuals and other information streams and resources. De Laat [13] highlighted the complexity of researching networked learning and emphasized as key problems the issues of human agency and the multitude of issues involved, such as the dynamics of the network, power-relations on the network, and the amount of content generated. Effective analysis would require a multi-method approach. He suggested the use of computer-generated content analysis to explore what people are discussing. In addition, interviews with an emphasis on critical event recall focusing on the experiences of participants to find out "why they are talking as they do" and Social Network Analysis to find out the dynamics of the network to see "who communicates with whom" [13, p. 110]. This seems a viable choice of research methods. Social Network Analysis would be a form of learning analytics, and a quantitative method, and could clarify who the central nodes on the network were, in other words which people on the network performed vital roles of connecting to the otherwise un-connected. It could also provide information on the importance of “connectors” to other networks, which would be important in finding out who the innovators on the network were, i.e. the ones to link vital information streams [14]. We would argue for the use of additional qualitative methods and that virtual ethnography would be the most appropriate method of qualitative research on learning networks. Researchers in this tradition work towards research data analysis that reflects as closely as possible what is happening in the chosen setting. The researcher is interested in the processes taking place, the perspectives and understandings of the people in the setting, the “details, context, emotion and the webs of social relationships that join persons to one another” [15, p. 55]. Hine [16] highlighted that in a technologically rich environment, such as the Internet, the technology itself and the artefacts it produces should be taken into consideration in the "online" ethnography as well, as these are part of the research setting and might influence the human interactions researched. As vast amounts of data are being generated in networked learning in an open environment, computational tools for analysis and interpretation will have to play a role in the research. Some argue for a mixed-method approach in educational research as “the theories we hold, and the training we have received, critically affect the data we collect and the lenses we choose in looking at such data" [17, p. 30]. They argue that the use of more than one method in research will increase its robustness [15]. Boyd [3, p.2-5], a social scientist researching "Big Data" highlights some other methodological concerns especially when analyzing Big Data collected on online networks: 1) bigger data is not always better data than obtained in other research as reliability will very much depend on the sampling strategies being used; 2) caution needs to be taken as not all data are created equally; 3) what people do is of limited importance unless you also ask people why they did what they did; 4) she argues that qualitative researchers are not the only ones interpreting data, that also quantitative researchers do this; dispelling the myth that "it is qualitative researchers [who] are in the business of interpreting stories and quantitative researchers [who] are in the business of producing facts’. Interpretation as part of analysis is the hardest of any data analysis, big or small. Boyd [3] would like to see computer experts working together with social scientists to avoid fallacies in interpretations. 
 Researching a MOOC: Analytics, Data mining or Qualitative Analysis.
 Background of the research.
 The research in this paper was carried out during The Personal Learning Environments Networks and Knowledge (PLENK2010 – http://connect.downes.ca) course in the fall of 2010. It was a free Massive Open Online course which lasted for 10 weeks. In total, 1641 participants were registered. The course was a joint venture between the National Research Council of Canada"s (NRC) Institute for Information Technology, Learning and collaborative Technologies Group, PLE Project, The Technology Enhanced Knowledge Research Institute (TEKRI) at Athabasca University, and the University of Prince Edward Island. Four facilitators, highly visible and knowledgeable in the field of study, were active on the course and would find resources, speakers and participate in all aspects of the course. PLENK2010 did not consist of a body of content and was not conducted in a single place or environment. It was distributed across the web. This type of learning event is called a "connectivist" course and is based on four major types of activity: 1) Aggregation: access to a wide variety of resources to read, watch or play, along with a newsletter called "The Daily", which highlighted some of this content; 2) Remixing: after reading, watching or listening to some content, it was possible to keep track of that somewhere-i.e., by creating a blog, an account with del.icio.us and creating a new entry, taking part in a Moodle discussion, or using any service on the internet – Flickr, Second Life, Yahoo Groups, Facebook, YouTube, iGoogle, NetVibes; 3) Repurposing: participants were encouraged to create something of their own. In the PLENK2010 the facilitators suggested and described tools that participants could use to create their own content. The job of the participants was to use the tools and just practice with them. Facilitators demonstrated, gave examples, used the tools themselves, and talked about them in depth. It was envisaged that with practice participants would become accomplished creators and critics of ideas and knowledge; and 4) Feed Forward: participants were encouraged to share their work with other people in the course, and with the world at large. Participants were able to work completely in private, not showing anything to anybody if they wished to do so. Facilitators emphasized that sharing would always be the participant"s choice. In addition, a tag would be used to identify anything that was created in relation to the course, using the course tag #PLENK2010. That is how content related to the course was recognized, aggregated, and displayed in "The Daily" newsletter for the course. If participants decided to use a tool such as Blogger, Flickr, or a discussion group they were asked to share the RSS feed. A separate post on how to produce and include their own RSS feed to the Daily was offered for those who did not know how to do this. All postings to a blog or forum would apply the #PLENK2010 tag. That is how information was recognized as being related to this particular course. When a connectivist course is working really well, one can see a great cycle of content and creativity begin to feed on itself, people in the course reading, collecting, creating and sharing. 
 Research methods and tools used.
 The NRC research team decided to use a mixed methods approach and a variety of research techniques and analysis tools to capture the diverse activities and the learning experiences of participants on PLENK2010. Learning analytics tools were used as a quantitative form of Social Network Analysis to clarify activity and relationships between nodes on the PLENK network. Three surveys were carried out at the end of the course and after it had finished to capture learning experiences during the course: End survey (N=62); "Active producers" survey (N= 31); "Lurkers" survey (N=74). In addition, qualitative methods in the form of virtual ethnography have been used. A researcher was an observer during the course, collecting qualitative data through observation of activities and engagement and also carried out a focus group in the final week of the course to gain a deeper understanding of particular issues related to the active participation of learners. As vast amounts of discursive data were generated and collected, analysis and computational tools have been used to represent large networks of activity in the PLENK, to identify themes in the data and for analysis and interpretation of the qualitative research data (e.g., SNAPP, Pajek, NetDraw and Nvivo). For the data analysis on the course the Moodle data mining functionality was used and provided participant details, their level of use and access of resources, information on course activities, and discussions taking place in the course forums. The gRSShopper aggregator statistics functionality provided details on course-related use of blogs and micro-blogging tools such as Twitter. Some analytics and visualization tools, such as the Social Networks Adapting Pedagogical Practice (SNAPP) tool, were also used to deliver real-time social network visualizations of Moodle discussion forum activity, while the visualization tool NetDraw was used to create an ego network to provide an understanding of the role of a particular actor in a discussion. 
 Findings.
 The professional background of participants on the PLENK course, were mainly employed in education, research and design, and development of learning opportunities and environments. They were teachers, researchers, managers, mentors, engineers, facilitators, trainers, and university professors. 
 Analyzing and visualizing participation on the course.
 When the course started 846 had registered, which steadily increased to 1641 at the end of the course, as shown in Figure

 1. People took part in the twice weekly meeting sessions that were hosted on Elluminate, once a week with an invited speaker and once as a discussion session amongst the group and facilitator(s). Actual presence at these synchronous sessions decreased over the weeks from 97 people in week two, when attendance was the highest, to 40 in the final week and there was a similar trend in the access of the recordings. Global participation and multiple time zones influenced who were present and who accessed the Elluminate recordings. A high number of blog posts were generated related to the course (949) and an even higher number of Twitter contributions (3459) as a means of connection participants inside and outside the course (see Figure 2). 
 Figure 1. Participation during PLENK. 
 Figure 2. Twitter activity in PLENK. 
 The #PLENK2010 identifier facilitated the easy aggregation of blog posts, del.icio.us links and Twitter messages produced by participants, which highlighted a wide number of resources and links back to participant"s blogs and discussion forums, and thus connecting different areas of the course. Although the number of course registrations was high, an examination of contributions across weeks (i.e., Moodle discussions, blogs, Twitter posts marked with #PLENK2010 course tag, and participation in live Elluminate sessions) suggested that about 4060 individuals on average contributed actively to the course on a regular basis by producing blog posts and discussion posts, while other"s visible participation rate was much lower. A total of 652 participants used Twitter and were linked to other #tag networks as suggested in Figure 3. 
 Figure 3. Twitter networks that participants were also linked to with #tags. 
 In the Moodle Forums for PLENK2010, general trends in posting behaviors indicate that there was a peak in activity in Week 2 in Moodle forums, with a slight upward trend in Blog and Twitter posts as well (Figure 3). This was followed by a sharp decline in the number of posts in all three mediums (Moodle, Blogs, and Twitter) in Week 3, a slight increase in Week 4, and a steady decline again in Weeks 5 and 6. Interestingly, the number of posts by course facilitators follows similar trends (Figure 4), with the number of posts by facilitators peaking in Week 2, then showing a steady decline in Weeks 5 and 6. The facilitator(s) played an important role in triggering discussion, questioning, providing feedback, and sustaining interaction amongst participants. 
 Figure 4. Postings across six weeks of PLENK. 
 Figure 5. Facilitator posts in PLENK. 
 Social Networks Adapting Pedagogical Practice (SNAPP) uses information on who posted and replied to whom, and what major discussions were about, and how expansive they were, to analyze the interactions of a forum and display it in a Social Network Diagram. Figures 5 and 6 provide a visual depiction of all interactions occurring among students and facilitators in PLENK2010 for Week 1-Discussion on PLE/PLNs. The social network diagram provides an aggregate visual representation of the connections that occurred between 69 participants for this particular discussion and is an aggregate visual representation of the interactions among participants but is not very comprehensive in describing the nature of the interaction (i.e., the quality). 
 Figure 5. Social network and connections between participants. 
 Figure 6. Relationships between main topic discussion in Week1. 
 Some people with experience in learning on a MOOC were very active and involved in the course, producing a Google map for PLENK participant place of residence, another created a concept map to represent their PLE, while others produced Wordles to visualize the content of a paper. Not all participants contributed in a visibly active way. A high number of people accessed resources but were not engaged in producing blog posts, videos or other digital artifacts; they seemed to be consumers rather than producers on the course. Only a small percentage of participants engaged in the production of digital artifacts. Between 40 and 60 were active producers, the other 1580 were not visibly active. This was unexpected to the course organizers as before the start they saw the production phase as vital to the learning on a networked environment. After all, as some participants mentioned in the discussion, if nobody is an active producer, it limits the resources that all participants can use to develop their ideas, to discuss, think, and be inspired by in their learning. Analyzing the word frequency in the Moodle discussion forum for Week 1 using Nvivo 9 highlighted the importance of “personal agency” in maintaining engagement, participation, and interaction with others. Keywords such as “learning”, “me”, “network”, “question”, and “exploration” in Week 1 discussions were focused on PLE/PLNs concept map activities. Connections between participants were made which supported the learning process as interactions generated many resources, including 49 links and 17 suggestions for useful tools. The use of the #PLENK2010 hash tag made it possible to aggregate blog post and Twitter messages and visualize and organize them into the Daily newsletter and a Twitter newsletter. The end of survey results confirm that although the Daily newsletter and Moodle helped 45% of participants understand the course content, learners needed a common space to create artifacts and connect back to their blog, such as Amplify for example, for social sharing and bringing together various media and resources. One participant commented in the survey: “I would have liked to see a thread each week called, How Can We Help You allowing the community to answer many of the questions and offer more support, mentoring, and evaluations”. 
 Discussion and Conclusions.
 This paper has highlighted some of the possible uses of current analytics tools in providing useful information to participants and facilitators about their participation and social connections within the Massive Open Online Course and outside it, but also the limitations of visualization, knowledge representation, and virtualization in providing meaningful information about learning. The mixed method approach used has highlighted the effectiveness of combining both quantitative and qualitative methods to achieve breadth and depth of data analyses. Quantitative analyses have exposed a basic level of assessment and reporting on learner activity, on whether participants are actually doing something, in this case either inside the Moodle environment and corresponding activity outside the environment including Blog and Twitter activity being tracked with the #PLENK2010 course tag. Qualitative tools and approaches (e.g., SNAPP, Nvivo, NetDraw) demonstrated how deep exploration of content can reveal the types of contributions made, as well as the knowledge, ideas, thinking, information, tools, and experience that promote learning along the way. But still, the need for human analysis and interpretation has also become apparent. From a research point of view, the time and efforts needed to conduct various analyses on two forum discussions was prohibitive, but yielded a detailed view of what actually occurred in one discussion, including the processes, the learning, and important outcomes. The use of tags in the Moodle environment would have been helpful in linking various contents across weeks, allowing participants to search for relevant content and to see how they were connected to various content and people with similar interests. Analytics can be applied to structure mining (link information), content mining (including text, images, audio files), usage mining and transaction data. Structure mining is often more valuable when it is combined with content mining of some kind to interpret the hyperlinks" contents. Vast amounts of data were being generated in this example of networked learning in an open environment, so much so that facilitators, participants, and researchers could not possibly attend to all the details but needed to focus on the most relevant information efficiently and effectively in order to encourage better outcomes of activity. Intelligent and automatic data analysis with powerful computational tools for analysis and interpretation should be explored as a valid option for informing learning in MOOC in a connectivist-type course. One of the limitations of the analytics approach, however, was the narrow scope of the analyses, as focused on one snapshot of the MOOC experience for Week 1 and 2 related to the topic of PLE/PLNs. It did expose what was occuring within the course when there was a peak in activity and when data was plentiful but did not expose gaps in the data that capture experiences that were lacking or nonexistant, and missing data, for instance in the case where people were "lurking" and their activity related to the course invisible. Qualitative methods will be applied to give more meaning to the experiences of those on the periphery, the non-active participants, and those who were perhaps lacking the skills or mechanisms for engaging wholeheartedly in the course. Interviews with participants who were either not connected, not visible on the “PLENK2010 social network” or on the fringes will provide them with a voice as the analytics used in our analysis could not capture their learning stories and made them into a marginalized, invisible group. Another limitation of analytics has been their inability to capture contextual nuances in data. Analytics can provide a view of what is happening, but it has problems representing the nature of connections between data sets and people. Human interpretation or artifical intelligence capacity will be necessary to achieve this. Facilitators and participants themselves were exposed to analytics tools within the PLENK2010 course and in one of the discussions concluded that although they provided a global view of social networks in the MOOC they lacked sufficient detail to be really informative. A greater understanding of how learners communicate, complete tasks and construct new knowledge in a Moodle environment, combined with blogs, and Twitter activity will inform the design and development of optimal learning experiences. Further analyses will be undertaken after the course has been completed with more options for analyses of data involving interpretation of meanings and human actions rather than a focus on the numbers. This work is expected to be completed in time for presentation at the conference on Learning Analytics and Knowledge, February 27-March 01, 2011. The use of learning analytics is only in its infancy, but from our use of the tools it seems that they can be powerful in giving meaning to interactions and actions in a learning environment such as was used on this MOOC, providing scope for personalized learning and the creation of more effective learning environments and experiences. Personalization and analysis of user interaction data is a key approach to overcoming the problems related to the overpowering plethora of information available and generated through technology in an open networked learning environment. More in depth analyses of the data from PLENK will feed into the development of support structures of optimal learning experiences in Personal Learning Environments. 
 Learning analytics tools have clearly provided scope for information filtering and visualization, as promising technologies to support people in clarifying and relating information, peer learners and digital artifacts and in doing so supporting people in pursuing their learning.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>The Value of Learning Analytics to Networked Learning on a Personal Learning Environment</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>analytics tools</dc:subject>
		<dc:subject>massive open online courses</dc:subject>
		<dc:subject>educational research</dc:subject>
		<dc:subject>big data</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/no_authors"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/no_authors"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/65/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/no_authors"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/66">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/proceedings"/>
		<dc:title>Using learning analytics to assess students' behavior in open‐ended programming tasks</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/66/authorlist"/>
		<swrc:abstract>In this paper, we describe a strand of the Project-Based Learning Analytics project, aimed at assessing student learning in unscripted, open-ended environments. We review the literature on educational data mining, computer vision applied to assessment, and emotion detection, to exemplify the directions we’re pursuing with the research. We then discuss the relevance of the work, and describe one case study, in which students created computer programs and snapshots of the code were recorded when the users attempted a compilation. The subjects were sophomore undergraduate students in engineering. Underlying this work is the assumption that human cognition can evolve in ways that are too subtle or too complex to be detected by conventional techniques, and computational techniques can not only help educators design systems with better feedback, but could also be a novel lens to investigate human cognition, finding patterns in massive datasets otherwise inaccessible to the human eye.</swrc:abstract>
		<led:body><![CDATA[ 1 Introduction and significance.
 Politicians, educators, business leaders, and researchers are unanimous to state that we need to redesign schools to teach the so-called 21st century skills: creativity, innovation, critical thinking, problem solving, communication, collaboration, among others. None of those skills are easily measured using current assessment techniques, such as multiple choice tests or even portfolios. As a result, schools are paralyzed by the push to teach new skills, and the lack of reliable ways to assess those skills. One of the difficulties is that current assessment instruments are based on end products (an exam, a project, a portfolio), and not on processes (the actual cognitive and intellectual development while performing a learning activity), due to the intrinsic difficulties in capturing detailed process data for large numbers of students. However, new sensing and data mining technologies are making it possible to capture and analyze massive amounts of data in all fields of human activity. Current attempts to use artificial intelligence techniques to assess human learning have focused on two main areas: text analysis and emotion detection. The work of Rus et al. (2009), for example, makes extensive use of text analytics within a computer-based application for learning about complex phenomena in science. Students were asked to write short paragraphs about scientific phenomena -- Rus et al. then explored which machine learning algorithm would enable them to most accurately classify each student in terms of their content knowledge, based on comparisons with expert-formulated responses. Similarly, speech analysis further removes the student from the traditional assessment setting by allowing them to demonstrate fluency in a more natural setting. Beck and Sison (2006) have demonstrated a method for using speech recognition to assess reading proficiency in a study with elementary school students that combines speech recognition with knowledge tracing (a form of probabilistic monitoring.) The second area of work is the detection of emotional states using non-invasive techniques. Understanding student sentiment is an important element in constructing a holistic picture of student progress, and it also helps to better enable computer-based systems to interact with students in emotionally supportive ways. Using the Facial Action Coding System (FACS), researchers have been able to develop a method for recognizing student affective state by simply observing and (manually) coding their facial expressions, and applying machine learning to the data produced (Craig et al., 2008). Researchers have also used conversational cues to realize student emotional state. Similar to the FACS study, Craig et al. (2008) designed an application that could use spoken dialogue to recognize the states of boredom, frustration, flow, and confusion. They were able to resolve the validity of their findings through comparison to emote-aloud (a derivative of talk-aloud where participants describe their emotions as they feel them) activities while students interacted with AutoTutor. Even though researchers have been trying to use all these artificial intelligence techniques for assessing students’ formal knowledge and emotional states, the field would derive significant benefit from three important additions: 1) student activity data (gestures, sketches, actions) as a primary component of analysis, 2) automation of data analysis processes, 3) multidimensional data collection and analysis. Educational data mining (EDM; Amershi & Conati, 2009; Baker, Corbett, Koedinger, & Wagner, 2004) has been used in many contexts to measure students’ learning and affect, but in Baker and Yacef’s review of its current uses, the majority of the work is focused on cognitive tutors or semi-scripted environments (Baker & Yacef, 2009). At the same time, qualitative approaches presents some crucial shortcomings: (1) there is no persistent trace of the evolution of the students’ artifacts (computer code, robots, etc.), (2) crucial learning moments within a project can last only seconds, and are easy to miss with normal data collection techniques, and (3) such methodologies are hard to scale for large groups or extended periods of time. Most of previous work on EDM, however, has been used to assess very specific and limited tasks – but the 21st century skills we need to assess now are much more complex, such as creativity, the ability to find solutions to ill-structured problems, to navigate in environments with sparse information, as well as dealing with uncertainty. Unscripted learning environments are well-known for being challenging to measure and assess, but recent advances in low-cost bio sensing, natural language processing and computer vision could make it possible to understand students’ trajectories in these environments. In this paper, I will present one first and simpler example of the possibility to use learning analytics and educational data mining (Baker & Yacef, 2009) to inspect students’ behavior and learning in project-based, unscripted, constructionist (Papert, 1980) learning environments, in which traditional assessment methods might not capture students’ evolution. The case study examines patterns in students programming scientific models. Snapshots of the code generated by students were automatically stored in a file, which was later analyzed using custom-built tools. One of such tools is the Code Navigator, which allows researchers to go back and forth in time, “frame-by-frame,” tracking students’ progression and measuring statistical data. Results show that we could not only identify phases in the development of a program (see Figure 2), and also some preliminary evidence of typical behaviors pertaining to novices and experts. Many researchers have attempted to automate the collection of action data, such as gesture and emotion. For example, Weinland et al. (2006) and Yilmaz et al. (2005) were able to detect basic human actions related to movement. Craig et al. (2007) created a system for automatic detection of facial expressions (the FACS study). The technique that Craig et al. validated is a highly non-invasive mechanism for realizing student sentiment, and can be coupled with computer vision technology and biosensors to enable machines to automatically detect changes in emotional state or cognitive-affect. Another area of active development is speech and text mining. For example, researchers have combined natural language processing and machine learning to analyze student discussions and writing, leveraging Independent Component Analysis of student conversations -- a technique whose validity has been repeatedly reproduced. The derived text will is subsequently analyzed using Latent Semantic Analysis (Rus et al. 2009). Given the right training and language model, LSA can give a clearer picture of each student’s knowledge development throughout the course of the learning activity. In the realm of exploratory learning environments, Bernardini, Amershi and Conati (2009; 2010) built student models combining supervised and unsupervised classification, both with log files and eye-tracking, and showed that meaningful events could be detected with the combined data. Montalvo et al., also using a combination of automated and semi-automated real-time coding, showed that they could identify meaningful meta-cognitive planning processes when students were conducting experiments in an online virtual lab environment. However, most of these studies did not involve the creation of completely openended artifacts, with almost unlimited degrees of freedom. Our study is one attempt in the direction of understanding the process of creation of these artifacts, especially by novices. Our goal in the paper will be to establish a proof of existence that automatically-generated logs of students programming can be used to infer consistent patterns in how students go about programming, and that by inspecting those patterns we could design better support materials and strategies, as well as detect critical points in the writing of software in which human assistance would be more needed. Since our data relies in just nine subjects, I don’t make claims of statistical significance, but the data points some clear qualitative distinctions between the students. 
 2 Methods and data collection.
 2.1 Dataset.
 The goal of the logfile analysis was to identify patterns in the model building process using the NetLogo (Wilensky, 1999) programming environment. NetLogo can log to an XML file all users’ actions, such as key presses, button clicks, changes in variables and, most importantly for this study, changes in the code. The logging module uses a special configuration file, which specifies which actions are to be logged. This file was distributed to students alongside with instruction about how to enable logging, collect the log-files, and send those files back for analysis. In what follows, I show some general data about the files collected and conduct one more detailed analysis for one student. I will try to show how the data collected can, rather than completely elucidate the problem, point researchers to instances during the model-building process in a more in-depth qualitative analysis could be worthwhile. Nine students in a sophomore-level engineering class had a 3-week programming assignment. The task was to write a computer program to model a scientific phenomenon of their choice. Students had the assistance of a ‘programming’ teaching assistance, following the normal class structure. The teaching assistant was available for about 3-4 hours a week for each student, and an individual, 1-hour programming tutorial session was conducted with each of the students on the first week of the study. 158 logfiles were collected. Using a combination of XQuery and regular expression processors (such as ‘grep’), the files were processed, parsed, and analyze (1.5GB and 18 million lines of uncompressed text files). Below is a summary of the collected data (in this order): number of files collected by students, the total size in megabytes for the set of files, total number of events logged, total number of global variable changes, and its proportion in relation to the total number of logged events (in percent and in absolute numbers). 
 Table.

 1. Number of events collected per student. 
 The overwhelming majority of events collected were global variable changes (99.6% of the total, and 60.8% on average per student. Note that in Table 1, “Globals” refer to the number of events containing only a variable change, while “Not Globals” refer to all other events.) This particular kind of event takes place when students are running or testing models – every single variable change gets recorded, what accounts for the very large number of events (almost 9 million.) Since the analysis of students’ interactions with models is out of the scope of this paper, all non-coding events were filtered out from the main dataset, so we were left with 1187 events for 9 users. 
 3 Data and discussion.
 For the analysis, I will first focus on one student and conduct an in-depth exploration of her coding strategies. Then, I will compare her work with other students, and show how differences in previous ability might have determined their experience. 
 3.1 Coding strategies.
 3.1.1 Luca.
 Luca is a sophomore student in materials science and built a model of how crystals grow. She had modest previous experience with computers, and her grade in the class was also around the average, which makes her a good example for an in-depth analysis of log files. Figure 2 summarizes Luca’s model building logs. The red curve represents the number of characters in her code, the blue dots represent the time between compilation (secondary y-axis to the right), green dots placed at y=1800 represent successful compilations, orange dots placed at y=1200 represent unsuccessful compilations. In the following paragraphs, I will analyze each of the 6 parts of the plot. The analysis was done by looking at the overall increase in character count (Figure 2), and then using the Code Navigator tool (Figure 1) to locate the exact point in time when the events happened. 
 Figure 1. The Code Navigator, which allows researcher to go back and forth in time tracking how students created a computer program. 
 Figure 2. Code size, time between compilations and errors, for Luca’s logfiles. 
 1. Luca started with one of the exemplar models seen in the tutorial (the “very simple” solidification model). In less than a minute, she deleted the unnecessary code and ended up with a skeleton of a new model (see the big drop in point A). 
 2. She spent the next half-hour building her first procedure, to generate one of the two types of crystal growth she purported to include in the model. During this time, between A and B, she had numerous unsuccessful compilations (see the orange dots), and goes from 200 to 600 characters of code. 
 3. The size of the code remains stable for 12 minutes (point B), until there is a sudden jump from 600 to 900 characters (just before point C). This jump corresponds to Luca copying and pasting her own code: she duplicated her first procedure as a basis for a second one. During this period, also, she opens many of the sample models with NetLogo. 
 4. Luca spends some time making her new procedure work. The frequency of compilation decreases (see the density of orange and green dots), the average time per compilation increases, and again we see a plateau for about one hour (point D). 
 5. After one hour in the plateau, the is another sudden increase in code size, from 900 to 1300 characters (between D and E). Actually, what Luca did was to open a sample model and copy a procedure that generates a hexagonal grid, which was needed for her model. Note that code compilations are even less frequent. 
 6. After making the “recycled” code work, Luca got to her final number of 1200 characters of code. She then spent about 20 minutes “beautifying” the code, fixing the indentation, changing names of variables, etc. No real changes in the code took place, and the are no incorrect compilations. 
   Luca’s narrative suggests, thus, four prototypic modeling events: - Stripping down an existing model as a starting point. - Long plateaus of no coding activity, during which students browse other models (or their own model) for useful code. - Sudden jumps in character count, when students import code from other model or copy an paste code from within the working model. - A final phase in which students fix the formatting of the code, indentation variable names etc. 
 3.1.2 Shana, Lian, Leen, and Che.
 This initial analysis is useful to examine logfiles from other students as well in search of similarities. In the following, I show plots (character count vs. time) from five different students (Luca, Che, Leen, and Shana, Figure 3) which include all of students' activity (including opening other models—the “spikes”—note tha the plot in Figure 2 did not show all of Luca’s activities but only her activities within her model, i.e excluding opening and manipulating other models). 
 Figure 3. Code size versus time for Luca, Shana, Che, and Leen.
 First, let’s examine Shana’s logfiles. After many ‘spikes,’ there is a sudden jump (at time=75) from about 200 to 4,000 characters of code. A closer, systematic examination revealed that Shana employed a different approach. After some attempts to incorporate the code of other models into her own (the spikes), she gave up and decided to do the opposite: start from a ready-made model and add her code to it. She then chose a very well-established model and built hers on top of it. The sudden jump to 4,000 characters indicates the moment when she opened and started making the model ‘her own,’ by adding her procedures. She seamless integrated the pre-existing model into her new one, adding significant new features. Leen, on the other hand, had yet another coding style. He did open other models for inspiration or cues, but did not copy and paste code. Instead, he built his procedures in small increments by trial-and-error. In Table 2 we can observe how he coded a procedure to “sprout” a variable number of white screen elements in his model (during a 30-minute period). The changes in the code are indicated in red. 
 Table 2. Leen’s attempts to write the “InsertVacancies” procedure 
 
 His trial-and-error method had an underlying pattern: he went from simpler to more complex structures. For example, he first attempts a fixed, “hardcoded” number of events (sprout), then introduces control structures (loop, while) to generate a variable number of events, and finally introduces new interface widgets to give the user control over the number of events. Leen reported having a higher familiarity with programming languages than Luca and Shana, which might explain his different coding style. Che, with few exceptions, did not open other models during model building. Similarly to Leen, they also employ an incremental, trial-and-error approach, but we can clearly detect many more long plateaus in Liam’s graph. Therefore, based on these four logfiles and the other five analyzed some canonical coding strategies were found in most of them: a. Stripping down an existing model as a starting point. b. Starting from a ready-made model and adding one’s own procedures. c. Long plateaus of no coding activity, during which students browse other models (or their own model) for useful code. d. Long plateaus of no coding activity, during which students think of solutions without browsing other models. e. Period of linear growth in the code size, during which students employ a trial-and-error strategy to get the code right. f. Sudden jumps in character count, when students import code from other models, or copy and paste code from within their working model. g. A final phase in which students fix the formatting of the code, indentation, variable names, etc. Based on those strategies, and the previous programming knowledge of students, the data suggest three coding profiles: - “Copy and pasters:” more frequent use of a, b, c, f, and g. - Mixed-mode: a combination of c, d, e, and g. - “Self-sufficients:” more frequent use of d, e. The empirical verification of these canonical coding strategies and coding profiles has important implications for design, in particular, constructionist environments. Each coding strategy and profile might entail different support strategies. For example, students with more advanced programming skills (many of which exhibited the “self-sufficient” behavior) might require detailed and easy-to-find language documentation, whereas “copy and pasters” need more working examples with transportable code. The data shows that students in fact are relatively autonomous in developing strategies for learning the programming language, and points designers to the need of having multiple forms of support (see, for example, Turkle and Papert (1991)). 
 3.2 Code compilation.
 Despite these differences, one behavior seemed to be rather similar across students: the frequency of code compilation. Figure 4 shows the moving average of unsuccessful compilations (thus, the error rate) versus time, i.e., the higher the value, the higher the number of unsuccessful compilations within one moving average period (the moving average period was 10% of the overall duration of the logfile—if there were 600 compilation attempts, there period of the moving average would be 60). 
 Figure 4. Error rate versus compilation attempts (time).
 For all four students, eliminating the somewhat noisy first instants, the error rate curve follows an inverse parabolic shape. It starts very low, reaches a peak halfway through the project, and then decreases reaching values close to zero. Also, the blue dots on top of y=0 (correct compilations) and y=1 (incorrect compilations) indicate the actual compilation attempts. Most of them are concentrated in the first half of the activity—approximately 2/3 in the first half to 1/3 in the second half. This further confirms the previous logfiles analysis in which the model-building process is not homogenous and simple, but complex and comprised of several different phases: an initial exploration characterized by few unsuccessful compilations, followed by a phase with intense code evolution and many compilation attempts, and a final phase of final touches and smaller fixes. 
 4 Conclusion.
 This paper is an initial step towards developing metrics (compilation frequency, code size, code evolution pattern, frequency of correct/incorrect compilations, etc.) that could both serve as formative assessments tools, and pattern-finding lenses into students’ free-form explorations in technology-rich learning environments. The frequency of code compilations, together with the code size plots previously analyzed, enables us to trace a reasonable approximation of each prototypical coding profile and style. Such an analysis has three important implications for the design of open-ended environments: ─ To design and allocate support resources, moments of greater difficulty in the modeling process should be identified. Our data indicate that those moments happens mid-way through the project. ─ Support materials and strategies need to be designed to cater to diverse coding styles and profiles. By better understanding each student’s coding style, we also have an extra window into students’ cognition. Paired with other data sources (interviews, tests, surveys), the data could offer a rich portrait of the model-building process and how it affects students’ understanding of the scientific phenomena and the programming language.]]></led:body>
		<swrc:month>Mar</swrc:month>
		<swrc:year>2011</swrc:year>
		<rdfs:label>Using learning analytics to assess students' behavior in open‐ended programming tasks</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>programming</dc:subject>
		<dc:subject>constructionism</dc:subject>
		<dc:subject>exploratory learning environments</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/paulo-blikstein"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/paulo-blikstein"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/66/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/paulo-blikstein"/>
	</rdf:Description>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/">
		<rdfs:label></rdfs:label>
		<foaf:name></foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-atkisson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/david-wiley"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/no_authors"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/paulo-blikstein"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/apollo-group">
		<rdfs:label>Apollo Group</rdfs:label>
		<foaf:name>Apollo Group</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-barber"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mike-sharkey"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/athabasca-university">
		<rdfs:label>Athabasca University</rdfs:label>
		<foaf:name>Athabasca University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dragan-gasevic"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/liaqat-ali"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/george-siemens"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nazim-rahman"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-dron"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sabine-graf"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/cindy-ives"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/arnold-ferri"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university">
		<rdfs:label>Carnegie Mellon University</rdfs:label>
		<foaf:name>Carnegie Mellon University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/stephen-e-fancsali"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/centre-for-applied-information-and-communication-technology---copenhagen-business-school">
		<rdfs:label>Centre for Applied Information and Communication Technology - Copenhagen Business School</rdfs:label>
		<foaf:name>Centre for Applied Information and Communication Technology - Copenhagen Business School</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-teplovs"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nobuko-fujita"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/copenhagen-business-school">
		<rdfs:label>Copenhagen Business School</rdfs:label>
		<foaf:name>Copenhagen Business School</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/european-schoolnet--eun-">
		<rdfs:label>European Schoolnet (EUN)</rdfs:label>
		<foaf:name>European Schoolnet (EUN)</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/riina-vuorikari"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology">
		<rdfs:label>Fraunhofer Institute for Applied Information Technology</rdfs:label>
		<foaf:name>Fraunhofer Institute for Applied Information Technology</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/katja-niemann"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/uwe-kirschenmann"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-wolpers"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/maren-scheffel"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/greek-research-and-technology-network--grnet-">
		<rdfs:label>Greek Research and Technology Network (GRNET)</rdfs:label>
		<foaf:name>Greek Research and Technology Network (GRNET)</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-palavitsinis"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vassilios-protonotarios"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/grockit,-inc">
		<rdfs:label>Grockit, Inc.</rdfs:label>
		<foaf:name>Grockit, Inc.</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ari-bader-natal"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-lotze"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/kuleuven">
		<rdfs:label>K.U.Leuven</rdfs:label>
		<foaf:name>K.U.Leuven</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-luis-santos"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sten-govaerts"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherland">
		<rdfs:label>Open University of the Netherland</rdfs:label>
		<foaf:name>Open University of the Netherland</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/the-open-university">
		<rdfs:label>The Open University</rdfs:label>
		<foaf:name>The Open University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/doug-clow"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/haiming-liu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-slade"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/fenella-galpin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-de-liddo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michelle-bachler"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/elpida-makriyannis"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/thompson-rivers-university---open-learning">
		<rdfs:label>Thompson Rivers University - Open Learning</rdfs:label>
		<foaf:name>Thompson Rivers University - Open Learning</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/griff-richards"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/irwin-devries"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/universita-degli-studi-napoli-federico-ii">
		<rdfs:label>Universita degli Studi Napoli Federico II</rdfs:label>
		<foaf:name>Universita degli Studi Napoli Federico II</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ivana-quinto"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lorella-cannavacciuolo"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-">
		<rdfs:label>Universitat Oberta de Catalunya (UOC)</rdfs:label>
		<foaf:name>Universitat Oberta de Catalunya (UOC)</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/german-cobo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/david-garcia-solorzano"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-antonio-moran"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/eugenia-santamaria"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-monzo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-melenchon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/josep-grau-valldosera"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/julia-minguillon"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-carlos-iii-of-madrid">
		<rdfs:label>University Carlos III of Madrid</rdfs:label>
		<foaf:name>University Carlos III of Madrid</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/derick-leony"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/abelardo-pardo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/luis-de-la-fuente-valentin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/david-sanchez-de-castro"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-birmingham">
		<rdfs:label>University of Birmingham</rdfs:label>
		<foaf:name>University of Birmingham</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/susan-bull"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-british-columbia">
		<rdfs:label>University of British Columbia</rdfs:label>
		<foaf:name>University of British Columbia</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-hawaii">
		<rdfs:label>University of Hawaii</rdfs:label>
		<foaf:name>University of Hawaii</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dan-suthers"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kar-hai-chu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/devan-rosen"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/victor-miagkikh"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-queensland">
		<rdfs:label>University of Queensland</rdfs:label>
		<foaf:name>University of Queensland</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/aneesha-bakharia"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-saskatchewan">
		<rdfs:label>University of Saskatchewan</rdfs:label>
		<foaf:name>University of Saskatchewan</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jim-greer"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carl-gutwin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carrie-demmans-epp"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/greg-logan"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-wollongong">
		<rdfs:label>University of Wollongong</rdfs:label>
		<foaf:name>University of Wollongong</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lori-lockyer"/>
	</foaf:Organization>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/michael-atkisson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/"/>
		<rdfs:label>Michael Atkisson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Michael</foaf:firstName>
		<foaf:lastName>Atkisson</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Michael Atkisson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/59"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/david-wiley">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/"/>
		<rdfs:label>David Wiley</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>David</foaf:firstName>
		<foaf:lastName>Wiley</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>David Wiley</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/59"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/no_authors">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/"/>
		<rdfs:label>NO_AUTHORS</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName></foaf:firstName>
		<foaf:lastName>O_AUTHORS</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>NO_AUTHORS</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/65"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/paulo-blikstein">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/"/>
		<rdfs:label>Paulo Blikstein</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Paulo</foaf:firstName>
		<foaf:lastName>Blikstein</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Paulo Blikstein</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/66"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mike-sharkey">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/apollo-group"/>
		<rdfs:label>Mike Sharkey</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Mike</foaf:firstName>
		<foaf:lastName>Sharkey</foaf:lastName>
		<foaf:mbox_sha1sum>d5412876c38269dfe20933f7ed43aa04611a9a31</foaf:mbox_sha1sum>
		<foaf:name>Mike Sharkey</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/64"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nazim-rahman">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/athabasca-university"/>
		<rdfs:label>Nazim Rahman</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Nazim</foaf:firstName>
		<foaf:lastName>Rahman</foaf:lastName>
		<foaf:mbox_sha1sum>66725a7ebaab80a8d8b8baa19842570ef1931fa7</foaf:mbox_sha1sum>
		<foaf:name>Nazim Rahman</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/63"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/arnold-ferri">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/athabasca-university"/>
		<rdfs:label>Arnold Ferri</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Arnold</foaf:firstName>
		<foaf:lastName>Ferri</foaf:lastName>
		<foaf:mbox_sha1sum>4bc5dd0ebcc65fed815cfb33dbbd7323b56be029</foaf:mbox_sha1sum>
		<foaf:name>Arnold Ferri</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/63"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sabine-graf">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/athabasca-university"/>
		<rdfs:label>Sabine Graf</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Sabine</foaf:firstName>
		<foaf:lastName>Graf</foaf:lastName>
		<foaf:mbox_sha1sum>7405fa503fd8f3bfd265c44d1e8bd94ae7d04f53</foaf:mbox_sha1sum>
		<foaf:name>Sabine Graf</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/63"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/cindy-ives">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/athabasca-university"/>
		<rdfs:label>Cindy Ives</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Cindy</foaf:firstName>
		<foaf:lastName>Ives</foaf:lastName>
		<foaf:mbox_sha1sum>5a0b2f28541bdcc3fc43d7ff6fa657016f896006</foaf:mbox_sha1sum>
		<foaf:name>Cindy Ives</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/63"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/stephen-e-fancsali">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Stephen E. Fancsali</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Stephen</foaf:firstName>
		<foaf:lastName>E. Fancsali</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Stephen E. Fancsali</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/45"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/christopher-teplovs">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/centre-for-applied-information-and-communication-technology---copenhagen-business-school"/>
		<rdfs:label>Christopher Teplovs</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Denmark"/>
		<foaf:firstName>Christopher</foaf:firstName>
		<foaf:lastName>Teplovs</foaf:lastName>
		<foaf:mbox_sha1sum>938fc51fd912b020316733038d07f8e0a77220c4</foaf:mbox_sha1sum>
		<foaf:name>Christopher Teplovs</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nobuko-fujita">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/centre-for-applied-information-and-communication-technology---copenhagen-business-school"/>
		<rdfs:label>Nobuko Fujita</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Denmark"/>
		<foaf:firstName>Nobuko</foaf:firstName>
		<foaf:lastName>Fujita</foaf:lastName>
		<foaf:mbox_sha1sum>5041d362ee6c47b880c7609bcfcc0f1e935b3096</foaf:mbox_sha1sum>
		<foaf:name>Nobuko Fujita</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/christopher-teplovs">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/centre-for-applied-information-and-communication-technology---copenhagen-business-school"/>
		<rdfs:label>Christopher Teplovs</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Denmark"/>
		<foaf:firstName>Christopher</foaf:firstName>
		<foaf:lastName>Teplovs</foaf:lastName>
		<foaf:mbox_sha1sum>938fc51fd912b020316733038d07f8e0a77220c4</foaf:mbox_sha1sum>
		<foaf:name>Christopher Teplovs</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nobuko-fujita">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/centre-for-applied-information-and-communication-technology---copenhagen-business-school"/>
		<rdfs:label>Nobuko Fujita</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Denmark"/>
		<foaf:firstName>Nobuko</foaf:firstName>
		<foaf:lastName>Fujita</foaf:lastName>
		<foaf:mbox_sha1sum>5041d362ee6c47b880c7609bcfcc0f1e935b3096</foaf:mbox_sha1sum>
		<foaf:name>Nobuko Fujita</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/copenhagen-business-school"/>
		<rdfs:label>Ravi Vatrapu</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Denmark"/>
		<foaf:firstName>Ravi</foaf:firstName>
		<foaf:lastName>Vatrapu</foaf:lastName>
		<foaf:mbox_sha1sum>08cf0c9e8732b1adb15733b679855704030d8bdd</foaf:mbox_sha1sum>
		<foaf:name>Ravi Vatrapu</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/56"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/copenhagen-business-school"/>
		<rdfs:label>Ravi Vatrapu</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Denmark"/>
		<foaf:firstName>Ravi</foaf:firstName>
		<foaf:lastName>Vatrapu</foaf:lastName>
		<foaf:mbox_sha1sum>08cf0c9e8732b1adb15733b679855704030d8bdd</foaf:mbox_sha1sum>
		<foaf:name>Ravi Vatrapu</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/56"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ravi-vatrapu">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/copenhagen-business-school"/>
		<rdfs:label>Ravi Vatrapu</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Denmark"/>
		<foaf:firstName>Ravi</foaf:firstName>
		<foaf:lastName>Vatrapu</foaf:lastName>
		<foaf:mbox_sha1sum>08cf0c9e8732b1adb15733b679855704030d8bdd</foaf:mbox_sha1sum>
		<foaf:name>Ravi Vatrapu</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/56"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/57"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/riina-vuorikari">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/european-schoolnet--eun-"/>
		<rdfs:label>Riina Vuorikari</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Belgium"/>
		<foaf:firstName>Riina</foaf:firstName>
		<foaf:lastName>Vuorikari</foaf:lastName>
		<foaf:mbox_sha1sum>792959715626d3c884b0cc72f0d2e33c03f29c24</foaf:mbox_sha1sum>
		<foaf:name>Riina Vuorikari</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/martin-wolpers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology"/>
		<rdfs:label>Martin Wolpers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Martin</foaf:firstName>
		<foaf:lastName>Wolpers</foaf:lastName>
		<foaf:mbox_sha1sum>61798a03ba9da369aafe3e416ecda9685c4e13ab</foaf:mbox_sha1sum>
		<foaf:name>Martin Wolpers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/47"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/katja-niemann">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology"/>
		<rdfs:label>Katja Niemann</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Katja</foaf:firstName>
		<foaf:lastName>Niemann</foaf:lastName>
		<foaf:mbox_sha1sum>bf1266883936a49888fbb3c464baeff7a05561f5</foaf:mbox_sha1sum>
		<foaf:name>Katja Niemann</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/47"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hans-christian-schmitz">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology"/>
		<rdfs:label>Hans-Christian Schmitz</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Hans-Christian</foaf:firstName>
		<foaf:lastName>Schmitz</foaf:lastName>
		<foaf:mbox_sha1sum>33e2d0fb50cce2b0894ca84b6c7c14e8226cd23f</foaf:mbox_sha1sum>
		<foaf:name>Hans-Christian Schmitz</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/47"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/martin-wolpers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology"/>
		<rdfs:label>Martin Wolpers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Martin</foaf:firstName>
		<foaf:lastName>Wolpers</foaf:lastName>
		<foaf:mbox_sha1sum>61798a03ba9da369aafe3e416ecda9685c4e13ab</foaf:mbox_sha1sum>
		<foaf:name>Martin Wolpers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/47"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/maren-scheffel">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/fraunhofer-institute-for-applied-information-technology"/>
		<rdfs:label>Maren Scheffel</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Maren</foaf:firstName>
		<foaf:lastName>Scheffel</foaf:lastName>
		<foaf:mbox_sha1sum>1f69aec2f0aaa2df56dc351210ffbfd056870119</foaf:mbox_sha1sum>
		<foaf:name>Maren Scheffel</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/47"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nikos-palavitsinis">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/greek-research-and-technology-network--grnet-"/>
		<rdfs:label>Nikos Palavitsinis</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Greece"/>
		<foaf:firstName>Nikos</foaf:firstName>
		<foaf:lastName>Palavitsinis</foaf:lastName>
		<foaf:mbox_sha1sum>4d97f7f43b4de180a4efc69a193c07a46626bf13</foaf:mbox_sha1sum>
		<foaf:name>Nikos Palavitsinis</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/48"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/vassilios-protonotarios">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/greek-research-and-technology-network--grnet-"/>
		<rdfs:label>Vassilios Protonotarios</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Greece"/>
		<foaf:firstName>Vassilios</foaf:firstName>
		<foaf:lastName>Protonotarios</foaf:lastName>
		<foaf:mbox_sha1sum>1a9bbcf45099d39cf18c2743077736cea2259d60</foaf:mbox_sha1sum>
		<foaf:name>Vassilios Protonotarios</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/48"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nikos-manouselis">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/greek-research-and-technology-network--grnet-"/>
		<rdfs:label>Nikos Manouselis</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Greece"/>
		<foaf:firstName>Nikos</foaf:firstName>
		<foaf:lastName>Manouselis</foaf:lastName>
		<foaf:mbox_sha1sum>e4749e14bcd5f2219506274ea7c7f7ce76c51dd6</foaf:mbox_sha1sum>
		<foaf:name>Nikos Manouselis</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/48"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nikos-manouselis">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/greek-research-and-technology-network--grnet-"/>
		<rdfs:label>Nikos Manouselis</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Greece"/>
		<foaf:firstName>Nikos</foaf:firstName>
		<foaf:lastName>Manouselis</foaf:lastName>
		<foaf:mbox_sha1sum>e4749e14bcd5f2219506274ea7c7f7ce76c51dd6</foaf:mbox_sha1sum>
		<foaf:name>Nikos Manouselis</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/48"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ari-bader-natal">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/grockit,-inc"/>
		<rdfs:label>Ari Bader-Natal</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ari</foaf:firstName>
		<foaf:lastName>Bader-Natal</foaf:lastName>
		<foaf:mbox_sha1sum>28c120084f8f5823674f14bf9f02e90be35e4296</foaf:mbox_sha1sum>
		<foaf:name>Ari Bader-Natal</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/58"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/thomas-lotze">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/grockit,-inc"/>
		<rdfs:label>Thomas Lotze</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Thomas</foaf:firstName>
		<foaf:lastName>Lotze</foaf:lastName>
		<foaf:mbox_sha1sum>6610d1e48ac3f6c005c24507f0ebd6139a358420</foaf:mbox_sha1sum>
		<foaf:name>Thomas Lotze</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/58"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/katrien-verbert">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/kuleuven"/>
		<rdfs:label>Katrien Verbert</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Belgium"/>
		<foaf:firstName>Katrien</foaf:firstName>
		<foaf:lastName>Verbert</foaf:lastName>
		<foaf:mbox_sha1sum>610730a9705def3b48bc4b17449cc5ac317d7bf8</foaf:mbox_sha1sum>
		<foaf:name>Katrien Verbert</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/erik-duval">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/kuleuven"/>
		<rdfs:label>Erik Duval</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Belgium"/>
		<foaf:firstName>Erik</foaf:firstName>
		<foaf:lastName>Duval</foaf:lastName>
		<foaf:mbox_sha1sum>33b04e59663928cd5da8044a9475463e09adfa3c</foaf:mbox_sha1sum>
		<foaf:name>Erik Duval</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherland"/>
		<rdfs:label>Hendrik Drachsler</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Hendrik</foaf:firstName>
		<foaf:lastName>Drachsler</foaf:lastName>
		<foaf:mbox_sha1sum>a0f12a89fc8bf9cfbd44b49a9e1bbcec40106def</foaf:mbox_sha1sum>
		<foaf:name>Hendrik Drachsler</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/46"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Simon Buckingham Shum</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Simon</foaf:firstName>
		<foaf:lastName>Buckingham Shum</foaf:lastName>
		<foaf:mbox_sha1sum>bbaf2c40d77ba4a21d7c6f19c0b973ed3624cdc5</foaf:mbox_sha1sum>
		<foaf:name>Simon Buckingham Shum</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/43"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/michelle-bachler">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Michelle Bachler</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Michelle</foaf:firstName>
		<foaf:lastName>Bachler</foaf:lastName>
		<foaf:mbox_sha1sum>e407f2c651017918494214e2fcaea2f9984b64a3</foaf:mbox_sha1sum>
		<foaf:name>Michelle Bachler</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/anna-de-liddo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Anna De Liddo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Anna</foaf:firstName>
		<foaf:lastName>De Liddo</foaf:lastName>
		<foaf:mbox_sha1sum>c88d7aa754cad26e485a99c7df0aaa2cb4020f5c</foaf:mbox_sha1sum>
		<foaf:name>Anna De Liddo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Simon Buckingham Shum</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Simon</foaf:firstName>
		<foaf:lastName>Buckingham Shum</foaf:lastName>
		<foaf:mbox_sha1sum>bbaf2c40d77ba4a21d7c6f19c0b973ed3624cdc5</foaf:mbox_sha1sum>
		<foaf:name>Simon Buckingham Shum</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/43"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/doug-clow">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Doug Clow</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Doug</foaf:firstName>
		<foaf:lastName>Clow</foaf:lastName>
		<foaf:mbox_sha1sum>50032dfb2365c3d7db9b3868e57e5b236f60f742</foaf:mbox_sha1sum>
		<foaf:name>Doug Clow</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/55"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Rebecca Ferguson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Rebecca</foaf:firstName>
		<foaf:lastName>Ferguson</foaf:lastName>
		<foaf:mbox_sha1sum>9a2a32c74e31c9e3a338d550c2619bae7273ec6d</foaf:mbox_sha1sum>
		<foaf:name>Rebecca Ferguson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/43"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/elpida-makriyannis">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Elpida Makriyannis</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Elpida</foaf:firstName>
		<foaf:lastName>Makriyannis</foaf:lastName>
		<foaf:mbox_sha1sum>c3b5a69dbefdf8c94a4eb7574dc690eec55b0b6c</foaf:mbox_sha1sum>
		<foaf:name>Elpida Makriyannis</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/55"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/griff-richards">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/thompson-rivers-university---open-learning"/>
		<rdfs:label>Griff Richards</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Griff</foaf:firstName>
		<foaf:lastName>Richards</foaf:lastName>
		<foaf:mbox_sha1sum>0eefddc7369a770771b1ee4f0e511430115fedd3</foaf:mbox_sha1sum>
		<foaf:name>Griff Richards</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/54"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/irwin-devries">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/thompson-rivers-university---open-learning"/>
		<rdfs:label>Irwin DeVries</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Irwin</foaf:firstName>
		<foaf:lastName>DeVries</foaf:lastName>
		<foaf:mbox_sha1sum>6f558578bc5889cdf79ba088209242079bf89780</foaf:mbox_sha1sum>
		<foaf:name>Irwin DeVries</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/54"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/lorella-cannavacciuolo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universita-degli-studi-napoli-federico-ii"/>
		<rdfs:label>Lorella Cannavacciuolo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Italy"/>
		<foaf:firstName>Lorella</foaf:firstName>
		<foaf:lastName>Cannavacciuolo</foaf:lastName>
		<foaf:mbox_sha1sum>b6d0972b94b8b93b9760da8275fee1123e5e8827</foaf:mbox_sha1sum>
		<foaf:name>Lorella Cannavacciuolo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ivana-quinto">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universita-degli-studi-napoli-federico-ii"/>
		<rdfs:label>Ivana Quinto</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Italy"/>
		<foaf:firstName>Ivana</foaf:firstName>
		<foaf:lastName>Quinto</foaf:lastName>
		<foaf:mbox_sha1sum>85a9e0bafbcee5db16a7ead67584c2e0b950f13c</foaf:mbox_sha1sum>
		<foaf:name>Ivana Quinto</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/50"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/josep-grau-valldosera">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Josep Grau-Valldosera</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Josep</foaf:firstName>
		<foaf:lastName>Grau-Valldosera</foaf:lastName>
		<foaf:mbox_sha1sum>46cbf499a706a7e6f72849013f9479d745554cbe</foaf:mbox_sha1sum>
		<foaf:name>Josep Grau-Valldosera</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/61"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/julia-minguillon">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universitat-oberta-de-catalunya--uoc-"/>
		<rdfs:label>Julia Minguillon</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Julia</foaf:firstName>
		<foaf:lastName>Minguillon</foaf:lastName>
		<foaf:mbox_sha1sum>fb7c44ca9aa720af095eed6d07cad60b6a087d22</foaf:mbox_sha1sum>
		<foaf:name>Julia Minguillon</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/61"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carlos-delgado-kloos">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-carlos-iii-of-madrid"/>
		<rdfs:label>Carlos Delgado Kloos</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Carlos</foaf:firstName>
		<foaf:lastName>Delgado Kloos</foaf:lastName>
		<foaf:mbox_sha1sum>1f8adb88df12d259e28c0e7aa8a84511ef753e39</foaf:mbox_sha1sum>
		<foaf:name>Carlos Delgado Kloos</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/49"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/abelardo-pardo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-carlos-iii-of-madrid"/>
		<rdfs:label>Abelardo Pardo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Abelardo</foaf:firstName>
		<foaf:lastName>Pardo</foaf:lastName>
		<foaf:mbox_sha1sum>5d52785ab83243bb537a2acaae36b12fb2abe935</foaf:mbox_sha1sum>
		<foaf:name>Abelardo Pardo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/49"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/susan-bull">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-birmingham"/>
		<rdfs:label>Susan Bull</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Susan</foaf:firstName>
		<foaf:lastName>Bull</foaf:lastName>
		<foaf:mbox_sha1sum>852d1af9892889ea13a1d656148cce6596ca1816</foaf:mbox_sha1sum>
		<foaf:name>Susan Bull</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/60"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/shane-dawson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-british-columbia"/>
		<rdfs:label>Shane Dawson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Shane</foaf:firstName>
		<foaf:lastName>Dawson</foaf:lastName>
		<foaf:mbox_sha1sum>ba08b6fe9285cb6790847f0b206fcb04bf4620f3</foaf:mbox_sha1sum>
		<foaf:name>Shane Dawson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/51"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/62"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/shane-dawson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-british-columbia"/>
		<rdfs:label>Shane Dawson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Shane</foaf:firstName>
		<foaf:lastName>Dawson</foaf:lastName>
		<foaf:mbox_sha1sum>ba08b6fe9285cb6790847f0b206fcb04bf4620f3</foaf:mbox_sha1sum>
		<foaf:name>Shane Dawson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/51"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/62"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/devan-rosen">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-hawaii"/>
		<rdfs:label>Devan Rosen</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Devan</foaf:firstName>
		<foaf:lastName>Rosen</foaf:lastName>
		<foaf:mbox_sha1sum>6e993f134f0924f9f4340a3b3d243f80b2a20cfd</foaf:mbox_sha1sum>
		<foaf:name>Devan Rosen</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/44"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/53"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/devan-rosen">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-hawaii"/>
		<rdfs:label>Devan Rosen</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Devan</foaf:firstName>
		<foaf:lastName>Rosen</foaf:lastName>
		<foaf:mbox_sha1sum>6e993f134f0924f9f4340a3b3d243f80b2a20cfd</foaf:mbox_sha1sum>
		<foaf:name>Devan Rosen</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/44"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/53"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/victor-miagkikh">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-hawaii"/>
		<rdfs:label>Victor Miagkikh</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Victor</foaf:firstName>
		<foaf:lastName>Miagkikh</foaf:lastName>
		<foaf:mbox_sha1sum>3b8def8e9e143a1c9c26c5abf449657758c007cf</foaf:mbox_sha1sum>
		<foaf:name>Victor Miagkikh</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/44"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dan-suthers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-hawaii"/>
		<rdfs:label>Dan Suthers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Dan</foaf:firstName>
		<foaf:lastName>Suthers</foaf:lastName>
		<foaf:mbox_sha1sum>acb5c9152cb21e56d54b1575ef1bfb36618e69ce</foaf:mbox_sha1sum>
		<foaf:name>Dan Suthers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/44"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/53"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dan-suthers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-hawaii"/>
		<rdfs:label>Dan Suthers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Dan</foaf:firstName>
		<foaf:lastName>Suthers</foaf:lastName>
		<foaf:mbox_sha1sum>acb5c9152cb21e56d54b1575ef1bfb36618e69ce</foaf:mbox_sha1sum>
		<foaf:name>Dan Suthers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/44"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/53"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/aneesha-bakharia">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-queensland"/>
		<rdfs:label>Aneesha Bakharia</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Aneesha</foaf:firstName>
		<foaf:lastName>Bakharia</foaf:lastName>
		<foaf:mbox_sha1sum>45bd5bd49243e3dfaf5429a6f89525bd9f0b00d5</foaf:mbox_sha1sum>
		<foaf:name>Aneesha Bakharia</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/51"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carrie-demmans-epp">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-saskatchewan"/>
		<rdfs:label>Carrie Demmans Epp</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Carrie</foaf:firstName>
		<foaf:lastName>Demmans Epp</foaf:lastName>
		<foaf:mbox_sha1sum>904fa665a646f61b87281b62d4e77275d007d6b4</foaf:mbox_sha1sum>
		<foaf:name>Carrie Demmans Epp</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/52"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/greg-logan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-saskatchewan"/>
		<rdfs:label>Greg Logan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Greg</foaf:firstName>
		<foaf:lastName>Logan</foaf:lastName>
		<foaf:mbox_sha1sum>d54d6e56248d7c2dac4fce2d0572215fe23a56d6</foaf:mbox_sha1sum>
		<foaf:name>Greg Logan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/52"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jim-greer">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-saskatchewan"/>
		<rdfs:label>Jim Greer</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Jim</foaf:firstName>
		<foaf:lastName>Greer</foaf:lastName>
		<foaf:mbox_sha1sum>da266e394507fe8b78f3f05dfd2dd5f8c1657cef</foaf:mbox_sha1sum>
		<foaf:name>Jim Greer</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/52"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/christopher-a-brooks">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-saskatchewan"/>
		<rdfs:label>Christopher A. Brooks</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Christopher</foaf:firstName>
		<foaf:lastName>A. Brooks</foaf:lastName>
		<foaf:mbox_sha1sum>83d2ec499449a83cd1952800a1849bbb2118683b</foaf:mbox_sha1sum>
		<foaf:name>Christopher A. Brooks</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/52"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/lori-lockyer">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-wollongong"/>
		<rdfs:label>Lori Lockyer</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Lori</foaf:firstName>
		<foaf:lastName>Lockyer</foaf:lastName>
		<foaf:mbox_sha1sum>d0695ebd3b450791d5c242ea456f72197c31a4f4</foaf:mbox_sha1sum>
		<foaf:name>Lori Lockyer</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/lak2011/paper/62"/>
	</foaf:Person>
</rdf:RDF>