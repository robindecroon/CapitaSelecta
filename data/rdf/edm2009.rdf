<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:dcterms="http://purl.org/dc/terms/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:ical="http://www.w3.org/2002/12/cal/ical#"
    xmlns:swrc="http://swrc.ontoware.org/ontology#"
    xmlns:bibo="http://purl.org/ontology/bibo/"
    xmlns:swc="http://data.semanticweb.org/ns/swc/ontology#"
    xmlns:led="http://data.linkededucation.org/ns/linked-education.rdf#"
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" >
	<swc:ConferenceEvent rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009">
		<swc:completeGraph rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/complete"/>
		<swc:hasAcronym>EDM2009</swc:hasAcronym>
		<swc:hasRelatedDocument rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<rdfs:label>Educational Data Mining 2009</rdfs:label>	<ical:dtend rdf:datatype="http://www.w3.org/2001/XMLSchema#date">0000-00-00</ical:dtend>
		<ical:dtstart rdf:datatype="http://www.w3.org/2001/XMLSchema#date">0000-00-00</ical:dtstart><foaf:homepage rdf:resource=""/>
	</swc:ConferenceEvent>
	<swrc:Proceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings">
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/177"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/178"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/180"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/181"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/182"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/184"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/185"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/186"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/192"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/193"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/195"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/196"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/197"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/199"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/200"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/201"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/203"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/204"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/207"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/208"/>
		<swc:relatedToEvent rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009"/>
		<swrc:booktitle>Proceedings of Educational Data Mining, 2009</swrc:booktitle>
		<swrc:month></swrc:month>
		<swrc:series></swrc:series>
		<swrc:year>2009</swrc:year>
	</swrc:Proceedings>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/177">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Learning Factors Transfer Analysis: Using Learning Curve Analysis to Automatically Generate Domain Models</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/177/authorlist"/>
		<swrc:abstract>This paper describes a novel method to create a quantitative model of an educational content domain of related practice item-types using learning curves. By using a pairwise test to search for the relationships between learning curves for these item-types, we show how the test results in a set of pairwise transfer relationships that can be expressed in a Q-matrix domain model. Creating these Q-matrices for various test criteria we show that the new domain model results in consistently better learning curve fits as shown by cross- validation. Further, the Q-matrices produced can be used by educators or curriculum designers to gain a richer, more integrated perspective on concepts in the domain. The model may also have implications for tracing student knowledge more effectively to sequence practice in tutoring/training software.</swrc:abstract>
		<led:body><![CDATA[ 1.1 POKS and LiFT (Learning Factors Transfer).
 This work is conceptually similar to work with knowledge spaces using the partial order knowledge structure (POKS) method [8]. In both cases we construct a partial order directed acyclic graph using pairwise tests to determine linkages and their directionality. In POKS the relationship between 2 item-types (A and B) is written AB, and allows inferences of the type "if A is known, then B must be known" and "if B is unknown, then A is unknown". In contrast, in the Learning Factors Transfer (LiFT) test a directional relationship is expressed in set notation so an analogous link is written A⊃ B and expresses the inference that the KCs required for A are a proper superset of the KCs required for B. For instance, in the dataset we examine, item-type A might be “What is 1/1 in percent? (Answer: 100%)”, which requires an understanding of whole number fractions and the percent conversion procedure, whereas item-type B is “What is 1/1 in decimal? (Answer: 1)” and only requires the understanding of whole number fractions. Because this example represents the superset relationship, it encodes a situation where learning of item-type B transfers fully to A, but where learning of item-type A transfers only partially to B. This example describes a model with 2 KCs for A and 1 KC for B. Thus, practice of B benefits A partially, while practice of A benefits both A and B. The POKS test is not fully applicable to repeated practice opportunities for a specific item-type because it requires single observations for each item from each subject to compute in standard form (the test assumes that observations are independent). In previous work we showed how it was possible to use the average performance rather than a single observation to compute implication relationships using POKS [9]. While this POKS analysis provided interesting information about the domain, like many previous works describing domain structure, the result necessarily abstracted over learning effects which the LiFT test explicitly analyzes. This inclusion of the effect of learning is a key advantage of our new method, since the LiFT test (assuming it shows transfer between 2 item-types) should therefore allow us to better answer questions such as which item-type should be practiced first and how much it should be practiced before it is optimal to switch to the other. For example, in some cases the quantitative model (Section 2.1) will predict, given A⊃ B, that B should be practiced because initial performance of A will be poor without some practice of B first. In other cases, A might be easy enough or the prior learning of B might be strong enough that A should be practiced first. Additionally, it seems plausible to suggest that if our test function does not include a learning component then a domain search using it will be less accurate when our data contain significant amounts of learning. 
 2 Learning Factors Transfer (LiFT) Analysis.
 LiFT analysis takes the form of a basic pairwise test that establishes the likelihood that any pairwise relationship is better represented as a transfer relationship, or whether it appears the item-types are unrelated. While the results of the pairwise test may be useful for human curriculum designers to consider, the LiFT test can also be employed to mine large datasets with many item-types being learned simultaneously. We will describe how this algorithmic usage can be performed, and compare the model fit for the transfer relationships discovered at various criterion settings with 3 alternative models: a single item/KC model, a independent item/KC model, and a random transfer models with the number of links in the Q’ matrix yoked to the LiFT found transfer models, but placed at random. Because transfer is a within-subject effect, occasionally we will see it occur due to general correlation among item-types caused by latent variables such as motivation, general intelligence, or generally better prior learning. This possibility may be minimized by setting a strict criterion for our transfer test. 
 2.1 PFA Item-type model.
 The model equation we will use here is similar to a model equation that has been recently described and shown to fit better than either the standard logistic regression equation used in LFA (Learning Factor Analysis) or the standard version of Bayesian knowledge tracing [10]. For this paper we have made a slight modification which specifies that prior learning parameters are assigned at the item-type level rather than the KC level. The model equation, which can be referred to as the Item-type PFA equation, is a logistic regression model of performance on each trial that includes a parameter to capture the initial strength for each item-type and 2 parameters that track the learning and performance with each KC. The PFA Item-type equation is shown in Equation 1, where m is a logit value representing the accumulated learning for a student i on one item-type k using one or more KCs j. The easiness of the item-types is captured by the β parameter for each item-type. The effect of learning and performance is captured by s, which tracks the prior successes for the KC for the student and f, which tracks the prior failures for the KC for the student. The 2 parameters γ and ρ scale the effect of these observation counts for each KC as a function of the s or f of prior observations for student i with KC j. Equation 2 is the logistic function used to convert m strength values to predictions of observed probability. It is useful to note that the model always assumes that each item- type has a “base KC” that matches to each item-type and the LiFT test tries to go beyond that base KC to propose other KCs that might be transferred to the item-type to better model performance. 
 FORMULA_(1).
 FORMULA_(2).
 The assignment of KC’s to item-types is described by a Q-matrix which describes which KC’s influence which item-types. To represent an independent component for each item- type we have specified that each KC is matched to a specific item-type. Therefore, our Q- matrix will be a square matrix with item-types as rows and matched KCs as columns. Since every item-type has its matching KC, the diagonal will be all 1s, indicating that each KC is present in its corresponding item-type. To distinguish this kind of Q-matrix (square where every item-type is assigned at least 1 KC) from the larger set of standard Q-matrices, henceforth we will refer to it as the Q’-matrix. 
 2.2 LiFT test.
 The LiFT test takes as input the sequence of practice data for 2 item-types in a tutor (their learning curves) and computes the relative likelihood that they have an overlapping KC by comparing the weights of the likelihood difference of alternative models. In the version we are presenting here, we considered the 2 alternative models, A⊃ B and A~B (where A and B do not share a KC) for each order pair of item-types (thus pair (X, Y) is distinguished from (Y, X) and the test is run on both pairs). While we consider these 2 models, there are other transfer assumptions that might be tested but these are beyond the scope of this paper. The A⊃ B model asserts that the A item-type is controlled by the 2 KCs, while the B item-type is only controlled by 1 KC (the 2x2 Q’-matrix is filled with 1’s on the diagonal and the upper right corner is also filled with a 1 to indicate item-type A shares the same KC as item-type B). In contrast, the A~B model supposes that each item-type is independent (the Q’-matrix is only filled with 1’s on the diagonal since each item-type has a single KC). When this test is computed it determines whether we can get an improvement in model fit by proposing that learning for one item-type transfers to the performance of the other item-type. At the same time as it determines whether the item-types share a KC, the directionality of the test determines which of the two item-types contains an additional independent KC, thus indicating that it contains a superset of the skills required relative to the other item-type. To compute the test, we fit these 2 models (each with 6 parameters) and compared them according to their BIC (Bayesian Information Criterion) weights to determine the evidence ratio in favor of A⊃ B. (Because model complexity was equal, this was equivalent to using AIC weights or likelihood ratio.) This evidence ratio gives us the likelihood of A⊃ B expressed as a probability. This use of BIC weights to compute evidence ratios has been described in detail previously [11]. Because the BIC weight test requires observations to be independent, we minimized BIC using the average loglikelihood for each subject rather than for each observation. This procedure is conservative since it overcompensates for the only partial dependence between observations within a single subject. 
 2.3 LiFT algorithm.
 Many possibilities exist for how this test could be applied to a dataset to determine the relationships between item-types. For this first attempt we did an exhaustive search for pairwise relationships, accepting those BIC weight test results that resulted in improvements above a probability criterion. Acceptance of the result of any pairwise test meant that the superset transfer implication was added to the Q’-matrix. For example, imagine that the criterion is .43 and we test A⊃ B and get a probability value of .32. In this case we have not passed criterion and we do not alter the Q’-matrix row for item-type A. However, if the A⊃ B test arrived at a result of .87 (thus passing the criterion), we would enter a 1 in the Q’-matrix at item-type row A and KC column B. This LiFT test is applied to a multi-item-type dataset according to the following steps. 
 1. Create diagonal matrix with 1’s on the diagonal assuming rows and columns equal the number of item-types. 2. Compute the pairwise test for all non-diagonal entries, entering a 1 in the matrix for any tests that pass. 3. Use the Q’-matrix from step 2 and maximize the likelihood of the entire dataset. 
 Because we wanted to get a perspective on what was an effective criterion, we tested criteria from 0 to 1 in .01 increments. We used 10 fold cross validation of the mean absolute deviation averaged by subject to compare the full models we tested. In applying this algorithm we used the following dataset. 
 3 Data.
 The dataset we used was gathered from a middle school in Florida which uses the Bridge to Algebra Cognitive Tutor by Carnegie Learning Inc. As part of a larger investigation we had supplemented the tutor with 9 problems sets each with 34 item-types. These supplemental units were closely matched to the tutor units that followed them, and future analysis will look at the potential for transfer into the Bridge to Algebra tutor from the supplemental units. For this investigation we choose 1 of these supplemental units (Unit 5, Fraction, Decimal and Percent Conversions) to investigate how our algorithm would work to improve the predictions of the model by filling in the Q’-matrix. The item-types were ideally sequenced for an analysis of this sort since they were randomized into a 4 by 2 within subjects design where there were 4 levels of practice (0, 1, 2 or 4 repetitions) and 2 levels of spacing (3 or 15 intervening trials). These conditions (with a few buffer trials) resulted in total of 64 practices for each supplemental lesson. 361 students produced valid data. The data for each subject took the form of sequential lists of which item-type was practiced and whether the result for that practice was correct or incorrect. 
 4 Results.
 Figure 1 shows the size of the resulting Q’-matrices found by the LiFT algorithm run on Florida dataset. At a criterion of 0, every BIC weight test passes and the matrix is saturated with 1 values. In this case the algorithm fits a model with 34 item-type parameters (β) and 34 γ and ρ parameters which are identical for every KC because they are shared across every item-type (a square Q’ matrix filled with 1s). As the criterion is made more stringent, Figure 1 shows how fewer and fewer links are proposed until at a criterion of 1, none of the BIC weight tests pass, and the algorithm proposes 34 item- types, each with an associated KC, each represented by its own β, γ, and ρ (a diagonal Q’ matrix of 1s). 
 Figure 1. Number of total links and number of bidirectional links A⊃ B found with pairwise BIC weight test. 
 Figure 2 shows 10 fold cross validated estimates of mean absolute probability deviation (averaged by subject) for the model fit at each criterion. For comparison, the 3 alternative models are also presented on this figure. The 1 KC/item model comparison is a 3 parameter model which assumes that all of the 64 practices for each student are actually best modeled as a single item-type with one KC. The no transfer model comparison is a 102 parameter model with 34 KCs/items and is represented by a diagonal Q’-matrix which assigns 1 KC to each item-type (equivalent to criterion = 1). The yoked random control comparison is a model calculated with a random square Q’-matrix yoked to the number of links in the LiFT found Q’-matrix at that criterion, but with those links placed randomly. Note: Just as with the found Q’-matrices, we began with the assumption that each item-type was associated with at least a single KC. The yoked random control is an important comparison since it helps to establish that selection using the LiFT test is causing the advantage seen, rather than it being caused merely by the presence of links in the Q’-matrix. The result shown in Figure 2 establishes that the transfer model (LiFT found Q’) fits the data better than the 1 KC/item, no transfer, or yoked random Q’-matrix models. Further, the cross validated comparison establishes that the result is likely to generalize to similar populations. Of some interest for further research is why the improvement in fit is relatively large even when using an extremely liberal BIC weight test value criterion. It seems likely that averaging of multiple low criterion transfer relations (multiple 1s in a Q’-matrix row) causes a reduction of the error compared to the pairwise test models. 
 Figure 2. 10-fold cross-validated fit of the model and yoked random control across the range of BIC weight test criterion values. 
 4.1 Interpretation of Results.
 In explaining the LiFT test, we provided an example where the two item-types showed an hypothetical subset relationship with one item-type requiring 1 KC and the other requiring that KC plus an additional KC. However, we did not find any clear superset relationships when we looked at the results. In contrast to this theory, our results were more varied but showed several specific patterns. We examined these patterns at a few test value criteria finding quantitative but not qualitative differences. The following description is given for a criterion of 0.6 on the BIC weight test. Group 1 (4 item-types) was the smallest item-type group where item-types were left completely unlinked. Because these item-types were unlinked, the found Q’-matrix models of these item-types (determined by β, γ, and ρ and Equation 1) is identical to the no transfer model of these item-types. This set includes questions that are relatively difficult for most students (a low β parameter). However, these item-types also tend to have high learning and performance parameters (a high γ and ρ). Based on these results we might suppose that the problems are badly worded, tricky, or beyond the level of the average student. For instance, the item-type (instantiated with different numerals in 6 versions that were randomly selected from with replacement) “If we are given that 4% of y is 1, one fraction of our proportion is 4/100 and the other is what? (Answer: 1/y)” was found to be in this category. Compared to other item-types it is wordy and highly complex involving fraction KCs, percent KCs, and proportion solving KCs. Group 2 (15 item-types) was similar to Group 1 in that these item-types did not share their KC component with any other item-types. Unlike Group 1 however, these item- types had between 2 and 10 input KC’s that other item-types shared with them. As we will see, Group 3 provides these inputs KCs. Group 2 item-types are distinguished by being primarily repetition based vocabulary practice item-types. Further, we see less fundamental conversions (“What is 1/1000 in percent? (Answer: 0.1%)” and “What is 10/1 in percent? (Answer: 1000%)”) are included in this group, perhaps because their performance and ability to be learned depends on understanding more fundamental frequency conversions (e.g. 1/10 in decimal), while the converse is less true. Group 3 (15 item-types)  was composed of item-types that were strongly connected with other item-types both by sharing their matched KC with between 2 and 22 item-types and by receiving KC input from between 3 and 11 item-types. Group 3 is composed of item- types that start out at moderately higher βs (logit greater than 0, i.e. >50% initial performance) and have much lower γ and ρ parameters for their matched KCs. These item-types appear to describe 3 conceptual categories that can be distinguished by the dominant sharing of inputs and outputs within each category. Essentially what has happened in each case is that inputs and outputs form loose conceptual categories by sharing that is primarily within category. Category 3a includes two item-types “What is a ratio of the amount of decrease to the original value, written as a percent? (Answer: percent decrease)” and the corresponding question about an increase. This category seems to depend on the general form of the question (identical) which allows direct transfer of the solution pattern. Category 3b is similar, but has to do with 3 questions that required the partial solution of proportions (e.g. “Given the proportion 1/y = 3/4, what is the product of the extremes? (Answer: 4)”) One of these questions also interacted heavily with category 3c. Category 3c can most closely be aligned with a fundamental proficiency in the domain, or with general intelligence, since these item-types all shared their KC with at least 13 item-types and received inputs from at least 8 item-types. While each of these item-types had its own β parameter, this extensive sharing means that learning and performance change for these item-types occurs nearly as a unit. 8 of these item-types involved simple place value problems such as “What digit is in the 10s place in the number 25046.37189? (Answer: 4)“ and “What is 1/10 in decimal? (Answer: 0.1)” Also in this category was a simple pre-algebra problem: “What is y in the equation 3 × y = 9? (Answer: 3) (with 6 versions). Finally, the item-type “If 54 can be completed in 10 hours, what is the amount completed in 1 hour (as a decimal)? (Answer: 5.4)” (with 6 versions) appeared to be part of both 3b and 3c, which seems consistent, since it involves aspects of both proportions and place value (the set of item-types always required a power of ten calculation). The three groups found show that understanding the model has interesting implications for the curriculum designer. First, it appears that item-types in Group 1 need to be improved in some way. While we cannot tell for sure what the problem is, the lack of connections to the other groups establishes that these item-types are either in a different domain, too complex to relate to the more simple item-types in the set, or badly worded so that students cannot transfer in related knowledge. Group 2, items on the other hand receive KC from Group 3 item-types, so we can surmise that they are at least marginally related to the domain. The fact that these questions did not share their KCs might have more to do with the limitations of the question set overall (which did not require much application of these mostly vocabulary-based item-types in other questions) or the model (see Discussion) rather than any specific lack in the items themselves. Finally, Group 3 items are useful to consider because the model here suggests that the 3 conceptual categories (3a, 3b and 3c) are each better modeled as single units rather than as independent skills. Knowing these item-type clusters are closely related allows curriculum designers to have more information as they make curriculum design decisions. 
 5 Discussion.
 In general, the results were a qualified success because the model found produced a better fit that generalized and because the model structure provided other reflections on the content subdivisions in the domain of item-types studied. Despite this, there were problems with the current logistic regression model. Specifically, because of the way the equation uses the Q’-matrix to share KCs as whole units, the parameter magnitudes changed as a function of whether or not each item-type shared KCs with other item-types. Item-types that shared their KC’s (especially when they provided their KC to many other item-types) had lower performance parameter values (γ and ρ) than when they were fit in the no transfer model. The reason for this is simply that when the probability is determined from multiple inputs each input must be scaled down so the total growth still resembles the situation with one KC. However, one unfortunate consequence of this is that the model is then insensitive to repetition effects for a single item-type and therefore predicts much slower growth when the same item-type is repeated. This is because the item-type is no longer being controlled by a single KC, but rather has become tied to a collection of KCs that control it. One solution to this problem in future work may be to take a knowledge decomposition approach that does not insist on this KC sharing through the Q’-matrix [12]. Such an approach might instead propose that the magnitude of transfer for each KC is different depending on what item-type the KC transfers to. While this would add parameters to our model, it might also greatly improve the fit of the model. This work may apply directly to the educational problem of sequencing item-types to maximize learning because the resulting model captures learning, is adaptive to performance, and captures the domain structure together in a single model. Unlike other automatically determined domain models, which determine performance dependencies and might be used for ordering practice, our model explicitly tracks learning also. By adding learning to our domain model, our model has the potential to answer not just the question of what item-type is best next, but also the question of how much more should the current item-type be practiced. 
 Acknowledgements.
 This research was supported by the U.S. Department of Education (IES-NCSER) #R305B070487 and was also made possible with the assistance and funding of Carnegie Learning Inc., the Pittsburgh Science of Learning Center and DataShop team (NSF-SBE) #0354420, and Ronald Zdrojkowski.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Learning Factors Transfer Analysis: Using Learning Curve Analysis to Automatically Generate Domain Models</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hao-cen"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hao-cen"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/177/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/hao-cen"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/178">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Improving Student Question Classification</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/178/authorlist"/>
		<swrc:abstract>Students in introductory programming classes often articulate their questions and information needs incompletely.  Consequently, the automatic classification of student questions to provide automated tutorial responses is a challenging problem.  This paper analyzes 411 questions from an introductory Java programming course by reducing the natural language of the questions to a vector space, and then utilizing cosine similarity to identify similar previous questions.  We report classification accuracies between 23% and 55%, obtaining substantial improvements by exploiting domain knowledge (compiler error messages) and educational context (assignment name).   Our mean reciprocal rank scores are comparable to and arguably better than most scores reported in a major information retrieval competition, even though our dataset consists of questions asked by students that are difficult to classify.  Our results are especially timely and relevant for online courses where students are completing the same set of assignments asynchronously and access to staff is limited.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Improving Student Question Classification</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/cecily-heiner"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/cecily-heiner"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-l-zachary"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-l-zachary"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/178/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/cecily-heiner"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-l-zachary"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Using Dirichlet priors to improve model parameter plausibility</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179/authorlist"/>
		<swrc:abstract>Student modeling is a widely used approach to make inference about a student’s attributes like knowledge, learning, etc. If we wish to use these models to analyze and better understand student learning there are two problems.  First, a model’s ability to predict student performance is at best weakly related to the accuracy of any one of its parameters.  Second, a commonly used student modeling technique, knowledge tracing, suffers from having multiple sets of parameters providing equally good model fits.  Furthermore, common methods for estimating parameters, including conjugate gradient descent and expectation maximization, suffer from finding local maxima that are heavily dependent on their starting values.  We propose a technique that estimates Dirichlet priors directly from the data, and show that using those priors produces model parameters that provide a more plausible picture of student knowledge. Although plausibility is difficult to quantify, we employed external measures to show the parameter estimates were indeed improved, even if our model did not predict student behavior any more accurately.</swrc:abstract>
		<led:body><![CDATA[ 1.1 Knowledge tracing model.
 Knowledge tracing [2], shown in Figure 1, is an approach for taking student observations and using those to estimate the student’s level of knowledge. There are two parameters slip and guess, which mediate student knowledge and student performance. These two parameters are called the performance parameters in the model. An assumption of the model is that even if a student knows a skill, there is a chance he might still respond incorrectly to a question that utilizes that skill. This probability is the slip parameter. There are a variety of reasons for an incorrect response, for example, the student could have made a simple typo (e.g. typed ‘12’ instead of ‘21’ for “7 x 3”). 
 Figure 1.  Knowledge tracing model. 
 Conversely, a student who does not know the skill might still be able to generate a correct response. This probability is referred to as the guess parameter. A guess could occur either through blind chance (e.g. in a 4- choice multiple choice test there is a ¼ chance of getting a question right even if one does not understand it), or the student being able to utilize a weaker version of the correct rule that only applies in certain circumstances. In addition to the two performance parameters, there are two learning parameters. The first is prior knowledge (K0), the likelihood the student knows the skill when he first uses the tutor. The second learning parameter is learning, the probability a student will acquire a skill as a result of an opportunity to practice it. Every skill to be tracked has these four parameters, slip, guess, K0, and learning, associated with it. 
 1.2 The problem.
 One issue is how to estimate the model parameters.  One approach is to use the expectation maximization (EM) algorithm to find parameters that maximize the data likelihood (i.e. the probability of observing our student performance data).  However, in EM, we have to start with some initial value of the parameter, and final parameter estimations are sensitive to those initial values.  Furthermore, one flaw of a knowledge tracing model is that it has multiple global maxima. That is to say, there can be more than one set of learning/performance parameters that fit the data equally well. Consider the three sets of hypothetical knowledge tracing parameters shown in Table 1, the knowledge model reflects a set of model parameters where students rarely guess. The guess model assumes that 30% of correct responses are due to randomness. This limit of 30% is the maximum allowed in the knowledge tracing code used by the Cognitive Tutors [2]. The third model has parameters similar to data from Project Listen’s Reading Tutor [3]. 
 By using the four parameters and the knowledge tracing equations, we can compute the theoretic learning and performance curves for each model. Specifically, we initialize P(know) to be K0.  After each practice opportunity, we use formula I to update P(know) as the new likelihood of the student knows the skill after the previous practice. Also we compute P(correct), the probability of the student will respond correctly in the current practice opportunity, by using the knowledge tracing formula to combine the estimated knowledge with the slip and guess parameters shown in formula II. 
 FORMULA_II.
 For example, the knowledge model’s prior knowledge (K0) is 0.56. At the second practice opportunity the knowledge model would have a P(know) of 0.56 + (1 – 0.56) * 0.1 = 0.604.  Furthermore, the likelihood for the student making a correct response would be 0.604 *(1-0.05) + (1-0.604) * 0.00 = 0.574.  As seen in Figure 2, the three models have identical student performance (in the left graph), but their estimates of student knowledge (right graph in Figure 2) are very different. 
 Table 1.  Parameters for three hypothetical knowledge tracing models. 
 Given the same set of performance data, we have presented three knowledge tracing models that fit the data equally well, i.e. all three sets of estimated parameters have equally good predictive power. Unfortunately, for drawing conclusions about student learning, they make very different claims. Statistically there is no justification for preferring one model over the others, since all three of the sets of parameters fit the observed data equally well. This problem of multiple (differing) sets of parameter values that make identical predictions is known as identifiability [4]. 
 1.3 Proposed solution: Dirichlet priors.
 Dirichlet prior is an approach used to initialize conditional probability tables when training a Dynamic Bayesian network. Dirichlet distributions are specified by a pair of numbers (α,β).  Figure 3 shows an example (the dashed line) of the Dirichlet distribution for (9,6).  If this sample distribution were of K0, it would suggest that few skills have particularly high or low knowledge, and we expect students to have a moderate probability of mastering most skills. Conceptually, one can think of the conditional probability table of the graphical model being as seeded with 9 instances of the student knowing the skill initially and 6 instances of him not. If there is substantial training data, the parameter estimation procedure is willing to move away from an estimate of 0.6. If there are few observations, the priors dominate the process. The distribution has a mean of α/(α+β). Note that if both α and β increase, as in the solid curve in Figure 3, the mean of the distribution is unchanged (since both numerator and denominator are multiplied by 3) but the variance is reduced. Thus, Dirichlets enable researchers to not only specify the most likely value for a parameter but the confidence in the estimate. 
 Figure 2 performance & learning curve. 
 Figure 3.  Sample Dirichlet Distributions demonstrating decreasing variance. 
 Dirichlets provide bias towards the mean of the distribution.  Since we estimated a set of parameters for each skill, for models with few training data, the parameter estimates can get wacky, since sparse data provide few constraints on the parameters. Hence, those parameters are sometimes estimated as extreme values. In this situation, we prefer to have parameters which are more similar to other, better-estimated, skills. With Dirichlet priors, the observations for each case are weighted against prior α, β values, i.e. models with few data are more influenced by the priors towards the mean. Therefore, we expect those estimates will be become more reasonable. It is important to note that researchers can use Dirichlets to set confidence on priors. If the variance is less, we are surer about the priors, whereas if the variance is high, we are less sure about the priors.  Each of the four parameters will not only have different mean values, but different degrees of certainty.  Suppose, in a group of students if they start with similar incoming knowledge but have variable learning. Then Dirichlet prior will set higher confidence in students’ prior knowledge (e.g.: α, β = 20, 34) but lower confidence in students’ learning (e.g.: α, β = 1, 4). As a result, prior knowledge parameter estimation will be more biased towards prior or distribution’s mean whereas learning will have more tendency to move away from prior value. 
 2 Methodology.
 There are several sources of setting Dirichlet prior values.  One approach is using knowledge of the domain [e.g. 4]. If someone knows how quickly students tend to master a skill or the likelihood of knowing a skill, that knowledge can be used to set the priors. One complaint is that such an approach is not necessarily replicable as for different domains and different subjects, different experts may give different answers. 
 2.1 An automatic approach for selecting priors.
 To compare estimations from fixed and Dirichlet prior models, we trained two KT models initialized with fixed and with Dirichlet priors. We used the following approach: 
 1. Initialize EM with fixed priors from our rough estimates of the domain. Then use EM to estimate the model parameters for each skill in the domain 2. For all four parameters (guess, slip, K0, learning) • Compute the mean (μ) and variance (σ2) of the parameter estimates  • Weight the mean and variance by the number of cases (n) of each skill.  Specifically, for each parameter P of skill i,  • weighti =   √ni  • μ’ = ∑ Pi * weighti /∑ weight  • σ2’ = ∑ weighti * (Pi – μp)2/∑ weight  • Select α and β to generate a Dirichlet with the same mean and variance as the estimates  Specifically, solve for α and β such that:  • α  =  (μ’2 / σ2’) * (1‐ μ’) ‐ μ’  • β = α *((1/ μ’)‐1)  
 3. We now have one Dirichlet distribution described by (α, β) for each of the four parameters 4. Reestimate two kinds of knowledge tracing models: a fixed prior model with initial value of µ' and Dirichlet prior model using the (α, β) pairs. 
 We calculated the mean and variance of the data. Based on those two values, we calculated α, β parameters (using the equations in step #2).  However, simply calculating the mean gives all data points equal weight.  This can be problematic, since as we mentioned earlier, skills with few cases are susceptible to error: going to extreme values such as getting 0 as student’s learning parameter.  Therefore, we weight each estimate by the square root of the number of cases used to generate the estimate, since √N is how the standard error decreases. 
 2.2 Iterating the algorithm.
 Rather than just stopping after step #4, it is possible to loop back to step #2.  We were interested to see how the parameter estimates change by iterating the algorithm with new prior values. We ran a number of iterations on both fixed and Dirichlet prior. 
 Table 2.  Results of iterating automatic process approach for K0 and slip parameters. 
 As shown from Table 2, the parameters do not change much across iterations, although the variance decreases.  The amount of bias towards the mean is proportional to how large α and β are, which is inversely related to the population variance. That is, if the population has a high variance then there is a small bias. Conversely, if a parameter value is already tightly clustered, there will be a strong bias towards the mean.  Therefore, at each iteration estimates will move towards the mean, and the values of α, β will increase. We discuss this problem further in the future work section. 
 3 Validating the models.
 For this study, we used data from ASSISTment, a web-based math tutoring system. The data are from 199 twelve- through fourteen- year old 8th grade students in urban school districts of the Northeast United States.  They were from two classes, each of which only lasted one month. These data consisted of 92,319 log records of ASSISTment during January 2009 to February 2009. Performance records of each student were logged across time slices for 106 skills (e.g. area of polygons, Venn diagram, division, etc). We split our data into training set and test set with the proportion of 2:1. Using our approach, we ran the fixed prior model and the Dirichlet prior model for a number of successive iterations and compared their predictive accuracy and parameter plausibility. 
 3.1 Predictive Accuracy.
 Predictive accuracy is the measure of how well the instantiated model fits the data. We used two metrics to examine the model performance on test set: AUC (Area Under Curve) and Summed Squared Error (SSE). As seen in We also computed the SSE = Σ (observed performance – P (correct)) 2. We found the first iteration of the Dirichlet prior model shows a slightly better, but not meaningfully better SSE than the first iteration of fixed prior model: 8008 vs. 8016.  With more iteration, SSE marginally decreases for fixed prior whereas it increases in Dirichlet. Table 3, the AUC values don’t show any difference in performance of fixed prior model and Dirichlet prior model. The values remain unchanged even for successive iterations. We also computed the SSE = Σ (observed performance – P (correct)) 2. We found the first iteration of the Dirichlet prior model shows a slightly better, but not meaningfully better SSE than the first iteration of fixed prior model: 8008 vs. 8016.  With more iteration, SSE marginally decreases for fixed prior whereas it increases in Dirichlet. 
 Table 3. Comparison of SSE and AUC. 
 These results show that predictive accuracy is not meaningfully better with Dirichlet priors and the accuracy does not seem to be improving with successive iterations. 
 3.2 Parameter plausibility.
 Predictive accuracy is a desired property, but EDM is also about interpreting models to make scientific claims. Therefore, we prefer models with more plausible parameters when we want to use those for scientific study.  Unfortunately, quantifying parameter plausibility is difficult since there are no well-established means of evaluation.  In our study, we explored two metrics for this analysis. For our first metric, we inspected the number of practice opportunities required to master each skill in the domain. We assume that skills in the curriculum are designed to neither be so easy to be mastered in three or fewer opportunities nor too hard as to take more than 50 opportunities. We define mastery as the same way as was done for the mastery learning criterion in the LISP tutor [5]: students have mastered a skill if their estimated knowledge is greater than 0.95. Based on students’ prior knowledge and learning parameters and knowledge tracing equations described before, we calculated the number of  practice opportunities required until the predicted value of P(know) exceeds 0.95. Then, we compared the number of skills with unreliable extreme values in both cases (fewer than 3 and more than 50). As seen in Table 4, fixed priors result in more extreme cases than Dirichlet priors. This result implies that Dirichlet prior model estimates more plausible parameters. . With more iteration, the extreme cases remain constant with fixed prior whereas the number slightly decreases with Dirichlet priors. The skills that are found implausible by Dirichlet are a subset of those found by fixed priors. Hence, Dirichlet is fixing the implausibility of fixed priors and is not introducing new problems of its own. Along with this method, we had tried to make an evaluation based on the correlation between estimated the model’s K0 and the skill difficulty. We consulted two domain experts to rate skill difficulties. But their ratings were not consistent (correlation <0.4) with each other and so we abandoned this approach. 
 Table 4.  Comparison of extreme number of practice until mastery. 
 Next, we tried to model students instead of skills since we it is easier to objectively rate characteristics of students rather than skills.  We trained KT model per student by observing his responses in all questions across skills. The model then estimated a set of parameters (prior knowledge, guess, slip and learning) for each student (rather than for each skill) which represents his aggregate performance across all skills. The students in our study had taken a 33-item algebra pre-test just before using the tutor. Taking the pre-test as external measure of incoming knowledge, we calculated the correlation between students’ prior knowledge (K0) as estimated by KT models and their pretest scores. In Table 5, we can see that the Dirichlet prior model produces slightly stronger, but not reliably so, correlations than the fixed prior. Neither method improves with more iterations. 
 Table 5  Comparison of correlation between prior knowledge and pretest. 
 4 Contributions.
 This paper extends prior work in automatically generating Dirichlet priors [6] in several ways.  First, this study has been scaled up both in terms of more students and more skills. Prior work found a small positive, but non-reliable, gain in predictive accuracy from using Dirichlets.  This paper provides evidence that the improvement was illusory.  We have also improved the estimation of the α and β parameters by weighting the parameter estimates by the number of observations  we have for the skill.  In this way we reduce the effect of skills that only have few estimates of skewing the mean and increasing the variance. This paper also presents a new method for evaluating student models for parameter plausibility.  Although prior work [4,6] in this area proposed and used a variety of metrics, there is still a need for additional methods.  Our new method was to essentially swap the knowledge tracing problem, and estimate a set of model parameters for the students rather than the skills.  We then correlated the K0 parameter for each student with his pretest score.  There are many ways of estimating how much knowledge students have, and many research efforts will have approaches for doing this.  Therefore, we expect this technique to have broad applicability. Finally, we are able to extend the result that EM produces more predictive models than Conjugate Gradient Descent [8], the approach used to estimate parameter in the CMU cognitive tutors.  We are now able to say that EM + Dirichlet priors is better than EM alone.  Using Dirichlets we are not able to predict student behavior any better, but the parameters are generally more plausible than with fixed priors. 
 5 Future work and Conclusions.
 There are several interesting open issues regarding the estimation of Dirichlet priors. First, our method of weighting the parameter estimation process by √N, although inspired by the relative standard error of each skill’s parameters, could use more theoretic grounding.  Second, neither the current nor past attempt [6] at automatically extracting α and β values from the data have shown improvements in model predictive performance. However, the single attempt at human-generated Dirichlet priors [4] did show such gains. Perhaps people have useful knowledge to bring to bear on this task?  Some means of incorporating human experts, and perhaps combining their insight with computer- suggested priors could be a positive step. The notion of iterating our process of fitting the data, estimating α and β, and refitting the data seems like it should work, and was in fact inspired by the expectation maximization recipe.  That it did not work was something of a disappointment, but we think we understand why:  at each iteration the population variance shrinks, increasing α and β, which further shrinks the population variance on the next iteration.  We need some mechanism of preventing α and β from increasing arbitrarily high, or some better metric that suggests what a “good” value of those parameters would look like. Finally, the assumption that we can estimate the shape of the Dirichlet distribution from which the parameters were drawn is certainly more relaxed than the standard assumption that we can correctly estimate the parameter values for each skill, however it is still somewhat naïve.  For example, consider the initial knowledge of a skill.  It is plausible that some skills will not have been covered in class by the students:  those skills could be described by a Dirichlet with a low average.  Other skills, that were covered in class, could be well described by a Dirichlet with a high average.  There is no single distribution that would handle both cases.  Therefore, it might be productive to consider mixtures of Dirichlets. This paper has shown that automatically generated Dirichlets are a method for generating more plausible parameters.  We found that, with Dirichlets, fewer skills were estimated to require too many or too few practice opportunities to master.  We have also introduced a new evaluation technique for evaluating parameter plausibility, and expect this technique to be widely applicable. 
 Acknowledgements.
 We would like to thank all of the people associated with creating the ASSISTment system listed at www.ASSISTment.org.  We would also like to acknowledge funding from the National Science Foundation, the Fulbright Program for funding the first author and the US Department of Education and the Office of Naval Research for funding the second and third authors.  All of the opinions expressed in this paper are those solely of the authors and not those of our funding organizations.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Using Dirichlet priors to improve model parameter plausibility</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/180">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Differences Between Intelligent Tutor Lessons, and the Choice to Go Off-Task</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/180/authorlist"/>
		<swrc:abstract>Recent research has suggested that differences between intelligent tutor lessons predict a large amount of the variance in the prevalence of gaming the system [4]. Within this paper, we investigate whether such differences also predict how much students choose to go off-task, and if so, which differences predict how much off-task behavior will occur. We utilize an enumeration of the differences between intelligent tutor lessons, the Cognitive Tutor Lesson Variation Space</swrc:abstract>
		<led:body><![CDATA[ 1.1 (CTLVS1.1), to identify 79 differences between tutor lessons, within 20 lessons from an intelligent tutoring system for Algebra. We utilize a machine-learned detector of off-task behavior to predict 58 students’ off-task behavior within that tutor, in each lesson. Surprisingly, the best model predicting off-task behavior from lesson features contains only one feature: lessons that involve equation-solving. We discuss possible explanations for this finding, and further studies that could shed light on this relationship. 
 1 Introduction.
 What underlies students’ choices, while they use educational software? In particular, why do students choose to game the system or go off-task, while using educational software? Much of the research on these questions has focused on the role that stable or semi-stable student individual differences play in driving these types of behaviors [2, 3, 8, 9]. Take, for example, the case of gaming the system (“attempting to succeed in an interactive learning environment by exploiting properties of the system rather than by learning the material” [cf. 5]). Several studies have been published that attempt to explain gaming behavior in terms of stable or semi-stable individual differences between students, such as a student’s attitude towards mathematics or goal orientation [2, 8, 9]. These studies have generally found statistically significant relationships. However, the relationships found in these studies only explain 5-9% of the variance in gaming behavior (r2 = 0.05 to 0.09) [2,8], a relatively low degree of explanatory power. By contrast, [7] found that the differences between intelligent tutor lessons predict a large proportion of the variance in gaming behavior. In an analysis of 58 students’ behavior within 20 lessons in an intelligent tutor for algebra (corresponding to the majority of a year’s curriculum), a combination of features of tutor lessons was found to predict 56% of the variance in gaming behavior (r2 = 0.56). In particular, lessons that incorporated interest-increasing text into problem scenarios had significantly less gaming; lessons with various types of ambiguity had more gaming; lessons with ineffective hints had more gaming; and lessons based on equation-solving had less gaming. These results suggest that it may be possible to bypass the intrusiveness and high development costs of interactive responses to gaming [cf. 1, 4, 22] simply by altering these features of lessons, designing lessons with less extraneous ambiguity and more attempts to increase student interest. The discovery that gaming the system can be well predicted by small-scale differences in educational software design raises the question of whether other prominent learner behaviors are similarly associated with small-scale features of software design.  In this paper, we investigate whether small-scale differences in software design can predict variance in off-task behavior. Off-task behavior shares many characteristics with gaming behavior. Both behaviors have been found to be associated with poorer learning in intelligent tutoring systems, although gaming the system’s impact on learning is both larger and more immediate [6, 11]. Additionally, the two behaviors have each been found to be weakly associated with some of the same student individual differences [3], in particular negative attitudes towards computers and mathematics. In this study, we apply a previously validated detector of off-task behavior [3] to data obtained from the PSLC DataShop [15], representing an entire school year of use of Cognitive Tutor Algebra, a widely used intelligent tutoring system. During the school year, students worked through a variety of lessons on different topics. These lessons had moderate variation in subject matter and considerable variation in design, making it possible to observe which differences in subject matter and/or design are associated with differences in how much off-task behavior occurs. We apply an existing taxonomy of the differences between tutor lessons [7] to these lessons, and investigate which lesson features are most strongly associated with off-task behavior. 
 2 Data and Models Applied.
 Data was obtained from the PSLC DataShop [15] (dataset: Algebra I 2005-2006 Hampton Only), for 58 students’ use of Cognitive Tutor Algebra during an entire school year. The data set was composed of approximately 437,000 student transactions (entering an answer or requesting help) in the tutor software. All of the students were enrolled in algebra classes in one high school in the Pittsburgh suburbs. The school used Cognitive Tutors two days a week, as part of its regular mathematics curriculum. None of the classes were composed predominantly of gifted or special needs students. The students were in the 9th and 10th grades (approximately 14-16 years old). The Cognitive Tutor Algebra curriculum involves 32 lessons, covering a complete selection of topics in algebra, including formulating expressions for word problems, equation solving, and algebraic function graphing. Three lessons from Cognitive Tutor Algebra are shown in Figure 1. Data from 8 lessons was eliminated from consideration, as taxonomy codings were not available for those lessons (these lessons were not coded in [7], due to having limited data from those lessons available for that paper’s analyses of interest). On average, each student completed 10.7 tutor lessons (among the set of 24 lessons considered), for a total of 619 student/lesson pairs. 
 Figure 1.  Three lessons from Cognitive Tutor Algebra. Top: The Equation-Solver. Middle: Story Problem with Worksheet. Bottom: Function Graphing. 
 To determine how often each student was off-task, in each lesson, each student’s actions were labeled using Baker’s [3] detector of off-task behavior. The detector was developed using data from 429 students’ classroom use of three lessons from an intelligent tutor on middle school mathematics. Applying this detector makes it tractable to study off-task behavior across a wide variety of tutor lessons. By contrast, other well-known methods are intractable – for instance, conducting quantitative field observations on a similar number of tutor lesssons and students would involve sending out two or more research assistants to classrooms for an entire year. The detector, under cross-validation, achieved a correlation of 0.55 to field observations of off-task behavior – hence, it can be considered reasonably reliable for these purposes. The detector is also able to distinguish off-task behavior from on-task conversation, by looking at the student actions that occur immediately before and after a seemingly idle pause. We show the model that predicts off-task behavior within the detector in Table 1. The detector makes a prediction as to whether each action is off-task, and then aggregates across actions to indicate what proportion of student actions was off-task (or, alternatively, what proportion of student time was off-task). Full details on this detector are available in [3]. Two features (F3 and F6) involved features that were not available for this data set (string and generally-known). However, F3 and F6 together accounted for only 4.4% of the cross-validated correlation accounted for by this model [3] – hence, this model can still be expected to be accurate even in the absence of these features. 
 Table 1.  The model of off-task behavior (OT) used in this paper, from [3]. In all cases, param1 is multiplied by param2, and then multiplied by value. Then the six features are added together. If the sum is greater than 0.5, the action is considered to be off-task. Features that were not applicable to the current data set are indicated in gray. “Pknowretro”, a feature found in many behavior detectors, refers to the probability the student knew the skill if the action was the first opportunity to practice the current skill on the current problem step, and is -1 otherwise. 
 Table 2. The 79 features of the Cognitive Tutor Lesson Variation Space (CTLVS1.1) used in study. Features captured using data mining methods (as opposed to hand-coding) marked with *. 
 Each tutor lesson’s attributes was represented using the Cognitive Tutor Lesson Variation Space version 1.1 (CTLVS1.1) [7], an enumeration of how Cognitive Tutor lessons can differ from one another. The CTLVS1.1 was developed by a diverse design team, including cognitive psychologists, educational designers, a mathematics teacher, and EDM researchers. The CTLVS1.1, shown in Table 2, consists of 79 features for how cognitive tutors differ from each other. The CTLVS1.1 was labeled with reference to the 24 lessons studied in this paper by a combination of educational data mining and hand- coding by the educational designer and mathematics teacher. 
 3 Analysis Methods and Results.
 The goal of our analyses was to determine how well each difference in lesson features predicts how much students will go off-task in a specific lesson. To this end, we combined the labels of the CTLVS1.1 features for each of the 22 lessons in Cognitive Tutor Algebra, and the assessments of how often each of the 58 students in the data set were off-task in each of the 22 lessons. Our first step in conducting the analysis was to determine if the 79 features of the CTLVS1.1 grouped into a smaller set of factors. We empirically grouped the 79 features of the CTLVS1.1 into 6 factors, using the implementation of Principal Component Analysis (PCA) given in SPSS. These same 6 factors were previously successful in discovering a factor that was statistically significantly associated with gaming the system [7]. We analyzed whether the correlation between any of these 6 factors and the frequency of off-task behavior was significant. However, none of the factors was statistically significantly associated with off-task behavior – the closest factor to significance had F(1,21)= 0.37, p=0.55. Taking the 79 features individually, only two were found to be statistically significantly associated with the choice to go off-task. Using an (overly conservative) Bonferroni adjustment [20] to control for the number of statistical tests conducted, only one feature was still found to be statistically significant. This feature was whether the lesson was an equation-solver lesson (as opposed to other types of lessons, such as story problems). An equation-solver lesson is shown at the top of Figure 1. Students were statistically significantly less likely to go off-task within equation-solver lessons, r2 = 0.55, F(1, 21)=27.29, p<0.001, Bonferroni adjusted p<0.001. To put this relationship into better context, we can look at the proportion of time students spent off-task in equation-solver lessons as compared to other lessons. On average, students spent 4.4% of their time off-task within the equation-solver lessons, much lower than is generally seen in intelligent tutor classrooms [5,6] or, for that matter, in traditional classrooms [cf.17, 18]. By contrast, students spent 14.1% of their time off-task within the other lessons, a proportion of time-on-task which is much more in line with previous observations. The difference in time spent per type of lesson is, as would be expected, statistically significant, t(22)=4.48, p<0.001. The other feature found to be statistically significantly associated with off-task behavior, prior to the Bonferroni adjustment, was the proportion of hints that are solely bottom-out hints (more bottom-out-only-hints, less off-task behavior). However, a model including both of these two features was not statistically significantly better than the model that only considered whether the lesson was an equation-solver lesson, F(1, 21)=0.73, p=0.40. 
 4 Discussion and Conclusions.
 The results found here suggest that differences between lessons explain a large proportion of the variance in how much off-task behavior occurs, just as with gaming the system. However, the nature of the models found is quite different. Whereas the model that best explains how much gaming occurs was a complex set of fine-grained features [7], the model that best explains off-task behavior consists of a single, very coarse-grained difference. This leaves us with a problem of interpretation. Why were students off-task so much less within these equation-solver lessons? One hypothesis is that there is some combination of features distinct to equation-solver lessons that produce less off-task behavior, but only when the full combination is encountered. For example, it is possible that the combination of features found in the equation-solver lessons (such as less complex hints, in combination with direct interaction with the equations, in problems that are generally shorter), combine to produce a state of very positive continued engagement (e.g. flow [13]) that precludes off- task behavior.  It may be that this positive engagement is promoted by a specific combination of features only found in these lessons, explaining why off-task behavior was not associated with any of the finer-grained features in the CTLVS1.1, once the coarser feature of whether the lesson used the equation-solver was included. Relatedly, it might be that the task of equation-solving is somehow more engaging, in and of itself, than other mathematical problem-solving tasks, leading students to engage in a lower degree of off-task behavior. A second hypothesis is that teacher behavior causes the lower off-task behavior within the equation-solver lessons. A conversation with a colleague with school teaching experience indicated that teachers in the United States are often particularly worried about students’ performance on equation-solving on state standardized exams (personal communication, L.A. Sudol). This concern may lead teachers to monitor a student more closely, if the student is working through an equation-solver lesson. This hypothesis could be tested through observing teachers’ behavior with quantitative field observations [cf. 5], as students use either equation-solver lessons or other lessons. It is worth noting that this hypothesis may also help explain the lower incidence of gaming the system in equation-solving lessons [e.g. 7]. Determining which of these hypotheses best explains the lower incidence of off-task behavior in equation-solver lessons has the potential to help us understand this behavior better. In turn, this knowledge has the potential to aid us in developing learning software that students engage with to a greater degree. In doing so, it is essential to avoid decreasing off-task behavior in ways that could increase the prevalence of other behaviors associated with poorer learning, such as gaming the system. It is also essential to avoid reducing off-task behavior in ways that would make instruction generally less effective – a potential danger in many visions of educational games in the classroom. More broadly, we believe that the methods used in this paper point to new opportunities for the field of educational data mining. The creation of taxonomies such as the CTLVS1.1 will enable an increasing number of data mining analyses about how differences in educational software concretely influence student behavior. In turn, these analyses can inform a deeper scientific understanding of the interactions between students and educational software. Acknowledgements The author would like to thank Leigh Ann Sudol, Kenneth R. Koedinger, Vincent Aleven, and Albert Corbett for very helpful comments and suggestions. This work was funded by NSF grant REC-043779 to “IERI: Learning-Oriented Dialogs in Cognitive Tutors: Toward a Scalable Solution to Performance Orientation”, and by the Pittsburgh Science of Learning Center, National Science Foundation award SBE-0354420.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Differences Between Intelligent Tutor Lessons, and the Choice to Go Off-Task</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/180/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/181">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Unsupervised MDP Value Selection for Automating ITS Capabilities</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/181/authorlist"/>
		<swrc:abstract>We seek to simplify the creation of intelligent tutors by using student data acquired from standard computer aided instruction (CAI) in conjunction with educational data mining methods to automatically generate adaptive hints. In our previous work, we have automatically generated hints for logic tutoring by constructing a Markov Decision Process (MDP) that holds and rates historical student work for automatic selection of the best prior cases for hint generation. This method has promise for domain-independent use, but requires that correct solutions be assigned high positive values by the CAI or an expert. In this research we propose a novel method for assigning prior values to student work that depends only on frequency of occurrence for the component steps, and compare how these values impact automatic hint generation when compared to our MDP approach. Our results show that the utility metric outperforms a classic MDP solution in selecting hints in logic. We believe this method will be particularly useful for automatic hint generation for ill-defined domains.</swrc:abstract>
		<led:body><![CDATA[ 1. We extracted 537 of students’ first attempts at direct solutions to proof 1. An example attempt of proof 1 is shown in Figure 1. 
 Figure 1. Sample student attempt to NCSU Proof. 
 The data were validated by hand, by extracting all statements generated by students, and removing those that 1) were false or unjustifiable, or 2) were of improper format. We also remove all student steps using axioms Conjunction, Double Negation, and Commutative, since students are allowed to skip these steps in the tutorial. After cleaning the data, there were 523 attempts at proof 1.  Of these, 381 (73%) were complete and 142 (27%) were partial proofs, indicating that most students completed the proof. The average lengths, including errors, were 13 and 10 steps, respectively, for completed and partial proofs. When excluding errors and removed steps, the average number of lines in each student proof is 6.3 steps. The validation process took about 2 hours for an experienced instructor, and could be automated using the existing truth and syntax-checking program in our tutorial.  We realized that on rare occasions, errors are not properly detected in the tutorial (less than 10 premises were removed). 
 Table 1. Sample states derived from example student attempt in Figure 1. 
 An MDP was created from this data using our MDP method resulting in 821 unique states. Table 1 shows the states created in our MDP for the student attempt shown in Figure 1. In the logic proofs domain, a step in the solution is considered to be a new statement added to the previous state. For example, in state 2, the statement ~a v d is the next “step” in the problem, however, since it is an error detected by the software, this statement is deleted and the problem is returned to state 1. 
 4.2 Utility Process.
 If our data are labeled, we simply connect all valid solutions to a synthetic goal state. However, when goal states are unknown, we need a way to label or measure correct attempts. Our proposed utility metric is one way that assumes that frequent features are important in the problem solution. From our 523 attempts, we extracted 50 unique statements (including 3 given statements) and calculated their frequencies. A partial sample of the statement-attempts matrix is shown in Table 2. Note that only the first three attempts and only those statements appearing in those three attempts are shown. The complete statements-attempts matrix would contain all 50 statements in rows and all 523 attempts in the columns. To determine statement frequency, we sum each column. 
 Table 2. Sample matrix showing the occurrence of elements in student solution attempts. 
 We then graphed the frequency of each statement, and the frequencies of statements (number 1-47) with more than 1 usage are shown in Figure 2. Statements 1-22 occurred only once in the data, while statements 43-47 occur in over 370 unique student attempts. Since there is variation in correct solutions, we set a low threshold frequency of 8 attempts for statements we might consider “useful” in a proof, and this is true for statements 29-47 and higher. A logic instructor verified that all the statements 29-47could be expected to occur in correct student solutions, while those with fewer were not as useful. The threshold value could be chosen automatically using the frequency profile. 
 Figure 2. Frequency of Statements in Proof. 
 Next we calculate the initial values for MDP states. For the possible goal states (valid terminal states), the initial value was a sum of the individual scores given to the component statements. Each statement score was +5 if its frequency was above the threshold and was -1 for those below. Error states received a value of -2, and all other states started at zero. Finally, after the initial values were set we ran a value iteration algorithm until the state values stabilized. Note that during value iteration, a -1 transaction cost was associated with each action taken. 
 4.3 Comparing Utility Method to MDP Method.
 We use an MDP along with its state values to generate hints that provide students with details of the best next state reachable from their current state [3]. To compare the utility method to our traditional MDP method we compared the effects of state values on the choice of the “best” next state. Both methods create the same 821 states, of which 384 were valid, non-error states. From the valid states, 180 states had more than one action resulting in new state. These 180 states are the ones that we focused on since these are the only states that could lead to different hints between the two methods. Comparing the two methods, they agree on the next best state in 163 states out of 180 (90.56%). For the remaining 17 states where the two methods disagreed, experts identified 4 states where the MDP method identified the better choice, 9 states where the utility method identified the better choice, and 4 states where the methods were essentially equivalent. These 17 states can be seen in Table 3, with the highlighted cells marking the expert choice. 
 Table 3. States where the methods disagree (17 total states). 
 These results show that the unsupervised utility metric does at least as good a job as the traditional MDP method in determining state values even when it is not known if the student attempt was successful. In all cases, the hints that would be delivered with either method would be helpful and appropriate. We believe that the utility metric provides a strong way to bias our hint selection toward statements derived by a majority of students, which may give students hints at a more appropriate level. Before we derived the utility metric presented here, we considered modifying MDP values by combining them in a weighted sum with a utility factor after value iteration had been completed. In our first attempt to integrate frequency and usefulness into a single metric, we analyzed all of our attempts to find derived statements that were necessary to complete the proof, by doing a recursive search for reference lines starting from the conclusion back through a student’s proof. For each attempt, this “used again” value was set to 1 if a derived statement could be reached backward from the goal, and zero otherwise. We summed the total times a statement was used again, and compared this with the total times a statement occurred in attempts. Table 4 shows the comparison of the frequency and used again values for all statements where used again was more than 1. The values have no real correlation, but most items that were used again had high (>7) frequencies, so we decided that frequency was a relatively good indictor of usefulness in the logic proof domain. The “used again” calculation is possible in the logic domain because students must provide a justification for the current statement using rules and references to prior statements. In other domains, this may not be possible but we believe that frequency of occurrence in student solutions indicates that a step is either needed, or is a very common step that will only skew state values in a consistent way. 
 Table 4. Comparison of frequency and used again. 
 5 Conclusion and Future Work.
 The most important feature of the MDP method is the ability to assign a “value” to the states. This allows the tutor to identify the action that will lead to the next state with the highest value. In this research we have shown that the utility metric that assigns values to terminal states based on the component steps in the state can be used to achieve hint- source decisions as one that assigns a single value to all goal states. The main contribution of this paper is to show how this new utility metric can be used to generate MDP values based on features of student solution attempts. Our results show that the utility metric could be used to achieve equivalent or better hints than our prior single-goal MDP approach. This is significant because the utility metric does not require a known goal state, so it can be applied in domains where the correctness of the student attempts is unknown, or difficult or costly to compute. We believe that this utility metric combined with our MDP method can be used to generate hints for a computer programming tutor. In this domain, it is difficult to say that a program is complete, but it is possible to say whether specific features are represented. The method of using a term- document matrix to determine utility could also be extended into using more complicated LSI techniques which would be a natural fit for tutors using textual answers such as essay response questions. Text based answers are prevalent in legal reasoning and medical diagnosis tutors. In our future work, we plan to construct and compare traditional and utility-based MDPs for other proofs and for student work in other domains. We also plan to analyze our logic tutor hint data to see if the utility method would result in different hints. This will give an indication of how much the utility technique is needed for our logic tutor. We also plan to analyze log data compiled from a C++ programming course to determine what kind of features we might extract and how well we can calculate the utility of those features.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Unsupervised MDP Value Selection for Automating ITS Capabilities</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/181/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/182">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Consistency of Students' Pace in Online Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/182/authorlist"/>
		<swrc:abstract>The purpose of this study is to investigate the consistency of students' behavior regarding their pace of actions over sessions within an online course. Pace in a session is defined as the number of logged actions divided by session length (in minutes). Log files of 6,112 students were collected, and datasets were constructed for examining pace rank consistency in three main situations: day/night sessions, beginning/end (for both situations, sessions of the same learning mode were taken), and a comparison between sessions from different learning modes. For each dataset, students were ranked twice, according to their pace in the two sub-groups, and these ranks were correlated. Results obtained with this study's data suggest that pace is sometimes not consistent, hence might not be considered as a characterizing measure for the whole learning period. A discussion of this study and further research is provided.</swrc:abstract>
		<led:body><![CDATA[ 1. 
 Table 1. Description of the datasets for investigating pace rank consistency. 
 For each dataset, we sorted the students twice, according to their pace in the relevant sub- groups (the student with the highest pace was ranked as "1", the student with the second- highest pace was ranked as "2", and so on). These two ranks were correlated using Spearman's rho (ρ) and Kendall's tau (τ), two common alternatives for non-parametric correlation coefficients ([-1,1]) which are often being compared, however without a sharp recommendation towards neither of them [9, 12, 17]; it is known that the Kendall's coefficient is usually lower than the Spearman's. 
 4 Results.
 Day/Night Consistency.
 Results for Dataset1M and Dataset1P, in which day/night situation was examined in the two learning modes, are given in Table 2. It might be concluded from the results that there is a significant relatively high correlation between pace ranks between day and night in both modes. It was also found that there is a significant difference when comparing means of pace values between day and night groups: Mean pace over night sessions was higher than the mean pace over day sessions; t values were 2.11* (df=330) for Dataset1M, and 2.33* (df=284) for Dataset1P. 
 Table 2. Day/night consistency of pace rank. 
 Results for Dataset2M and Dataset2P, examining consistency of pace ranks over time, are given in Table 3. As might be seen, correlation coefficients are pretty low. On average, beginning and last sessions are differed by pace of action within them: Students tend to work faster at the end, as shown by t values of 3.33** (df=2,649) for Dataset2M, and 3.64** (df=1,357) for Dataset2P. 
 Table 3. Over time consistency of pace rank. 
 Another way of looking at these results is to scatter plot a two-dimension representation of the students according to their ranks in both groups, and to look at the four quadrants formed by the median lines. If pace rank is consistent, it is anticipated that the faster students will be faster in both dimensions, and same for the slower students, hence quadrants I (top-right) and III (bottom-left) should be occupied with most of the dots (students). For example, let's take a look at such a scatter plot for Dataset2P, which relates to the beginning/end situation for the Practicing learning mode. The examination of pace rank consistency for this dataset showed a low yet significant correlation (ρ=0.20**). The scatter plot for this example is presented in Figure 1. According to our calculations, the first and the third quadrants each holds 30% of the dots, which means that the second and fourth quadrants hold together 40% of the students. 
 Figure 1. Scatter plot of pace ranks at the beginning (x) and the end (y) for Dataset2P (Practicing learning mode), N=1,358. 
 Across Modes Consistency.
 Results for Dataset3 are given in Table 4, representing the examination of pace rank consistency across learning modes. Correlation coefficients are relatively low for this situation. Furthermore, there is a significant difference between the means of the two groups: On average, Memorizing sessions were faster than Practicing sessions with t(767)=7.99**. It is a good point to recall the similarities and differences between the two learning modes being discussed here. While Memorizing and Practicing modes share a very similar GUI, and work according to the same principle (browsing over pages each consisting of a 10-row table of words/phrases), the main difference is that the Memorizing tables show the meaning of the term, while the Practicing tables hide it. As suggested by the results, students spend more time on Memorizing pages than on Practicing pages, and pace ranks across modes have a low correlation. This might imply that pace of action is affected by a set of skills needed for progressing in either of the modes. 
 Table 4. Across modes consistency of pace rank. 
 Random Division Consistency.
 Results for Dataset4A, Dataset4M and Dataset4P are given in Table 5. These three datasets relate to a more technical situation than the previous ones: random division of each student's sessions to two groups, and examination of pace rank consistency between these two groups. While Dataset4A takes into consideration all the sessions from the log file, Dataset4M and Dataset4P relate only to Memorizing and Practicing sessions, accordingly. 
 Table 5. Random division consistency of pace rank. 
 It might be seen that for the general case – correlation is relatively low, however when examining pace ranks within the same learning mode, correlation is resulted with relatively high values of coefficients. Also, no significant difference was observed in the means between the two groups within each of the datasets. To conclude the results of this study, there were only two situations in which pace rank was found to be consistent with relatively high values of correlation coefficients: a) Day/night division within the same learning mode; and b) Random division of each student's sessions within the same learning mode. In all the other situations - namely: over time, across modes, and all-inclusive random division - pace rank consistency was found to be relatively low, with correlation coefficients (ρ) between 0.20** and 0.36**. 
 5 Discussion.
 Many EDM studies often handle fine-grained data in the action/session level, like pace measures. However, when examining the student level, mainly since vector variables are not easy to cope with while applying data mining algorithms, scalar measures of these variables are often being used (e.g., average or median pace over different sessions). Time-related variables (usually describing the time taken for answering a question or for completing a task) are quite common in EDM research [1, 8, 11], but others are also often being averaged, for example: attempts for answering a question [1, 11], hint/help usage (usually per question) [1], and intense of activity (usually in terms of number of actions per session or frequency of certain activities) [6, 15]. While doing this, a hidden assumption – regarding the variable in question being a trait – is lying behind the calculations. It is our obligation to deeply investigate the consistency of each variable before projecting it on a 1-dimensional measuring scale and assuming it is of a trait type, as was clearly presented by Baker [2]. This is why we choose a rather primitive variable, namely pace of actions, in order to study its consistency. As the results obtained with our data suggest, correlation between pace ranks in different situations was sometimes very low. The minimal correlation coefficient (for Dataset2P) was 0.20**, which is almost a zero correlation. The maximal correlation coefficient (for Dataset4M) was 0.62**, which is relatively high but still quite far from a perfect correlation. To be honest, these results was, at first, very surprising, as we expected to see much higher correlation values. The fact that for one situation (beginning/end consistency, Practicing mode) 40% of the students were located at the second and fourth quadrants of the pace ranks scatter plot (Figure 1) – indicating they were above the median rank in the beginning and below it in the end, or vice versa – is thought-provoking, and explicitly shedding light on the questionability of the assumption of pace rank consistency. Furthermore, the surprisingly low correlations might imply that our choice of pace was not at all of a simple variable as we first thought, as pace of actions depicts different kinds of processes in which the online student is involved while learning, e.g., reading, memorizing, recalling previous knowledge, thinking, processing, typing, and navigating. Besides the clear effect of different learning components on learning time/pace, individual components also heavily affect it, such as ability to understand instruction or quality of instruction events, as was seminally proposed by Carroll [5]. Considering that pace measurement embodies different task-related and/or student-related components (and potentially others), it is clear that replicating this study with different learning systems and/or with different pace metrics is necessary before generalizing any conclusion regarding the consistency phenomenon. In general, many educational studies investigate all kinds of students' attributes; however, EDM researches often analyze data drawn from relatively long periods of time, therefore our hand on the reduction trigger is likely to be more itchy. Further research and a deeper investigation is needed in order to better understand which behavioral attributes in online learning are indeed students' traits and which are heavily situation dependent.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Consistency of Students' Pace in Online Learning</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/182/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Conditional Subspace Clustering of Skill Mastery: Identifying Skills that Separate Students</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183/authorlist"/>
		<swrc:abstract>In educational research, a fundamental goal is identifying which skills stu- dents have mastered, which skills they have not, and which skills they are in the process of mastering. As the number of examinees, items, and skills increases, the estimation of even simple cognitive diagnosis models becomes difficult. We adopt a faster, simpler approach: cluster a capability matrix estimating each student’s individ- ual skill knowledge to generate skill set profile clusters of students. We complement this approach with the introduction of an automatic subspace clustering method that first identifies skills on which students are well-separated prior to clustering smaller subspaces. This method also allows teachers to dictate the size and separation of the clusters, if need be, for practical reasons. We demonstrate the feasibility and scalabil- ity of our method on several simulated datasets and illustrate the difficulties inherent in real data using a subset of online mathematics tutor data.</swrc:abstract>
		<led:body><![CDATA[ 1. Let γN = λi − λ j be the total descent gradient from a peak (Bin i) to a valley (Bin j). Let γP = λi − λ j be the total ascent gradient from a valley (Bin i) to a peak (Bin j). Let Lm be the location of the mode preceding the current valley (scan’s startpoint). Let Lv be the location of the lowest height of the current valley. Initialize Lm = Lv = Bin 1. 1) Scan γi,i+1 until γi,i+1 < 0. If no such gradient exists, there are no remaining valleys. 2) Else, scan γi,i+1 until γi,i+1 ≥ 0 (end of valley) or out of bins; compute γN. If |γN | > τd, have found a “significant” descent. Set Lv = Bin i + 1. 3) Scan γi,i+1 until γi,i+1 < 0 (end of peak) or out of bins; compute γP. If |γP| > τd, we have found a “significant” ascent. Find valley width w. If w > τw, significant valley; store mode locations. Else, do not store. In either case, set Lm = Lv = Bin i + 1. Scan for next valley (return to 1). Else, have not found significant ascent. Scan γi,i+1 until γi,i+1 ≥ 0 (end of next valley) or out of bins. If λi+1 < λLv, current valley is lower than valley at Lv. Set Lv = Bin i + 1. (return to 3) Else, current valley is higher than valley at Lv; have “hiccup mode”. (return to 3) Else, have not found a significant descent. Scan γi,i+1 until γi,i+1 < 0 (end of next peak) or out of bins. If λi+1 > λLm , current peak is higher than peak at Lm. Set Lm = Bin i + 1. Scan for next valley (return to 1). Else, current peak is lower than peak at Lm; have “hiccup mode”. (return to 2) 
 Figure 2: Marginal Skill Distributions: Illustrative Example, Three Assistment Skills. 
 The spirit of our algorithm is similar to mode-hunting (e.g. [12]) excepting that we only want to identify modes that are separated by a valley of substantial depth and width. In a sense, we are “valley-hunting”. For example, if while searching for a descent of substantial depth we find a “hiccup mode” where the marginal distribution slightly increases and then continues to decrease, the algorithm does not view that small valley to be important. (A “hiccup mode” might similarly be found when searching for a substantial ascent.) Figure 2a contains an example marginal distribution of Skill k, a histogram with bin width = 0.10. For example, say a teacher will only adapt classroom strategies for groups of students who are at least 10% of the class and whose capability values are separated by at least 20%. Given τd = 0.1, τw = 0.2, we start at Bin 1 and immediately find a descent of 0.14 (1.5 · 0.10 − 0.1 · 0.10). We know that there is at least one bin in the preceding mode with at least 10% of the students (our depth threshold). We continue scanning to find a total ascent of 0.135 (1.45 · 0.10 − 0.1 · 0.10) at Bin 4, evidence that the next mode also has at least 10% of the students. As both gradients exceed τd, we check that the valley is wide enough by measuring the distance between the two modes (0.0, 0.3). Since 0.3 > 0.2 = τw, both modes are separated by at least 20% capability, and we have identified a “significant valley”. Continuing to scan, we find another descent and valley at Bin 6. In this case, the descent is not large enough yet to indicate a well-separated group (Bin 7 is a “hiccup mode”). A large enough descent is eventually found between Bin 4 and Bin 8, followed by a significant ascent. The next significant valley is then from Bin 4 to Bin 10. We partition the skill at Bin 2 (0.15) and Bin 8 (0.75) to create three groups of students of size at least 10% of the class separated by at least 20% capability on Skill k. If our thresholds were τd = .045, τw = 0.10, four groups would have been found (cutpoints: 0.15, 0.55, 0.75). Figure 2 also includes the three Assistment skill marginal distributions. While Unit Con- version (Figure 2d) has three well-separated peaks, given reasonable depth/size thresholds, our algorithm would not partition this skill since two non-zero bin counts are very small (i.e. modes of trivial mass). We also would likely not partition the skewed Multiplication distribution. Given τd=0.1, τw=0.2, we do partition Evaluate Functions at 0.15, 0.75 for three groups of students and cluster the three subsequent two-dimensional subspaces. Fig- ure 3 shows the methods’ respective results. There is less cross-plane clustering in HC and k-means without partitioning Unit Conversion (Figures 3a,b). MBC again chose 14 total with similar results; however, the subspace clustering (including both finding the partitions and clustering the subspaces) took ≈ 6 seconds (vs. 21) for computational savings of 71%. 
 Figure 3: Cluster Assignments: a) HC, Complete G=3 · 22; b) K-means G=3 · 22; c) MBC G=14. 
 
 4 Recovering the True Skill Set Profiles.
 In this section, we simulate data from the DINA model, a common educational research model, to compare the methods’ ability to recover the students’ true skill set profiles. The deterministic inputs, noisy “and” gate model (DINA) is a conjunctive cognitive diagnosis model used to estimate student skill knowledge [10]. The DINA model item response form is P(yi j = 1 | ηi j, s j, g j)= (1 − s j)ηi jg1−ηi jj where αik = I{Student i has skill k} and ηi j =∏K k=1 α q jk ik indicates if student i has all skills needed for item j; s j= P(yi j=0 | ηi j=1) is the slip parameter; and g j= P(yi j=1 | ηi j=0) is the guess parameter. If student i is missing any of the required skills for item j, P(yi j = 1) decreases due to the conjunctive assumption. Prior to simulating the yi j, we fix the skills to be of equal medium difficulty with an inter-skill correlation of either 0 or 0.25 and generate true skill set profiles Ci for each student. In our work thus far, only a perfect inter-skill correlation has a non-negligible effect on the results. These parameter choices evenly spread students among the 2K natural skill set profiles. We randomly draw our slip and guess parameters (s j ∼ Unif(0,0.30); g j ∼ Unif(0,0.15)). Given the true skill set profiles and slip/guess parameters, we generate the student response matrix Y . Then, using a fixed Q matrix, we calculate and cluster the corresponding B matrix. For the first three methods, no partitioning is done (HC, k-means: G = 2K; MBC: searches from 1 to G > 2K). In conditional subspace clustering, we initially use τd= 0.1, τw= 0.2 and then cluster the resulting subspaces (if any). To gauge performance, we calculate their agreement to the true profiles using the Adjusted Rand Index (ARI), a common measure of agreement between two partitions [9]. Under random partitioning, E[ARI] = 0, and the maximum value is one. Larger values indicate better agreement. Table 1 presents selected simulations for K = 3, 7, 10 for varying J, N. In the Cond (MBC) column, the first ARI corresponds to the partitioning alone, the second to the clustering of the partitioned subspaces (with MBC). We also vary the Q-matrix design to include only single skill items, only multiple skill items, or both. In addition, the Q-matrix was balanced (bal) or unbalanced (unbal). If balanced, all skills and skill combinations occur the same number of times. Unbalanced refers to uneven representation of or missing skills (miss). 
 Table 1: Comparing Clustering Methods with the True Generating Skill Set Profiles via ARIs. 
 Excepting the multiple unbalanced design, the subspace algorithm selected one or more skills for partitioning (in some cases, all skills were correctly selected). In almost all sim- ulations, MBC was comparable to or better than HC and k-means for true skill set profile recovery. The partitioning method coupled with using MBC on the reduced subspaces gave comparable or better results in all cases except the balanced single and multiple skill design. In addition, subspace partitioning/MBC was always faster than MBC alone. 
 Table 2: Comparison of Depth, Width Thresholds. 
 In addition, for the fourth K= 3, J= 30 Q matrix design, we vary the depth and width thresholds. Smaller values of τd, τw will find narrower, shallower separations; in addition, smaller isolated clusters will be found. In this particular example, we found that as we decreased the depth threshold, more skills were (correctly) selected, and the performance of the partitioning by itself improved. While the parameters are designed to be user-specified, we are currently exploring their behavior in order to make good default suggestions. 
 5 Thirteen Skill Assistment Example.
 Finally, we briefly look at a higher dimensional Assistment example with K=13 skills, N=344 students, and J=135 items. This data set included multiple skill items and a large amount of missing response data. HC and k-means are not appropriate choices; finding 213=8192 clusters is unreasonable (without, say, allowing for empty clusters as in [1]); MBC will largely depend on choosing an appropriate search range. The conditional sub- space clustering algorithm, however, searches the space for obvious separation and parti- tions 9 of the 13 skills for a total of 221 subspaces (1 sec). All subspaces contained ≤ 13 students and so could likely be used alone or as subspaces for further clustering if needed. 
 6 Conclusions.
 We presented a conditional subspace clustering algorithm for use with the capability matrix (or similar skill knowledge estimate). The method selects skills that separate students well and reduces dimensionality for subsequent clustering. Our work so far shows that for most Q-matrix designs, the recovery of true skill set profiles is comparable or better than other clustering methods while also including skill selection. Since the true profiles in the Assist- ment examples are unknown, we cannot judge their recovery. However, visual inspection indicates that the partitions and skill selection seem sensible. To our knowledge, work in this area has not adequately addressed the need to analyze high-dimensional Q-matrices. The approach presented, while allowing for real time estimation of student skill set profiles, can handle large numbers of skills as well as incorporate practical user specifications.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Conditional Subspace Clustering of Skill Mastery: Identifying Skills that Separate Students</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/184">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Visualization of Differences in Data Measuring Mathematical Skills</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/184/authorlist"/>
		<swrc:abstract>Identification of significant differences in sets of data is a common task of data mining. This paper describes a novel visualization technique that allows the user to interactively explore and analyze differences in mean values of analyzed attributes. Statistical tests of hypotheses are used to identify the significant differences and the results are then presented using Hasse diagrams. The presented technique has been tested on real data coming from pedagogical tests focused on evaluation of mathematical skills of secondary school students in Czech Republic. The results show that the proposed tool provides comprehensible representation of the data.</swrc:abstract>
		<led:body><![CDATA[ 1.1. Related work.
 An extensive amount of research has been done on data exploration and data mining. Let us focus on visualization techniques related to the main objective of this paper only. Eick in [3] presents three interesting techniques, where 3D bar chart, scatterplot and a combination of para-boxes, bubble plots and box plots allow to visually analyze values of quantitative attributes. 
 Table 1. Number of students depending on the type of school and sex. 
 The test questions have been specially prepared in cooperation with pedagogical experts so as to cover eight important mathematical skills. They can be characterized as follows: • Understanding of the number as a concept expressing quantity (skill1); • Numerical skills (skill2); • Understanding of mathematical symbols and signs (skill3); • Orientation and work with table (skill4); • Graphical reception and work with graph (skill5); • Understanding of plane figures and work with them, spatial imagination (skill6); • Function as a relation between quantities (skill7); • Logical reasoning (skill8). In the next step of data preparation, each of the eight mathematical skills presented above has been evaluated depending on the corresponding answers. For each student, the skills have been evaluated separately. Each of the skills has been characterized by a percentage (0-100) representing the level of the skill. The evaluation strategy has been prepared again in cooperation with pedagogical experts. So, at the end, each student has been represented by a vector of eight values corresponding to eight skills (attributes). 
 3 The method.
 On the above described data, a method for searching statistically significant differences among mean values has been applied. We have been searching for significant differences among the means (averages) of mathematical skills. To identify significant differences, a statistical test of hypotheses could be used. For our purpose, a two sample Student’s t-test for testing the equality of means has been used [9]. Thus, for sufficiently high |t|, say |t| > Tf(1-0.05), where Tf is a cumulative distribution function of the Student’s distribution with f degrees of freedom, we can reject the hypothesis of equal means, that is, we can consider X  and Y  to be statistically significantly different. This way we can test each combination of mean values. Consider e.g. data in the following table: 
 Table 2. Table shows aggregated data representing skill1. (Variance is a square of stdev). 
 Generally, the described technique proceeds as follows: 1. A test characteristic c is selected, i.e. the attribute whose average differences we would like to explore (e.g. some mathematical skill, in our case). 2. Optionally, a selection condition is defined. Selection condition determines, which data rows will be processed only (e.g. grammar schools only). 3. A partitioning attribute is selected (e.g. sex). The partitioning attribute is a categorical attribute that is used to partition the data into groups G1, G2, …, Gn, among which the differences of means would be analyzed. 4. A statistical testing of differences among c’s mean values of groups G1, G2, …, Gn is performed. That is, the difference of mean values among all combinations of groups Gi and Gj are tested. We have used two-sample Student’s t-test with level of significance α = 0.05. 5. As the result, a relation describing statistically significant inequalities among the groups is obtained: Gi > Gj with respect to c. 
 Thus, the obtained inequalities are based on statistical testing of hypotheses. The results may be very interesting to the analyst. Unfortunately, plain textual representation of the obtained relationships seems not to be very synoptic. Is there any way of representing them graphically? The obtained inequalities may be visualized using a Hasse diagram. Hasse diagram is a graph with each group Gi being represented with a vertex. A downward line is drawn from Gi to Gj, if the statistical test has indicated that the mean value computed for group Gi is significantly greater than mean value computed for Gj (i.e. Gi > Gj) and there is no such Gk that Gi > Gk and Gk > Gj. Generally, the Hasse diagram should be understood as follows: a node X is significantly greater than Y, if there exists a downward path from X to Y. The path from X to Y may lead through other nodes – however, it must be always downward. Thickness of the line represents intensity of the difference. For instance, see Figure 2 depicting inequalities extracted from example data characterized in Table 2. From Figure 2 can be for example seen, that grammar schools (GRA) have the greatest average skill level, whereas natural science (NAT) and social and health studies (SAH) are the worst, but there is not a significant difference among them. Similarly, lyceum (LYC) and technical schools (TEC) are not different either. Please note that accordingly to the theory of statistics, performing large amount of simultaneous statistical tests increases the test error far beyond the level of significance α [8]. Therefore, the obtained inequalities should be considered only as hypotheses indicating some interesting relationship within data – we can never treat the results as a sure and proven knowledge, if obtained that way. 
 4 Results.
 This section presents the results of the proposed tool when applied to a set of real data. The data characterizing mathematical skills of secondary school students have been analyzed from three points of view. 
 4.1. Male or female.
 The aim of the first test is to analyze difference between male and female students over the eight mathematical skills analyzed. In the first part, the type of secondary school has not been considered for the test. The results show significant differences in average values of levels for all analyzed skills. For all skills, the average values computed for male students are significantly higher. Hasse diagram characterizing this situation is shown in Figure 1. The lowest difference (2.79%) is obtained for skill4 (males 71.88%; females 69.09%). On the contrary, the maximal difference (5.57%) between males and females is in the case of skill6 (males 53.49%; females 47.92%). 
 Figure 1. Hasse diagram representing the situation, when the average level computed for male students is significantly different compared to female students. 
 The results of the detailed analysis, when the different types of secondary school have been separated, show, that the secondary schools could be sorted into three groups. Grammar schools (GRA), lyceums (LYC) and economic schools (ECO) can be characterized by the fact that the average skill level characterizing all analyzed skills is significantly higher for male students. In the case of natural science (NAT), trade and service (TAS), social and health studies (SAH), and technical schools (TEC), only for some skills is the average level computed for males significantly higher than for females. The concrete skills and types of school are summarized in the Table 3. The results for remaining schools (art studies (ART), social science (SOC)) do not show significant difference of average skill level for any skill. Unfortunately, relevancy of the data characterizing male students at social science secondary school is low because of very small number of recordings (only eight male students). 
 Table 3. In the case of four schools, only several skills show significant difference of average skill levels. 
 4.2. Difference of the skills.
 In the second part, individual skills have been evaluated and compared. For this analysis, male and female students are not separated into two groups. From the eight skills to be analyzed, two skills (skill1 and skill4) are characterized by the highest average level. Both skill1 and skill4 are significantly different from the remaining six skills, while not being significantly different each other. On the other hand, the students have reached the lowest average level for the skill5. The mean value is again significantly different from all the other analyzed skills. Table 4 shows order of the skills depending on the average skill level. When two or more skills are not significantly different, they are presented on the same line. As it can be seen, the difference between skill1 and skill4, and skill2 is only 2%. Due to the fact, that the number of items is high (N = 7 906), this difference is evaluated by the statistic test as significantly different. 
 Table 4. Average skill levels computed for the skills analyzed in the research. 
 There are only slight differences in the order of the individual skills when the type of school or the sex is considered as an attribute. As we can expect, the values of average level vary for different types of school engaged in the research. This effect is analyzed in the next section. 
 4.3. Effect of the secondary school.
 To provide complete analysis of the data, the effect of the school type on the skills has been also evaluated using the presented tool. The average level of the grammar school (GRA) students (both male and female students) is the highest for all the analyzed skills. It is significantly different compared to the other schools. Then, it could be said, that technical schools (TEC) and lyceums (LYC) are characterized with the second highest average level for most of the skills. The values are again significantly different from the remaining schools. The order of the other types of school depends on the concrete skill and no general rule can be derived from the data. Figure 2 shows the Hasse diagram prepared from the data characterizing skill1. Grammar schools (GRA) are placed alone on the top of the diagram, which represents the highest average level obtained for the skill. Lyceums (LYC) and technical schools (TEC) are placed together on the same level just below the grammar schools (GRA). Absence of a path between them corresponds to the fact, that there is no significant difference between them for skill1. For skill2 and skill7, the average level obtained for lyceum (LYC) students is significantly different (higher) compared to the average value obtained for technical school (TEC) students. 
 Figure 2. Hasse diagram created for skill1 (understanding of the number as a concept expressing the quantity). 
 Figure 3. Hasse diagram created for skill7 (function as relation between quantities). 
 Only in the case of skill5, the result is markedly different. Figure 4 shows the Hasse diagram obtained. 
 Figure 4. Hasse diagram created for skill5 (graphical reception and work with graph). 
 In the next step of the analysis, we focused on evaluation of absolute differences between various types of schools. This analysis shows another two interesting facts. In the case of skill5, the difference between the highest average level (grammar school (GRA)) and the lowest average level is only about 8.5%. It represents the smallest difference among the analyzed skills. For grammar schools (GRA), the average skill level reached 46.5%. On the contrary, the worst average level has been obtained for art (ART) and natural science (NAT) and social and health studies (SAH) students (about 38%). This fact strongly corresponds to the results presented in the previous parts, where the average level representing the skill5 has been determined as very poor compared to the other skills and also the Hasse diagram (Figure 4) representing order of the schools is slightly different. The greatest difference (over 21%) has been reached for skill3 and skill7. For both the skills, the maximal average level characterizes grammar schools (65% and 64% respectively) and the minimal average level reached art schools (about 43%). In the case of skill3, the average level reached for art school is significantly different from the values obtained for other types of school. For the other skills, the difference varies between 14% and 17%. The variety of absolute difference between types of school is also evident from the diagrams obtained. When the absolute difference is minimal (skill5, Figure 4), the structure of the diagram is much wider compared to the skills characterized with maximal absolute difference (e.g. skill7, Figure 3). The skill7 is represented with very narrow structure of the diagram representing the significant differences among averages of the skill levels. 
 5 Conclusion.
 We have introduced a new tool for visualization of statistically significant differences among the mean values of quantitative attributes. The method is based on statistical tests of hypotheses of equal means. Firstly, a set of tests is performed in order to determine significant differences among all combinations of tested mean values. The results are then visualized in the Hasse diagram which represents the extracted information in easily understandable format. The proposed technique has been applied on data characterizing mathematical skills of secondary school students. From the results obtained, we can pick up very poor work with graphs (skill5) typical for all types of secondary schools. In the future, the authors of this paper plan to utilize Hasse diagrams to visualize other types of knowledge (e.g. impact rules).]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Visualization of Differences in Data Measuring Mathematical Skills</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/lukas-zoubek"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/lukas-zoubek"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/michal-burda"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/michal-burda"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/184/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/lukas-zoubek"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/michal-burda"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/185">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>A Data Mining Approach to Reveal Representative Collaboration Indicators in Open Collaboration Frameworks</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/185/authorlist"/>
		<swrc:abstract>Data mining methods are successful in educational environments to discover new knowledge or learner skills or features. Unfortunately, they have not been used in depth with collaboration. We have developed a scalable data mining method, whose objective is to infer information on the collaboration during the collaboration process in a domain-independent way and to improve collaboration process management and learning in an open collaborative educational web environment. Thus, we used statistical indicators of learner’s interactions in forums as the data source and a clustering algorithm to classify the data according to learner’s collaboration. We showed the information on learner’s collaboration to the tutor and learners to help them with collaboration process management. The experimental results support this method.</swrc:abstract>
		<led:body><![CDATA[ 1. Schema of the collaborative learning experience. 
 We provided a learning platform dotLRN (http://dotlrn.org/), which supports all learning experience activities, provides communications services such as forums, and stores all the interactions that take place on the platform in a relation database. During the 1st phase a general virtual environment was opened for all learners of the subject with common services (FAQs, news, surveys, calendar and forums). During the 2nd phase virtual spaces for each three-member team were opened, where the teams could perform the tasks. The specific virtual spaces include documents, surveys, news, a task manager and forums. 
 4 Method.
 We developed the inferring method with these objectives in mind: 1) the method should obtain information on learner’s collaboration; 2) the method should be domain independent; 3) the method should provide information on collaboration before the collaboration process finished. Thus, it is possible reusing and applying the approach in other e-learning environments. We were looking approaches that could be applied to others at UNED, where there are 4000 curses and over 190000 students enrolled. The learners of the collaborative learning experience were encouraged to use the forums on the dotLRN platform as the main communication media. The platform stores the forum messages giving information on what thread the messages are in and what message the message has replied to. We focused on forum interactions, because they are a very common service in a collaborative learning environment and the statistics from forums can be obtained just after the interaction has happened and data mining analysis is possible with these indicators [6, 8]. Since the statistics from forums do not give any semantic information, they are domain independent. In line with the objectives explained above, we used statistical indicators of learner interaction in forums as a data source. According to [17], the features of collaborative learners in these environments are: activity, initiative, regularity and promoting team- work. We proposed these attributes as indicators of the above features: number of threads or conversations that the learner started (num_thrd), and their average, square variance and the number of threads divided by their variance; the number of messages sent (num_msg), and their average, square variance and the number of messages divided by their variance; the number of replies in the thread started by the user (num_reply_thrd), and divided by the number of user threads; the number of replies to messages sent by the user (num_reply_msg), and divided by the number of user messages. The number of threads started and their associated indicators are related to learner initiative. The square variance of the number of threads is related to the regularity of the initiative. The number of messages sent and their associated indicators are related to learner activity and regularity of activity. The number of replies to messages sent and their associated indicators are related to the activity caused by the learner. We built datasets with the above statistical indicators from every year (2006-07, 2007-08 and 2008-09). The characteristics of the datasets were: Dataset-06-07, 117 instances; Dataset-07-08, 122 instances; Dataset-08-09, 112 instances. Every instance is the statistical indicators of the interactions of one learner. We focused our research on the collaborative period, which started at the end of November and finished at the end of January. We collected the values of these statistical indicators in datasets during the whole collaborative period. We used a clustering algorithm as the data mining method. We used a clustering method because it classifies data collection without help from any expert, which delays the inferring process. We employed the EM clustering algorithm because of its good results when the method is applied in the learning environment to reveal collaboration. [20, 14, 13]. We obtained a classification of the instances with the EM clustering algorithm. We used the WEKA data mining software [21] and the EM clustering algorithm [7]. We checked the relation of the classification obtained with collaboration. We needed to know student collaboration from another source to be able to compare their results and validate the approach as a collaborative inferring method. For this reason an expert identified student collaboration in the experiences. The expert read all the forum messages and labeled students according to their collaboration levels. Thus, we obtained a list of most of the students labeled according to their collaboration level. The expert used a scale of 8 values (1, low collaboration level; 9, high collaboration level). Finally, the method finished by comparing the clustering classification of the learners with the labeled list of learner’s collaboration levels. The objective was to measure the average collaboration level of each cluster and to realize that the average collaboration level is different in each cluster. 
 5 Results.
 We have conducted this research during the last three years. In 2006-07 and 2007-08 we focused on the aforementioned inferring method in order to prove the usefulness of the method as a collaboration inferring method. During 2008-09 we applied the method to improve collaborative process management and learning. We proved that the clusters obtained from statistical indicators were related to learner collaboration in the last two years [1] and the data for 2008-09 support these conclusions. We classified the learners into 3 clusters, because the meaning of the classification is easier to understand in relation to collaboration. One cluster represents the low collaboration level, another cluster the medium collaboration level and the third cluster the high collaboration level. Then we run the clustering algorithm EM to obtain 3 cluster and we supplied with the datasets of every year (D-06-07, D-07-08 and D-08-09). These datasets collected the above statistical indicators for every learner. First of all, we note that the cluster algorithm classifies according to the interaction. One cluster (cluster-0 in the next table) collects learners with low interaction (low values in the statistical indicators), another (cluster-1) collects learners with a medium level of interaction, and the third (cluster-2) collects learners with high interaction (high values of statistical indicators). Then we measured the average collaboration level in each cluster (column “Level” of the next table). 
 Table 1. Cluster collaboration level average. 
 Table 1 shows the average of the statistical indicator “num_msg” (number of messages sent to the forums), “num_reply_msg” (number of replies to the messages sent to the forums), and the average collaboration level (Level), which was supplied by the expert, in every cluster. The table shows just two statistical indicators because they define the clusters better, although the clustering algorithm EM used datasets with the 12 statistical indicators, which were explained above. We concluded that the relation between collaboration (collaboration level supplied by the expert) and the clusters, and the statistical indicators is clear. Therefore, the most active learners (cluster-2), i.e., who sent more messages and whose statistical indicator “num_msg” is higher, and who caused more activity (statistical indicator “num_reply_msg” is higher) are the most collaborative learners. From this we can label learners according to their collaboration. Clusters-0 learners are labeled with low collaboration level, cluster-1 learners are labeled with medium collaboration level, and cluster-2 learners are labeled with high collaboration level. Considering the coverage of the evaluations performed over three consecutive academic years and the number of students involved, we can conclude that the relation between the collaboration level and the inferred representative collaboration indicators can be measured automatically, which was done this 2008-09. 
 6 Result Management.
 The year 2008-09 we used this method and learner collaboration levels were calculated during the collaborative period. The objective was not to calculate the exactly collaboration level. We argue that calculating the exact value of one variable in an environment, which is in imperfect scientific conditions, is very complicated. The method used offers rough information on the collaboration level, which can be used to improve learning. We thought that we could show the collaboration level to the tutor of the collaborative environment so that the tutor improved the teaching. The same idea, however, could be applied to learners. Thus we showed learner’s collaboration levels to the tutor and learners. We prepared different ways of showing the information to learners. • Statistical indicator portlet. We prepared a tool displaying the value of only 4 statistical indicators (num_thrd, num_msg, num_reply_thrd and num_reply_msg) of every week during the collaboration period. The objective was to give information on the interaction during the collaborative process to team-members. • Collaboration level portlet. We proved that our data mining method reveals the rough learner collaboration level. This tool displays the collaboration level of team-members and the information was updated every week until the end of the collaboration process. The objective was to give information on the collaboration behavior of team-members. We offered these tools to 2008-09 students. The statistical indicator portlet was offered to 6 teams (18 learners), the collaboration level portlet was offered to 8 teams (24 learners), and both portlets were offered to 6 teams (18 learners). The collaborative learning experience finished, but the academic year has not finished. We are currently analyzing learners’ answers to an opinion questionnaire and the collaboration learning experience results to prove the usefulness of the portlets. We offered these questionnaires to teams who had used some tool. The results are explained in the next table. 
 Table 2. Evaluation of tools.
 Half of the learners or more, to whom some tool was offered, answered the questionnaire and they had to rank the tools between 5 (highest value) and 0 (lowest). The average rank of every tool is not really high but it is always over half values (2.5). The results are positive but the poor number of answers means that we should be cautions on their analysis. To improve the analysis of the questionnaire we are comparing the above results with the marks and the collaboration period evaluation by the tutor. The aforementioned questionnaire will be contrasted with students' marks from tutors' evaluations and final exams. The latter will be available next June. 
 7 Conclusion and Future Work.
 In this paper we have proposed a data mining approach to improve teaching and learning awareness on collaboration features in open collaborative learning frameworks. It infers learner collaboration levels and shows this information to tutors and learners. We thought that the data mining method covers the objective needed to improve the collaboration process. The objectives are: obtaining information on learner collaboration just after collaboration interactions have finished and guarantee domain independency. These objectives guarantee the data mining method can apply to others. This research focused on obtaining information on the collaboration process using statistical indicators of learner interaction in forums, machine learning technology as the inferring method, and showing the inferred information to tutors as the approach to improve the collaboration process. We have proposed statistical indicators, which are related to the activity: initiative, regularity of the learners and the activity caused by the learners. We think the above features explain the collaborative work [17]. An EM clustering algorithm classified the learner statistical indicators and learner collaboration levels, which were provided by an expert, were used to validate the clustering classification as a collaboration level classification. This research took place over three academic years 2006-07, 2007-08 and 2008-09, and more than 100 students took part in the collaborative learning experience each year (125 in 2006-07, 140 in 2007-08 and 115 in 2008-09). During 2006-07 and 2007-08 the research focused on the inferring method [1] and this 2008-09 the results inferred were shown to learners and their usefulness measured. The results have proved that the data mining method could reveal representative collaboration indicators and help learners to improve collaboration learning management. We have proved the clustering approach infers information on the learners’ collaboration, but we do not have any empirical conclusion claim that the clustering method is better than other machine learning methods, which can adapt itself to the problem. To clarify this issue we are carrying out parallel research where the inferring method relies on decision tree algorithms [2]. We are currently collecting results from the datasets so that we can subsequently compare the new results from the application of decision tree algorithms with the results reported in this paper. Another open issue is evaluating the tools offered. To date the evaluation has given satisfactions, but the tools could be improved. However, we must be cautions and wait until the results from the opinion questionnaire and the results from the exams and collaboration experience evaluation by the tutor are compared and analyzed.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>A Data Mining Approach to Reveal Representative Collaboration Indicators in Open Collaboration Frameworks</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/antonio-r-anaya"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/antonio-r-anaya"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jesus-g-boticario"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jesus-g-boticario"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/185/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/antonio-r-anaya"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jesus-g-boticario"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/186">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Predicting Correctness of Problem Solving from Low- level Log Data in Intelligent Tutoring Systems</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/186/authorlist"/>
		<swrc:abstract>This paper proposes a learning based method that can automatically determine how likely a student is to give a correct answer to a problem in an intelligent tutoring system. Only log files that record students’ actions with the system are used to train the model, therefore the modeling process doesn’t require expert knowledge for identifying domain specific skills that are needed to solve the problem or students’ possible solution methods etc. The model utilizes a set of performance features, problem features, time and mouse movement features and is compared to i) a model that utilizes performance and problem features, ii) a model that uses performance, problem and time features. In order to address data sparseness problem, a robust Ridge Regression algorithm is designed to estimate model parameters. An extensive set of experiment results demonstrate the power of using multiple types of evidence as well as the robust Ridge Regression algorithm.</swrc:abstract>
		<led:body><![CDATA[ 1. Details of the problem solving worksheets. The information of whether problems of a worksheet include i) diagram boxes can be seen under the Include Diagram Boxes column; ii) equation boxes can be seen under the  Include Equation Boxes column; iii) unknown number to be solved for can be seen under the Include Unknown "umber column. Shows Correct Answer column shows whether the correct answer to a question is shown to students after they submit their answer. 
 Such partial skills for a problem include answers to i) diagram boxes which check student’s mapping of the information given in a problem into an abstract model; ii) equation box which checks whether a student can form a correct equation from the information given in a problem; iii) final answer box which checks whether a student can solve the asked unknown in a problem correctly. Details about which worksheets include which groups of boxes are shown in Table 1. In some of the worksheets, after the student answers the question, the correct answer is also shown to the student after each question in the worksheet (regardless of whether the student’s answer is correct or not) to make the student learn from his/her mistake. Details about which worksheets show correct answers are also shown in Table 1. The study with the tutoring system included 8 students which include 3 students with learning disabilities, 1 student with emotional disorder and 1 student with emotional disorder combined with mental retardation.  Students used the tutor for several 30 minute class sessions (on average 18.1255 sessions per student with standard deviation of 3.4408 sessions) during which their interaction with the tutoring system was logged in a centralized database. A total of 1960 problems that were solved, 1291 of which were correctly solved and 669 of which were incorrectly solved. The average number of correctly solved problems per student is 161.37 (with a standard deviation of 42.07) and the average number of incorrectly solved problems per student is 83.62 (with a standard deviation of 27.07). Data from 4 students were used as training data to build the models for making predictions for the other 4 students (who are used as the test data). Details about the training and test splits are given in Table 2. 
 3 Methods: Least Squares and Ridge Regression.
 Data sparseness is an important problem which is caused by using limited training data to learn parameters of a model and leads to the common problem of over-fitting [9]. Over- fitting as the name implies is the problem of having an excellent fit to the training data which may not be a precise indicator of future test data especially in the case of data sparseness. Regularization is a technique to control the over-fitting problem by setting constraints on model parameters in order to discourage them from reaching large values that lead to over-fitting. We will briefly discuss the Least Squares technique followed by Ridge Regression technique that controls over-fitting [9]. The simplest linear model for regression involves a linear combination of input variables as follows: 
 FORMULA_1.
 FORMULA_2.
 which can be minimized with a maximum likelihood solution that gives the Least Squares solution of the model parameters as follows: 
 FORMULA_3.
 FORMULA_4.
 where + is the regularization coefficient that controls the relative importance of data- dependent error and the regularization term. The regularization coefficient in this work is learned with cross validation in the training phase (i.e. splitting the training data into smaller training and test datasets). The exact minimizer of the total error function can be found in closed form as follows: 
 FORMULA_5.
 which is the Ridge Regression solution of the parameters of the model. 
 4 Modeling Approaches.
 This section describes the models that are used for evaluation: i) a model that considers performance and problem features, ii) another modeling approach that considers time features as well as performance and problem features, and finally iii) a more advanced model that incorporates mouse movement features with performance, problem and time related features. 
 4.1 Performance and Problem Based Modeling (PerfProb_Mod).
 Using performance and problem based features has been shown to be a useful approach for student modeling in the prior work [3, 14]. The idea of using performance features is quite intuitive since students’ performance up to a certain problem is a good indicator of their performance for that problem. Similarly problem related features such as problem difficulty or number of sub skills (types of question boxes in this work) required etc., are very informative to see whether a current student can correctly answer a problem or not. In this work; 4 performance features are used. The set of 4 performance features are used as a measure of the probability that the student knew the skills asked in a question. The first feature is the # of correct answers so far in a problem solving worksheet. Each problem solving worksheet consists of 12 math word problems and a problem is counted as correct only if all question boxes for the problem are filled correctly. The number of correctly solved problems up to a current problem in a worksheet is a good indicator for student’s success for the current problem. Second, third and fourth performance features help to assess student’s partial skills that are needed for the solution of a problem when they can’t give a full answer. Such partial skills for a problem include the abilities to give answers to, as mentioned before, i) diagram boxes which check student’s mapping of the information given in a problem into an abstract model; ii) equation box which checks whether a student can form a correct equation from the information given in a problem; iii) final answer box which checks whether a student can solve the asked unknown in a problem correctly. The corresponding features are percentage of correct diagram answers so far, percentage of correct equation box answers so far, percentage of final answers so far in a problem solving worksheet. They provide the percentage of correct answers given by a student for the associated partial skill boxes of all the solved problems of a current worksheet up to the current problem. In addition to the 4 performance features, 11 problem features are also used indicating which problem solving worksheet the current problem belongs to. In our model, there are 11 binary variables corresponding to 11 worksheets. If a current problem belongs to 5 th worksheet (i.e. MC Worksheet 1), then 5th binary variable will be 1 and all others will be 0. This encoding approach enables the model to associate each problem with the different characteristics of different worksheets. This encoding scheme is also mentioned in Beck’s work as “one hot” encoding [3]. Performance and problem based modeling in this work serves as the baseline for all other models and will be referred as PerfProb_Mod. 
 4.2 Performance, Problem and Time Based Modeling (PerfProbTime_Mod). 
 Performance and problem based modeling approach is useful in many situations however there are lots of other possible data that can be good indicators of students’ success for a current problem such as the time that a student spends while solving a problem. Although not all the prior work used time related features [14], it has been used as a feature by Beck [3]. In addition to the 15 performance and problem features mentioned before, this modeling approach also incorporates the time feature for student modeling. The time feature in this work is defined as the time a student spends while solving a problem. Performance, problem and time based modeling approach will be referred as PerfProbTime_Mod. 
 4.3 Performance, Problem, Time and Mouse Tracking Based Modeling (PerfProbTimeMouseT_Mod). 
 Incorporation of the time feature into the performance and problem based modeling is an effective way of improving student modeling; however there is still more room to improve. Both performance & problem based modeling and performance, problem & time based modeling approaches ignore an important data source with which students are almost always in interaction while they are solving problems in a problem solving environment: the mouse. As far as we know there is no prior research on student modeling that utilize mouse tracking data.  More details about the prior work on this modeling approach as well as utilizing mouse movement data can be found in the Introduction section. In addition to the 4 performance related features, 11 problem related features and 1 time feature that have been mentioned; this modeling approach incorporates 3 more features as mouse tracking data. The first feature is the maximum mouse off time in a problem which provides the knowledge of the biggest time interval (in seconds) in which mouse is not used for a current problem. Second and third mouse tracking features are the average x movement and average y movement respectively. They basically assess average number of pixels the mouse is moved along the x and y axes in 0.2 second intervals. Performance, problem, time and mouse tracking based modeling that we propose will be referred as PerfProbTimeMouseT_Mod. 
 5 Experimental Methodology: Evaluation Metric. 
 To evaluate the effectiveness of the off-task behavior detection task, we use the common 1F  measure, which is the harmonic mean of precision and recall [2,13]. Precision (p) is the ratio of the correct categorizations by a model divided by all the categorizations of that model. Recall (r) is the ratio of correct categorizations by a model divided by the total number of correct categorizations. 
 FORMULA_6.
 6 Experiment Results.
 This section presents the experimental results of the methods that are presented in Methods section. All the methods were evaluated on the dataset described in Data section. 
 Table 3. Results of PerfProbTimeMouseT_Mod method is shown in comparison to PerfProb_Mod and PerfProbTime_Mod methods for high level student modeling to detect whether a student can correctly solve a given problem. Note that the results for each model for the technique of least squares are shown under the Least Squares column, and the results for each model for the technique of Ridge Regression are shown under the Ridge Regression column. The percentages in the parenthesis show the relative improvements of each method with respect to the Least Squares version of the PerfProb_Mod model. 
 An extensive set of experiments are conducted to address the following questions: • How effective is the PerfProbTime_Mod method that utilizes performance, problem and time features with respect to PerfProb_Mod method that utilizes performance and problem features? • How effective is the PerfProbTimeMouseT_Mod method that utilizes mouse tracking data as well as performance, problem and time features with respect to PerfProb_Mod and PerfProbTime_Mod methods? • How effective is the approach of utilizing the Ridge Regression technique to estimate the model parameters? 
 6.1 The Performance of Performance, Problem and Time Based Modeling (PerfProbTime_Mod). 
 The first set of experiments was conducted to measure the effect of including the time feature in the PerfProb_Mod model. The details about this approach are given in detail in Section 4.1. More specifically, PerfProbTime_Mod model is compared with PerfProb_Mod and their performances are shown in comparison to each other in Table 3. It can be seen that the PerfProbTime_Mod model outperforms PerfProb_Mod model. The lesson to learn from this set of experiments is that time feature is helpful when it is combined with performance and problem related features for the task of predicting whether a student will be able to correctly answer a current problem. This explicitly demonstrates the power of incorporating the time feature into the performance and problem related based modeling. 
 6.2 The Performance of Performance, Problem, Time and Mouse Tracking Based Modeling (PerfProbTimeMouseT_Mod). 
 The second set of experiments was conducted to measure the effect of including the mouse tracking data in the PerfProbTime_Mod model. The details about this approach are given in detail in Section 4.2. More specifically, PerfProbTimeMouseT_Mod method is compared with the other two models and its performance is shown in comparison to the other two models in Table 3. It can be seen that the PerfProbTimeMouseT_Mod model outperforms both PerfProbTime_Mod and PerfProb_Mod models. This set of experiments show that mouse movement features are helpful when they are combined with performance and problem related features along with the time feature for high level student modeling. This explicitly demonstrates the power of incorporating the mouse tracking features into performance, problem and time based modeling. 
 6.3 The Performance of Utilizing the Robust Ridge Regression Technique. 
 The last set of experiments was conducted to measure the effect of utilizing the technique of Ridge Regression for learning the model parameters for each of the models. The details about this approach are given in detail in Section 3. More specifically, Ridge Regression learned models are compared to Least Squares learned models. The performance of Ridge Regression versions of each model is shown in comparison to Least Squares versions in Table 3. It can be seen that the Ridge Regression version of each model outperforms Least Squares versions with its regularization framework. This confirms that Ridge Regression models better solve the data sparseness problems in this application. 
 7 Conclusion and Future Work.
 This paper proposes a novel machine learning method for high-level student modeling (that doesn’t require any expert knowledge of the domain to extract skills, or possible solutions that students may follow) to detect if a student can correctly solve a current problem in a problem solving environment while using an intelligent tutoring system. This model relies only on the low-level log data that is available from the log files from students’ actions within the software. The proposed model makes use of a set of evidence such as performance, problem, time and mouse movement features and is compared to i) a model that utilizes performance and problem related features, ii) a model that uses performance, problem and time features together. To address data sparseness problem, the proposed model utilizes a robust Ridge Regression technique to estimate model parameters. An extensive set of empirical results show that the proposed method that automatically detects whether a student will be able to correctly answer a problem substantially outperforms the model that uses performance and problem related features as well as the model that utilizes performance, problem and time features together. Furthermore empirical results show that the proposed model attains a better performance by utilizing the technique of Ridge Regression over the standard Least Squares Regression technique. There are several possibilities to extend the research. For example, different students have different types of characteristics for solving problems (e.g. using more or less time to solve the problems; having difficulties with particular types of questions and/or problems or different mouse usage types etc.). Therefore, personalized models tend to provide more accurate detection results than a single model for all students. Future research work will be conducted mainly in this direction. 
 Acknowledgements.
 This research was partially supported by the NSF grants IIS-0749462 and IIS-0746830. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsor.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Predicting Correctness of Problem Solving from Low- level Log Data in Intelligent Tutoring Systems</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/suleyman-cetintas"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/suleyman-cetintas"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/luo-si"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/luo-si"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/yan-ping-xin"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/yan-ping-xin"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/casey-hord"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/casey-hord"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/186/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/suleyman-cetintas"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/luo-si"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/yan-ping-xin"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/casey-hord"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>A Comparison of Student Skill Knowledge Estimates</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187/authorlist"/>
		<swrc:abstract>A fundamental goal of educational research is identifying students’ current stage of skill mastery (complete/partial/none). In recent years a number of cognitive diagnosis models have become a popular means of estimating student skill knowledge. However, these models become difficult to estimate as the number of students, items, and skills grows. There exist alternatives such as sum-scores and the capability matrix. While initial theoretical work on sum-scores has been done, the behavior of sum-scores and the capability matrix is not well understood with respect to each other or to estimates from cognitive diagnosis models. In this paper we compare the performance of the three estimates of student skill knowledge under a variety of clustering methods using simulated data with varying levels of missing values.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>A Comparison of Student Skill Knowledge Estimates</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Why, What, and How to Log?  Lessons from LISTEN</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188/authorlist"/>
		<swrc:abstract>The ability to log tutorial interactions in comprehensive, longitudinal, fine- grained detail offers great potential for educational data mining – but what data is logged, and how, can facilitate or impede the realization of that potential.  We propose guidelines gleaned over 15 years of logging, exploring, and analyzing millions of events from Project LISTEN’s Reading Tutor and its predecessors.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Why, What, and How to Log?  Lessons from LISTEN</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Detecting and Understanding the Impact of Cognitive and Interpersonal Conflict in Computer Supported Collaborative Learning Environments</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189/authorlist"/>
		<swrc:abstract>This paper presents a model which can automatically detect a variety of student speech acts as students collaborate within a computer supported collaborative learning environment. In addition, an analysis is presented which gives substantial insight as to how students’ learning is associated with students’ speech acts, knowledge that will significantly influence how this model is utilized by running learning software. Within Piagetian theory, the cognitive conflict of ideas between students is seen as beneficial for learning. Which sorts of interpersonal behaviors lead to most effective learning, however, is open to debate, with some researchers arguing that cooperation is most effective and others arguing that interpersonal conflict is a natural part of collaborative learning. We find that, in fact, interpersonal conflict is associated with positive learning, a finding that must be taken into account, in designing interventions that rely upon detectors of students’ speech acts in CSCL environments.</swrc:abstract>
		<led:body><![CDATA[ 1.1 Cognitive and Social Conflict in CSCL.
 Within this paper, we focus on cognitive and social conflict within computer supported collaborative learning, categories found to have relevant impacts on student learning within non-computer-mediated collaborative learning. Studies from the 1970’s have shown that cognitive conflict promotes cognitive development [11,13,21,23,25], in line with Piaget’s [16] writings on the equilibration process. Piaget claimed that one source of progress in the development of knowledge is found in the imbalance that forces a subject to seek new equilibrium through assimilation and accommodation. Many researchers have found results indicating that cognitive conflict and learning emerges from the process of collaboration, when students mutually engage to co- construct shared knowledge [5,14,19].  In fact, Moshman and Geil [12] and Kruger [10] have argued that the conceptualization of cognitive change as either a process of conflict or a process of cooperation is a false dichotomy, claiming that productive cognitive conflicts take place solely within a cooperative context, and not via competition or interpersonal conflict. In Moshman and Geil’s view, productive cognitive conflict does not emerge from students arguing in favor of their own views, but from co-constructing a consensus solution. Similarly, Howe [7] suggested a separation between types of conflict that involves transactive [3] dialogues (e.g. cognitive conflict) and interpersonal conflict that involves aggression. These studies argued that these two types of interaction occur in distinct groups, depending on students’ gender and temperament. However, other studies have provided evidence suggesting that cognitive conflict does not solely occur in purely collaborative and consensus-based process. For example, Arsenio and Lover [2] and Shantz [22], give evidence that the conflict of ideas often leads to interpersonal conflict. Hence, it appears to still be an open question whether productive cognitive conflict, and the learning that emerges from it, only occurs when students show collaborative behaviors, or whether it still occurs in conjunction with interpersonal conflict. This question is especially relevant within the context of online collaborative learning. Online learning has different affordances than the face-to-face learning settings where much prior collaborative learning research has taken place – in particular, online collaboration, due to the anonymity potentially afforded, is prone to a very high rate of insults, often called “flaming” in the online collaboration literature (e.g. O’Sullivan & Flanagin [15]).  To investigate these questions, we consider the relationship between learning and the different behaviors students display in anonymous online collaborative learning, focusing on the insults indicative of interpersonal conflict. Our hypothesis is that the cognitive conflict which occurs in online learning is highly likely to produce interpersonal conflicts, demonstrated by insults; however the online medium may also reduces the social consequences associated with insults, meaning that insults may not impede learning gains. 
 2 Analysis of Learning Associated With Speech Acts.
 2.1 Methods.
 Twenty four sixth-grade students from a suburban elementary school near Pittsburgh, PA, participated in this study. The study was conducted in a genuine setting of learning, involving authentic learning materials. Because one of the students did not use the chat interface during the two lab days, that student was excluded from analysis (that student’s partner was still included in the sample because this student used the chat interface to discuss the problem solving and complain about his partner’s lack of interactivity), and the sample was reduced to 23 students. Each student made, on average, 84.3 utterances, for a total of 1940 utterances. Each student used a mathematics tutoring program covering problems on fraction addition, subtraction, multiplication, and division [9], in collaborative pairs mediated through TuTalk. TuTalk [8] is a collaborative problem solving interface that include two online panels: a chat, and a collaborative interface for the problem solving built in the CTAT authoring tool [1]. 
 Figure 1. Problem-solving interface [9]. 
 The students worked in their school lab computer, in pairs, using TuTalk, with their chat dialogues and problem solving contributions (within the interface) logged for later analysis. The arrangement of the lab was designed so that the students could not easily talk with their pairs outside of the chat interface, with the identity and the seat location of the collaborating pairs hidden from their partners. Each student individually took nearly isomorphic pre-tests and post-tests, covering knowledge of the material covered in the tutor, during a 30 minute period during on separate days from tutor usage. The students collaborated in learning fractions within TuTalk within two lab sessions, each lasting 45 minutes. This design enabled us to investigate the student’s knowledge gains based on the pre- and post-tests, and to analyze students’ collaborative and individual learning behaviors. 
 2.2 Analytical Method.
 In analyzing these dialogues, we divided student behavior into a selected number of categories relevant to our analyses. Cognitive communicative categories were split in accordance with Youniss and Damon’s [26] interpretation of Piaget’s views on social relations in the individual construction of knowledge, where cognitive conflict and knowledge construction can occur either through disagreement, where one student perceives a misconception or other error in his partner’s thinking and disagrees, attempting to express why it is wrong (called disagree with concept in our coding scheme).  Within this type of disagreement, a student is arguing in favor of his or her own views, an ultimately competitive act. In one example, one student said, “i dont think thats the common denominator”. By contrast, a student may also refine their partner’s ideas by expressing their perspective on an idea, and informing the partner as to their beliefs (called inform belief in our coding scheme), attempting to co-construct a solution – a more collaborative manner of expression. One example of this within our corpus is, “the common denominator is 54”. These two categories are shown in Table 1. The two categories do not form an exhaustive list of possible cognitive communicative acts, but are particularly relevant to the analysis presented here. A fuller taxonomy of speech acts is given in the first author’s doctoral dissertation [17]. 
 Table 1. Description of the cognitive communicative categories. 
 Table 2 Description of social communicative categories. 
 The offer collaborative act and insult categories are social communicative categories (Table 2). A student who offers collaborative act offers to do something to forward the problem-solving goals, generally without having specifically been asked to do so. As such, offer collaborative act is a reflection of the peers’ social collaboration. By contrast, an insult reflects interpersonal conflict within the dialogue, and is in contrast to social collaborative behavior. This set of four categories was coded by the first and sixth authors. Both authors coded a subset of 225 utterances made by students during collaborative learning. Cohen’s [4] Kappa was 0.80, indicating good inter-rater reliability. Afterwards, the protocol analyses were based on the first author’s codes for the entire corpus. We also developed a machine-learned model that was able to accurately code these categories, discussed later in the paper – however, for this analysis human coding was used, as a tractable gold- standard. 
 2.3 Results.
 We analyzed the correlations between pre- and post-test learning gains and the frequency of each category of our coding scheme in each pair’s dialogue. The overall pattern of results is shown in Table 3. As can be seen, the number of inform belief speech acts a student made or received was not significantly correlated with learning gains, respectively t(22)=-0.31, p=0.75, t(22)=-0.10, p=0.92 (all tests reported are two-tailed). The number of offer collaborative act speech acts a student made or received also was not significantly correlated with learning gains, t(22)=0.00, p=0.99, t(22)=0.64, p=0.52, for a two-tailed t-test.) Hence, neither of the two cooperative behaviors coded were associated with significantly higher learning gains. By contrast, the two non-cooperative behaviors were associated with positive learning – but only in the student being non-cooperative. The amount a student disagreed with concept was associated with statistically significantly higher learning gains for the disagreeing student, r=0.53, t(22)=2.93, p<0.01, but not for their partner, t(22)=-0.38, p=0.70. The disagree with concept act has the intention to alter the peer’s reasoning with conflicting ideas, but appears to have been more of a marker of the disagreeing student’s learning than a learning opportunity for their partner. Interestingly, the amount a student made insults was also associated with significantly higher learning gains, r=0.70, t(22)=4.53, p<0.001, but receiving insults from another student was not associated with higher learning gains, r=0.26, t(22)=1.26, p=0.21. Hence, students who acted in ways that create or indicate interpersonal conflict appeared to have higher learning gains in this study. Students who behaved in a more cooperative fashion did not appear to have higher gains. However, the mechanism explaining this result is not clear. Did students learn more because they allowed cognitive conflict to move into interpersonal conflict, or did students engage in interpersonal conflict because they had learned more than their partner, and were impatient with them? It is possible, in particular, that the anonymity of the online learning system facilitated students who had just learned the material in choosing to insult their partner rather than help them. 
 Table 3. The relationship between learning gains and different dialogue acts (p values shown). Statistically significant results (p<.05) in boldface. 
 3 Development of Collaboration Behavior Detectors.
 Having coded a significant number of utterances, the next step was to determine whether it would be possible to develop a machine learned model that could automatically detect these four categories. Such a detector could be used to drive automated interventions by the CSCL environment. (Possible interventions will be discussed in the discussion section). These four categories were combined with additional data coded with twenty-eight other categories, representing a wide span of possible dialogue acts within collaborative learning. The full coding scheme is discussed in detail in the first author’s doctoral dissertation [17]. In total, there were 170 utterances coded with the 4 speech acts discussed above, and a total of 1940 utterances coded with the full set of 32 speech acts. Rosé et al’s [20] TagHelper tool kit was used to develop a machine learned model that could identify the set of speech acts in students’ utterances. TagHelper provides text classification services, designed for use with several languages, and access to a variety of metrics for validating model goodness. It also automatically distills features previously found to be useful for linguistic analyses, such as bigrams and the presence of “stop” words. We used TagHelper to develop models, and to quantify our success in terms of agreement with the hand-coded gold standard corpus. The Naïve Bayes classification algorithm was selected, and applied to the 32 speech acts on the dialogue data. The Kappa [4] statistic was used, in combination with 10-fold cross validation, to assess reliability of the model’s coding. Non-stratified cross-validation was used, under the assumption that multiple utterances by a single student on a single topic are unlikely to be highly correlated to each other (as opposed to other types of behavior, where a student’s responses may be more characteristic), especially when all students are discussing the same mathematical topics. For instance, terms used to discuss fractions, or to disagree about fractions, are likely to be similar between students. The model was successful at classifying student utterances. Within the whole set of 32 speech act categories, kappa was a respectable 0.65. Within the set of four utterances that were previously thought to be particularly relevant for modeling and understanding learning (two were indeed found to be statistically significantly associated with learning), kappa was an excellent 0.91. This was better than our human judges’ degree of agreement, suggesting that the model was highly successful. 
 4 Discussion and Conclusions.
 In the previous section, an automated detector of a variety of speech acts was presented and shown to be reasonably effective at distinguishing between a variety of speech acts, including the four categories discussed in detail in this paper: inform belief, disagree with concept, offer collaborative act, and insult. With Kappa values between 0.65 (all categories) and 0.91 (categories discussed in this paper), it seems quite feasible to use the model for detecting and responding to different types of speech acts. However, using the model to drive appropriate interventions depends on understanding the implications of each type of speech act, which leads to a need for analyses such as the one presented here. In the analysis in section 2 of this paper, learning gains were correlated with speech acts. Understanding this gives us an important first piece in the puzzle of deciding how a CSCL system should respond to those acts.  Developing a full understanding of what prompts different speech acts will help us even further. Within this study, learning gains were positively related to interpersonal conflict. Arsenio and Lover [2] and Shantz [22] previously found, in face to face collaboration, that the conflict of ideas can lead to aggressive behavior, including the types of interpersonal conflict observed here. In those studies, the aggressive behavior harmed students’ interpersonal relationships. However, the anonymity of communication in our study may have enabled students to insult each other with lower interpersonal cost, eliminating one of the negative factors associated with aggressive behavior in collaborative learning. In broader usage of such a CSCL system, this anonymity could persist within internet usage while not persisting in classroom usage (because over time, students can determine who they are collaborating with in a classroom, eliminating anonymity). This does not mean, however, that insults are to be encouraged. Insults are clearly perceived as problematic in online communication (e.g. O’Sullivan and Flanagin [15]), and may be associated with the abandonment of usage of online learning environments (cf. Reinig et al. [18]). In general, this work supports the hypothesis that positive cognitive conflict can coincide with interpersonal conflict. It is not at all clear from our results that the interpersonal conflict had a positive impact on cognitive conflict or learning – for instance, it may have been a side-effect of one student’s greater learning, with no positive impact on the other student. Studying this issue in richer depth will require a combination of methods, including time series analysis on a significantly larger corpus of data, and perhaps experimentally manipulating interpersonal conflict via not transmitting students’ insults, in order to determine insults’ causes and impacts on learning. One clear implication of our results is that insults and interpersonal conflict play a prominent role in collaborative learning, which cannot be safely ignored. An overly harsh response to student insults may also interfere with the positive learning that insults appear to be associated with. One approach may be to attempt to develop designs which guide students in moderating their comments to others, without disrupting the cognitive conflict which insults appear to be associated with. However, if insulting another student produces pleasure for the insulting student and increases the insulting student’s desire to persist in the use of a learning environment [cf. 24], it may be feasible for a CSCL environment to automatically strip out insults from the text the insulted student actually receives. Further research on how software that supports CSCL can optimally handle insults and other interpersonal conflict behaviors, given the ability to detect those acts, appears to be warranted. 
 5 Acknowledgements.
 This work was funded in part by NSF grant REC-043779 to "IERI: Learning-Oriented Dialogs in Cognitive Tutors: Toward a Scalable Solution to Performance Orientation".]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Detecting and Understanding the Impact of Cognitive and Interpersonal Conflict in Computer Supported Collaborative Learning Environments</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/david-nadler-prata"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/david-nadler-prata"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/evandro-db-costa"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/evandro-db-costa"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carolyn-p-rose"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carolyn-p-rose"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-cui"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-cui"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/adriana-mjb-de-carvalho"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/adriana-mjb-de-carvalho"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/david-nadler-prata"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/evandro-db-costa"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/carolyn-p-rose"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-cui"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/adriana-mjb-de-carvalho"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Reducing the Knowledge Tracing Space</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190/authorlist"/>
		<swrc:abstract>In Cognitive Tutors, student skill is represented by estimates of student knowledge on various knowledge components. The estimate for each knowledge component is based on a four-parameter model developed by Corbett and Anderson [Nb]. In this paper, we investigate the nature of the parameter space defined by these four parameters by modeling data from over 8000 students in four Cognitive Tutor courses. We conclude that we can drastically reduce the parameter space used to model students without compromising the behavior of the system. Reduction of the parameter space provides great efficiency gains and also assists us in interpreting specific learning and performance parameters.</swrc:abstract>
		<led:body><![CDATA[ 1.95 for the fitted parameters and 1.63 for the clustered parameters. A paired t-test showed a significant difference: t(181) = 3.2, p < 0.01.  This may be due to the fact that clustering tends to move parameters away from extreme values, bringing them closer to delivered parameters, which generally avoid extremes. Another advantage of clustering is to avoid overfitting with smaller amounts of data. To test this, we developed 23 new clusters, using 1561 skills and 1312 students. We then found the best-fitting cluster for each of the 275 skills that were not used in developing the clusters, using varying numbers of students. We also found best-fitting parameters for these 275 skills on the subsets of students and tested the fit with another set of 200 students. As Figure 4 shows, when there are a small number of students contributing to the data, the clusters provide a substantially better fit to the data than the best-fit estimates. This provides evidence both that clusters developed with one set of skills will generalize to another set and that, with small amounts of student data, clusters can help prevent overfitting. 
 Figure 4: Comparison of clustered vs. best-fit estimates with differing numbers of students. 
 5 Conclusion.
 Previous work has shown that modeling student learning and performance parameters based on prior-year student data results in improved system efficiency. This paper explored the issue of how sensitive such effectiveness is to the particular sets of parameters used. Our results have shown that tutor performance is relatively insensitive to the particular parameter sets that are used. We were able to show that, using only 23 sets of parameters, we could produce virtually the same system behavior as we would see if we had used parameters found through exploring the full parameter space. This result does not argue against fitting these parameters based on data; rather it suggests that a quick estimate of such parameters can be sufficient to produce near-optimal behavior. It is worth pointing out that the parameters we are setting act as population parameters, which would likely benefit from adjustment for individual differences [1]. Indeed, these results may suggest that a more profitable route to accurate student modeling is to focus on individual differences, rather than population characteristics. We see clustering as complementary to both the Dirichet priors approach [2] and the use of contextual guess and slip [1]. The fact that we can model student behavior with a very small set of parameters helps us to extend the knowledge tracing model beyond simply a mathematical model of student behavior; we now have a better chance to interpret individual parameters within the set. For any knowledge component, we could calculate the goodness of fit to the data for each of the 23 parameter clusters. If we only see a good fit to one cluster, and that cluster has a high plearn parameter, then we can reasonably conclude that that the knowledge component is easily learned. Such a conclusion would be computationally expensive to reach in the full parameter space since, since we would need to explore a large part of the space before we could conclude that there is an almost-as-good fit to the data to be found with a low-plearn parameter set. Clustering parameters thus provides us a way to quickly examine knowledge components and determine which ones are problematic. Knowledge components with low plearn might suggest areas where we should refine our instruction. Ones with high pguess or high pslip might indicate areas where we need to reconsider the user interface. Ones with high pinitial might indicate areas where instruction is unneeded. We are optimistic that our work in reducing the parameter space for knowledge tracing will provide us with new ways to more quickly and confidently use knowledge tracing parameters to interpret student behavior.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Reducing the Knowledge Tracing Space</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/steve-ritter"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/steve-ritter"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-k-harris"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-k-harris"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tristan-nixon"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tristan-nixon"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-dickison"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-dickison"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/r-charles-murray"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/r-charles-murray"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/brendon-towle"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/brendon-towle"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/steve-ritter"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-k-harris"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/tristan-nixon"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-dickison"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/r-charles-murray"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/brendon-towle"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Collaborative Data Mining Tool for Education</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191/authorlist"/>
		<swrc:abstract>This paper describes a collaborative educational data mining tool based on association rule mining for the continuous improvement of e-learning courses allowing  teachers  with  similar  course’s  profile  sharing  and  scoring  the discovered information. This mining tool is oriented to be used by instructors non experts in data mining such that, its internal operation is transparent to the user and the instructor can be focused in to the analysis of the results and make decisions about how to improve e-learning courses.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 Nowadays,  there  are  a  variety  of  general  data  mining  tools  and  frameworks.  Some examples  of  commercial  mining  tools  are  DBMiner  [1],  SPSS  Clementine  [2],  DB2 Intelligent Miner [3], etc. And some examples of public domain mining tools are Weka [4],  RapidMiner  [5],  Keel  [6],  etc.  All  these  tools  are  not  specifically  designed  for pedagogical/educational purposes and it is cumbersome for an educator to use these tools which are normally designed more for power and flexibility than for simplicity. However, there are also an increasing number of mining tools specifically oriented to educational data  such  as:  Mining  tool  [7]  for  association  and  pattern  mining,  MultiStar  [8]  for association and classification, EPRules [9] for association,  KAON [10] for clustering and text  mining,  Synergo/ColAT  [11]  for  statistics  and  visualization,  GISMO  [12]  for visualization,  Listen  tool  [13]  for  visualization  and  browsing,  TADA-Ed  [14]  for visualizing and mining, O3R [15] for sequential pattern mining, Sequential Mining tool [16] for pattern mining, MINEL [17] for mining learning paths, Simulog [18] for looking for unexpected behavioral pattern. Moodle mining tool [19] for classification, clustering and association rule mining. All these tools are oriented to be used by a single instructor or course administrator in order to discover useful knowledge from their own courses. So, they don’t allow a collaborative usage in order to share all the discovered information between  other  instructors  of  similar  courses  (contents,  subjects,  educational  type: elementary  and  primary  education,  adult  education,  higher,  tertiary  and  academic education,  special  education,  etc.).  In this  way, the information discovered locally  by teachers could be joined and stored in a common repository of knowledge available for all instructors for solving similar detected problems. In  this  paper,  we describe  an educational  data  mining  tool  based  on association  rule mining and collaborative filtering for the continuous improvement of e-learning courses and it directed to teachers non experts in data mining. The main objective is to make a mining  tool  in  which  the  information  discovered  can  be  shared  and  scored  between different instructors and experts in education. 
 Implementation of the collaborative data mining tool.
 We have developed a data mining tool with two subsystems: client and server application (Figure 1). The client application uses an association rule mining tool for discovering interesting  relationships  through  student’s  usage  data  in  the  form  of  IF-THEN recommendation rules. The server application uses a collaborative recommender system to share and score the previously obtained rules by instructors of similar courses with other instructors and experts in education. 
 Figure.

 1. Collaborative data mining tool. 
 As we can see in  Figure 1,  the system is  based on client-server  architecture  with  N clients,  which applies an association rule mining algorithm locally  on students’ usage data. In fact, the client application uses the Predictive Apriori algorithm [20], because it does not require the user to specify parameters such as the minimum support threshold or confidence values. The only parameter is the number of rules to be discovered, which is a more intuitive parameter for a teacher non expert in data mining. The association rules discovered by the client application must be evaluated to decide if they are relevant or not, therefore the client application uses an evaluation measure [21] to classify the rules as  being  expected  or  unexpected,  comparing  them with  the  scored  rules  stored  in  a collaborative rules repository maintained on server side. Also, the expected rules found are then expressed in a more comprehensible format of recommendation about possible solutions to problems detected in the course. The teacher sees the recommendation and can  determine  if  it  is  relevant  or  not  for  him/her  in  order  to  apply/use  the recommendation.  On the other  side,  the server application allows managing the rules repository using collaborative filtering techniques with knowledge-based techniques [21]. The  information  in  the  knowledge  base  is  stored  in  form  of  tuples  (rule-problem- recommendation-relevance) which are classified according to a specific course profile. The course profile is represented as a three-dimensional vector related with the following characteristic of his/her course: Topic (the area of knowledge, e.g. Computer Science or Biology);  Level  (level  of  the  course,  e.g.  Universitary,  High  School,  Elementary  or Special Education); and Difficulty (the difficulty of the course, e.g., Low or High). These similarities  between  courses  are  available  to  other  teachers  to  assess  in  terms  of applicability and relevance. A group of experts in online education from University of Córdoba,  Spain,  propose the first  tuples of the rule repository and also vote on those tuples proposed by other experts. On the other hand, teachers could discover new tuples (in  the  client  application)  but  these  must  be  validated  by  the  experts  (in  the  sever application) before being inserted in the rule repository. 
 1.1 Client application.
 As we mentioned before, the client application is used by instructors in order to find association  rules.  The  main  feature  of  the  client  application  is  its  specialization  in educational environments. Before applying our mining algorithm, the data have to be pre- processed in order to adapt them to our specific data model. First, the teacher has to select the origin of the data to be mined. We have two different formats available for input data: 1) the Moodle relational database,  for teachers that  work with Moodle as well  as the INDESAHC authoring tool [22], so all our attributes are used directly; or 2) a Weka [4] ARFF text file, for teachers that use other LMSs and, therefore, other attributes. Also, the teacher can restrict the search field, we have also added a few parameters related with the analysis depth. Firstly, the teacher must select the level of granularity to carry out the analysis:  course,  unit,  lesson or a specific  table  of the data  base such as course-unit, course-lesson, course-exercise, course-forum, unit-exercise, unit-lesson, lesson-exercise among others. The rules repository (see Figure 2) is the knowledge database upon which the analysis of the discovered rules is based. Before running the algorithm, the teacher downloads from the server, the current knowledge database, according to his/her course profile. 
 Figure 2. Rules repository panel. 
 Finally, after downloading the rule repository and configuring the application parameters or using default values, the teacher executes the association rule algorithm. Then, client application shows the results obtained in a table (see Figure 3), with the following fields: rule (discovered IF-THEN rule), problem (detected by the rule), recommendation (about how to solve the problem), score (of experts and others instructors have set to the rule) and apply button (to use/apply the recommendation in his/her course). 
 Figure 3. Results panel. 
 We have distinguished between two types of recommendations: 1) Active, if it implies a direct modification of the course content or structure; or 2) Passive, if it detects a more general problem in the course or unit and it advices the teacher to consult more specific recommendations related with these didactic resources. Active recommendations can be linked to: modifications in the formulation of the questions (see Figure 3) or the practical exercises/tasks assigned to the students; changes in previously assigned parameters such as course duration or the level of lesson difficulty; or the elimination of a resource such as a forum or a chat room. 
 1.2 Server application.
 The server application is used by experts and instructors. The experts in education insert the  tuples  and they explicitly  vote  for  them by indicating  degrees  of  preference  (see Figure 4). The teachers vote implicitly when they push the “Apply” button, in order to side-step one of the main problems for collaborative filtering systems, that  is how to encourage  teachers  to  vote  or  evaluate.  In  this  case,  if  teachers  apply  one  of  the recommendations to their course, they are implicitly voting for this specific tuple. 
 Figure 4.  Vote panel. 
 The server application is a web-based application for managing the knowledge database or tuple repository (see Figure 5). In order to access easily to all the editing options for the  repository,  a  general  course profile  was  created  which  is  the  profile  used by the experts in educational domain. These experts have permission to introduce new tuples into the rule repository and vote explicitly for existing ones (see Figure 4). In order to allow information exchange (tuples) between client and server, we have developed a web service for downloading/uploading the repository. Each time a client application updates its repository, all the tuples are reordered in the repository. Finally, we must mention that an evaluation of this collaborative data mining tool can be found in [21]. 
 Figure 5. Server application interface. 
 2 Conclusions.
 In this paper we have shown a data mining tool that uses association rule mining and collaborative  filtering  in  order  to  make  recommendation  to  instructors  about  how to improve e-learning courses. This tool enables to share and score the discovered rules by other teachers of similar  courses. Currently,  the mining tool has been only used by a group of instructors and expert involved in the development of the own tool. So, in the future we want to test the tool with several groups of external instructors and experts in order to can test the usability of the tool with external users. 
 Acknowledgments.
 The  authors  gratefully  acknowledge  the  financial  support  provided  by  the  Spanish department of Research under TIN2008-06681-C06-03 and P08-TIC-3720 Projects.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Collaborative Data Mining Tool for Education</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/enrique-garcia"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/enrique-garcia"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/miguel-gea"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/miguel-gea"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-de-castro"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-de-castro"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/enrique-garcia"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/miguel-gea"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-de-castro"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/192">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Determining the Significance of Item Order In Randomized Problem Sets</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/192/authorlist"/>
		<swrc:abstract><![CDATA[ Researchers who make tutoring systems would like to know which sequences of educational content lead to the most effective learning by their students. The majority of data collected in many ITS systems consist of answers to a group of questions of a given skill often presented in a random sequence. Following work that identifies which items produce the most learning we propose a Bayesian method using similar permutation analysis techniques to determine if item learning is context sensitive and if so which orderings of questions produce the most learning. We confine our analysis to random sequences with three questions. The method identifies question ordering rules such as, question A should go before B, which are statistically reliably beneficial to learning. Real tutor data from five random sequence problem sets were analyzed. Statistically reliable orderings of questions were found in two of the five real data problem sets. A simulation consisting of 140 experiments was run to validate the method's accuracy and test its reliability. The method succeeded in finding 43% of the underlying item order effects with a 6% false positive rate using a p value threshold of <= 0.05. Using this method, ITS researchers can gain valuable knowledge about their problem sets and feasibly let the ITS automatically identify item order effects and optimize student learning by restricting assigned sequences to those prescribed as most beneficial to learning.]]></swrc:abstract>
		<led:body><![CDATA[ 1.1 The Tutoring System and Dataset.
 Our dataset consisted of student responses from The ASSISTment System, a web based math tutoring system for 7th-12th grade students that provides preparation for the state standardized test by using released math items from previous tests as questions on the system. Figure 1 shows an example of a math item on the system and tutorial help that is given if the student answers the question wrong or asks for help. The tutorial helps the student learn the required knowledge by breaking the problem into sub questions called scaffolding or giving the student hints on how to solve the question. The data we analyzed was from the 2006-2007 school year. Subject matter experts made problem sets called GLOPS (groups of learning opportunities). The idea behind the GLOPS was to make a problem set where the items in the problem set related to each other. They were not necessary strictly related to each other through a formal skill tagging convention but were selected based on their similarity of concept according to the expert. We chose the five three item GLOPS that existed in the system each with between 295 and 674 students who had completed the problem set. Items do not overlap across GLOP problem sets. Our analysis can scale to problem sets of six items but we wanted to start off with a smaller size set for simplicity in testing and explaining the analysis method. The items in the five problem sets were presented to students in a randomized order. Randomization was not done for the sake of this research in particular but rather because the assumption of the subject matter expert was that these items did not have an obvious progression requiring that only a particular sequence of the items be presented to students. In other words, context sensitivity was not assumed. We only analyzed responses to the original questions which meant that a distinction was not made between the learning occurring due to answering the original question and learning occurring due to the help content. The learning from answering the original question and scaffolding will be conflated as a single value for the item. 
 Figure 1. An ASSISTment item. 
 1.2 Knowledge Tracing.
 The Corbett and Anderson method of “knowledge tracing” [3] has been useful to many intelligent tutoring systems. In knowledge tracing there is a set of questions that are assumed to be answerable by the application of a particular knowledge component which could be a skill, fact, procedure or concept. Knowledge tracing attempts to infer the probability that a student knows a knowledge component based on a series of answers. Presumably, if a student had a response sequence of 0,0,1,0,0,0,1,1,1,1,1,1 where 0 is an incorrect first response to a question and 1 is a correct response, it is likely she guessed the third question but then learned the knowledge to get the last 6 questions correct. The Expectation Maximization algorithm is used in our research to learn parameters from data such as the probability of guess. 
 Figure 2. Bayesian network model for question sequence [2 1 3]. 
 Figure 2 depicts a typical knowledge tracing three question static Bayesian network. The top three nodes represent a single skill and the inferred value of the node represents the probability the student knows the skill at each opportunity. The bottom three nodes represent three questions on the tutor. Student performance on a question is a function of their skill knowledge and the guess and slip of the question. Guess is the probability of answering correctly if the skill is not known. Slip is the probability of answering incorrectly if the skill is known. Learning rates are the probability that a skill will go from “not known” to “known” after encountering the question. The probability of the skill going from “known” to “not known” (forgetting) is fixed at zero. Knowledge tracing assumes that the learning on a piece of knowledge is independent of the question presented to students, that is that all questions should lead to the same amount of learning.  The basic design of a question sequence in our model is similar to a dynamic Bayesian network or Hidden Markov Model used in knowledge tracing but with the important distinction that the probability of learning is able to differ between opportunities. This ability allows us to model different learning rates per question which is essential to our analysis. The other important distinction of our model is the ability to model permutations of sequences with parameter sharing, discussed in the next section. 
 2 Analysis Methodology.
 In order to represent all the data in our randomized problem sets of three items we must model all six possible item sequence permutations. If six completely separate networks were created then the data would be split into six which would degrade the accuracy of parameter learning. This would also learn a separate guess and slip for each question in each sequence despite the questions being the same in each sequence. In order to leverage the parameter learning power of all the data and define an individual question’s guess and slip values we will use parameter sharing2 to link the parameters across the different sequence networks. This means that question one as it appears in all six sequences will share the same guess and slip conditional probability table (CPT). The same will be true for the other two questions. This will give us three guess and slip parameters total and the values will be trained to reflect the questions' non sequence specific guess and slip values. In our item order effect model we also link the learning rates of item sequences. 
 2.1 The Item Order Effect Model.
 In the model we call the item order effect model we look at what effect item order has on learning. We set a learning rate for each pair of items and then test if one pair is reliably better for learning than another. For instance, should question A come before question B or vice versa? Since there are three items in our problem sets there will be six ordered pairs which are (3,2) (2,3) (3,1) (1,3) (2,1) and (1,2). This model allows us to train the learning rates of all six ordered pairs simultaneously along with guess and slip for the questions by using shared parameters to link all occurrences of pairs to the same learning rate conditional probability table. For example, the ordered pair (3,2) appears in two sequence permutations; sequence (3,2,1) and sequence (1,3,2) as shown in Figure 3. 
 2.2 Reliability Estimates Using the Binomial Test. 
 In order to derive the reliability of the learning rates fit from data we employed the binomial test3 by randomly splitting the response data into 10 by student. 
 Figure 3. A two sequence portion of the Item Order Effect Model (six sequences exist in total). 
 We fit the model parameters using data from each of the 10 bins separately and counted the number of bins in which the learning rate of one item pair was greater than its reverse, (3,2) > (2,3) for instance. We call a comparison of learning rates such as (3,2) > (2,3) a rule. The null hypothesis is that each rule is equally likely to occur. A rule is considered statistically reliable if the probability that the result came from the null hypothesis is <= 0.05. For example, if we are testing if ordered pair (3,2) has a higher learning rate than (2,3) then there are two possible outcomes and the null hypothesis is that each outcome has a 50% chance of occurring. Thus, the binomial test will tell us that if the rule holds true eight or more times out of ten then it is <= 0.05 probable that the result came from the null hypothesis. This is the same idea as flipping a coin 10 times to determine the probability it is fair. The less likely the null hypothesis, the more confidence we can have in the result. If the learning rate of (3,2) is greater than (2,3) with p <= 0.05 then we can say it is statistically reliable that question three and its tutoring followed by question two better help students learn the skill than question two and its tutoring followed by question three. Based on this conclusion it would be recommended to give sequences where question three comes before two. The successful detection of a single rule will eliminate half of the sequences since three comes before two in half of the sequence permutations. Strictly speaking the model is only reporting the learning rate when two comes directly after three however in eliminating half the sequences we make the pedagogical assumption that question three and its tutoring will help answer question two even if it comes one item prior such as in the sequence (3, 1, 2). Without this assumption only the two sequences with (2,3) can be eliminated and not sequence (2,1,3). 
 2.3 Item Order Effect Model Results.
 We ran the analysis method on our problem sets and found reliable rules in two out of the five problem sets. The results below show the item pair learning rate parameters for the two problem sets in which reliable rules were found. The 10 bin split was used to evaluate the reliability of the rules while all student data for the respective problem sets were used to train the parameters shown below. 
 Table 1. Item order effect model results. 
 As shown in Table 1, there was one reliable rule found in each of the problem sets. In problem set 24 we found that item pair (3,2) showed a higher learning rate than (2,3) in eight out of the 10 splits giving a binomial p of 0.0439. Item pair (1,3) showed a higher learning rate than (3,1) also in eight out of the 10 splits in problem set 36. Other statistically reliable relationships can be tested on the results of the method. For instance, in problem set 36 we found that (2,1) > (3,1) in 10 out of the 10 bins. This could mean that sequence (3,1,2) should not be given to students because question three comes before question one and question two does not. Removing sequence (3,1,2) is also supported by rule (1,3) > (3,1). In addition to the learning rate parameters, the model simultaneously trains a guess and slip value for each question. Those values are shown below in Table 2. 
 Table 2. Trained question guess and slip values. 
 3 Simulation.
 In order to determine the validity of the item order effect method we chose to run a simulation study exploring the boundaries of the method’s accuracy and reliability. The goal of the simulation was to generate student responses under various conditions that may be seen in the real world and test if the method would accurately infer the underlying parameter values from the simulated student data. This simulation model assumes that learning rates have distinct values and that item order effects of some magnitude always exist and should be detectable given enough data. 
 3.1 Model design.
 The model used to generate student responses is a six node static Bayesian network as depicted in Figure 2 from section 1.2. While the probability of knowing the skill will monotonically increase after each opportunity, the generated responses (0s and 1s) will not necessarily do the same since those values are generated probabilistically based on skill knowledge and guess and slip. Simulated student responses were generated one student at a time by sampling from the six node network. 
 3.2 Student parameters.
 Only two parameters were used to define a simulated student, a prior and question sequence. The prior represents the probability the student knew the skill relating to the questions before encountering the questions. The prior for a given student was randomly generated from a distribution that was fit to a previous year’s ASSISTment data [6]. The mean prior for that year across all skills was 0.31 and the standard deviation was 0.20. In order to draw probabilistic parameter values that fit within 0 and 1, an equivalent beta distribution was used. The beta distribution fit an α of 1.05 and β of 2.43. The question sequence for a given student was generated from a uniform distribution of sequence permutations. 
 3.3 Tutor Parameters.
 The 12 parameters of the tutor simulation network consist of six learning rate parameters, three guess parameters and three slip parameters. The number of users simulated was: 200, 500, 1000, 2000, 4000, 10000, and 20000. The simulation was run 20 times for each of the seven simulated user sizes totaling 140 generated data sets, referred to later as experiments. In order to faithfully simulate the conditions of a real tutor, values for the 12 parameters were randomly generated using the means and standard deviations across 106 skills from a previous analysis [6] of ASSISTment data. Table 3 shows the distributions that the parameter values were randomly drawn from and then assigned to questions and learning rates at the start of each run. 
 Table 3. The distributions used to generate parameter values in the simulation. 
 Running the simulation and generating new parameter values 20 times gives us a good sampling of the underlying distribution for each of the seven user sizes. This method of generating parameters will end up accounting for more variance than the real world since standard deviations were calculated for values across problem sets as opposed to within. Also, guess and slip have a correlation in the real world but will be allowed to independently vary in the simulation which means sometimes getting a high slip but low guess, which is rarely observed in actual ASSISTment data. It also means the potential for generating very improbable combinations of item pair learning rates. 
 3.4 Simulation Procedure.
 The simulation consisted of three steps: instantiation of the Bayesian network, setting CPTs to values of the simulation parameters and student parameters and finally sampling the Bayesian network to generate the students’ responses. To generate student responses the six node network was first instantiated in MATLAB using routines from the Bayes Net Toolbox package. Student priors and question sequences were randomly generated for each simulation run and the 12 parameters described in section 3.3 were assigned to the three questions and item pair learning rates. The question CPTs and learning rates were positioned with regard to the student’s particular question sequence. The Bayesian network was then sampled a single time to generate the student’s responses to each of the three questions; a zero indicating an incorrect answer and a one indicating a correct answer. These three responses in addition to the student’s question sequence were written to a file. A total of 140 data files were created at the conclusion of the simulation runs, all of which were to be analyzed by the item order effect detection method. The seeded simulation parameters were stored in a log file for each experiment to later be checked against the method's findings. An example of an experiment’s output file for 500 users is shown in Table 4 below. 
 Table 4. Example output from data file with N=500. 
 Each data file from the simulation was split into 10 equal parts and each run separately through the analysis method just as was done in analysis of real tutor data. This analysis step would give a result such as the example in Table 5 below. 
 Table 5. Example output from item order effect analysis. 
 In order to produce a p value and determine statistical reliability to the p < 0.05 level the binomial test is used. The method counts how many times (3,2) was greater than (2,3) for instance. If the count is greater than eight then the method considers this an identified rule. Even though there are six item pairs there is a maximum of three rules since if (3,2) > (2,3) is a reliable rule then (3,2) < (2,3) is not. In some cases finding two rules is enough to identify a single sequence as being best. Three rules always guarantee the identification of a single sequence. The method logs the number of rules found and how many users (total) were involved in the experiment. The method now looks "under the hood" at the parameters set by the simulation for the item pair learning rates and determines how many of the found rules were false. For instance, if the underlying simulated learning rate for (3,2) was 0.08 and the simulated learning rate for (2,3) was 0.15 then the rule (3,2) > (2,3) would be a false positive (0.08 < 0.15). This is done for all 140 data files. The total number of rules is three per experiment thus there are 420 rules to be found in the 140 data files. 
 3.5 Simulation Results.
 The average percent of found rules per simulated user size is plotted in Figure 2 below. The percentage of false positives is also plotted in the same figure and represents the error. 
 Figure 4. Results of simulation study.
 Figure 4 shows that more users allows for more rules about item order to be detected. It also shows that the false positive rate remains fairly constant, averaging around the 6% mark. From 200 users to 1,000 users the average percentage rules found was around 30% which would correspond to about 1 rule per problem set (0.30 * 3). This percentage rises steadily in a linear fashion from 500 users up to the max number of users tested of 20,000 where it achieves a 69% discovery rate which corresponds to about two rules per problem set on average. The error starts at 13% with 200 users and then remains below 10% for the rest of the user sizes. The overall average percent of rules found across users sizes is 43.3%. The overall average false positive rate is 6.3% which is in line with the binomial p value threshold of 0.05 that was used and validates the accuracy of the method's results and dependability of the reported binomial p value. 
 Limitations and Future Work. 
 One of the limitations of this permutation analysis method is that it does not scale gracefully. The number of network nodes that need to be constructed is exponential in the number of items. For a three item model there are six nodes per sequence and six sequences. For a seven item model there are fourteen nodes per sequence and 5,040 sequences (70,560 nodes). One potential optimization would be to only construct sequences for which there is data, which will be at most the number of students. The split 10 procedure has the effect of decreasing the amount of data the method has to operate on for each run. A more efficient sampling method may be beneficial, however, our trials using resampling with replacement for the simulation instead of splitting resulted in a high average false positive rate (>15%). A more sensitive test that takes into account the size of the difference between learned parameter values would improve reliability estimates. The binomial accuracy may also be improved by using a Bonferroni correction as suggested by a reviewer. This correction is used when multiple hypotheses are tested on a set of data (i.e. the reliability of item ordering rules). The correction suggests using a lower p value cut-off. There is a good deal of work in the area of trying to build better models of what students are learning. One approach [1] uses a matrix of skill to item mappings which can be optimized [2] for best fit and used to help learn optimal practice schedules [7] while another approach attempts to find item to item knowledge relationships [4] such as prerequisite item structures using item tree analysis. We think that the item order effect method introduced here and its accompanying paper [5] have parallels with these works and could be used as a part of a general procedure to try to learn better fitting models. 
 Contribution. 
 This method has been shown by simulation study to provide reliable results suggesting item orderings that are most advantageous to learning. Many educational technology companies [8] (i.e. Carnegie Learning Inc. or ETS) have hundreds of questions that are tagged with knowledge components. We think that this method, and ones built off of it, will facilitate better tutoring systems. In [5] we used a variant of this method to figure out what items are causing the most learning. In this paper, we presented a method that allows scientists to see if the items in a randomly ordered problem set produce the same learning regardless of context or if there is an implicit ordering of questions that is best for learning. Those best orderings might have a variety of reasons for existing. Applying this method to investigate those reasons could inform content authors and scientists on best practices in much the same way as randomized controlled experiments do but by utilizing the far more economical means of investigation which is data mining. 
 Acknowledgements.
 We would like to thank the Worcester Public Schools and the people associated with creating ASSISTment listed at www.ASSISTment.org including investigators Kenneth Koedinger and Brian Junker at Carnegie Mellon and also Dave Brown and Carolina Ruiz at Worcester Polytechnic Institute for their suggestions. We would also like to acknowledge funding from the U.S. Department of Education’s GAANN and IES grants, the Office of Naval Research, the Spencer Foundation and the National Science Foundation.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Determining the Significance of Item Order In Randomized Problem Sets</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/192/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/193">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Student Consistency and Implications for Feedback in Online Assessment Systems</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/193/authorlist"/>
		<swrc:abstract>Most of the emphasis on mining online assessment logs has been to identify content- specific errors. However, the pattern of general “consistency” is domain independent, strongly related to performance, and can itself be a target of educational data mining.  We demonstrate that simple consistency indicators are related to student outcomes, and suggest how consistency might be used in an online assessment framework to provide scaffolding to help students in need.</swrc:abstract>
		<led:body><![CDATA[ 1. The second factor loaded .96 on Final Part2.Q3. This pattern of loading suggests that the first factor represents general knowledge gained in the course, separate from the format of the exam (e.g., open-ended questions versus multiple choice). We use the first factor as an outcome measure of general class knowledge. 
 4  Results.
 Table 1 shows the mean and standard deviation for the outcome measures, measures of time spent completing the laboratory exercise, and the consistency measures. There was clear variation among the students on all measures, including the consistency measures that may seem obvious (for example, writing the same response on the worksheet that was used in the exercise). This variation is particularly substantial considering that these students are highly selected into a competitive state university, and are expected to have developed skills that would result in higher consistency measures than the population as a whole. There is also significant variation in scores on the final exam. We note that the average score on Final Part2.Q2, the more difficult of the two questions dealing with image processing, is lower than the average score of Final Part2.Q1. This suggests that the questions were difficult enough to avoid ceiling effects. 
 Table 1. Summary statistics for outcome measures, general logfile measures, and consistency measures. Maximum score or range of scores is given, where appropriate, in parentheses following the measure. 
 To examine the differences in outcome measures based on consistency, we split the students by the median sum score of consistency (7) forming two groups. We call these the low consistency group (N=14) and the high consistency group (N=13). We conducted a one-way ANOVA to determine whether outcome measures differ across the two groups. Results are summarized in Table 2. On average, the high consistency group scored higher on all outcome measures (though not all differences were significant). The high consistency group obtained significantly greater worksheet scores (p=.002). The difference in scores is particularly noteworthy, because the consistency measures do not measure correct or incorrect responses, whereas the worksheet scoring does. Furthermore, the high consistency group scored marginally significantly higher on the Final Part2.Q1 (p=.083) and significantly higher on the Final Part2.Q2 (p=.049). These two questions of the final were designed to measure the same concepts covered by the worksheet, and were administered two weeks afterward. The high consistency group also scored significantly higher on the class knowledge measure extracted from the midterm scores (p=.018). The effect size (measured by Cohen’s d) was quite large. Differences in consistency cannot be attributed solely to “carelessness” or greater or lesser time spent on the assignment. The high consistency group logged fewer events (not significant) with more parse errors (marginally significant, p=.062). The average time spent between logged events was virtually identical among the two groups. 
 Table 2. Analysis of variance. 
 5 Implications for Feedback.
 We have proposed some rough indicators of consistency and shown that the low consistency students perform worse on several important class outcomes than the high consistency students. It is particularly remarkable that we find such variation in consistency in such a highly selected population. We do not know the reasons why students are inconsistent, and we have not demonstrated whether higher consistency results in higher performance or whether it is a side-effect of something else (e.g., low level of engagement or interest in the class). This is a topic for future research. However, the inconsistencies that we have coded are rather blatant. When a student repeatedly types in the wrong command and does not recognize discrepancies in the results or attempt to reconcile them, it is reasonable to believe that such events may provide a learning opportunity. Furthermore, the needs of this student in this circumstance are not based on a model of how the student interacts with the pedagogical content of the assignment. We suggest that consistency across several dimensions may be dynamically monitored and used to adaptively control scaffolding (additional guidance) for the student. Scaffolding involves (a) reducing the number of steps required to solve a problem by simplifying the task (b) keeping the student motivated (c) marking discrepancies between actions taken and the desired solution (d) controlling frustration and (e) demonstrating an idealized version of the task [9]. It is a technique used primarily when students are learning new material, and cannot handle complex problems. Graesser et al showed that use of scaffolding, including good questions and answers, could promote deep inquiry, which students tended to avoid without prompting [10]. For example, suppose students did not follow the directions on the worksheet correctly and did not realize it. An ideal student would have recognized that “something was wrong” and taken some action to resolve the cognitive dissonance, checking the last command executed or re-entering the command. If still confused, the student could ask a classmate or the instructor for help. As part of an online assessment system, we wish to encourage such behavior. An appropriate first step would be to emphasize the cognitive dissonance. In the assignment in the experiment, expectations are outlined in the text, e.g., by writing “Apply a formula that makes a monochrome image in which the cedar foliage is white and everything else is black” before providing the formula. However, the discrepancy could be highlighted further by showing an image of the expected result and asking the student, “Does your image look like this?” before proceeding. In the extreme case when the same error is repeated, we can assume that the mistake is not inconsistency but represents some higher order misconception. For example, in the PixelMath interface there are buttons for common functions, such as XOR. Other functions can be typed in the window. One command asked students to apply the BXOR formula to exclusive-OR two images on a bit-by-bit basis, and many incorrectly applied the XOR function. It is possible that students who do not correct their error with appropriate scaffolding may not understand the difference between bitwise-exclusive-OR and straight exclusive-OR, or may not realize that they can type equations directly into the PixelMath formula area without clicking buttons. This might require specific formative feedback that reteaches these concepts to the student. A common misconception about image processing systems such as PixelMath is that the formula tells where to move each pixel of the source image.  In reality, the formula describes, for each destination pixel location, where or how to get its value.  This “push- instead-of-pull” notion is exhibited by students asked to come up with a formula to, say, reduce the size of an image by a factor of two.  Instead of writing Source1(x*2, y*2) which is correct, they write Source1(x/2, y/2). Similarly, to shift an image 5 pixels to the left, they should write Source1(x+5, y), but they put down Source1(x-5, y).  After seeing a consistent pattern of such incorrect formulas, an automatic feedback system could provide scaffolding to specifically address the “push-instead-of-pull” misconception. 
 6 Conclusions.
 We have identified a dimension of student performance, consistency, that is content independent, easily mined from educational logs, and that is related to performance outcomes. We suggest that because consistency is an assumption that underlies many educational interventions, the significance of lack of consistency may be overlooked as a potential opportunity to provide scaffolding. We give some suggestions for how an adaptive learning system might exploit consistency measures to scaffold instruction, and to identify when consistency of incorrect responses suggests moving to an intervention based on more sophisticated models of the learner, content, and their interaction. 
 Figure 1. The background window contains the original image used by students for the laboratory activity. The fence has been extracted into another window (above, and zoomed out by a factor of 2) and downsampled into another (below). The PixelMath calculator can also be seen here with the correct formula for the downsampling: Source1(5*x, 5*y). 
 Figure 2. Detail within the downsampled picket fence showing the effect of sampling at the Nyquist rate. Also, PixelMath’s display of the RGB values of each pixel can be seen.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Student Consistency and Implications for Feedback in Online Assessment Systems</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tara-m-madhyastha"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tara-m-madhyastha"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/steven-tanimoto"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/steven-tanimoto"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/193/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/tara-m-madhyastha"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/steven-tanimoto"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Process Mining Online Assessment Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194/authorlist"/>
		<swrc:abstract>Traditional data mining techniques have been extensively applied to find interesting patterns, build descriptive and predictive models from large volumes of data accumulated through the use of different information systems. The results of data mining can be used for getting a better understanding of the underlying educational processes, for generating recommendations and advice to students, for improving management of learning objects, etc. However, most of the traditional data mining techniques focus on data dependencies or simple patterns and do not provide a visual representation of the complete educational (assessment) process ready to be analyzed. To allow for these types of analysis (in which the process plays the central role), a new line of data-mining research, called process mining, has been initiated. Process mining focuses on the development of a set of intelligent tools and techniques aimed at extracting process-related knowledge from event logs recorded by an information system. In this paper we demonstrate the applicability of process mining, and the ProM framework in particular, to educational data mining context. We analyze assessment data from recently organized online multiple choice tests and demonstrate the use of process discovery, conformance checking and performance analysis techniques.</swrc:abstract>
		<led:body><![CDATA[ 1. The process mining spectrum supported by ProM. 
 3 Case Studies.
 We studied different issues related to authoring and personalization of online assessment procedures within the series of the MCQ tests organized during the mid-term exams at Eindhoven University of Technology using Moodle2 (Quize module tools) and Sakai3 (Mneme testing component) open source LMSs. To demonstrate the applicability of process mining we use data collected during two exams: one for the Data Modeling and Databases (DB) course and one for the Human- Computer Interaction (HCI) course. In the first (DB) test students (30 in total) answered to the MCQs (15 in total) in a strict order, in which questions appeared one by one. Students after answering each question were able proceed directly to the next question (clicking “Go to the next question”), or first get knowledge of correct response (clicking the “Check the answer”) and after that either go the next question (“Go to the next question”) or, before that, request a detailed explanation about their response (“Get Explanations”). In the second (HCI) test students (65 in total) had the possibility to answer the MCQs (10 in total) in a flexible order, to revisit (and revise if necessary) the earlier questions and answers. Flexible navigation was facilitated by a menu page for quick jumps from one question to any other question, as well as by “next” and “previous” buttons. In the MCQ tests we asked students to also include the confidence level of each answer. Our studies demonstrated that knowledge of the response certitude (specifying the student’s certainty or confidence of the correctness of the answer) together with response correctness helps in understanding the learning behavior and allows for determining what kind of feedback is more preferable and more effective for the students thus facilitating personalization in assessment [3]. For every student and for each question in the test we collected all the possible information, including correctness, certitude, grade (determined by correctness and certitude), time spent for answering the question, and for the DB test whether an answer was checked for correctness or not, whether detailed explanation was requested on not, and how much time was spent reading it, and for the HCI test whether a question was skipped, revisited, whether answer was revised or the certitude changed.4 In the remainder of this section we demonstrate how various ProM plug-ins supporting dotted chart analysis, process discovery (Heuristic Miner and Fuzzy Miner), conformance checking, and performance analysis [1][6] allow to get a significant better understanding of the assessment processes. 
 3.1 Dotted Chart Analysis.
 The dotted chart is a chart similar to a Gantt chart. It shows the spread of events over time by plotting a dot for each event in the log thus allowing to gain some insight in the complete set of data. The chart has three (orthogonal) dimensions: one showing the time of the event, and the other two showing (possibly different) components (such as instance ID, originator or task ID) of the event. Time is measured along the horizontal axis. The first component considered is shown along the vertical axis, in boxes. The second component of the event is given by the color of the dot. Figure 2 illustrates the output of the dot chart analysis of the flexible-order online assessment. All the instances (one per student) are sorted by the duration of the online assessment (reading and answering the question and navigation to the list of questions). In the figure on the left, points in the ochre and green/red color denote the start and the end (passed/failed) of the test. Triangles denote the moment when the student submits an answer or just navigates to another question. Green triangles denote correct responses with low (LCCR – light green) and high (HCCR – dark green) certainty, red triangles correspondingly – wrong responses (light red – LCWR, dark red – HCWR), white triangles – the cases when the student navigated to the next question without providing any response. The blue squares show the moments when the students navigated from the list of the questions (menu) to a question of the quiz (or just submitted the whole test). 
 Figure 2. Two dotted charts extracted from the test with flexible order navigation; (1) the overall navigation and answering of questions (left chart), and (2) the effects of changes (right chart). 
 We can clearly see from the figure that most of the students answered the questions one by one, and provided more correct answers for the first questions of the test than for the last questions. They used the possibility to flexibly navigate mainly at the end of the test: students navigating to the list of the questions and then to the different questions from the list. It can be also clearly seen that only few students read and skipped some questions, not providing their answers first, and then returning to those questions back to provide an answer. In the figure on the right, we can see the when students revisited the questions.  Points in yellow correspond to the situations when correctness of the answers did not change, and points in red and green correspond accordingly to changes to wrong and correct answers. We can see that in a very few cases the correctness was changed, most changes do not affect correctness (e.g., a wrong answer was changed to another wrong answer). Moreover, changes from right to wrong or from wrong to write had similar frequencies, thus not significantly changing the end results. 
 3.2 Process discovery.
 In some cases, given a usage log we may have limited knowledge about the exact procedure of the assessment but want to discover it based on the data from the log. There exist several algorithms that can automatically construct a depiction of a process. This process representation typically comes in form of a (formal) mathematical model supporting concurrency, sequential and alternative behavior (like, e.g., the model of Petri nets, Heuristic or Fuzzy miner). Figure 3 illustrates for the DB test a part (for the first 3 questions) of the discovered process (left) as a Heuristic net, and animation of the same part after conversion to the Fuzzy model (middle), and for the HCI test the complete Heuristic net (right), abstracted from the type of the answer, but from which it is clear which jumps between the questions were popular. From the visualization of the DB test process we can see what possibilities students had, and what the main “flows” were globally or at a particular time. 
 Figure 3. Heuristic nets of strict order (left) and flexible order tests (right) 
 3.3 Process analysis.
 In some cases, the goal is not to discover the real learning process but to analyze some normative or descriptive model that is given a-priori. For example, the Petri net shown in Figure 4 (formally) describes the generic pattern of answering questions in the DB test allowing for answer-checks and feedbacks. Now it is interesting to see whether this model conforms to reality (and vice versa) and augment it with additional information learned from the event logs. The advantage of having the answering pattern represented as a Petri net is that this allows for many different analysis techniques. ProM offers various plug-ins to analyze Petri nets (verification, performance analysis, conformance, etc.). Models like the one in Figure 4 can be discovered or made by hand. It is also possible to first discover a model and then refine it using the tool Yasper (incorporated into ProM). Figure 4 was constructed using Yasper and this was a one-time task for this test-type and in principle an authoring tool can be developed to facilitate an automatic translation of the multiple-choice tests with varying properties to Petri nets. As every question can be answered correctly or wrongly, and with either high or low confidence, there are four possibilities for the first step in the net from Figure 4. The transition HCCR, for example, denotes that the answer is given with high confidence and that it was correct; the other three starting transitions are similar. After answering the question the student can check his answer or just go the next question. The latter decision is modeled by an internal transition (painted in black) that goes to the final place of the net. In case the student has decided to check the answer, he can also ask for some feedback afterwards. 
 Figure 4. A Petri net representing the question pattern. 
 To illustrate the many analysis possibilities of ProM, we show some results obtained using the Conformance checker and the Performance Analysis with Petri net plugin. The purpose of conformance analysis is to find out whether the information in the log is as specified. This analysis may be used to detect deviations, to locate and explain these deviations, and to measure the severity of these deviations. We are mostly interested in the notion of fitness which is concerned with the investigation whether a process model is able to reproduce all execution sequences that are in the log, or, viewed from another angle, whether the log traces comply with the description in the model (the fitness is 100% if every trace in the log corresponds to a possible execution of the model). This notion is particularly useful for finding out whether (or how often) the students respected the specified order for answering questions (to discover frauds, for example). Figure 5 shows the result of conformance checking when applied on our log and the Petri net from Figure 4. In this, so-called log perspective of the result, each trace from the log has all its mismatched events colored in orange. In our case, however, there are no orange events, therefore there are no mismatches between the specified answering pattern and the actual exam data. 
 Figure 5. Result of conformance checking showing a 100% fitness. 
 Our next analysis is of a different kind. Instead of checking for the correctnes of the exam behavior, we provide a means to assess the performance of the answering process. The Performance analysis with Petri net plugin can extract the Key Performance Indicators from the log, summarizing them in an intuitive way, and graphically present them on a Petri net describing the process under consideration. For our purpose we apply the plugin with the exam data log and the answering pattern from Figure 6 (only for the first question of the test). 
 Figure 6. Results of applying the Performance analysis with Petri net plug-in. 
 The result of the analysis is shown in Figure 6. In the right panel different throughput- type metrics are displayed; from there we, e.g., see that the average duration of the test was 64.41 minute. The central panel shows the answering pattern, colored and annotated with performance information. The numbers on the arcs represent probabilities. As shown, 35% percent of the students answered the first question right and had high confidence.  We could also see that almost all students checked their answers and asked for feedback afterwards. Places are colored with respect to their soujourn time, i.e., with respect to the time the process spends in this place. From the picture we can thus see that the answering time was short (the first question was easy), and that the students who answered with high confidence spent more time on the feedback (regardless on the correctness of the answer). 
 4 Conclusions and Future Work.
 Data mining techniques have been successfully applied to different types of educational data and have helped to address many issues by using traditional classification, clustering and association analysis techniques. Although the process perspective in educational domains has received some attention, most of the traditional intelligent data analysis approaches applied in the context of educational data mining do not consider the process as a whole (i.e., the focus is no data or simple sequential structures rather than full- fledged process models). In this paper, we illustrated some of the potential of process mining techniques applied to online assessment data where students in one of the tests were able to receive tailored immediate EF after answering each of the questions in the test one by one in a strict order, and in the other test – to receive no feedback but to answer question in a flexible order. This data was of a sequential nature, i.e. it did not include concurrency. However, other educational processes have lots of concurrency and this can be discovered by ProM. Applying process mining techniques for other types of assessment data, e.g. grades for traditional examinations is therefore an interesting possibility. ProM 5.0 provides a plugable environment for process mining offering a wide variety of plug-ins for process discovery, conformance checking, model extension, model transformation. Our further work includes the development of EDM tailored ProM plug- ins. On the one hand, this would help bringing process mining tools closer to the domain experts (i.e. educational specialists and researchers), who not necessarily have all the technical background. On the other hand, this will help to better address some of the EDM specific challenges related to data preprocessing and mining. Besides this, the development of the authoring tools for assessment modules with specialized ProM plug- ins would allow to significantly simplify some of the processes for conformance analysis as e.g. a Petri net representing certain assessment procedure can be generated completely automatically. 
 Acknowledgements.
 This work is supported by NWO (the Dutch Science Foundation). We would like to thank the many people involved in the development of ProM.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Process Mining Online Assessment Data</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/n-trcka"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/n-trcka"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/wil-van-der-aalst"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/wil-van-der-aalst"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/p-de-bra"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/p-de-bra"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/n-trcka"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/wil-van-der-aalst"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/p-de-bra"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/195">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Predicting Students Drop Out: A Case Study</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/195/authorlist"/>
		<swrc:abstract>The monitoring and support of university freshmen is considered very important at many educational institutions. In this paper we describe the results of the educational data mining case study aimed at predicting the Electrical Engineering (EE) students drop out after the first semester of their studies or even before they enter the study program as well as identifying success-factors specific to the EE program.  Our experimental results show that rather simple and intuitive classifiers (decision trees) give a useful result with accuracies between 75 and 80%.  Besides, we demonstrate the usefulness of cost-sensitive learning and thorough analysis of misclassifications, and show a few ways of further prediction improvement without having to collect additional data about the students.</swrc:abstract>
		<led:body><![CDATA[ 1. The OneRule classifier reached the accuracy of 68% taking the VWO Science mean as a predictor. None of the other classification algorithms was able to learn a model which would outperform it (statistically) significantly. Attribute ranking (with respect to the class attribute) according to the information gain criterion showed that the VWO Science mean, VWO main and VWO Math mean were by far the best attributes in information gain (information gains 0.16, 0.13, 0.12 respectively), with the next “closest” attribute VWO Year lagging behind (0.05). Furthermore, these three attributes are highly correlated and therefore it is logical to expect it would be hard to learn a more complex and yet generalizable classifier with a relatively small dataset. Learning a classifier with feature selection also does not improve the results a lot. Learning a J48 tree using only the three mentioned attributes gives an average accuracy of 71%. 
 Table 1. Classification accuracy on pre-university dataset. 
 The same classification techniques were applied to the dataset with the university grades (Table 2). The OneRule algorithm results in the classifier which checks the grade for Linear Algebra (LinAlgAB), and decides positive if this grade is bigger than 5.5 (that is exactly the minimum for passing a course). Again we can see that more sophisticated classification techniques do not improve accuracy very much. However, it is worth noticing that the CART classifier is statistically significantly better than the base line with a classification accuracy that is 4.8% higher on average. 
 Table 2. Classification accuracy on university grades dataset. 
 The CART classifier learnt a compact tree with five leaves and uses LinAlgAB as root of the tree, and CalcA, Calc1 and Project nAttempts as further discriminators. It is worth noticing that the grades of the Networks course are not used at all, while some of its attributes have higher information gains. Correlation analysis however does show that correlation between Linear Algebra and Networks attributes is rather strong, but weak between Linear Algebra and Calculus attributes. 
 3.2 Classification with complete data. 
 Classification accuracies for the dataset containing both pre-university and university related data are shown in Table 3 (column indexes correspond to those in Tables 1 and 2). 
 Table 3. Accuracy and rates of total dataset. 
 It can be seen that these accuracies are comparable with those achieved on the dataset with university related data only. Apparently, the pre-university data does not add much independent information that can improve classification accuracy. However, we can see that the trees learnt with J48 are now statistically significantly better than the base line model. The other tree-based classifiers also achieve reasonable accuracy, while the Bayes Net and JRip algorithms slightly fall behind. To get a better insight on the performance of classifiers, the scoring of the algorithms is shown in more detail now. A remarkable fact is that the base line model has a higher false negative rate than all other models. This is an interesting finding, because according to the student counselor it is better to give an erroneous positive advice to a student who should actually be classified as negative, than to give a erroneous negative advice to a student who should be classified as positive. Cost-sensitive learning can be used to balance classification accuracies or boost the accuracy for a particular type of prediction. 
 3.3 Boosting accuracy with cost-sensitive learning. 
 In order to “advice” a classification algorithm to prefer one type of misclassification to another a cost matrix (that has a direct mapping to the confusion matrix) is commonly used as an input to a meta classifier: classified as negative classified as positive actual negative C(−,−) C(−,+) actual positive C(+, −) C(+,+) By choosing the weights C(i, j) in a certain way we can achieve a more balanced classification in case of severe class imbalances (using the diagonal entries), or a more cost-effective classification (using the off-diagonal entries). Since cost matrices are equivalent under scaling, and we only want to increase the cost of false negatives over false positives, it suffices to build a matrix with only one free coefficient and structure [[0 1] [C 0]], with C > 1. Since our experiments favored tree-based learners we used J48, J48graft and CART as base classifiers in Weka’s CostSensitiveClassifier. To prevent the tree from growing too big, we used the CfsSubsetEval feature subset selection algorithm that tries to select the most predictive attributes with low intercorrelation. The J48 and J48graft classifiers were forced to have at least 10 instances for each node in order to prevent overfitting and unnecessarily complex models. Combining these CART, J48 and J48graft with the two ways of using the cost matrix in cost-sensitive approach (data weighing and model cost), six experiments were conducted using F measure for defining the precision-recall tradeoff (we used β = 1.5). For each combination, the settings giving the highest F measure is presented in Table 4. The tree learnt with the “plain” J48 is presented in the first data column. The results indicate that it is necessary to sacrifice some of the achieved accuracy to be able to shape the misclassification. Only model 5 achieves a high accuracy and a high F measure, all other models lose in accuracy if F is increased. During the experiment, it became clear that there is not much room for enhancement: if recall increased to values higher than 85%, the overall accuracy results were unacceptable. The only exception is model 7 (notice the size of this tree being much larger comparing to other models and also seem to be too detailed to be meaningful for decision making). In some cases, small trade-offs could be made changing C. Compare for instance model 5 with model 6: a three percent point drop in accuracy gives a three percent rise in recall. The created decision trees are remarkably similar: in every tree the LinAlgAB attribute is dominant, with CalcA as first node in most of the cases. When NetwB is chosen as the first node, the recall is lower, although the difference is too small to draw decisive conclusions. 
 Table 4.  Accuracy results with cost-sensitive learning. 
 4 Further evaluation of the obtained results. 
 As the final step, we examined one of the models (model 7 from Table 4) in more detail to see if we can gain better understanding of the classifier errors. The student counselor compared all the wrongly classified instances of model 7 with his own given advices to check for interesting patterns. One of the first assessed things was the question whether the learned model is incorrect or the classification criterion is chosen incorrect. To examine this, two methods were used. Firstly, the false negative and false positive sets have been checked manually by the student counselor. His conclusions were that about 25% of the false negatives should be true negatives instead. This finding might indicate a wrong classification measure. Concerning the false positive set a conclusion is less obvious: about 45% of this set was classified as positive by the student counselor as well as by the tree, but did not meet the classification criterion. A substantial subset of these students have chosen not to continue their bachelor program in Electrical Engineering although all indications for a successful continuation were present. Qualifying these students as false positive does not seem to be appropriate. So from this evaluation based on domain expertise we can conclude that some of the mistakes might be due to the classification measure, and some of them raise suspicion on behalf of the learned model. The second way to check the viability of the model is to compare the results obtained with this classifier with respect to the three class classification problem, i.e. identifying first manually the third so-called risk group and then checking whether wrongly classified students will be in the risk class (that would indicate that the learned model is actually more accurate and also that it has difficulties in predicting the students who are difficult to classify into success or failure categories per se). However, we observe that only 25% of the misclassified instances are in this category. It should be noted that this is still twice as much as the risk students ratio in the total dataset. Therefore, this also indicates that the learned model should be improved. Furthermore, 25% of the instances in the false positive class would be classified as good using the three-class classification thus indicating a real difference between two classifiers. So from this test we can also conclude that the model as well as the classification criterion should be revised. After the analysis of errors, the misclassified sets are looked up in the database to search for meaningful patterns manually. A very clear pattern popped up immediately: almost all misclassified students did not have a database entry concerning LinAlgAB (and therefore were mapped to zero). Checking out different students showed that there are many possible reasons now to have a zero value in the LinAlgAB record: a) a student might be of a cohort in which the LinAlgAB exam was in January or later; b) a student might have not shown up during the exam; and c) a student might have taken another way to get its LinAlgAB grade: in some years it was possible to bypass the regular exam by doing the subexams LinAlg1, LinAlg2, LinAlg3, LinAlg4 and LinAlg5. A student succeeding in taking this path can well be an excellent student, but gets a zero mark for the LinAlgAB attribute. Due to this effect, 216 of the 516 students do have a zero entry in their LinAlgAB record (of which 155 instances were classified as unsuccessful and 61 instances as successful). Moreover, the same effect will play a role for the other courses too. Given the dominant position of the LinAlgAB attribute in the decision trees generated in section 3.3, attempts in completing the data-set should be considered worthwhile. 
 5 Conclusions and Future work.
 Student drop out prediction is an important and challenging task. In this paper we presented a data mining case study demonstrating the effectiveness of several classification techniques and the cost-sensitive learning approach on the dataset from the Electrical Engineering department of Eindhoven University of Technology. Our experimental results show that rather simple classifiers give a useful result with accuracies between 75 and 80% that is hard to beat with other more sophisticated models. We demonstrated that cost-sensitive learning does help to bias classification errors towards preferring false positives to false negatives. Surprisingly (according to the student counselor) the strongest predictor of success is the grade for the Linear Algebra course, which has in general not been seen as the decisive course. Other strong predictors are grades for Calculus, Networks and the mean grade for VWO Science courses. The most relevant information is collected at the university itself: the pre-university data can be summarized into a few attributes. The in depth model evaluation pointed to three major improvements that can be assessed. Firstly, a key improvement in this dataset would be to find a solution for the changing course organization over the set. Aggregating the available information about student performance for a course in a way that can be used for all students in the dataset might prevent the type of misclassifications that is now strongly prevalent. A second, related improvement would be a better way to encode grades in general. Mapping all unknown or not available information to zero showed to be not effective. Specifically, Linear Algebra grades should be available. A more advanced solution dealing with missing values also can be considered in this respect. The quality of the classification criterion is the third improvement that might be considered. The simple binary classification as used in this study has some disadvantages: a negative classification can only be given after three years, and there is no guarantee that a student who does not get his propedeuse after three years will be not successful in the long run. Also, students who do not receive a propedeutical diploma, should not necessarily be “disqualified”: they may have had different motives to discontinue their studies. This touches on a more fundamental topic: it is not easy to find an objective way of classifying students. In this paper we experimented with the so-called 0/1 loss and cost-sensitive classification. AUC optimization is also one of the directions of further work. As a final remark we would like to point out that this study shows that learning a model on less rich datasets (i.e. having only pre-university and/or first-semester data) can be also useful, provided the data preparatory steps are carried out carefully.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Predicting Students Drop Out: A Case Study</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/gerben-w-dekker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/gerben-w-dekker"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jan-m-vleeshouwers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jan-m-vleeshouwers"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/195/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/gerben-w-dekker"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/jan-m-vleeshouwers"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/196">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Dimensions of Difficulty in Translating Natural Language into First Order Logic</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/196/authorlist"/>
		<swrc:abstract>In this paper, we present a study of a large corpus of student logic exercises in which we explore the relationship between two distinct measures of difficulty: the proportion of students whose initial attempt at a given natural language to first-order logic translation is incorrect, and the average number of attempts that are required in order to resolve the error once it has been made. We demonstrate that these measures are broadly correlated, but that certain circum- stances can make a hard problem easy to fix, or an easy problem hard to fix. The analysis also reveals some unexpected results in terms of what students find dif- ficult. This has consequences for the delivery of feedback in the Grade Grinder, our automated logic assessment tool; in particular, it suggests we should provide different kinds of assistance depending upon the specific ‘difficulty profile’ of the exercise.</swrc:abstract>
		<led:body><![CDATA[ 1. The 20 sentences in Exercise 7.12. 
 We are interested in developing a richer understanding of what it is that makes a translation exercise difficult. We identify two different measures of difficulty, and explore the relation- ship between them; this allows us to characterize each translation problem in terms of its difficulty profile, which in turn can be used to vary the kind of feedback provided. Section 2 provides some background to the present study and introduces the corpus we use. Section 3 introduces our two measures of difficulty; Section 4 then discusses the relation- ship between these two measures, and what they reveal about the nature of the problems students face. In Sections 5 and 6, we focus on two particular difficulty profiles, exploring what they mean for our assessment tool. Finally, Section 7 provides some conclusions and points to future work. 
 2 The Data.
 The corpus consists of student-generated solutions to exercises in Language, Proof and Logic (LPL) [3], a courseware package consisting of a textbook together with desktop ap- plications which students use to complete exercises.1 Students may submit answers to 489 of LPL’s 748 exercises online; the other exercises require that students submit their answers on paper to their instructors. The electronic submissions are processed by the Grade Grinder (GG), a robust automated assessment system that has assessed approximately 1.8 million submissions of work by more than 46,000 individual students over the past eight years; this population is drawn from approximately a hundred institutions in more than a dozen countries. These submissions form an extremely large corpus of high ecological validity. For the work reported here, we focus on a specific exercise we selected from LPL; this is a natural language (NL) to first-order logic (FOL) translation exercise of moderate difficulty, i.e. one that psychometrically discriminates between students. Exercise 7.12 from Chapter 7 (which introduces conditionals) was selected because it has the highest proportion of erroneous submissions of all translation exercises. This exercise involves translating each of twenty English sentences into FOL. Our desktop applications offer relatively little useful feedback for exercises of this type, so compared to other exercise types, a higher proportion of submissions to the Grade Grinder contain errors. A submission for Exercise 7.12 consists of a solution for all twenty sentences, and is considered erroneous if the student makes an error on at least one of the solutions. For this study we focus on the calendar year 2007, during which period a total of 2558 students attempted this exercise. 14% got the exercise right without making a single error. The high proportion of submissions containing errors is in part a reflection of the fact that the exercises contains twenty translation tasks, all of which the student would have to have correct first time to avoid being found erroneous. For this study, we examined the corpus of erroneous submissions of Exercise 7.12, representing the work of a set of 2221 students. A translation for a sentence (which we refer to here as a solution) is considered correct if it is equivalent to a reference solution; there are infinitely many correct answers for any sentence, so a theorem prover is employed to determine equivalence. The sentences from Exercise 7.12 are presented in Figure 1. The reference solution for Sentence 1 in Figure 1 is Tet(a) → FrontOf(a,d). The emphasis in the Grade Grinder is on self-remediation of errors. The Grade Grinder’s response to an erroneous submission of the form FrontOf(a, d) → Tet(a), a common error, takes the form: *** Your first sentence, "FrontOf(a, d) -> Tet(a)", is not equivalent to any of the expected translations. This very uninformative, ‘canned’ feedback is typical of the Grade Grinder’s responses. Students using the Grade Grinder may make as many submissions of a given exercise as they need to obtain the correct answer. This means that the corpus contains repeated submissions by the same student of the same task, and enables us to track their path to a solution. A student’s work is reported to their instructor only when the student chooses, typically because it is reported as correct or because their deadline has arrived. Further information on the Grade Grinder, and samples of feedback reports, can be found on the GG website.2 
 Figure 2. Proportion of students who get each sentence wrong (left), and average stickiness values for the 20 sentences (right). 
 3 Measuring Difficulty.
 How do we measure the difficulty of a translation exercise? Of course, the author of a textbook uses intuitions about difficulty when preparing the exercises used in that textbook. However, even when based on extensive experience, this invariably involves some degree of subjectivity, and it fails to acknowledge that different students may find different problems difficult to different degrees. Ideally we would like to determine the difficulty of an exercise on the basis of empirical data, and further, be able to take account of the fact that different students may face different problems. We consider here two possible measures of difficulty based on the data we have available. • First, we can look at the proportion of students who get a particular exercise wrong; the assumption here is that the more students who get an exercise wrong, the more difficult that exercise must be. We refer to this value as the sentence’s PSI (for ‘the Proportion of Students who provide an Incorrect answer’.) Figure 2 (left) shows, for each of the 20 sentences, the proportion of this sample whose initial attempt at that sentence resulted in an incorrect answer. On this basis, we can observe, for example, that Sentences 1 and 12 are relatively easy, whereas Sentences 5, 6 and 7 are relatively difficult. • A second measure can be obtained by considering how many attempts it takes for a student to determine the correct answer once they have made their initial mistake. We call this the stickiness of the error. Thus, every student has a stickiness value for every sentence they get wrong; for any sentence they correct on their second attempt, the stickiness value of that sentence for that student is 1. If we average this value over all students, we obtain a stickiness measure for the sentence. Figure 2 (right) shows the average stickiness values for the 20 sentences in the exercise based, for each sentence, on the subset of the 2221 students in the sample that made an error on it. This illustrates that Sentence 6 is much stickier—which is to say that it is ‘harder to fix’, even given the Grade Grinder’s feedback—than Sentence 5 which with it shares a high PSI. Since we have both values for each sentence in the data, we can combine these to produce a tuple we refer to as the difficulty profile of the sentence; this captures both the likelihood of a student getting the sentence wrong, and the average number of attempts it takes to fix the error. 
 4 Correlating the Dimensions.
 As noted above, 2221 students who made submissions to Exercise 7.12 made one or more errors. For each of these students, a co-occurrence matrix was constructed for the 20 sen- tences. Each cell of an individual student’s matrix coded the relationship between one distinct pair of sentences. For example, if the student made an error on Sentence 3 and an error on Sentence 12, then the cell at [3,12] would be coded 1 (co-occurrence of error), oth- erwise zero. The individual subject matrices were summed to produce a combined matrix for all 2221 subjects. The summed matrix was input to the SPSS Proximities procedure to produce a similarity matrix. The similarity matrix formed the input to the SPSS Cluster procedure, which was used to compute a multilevel, agglomerative, hierarchical cluster analysis using Ward’s method (see, e.g., [4]). The item clusters are arranged hierarchically with individual items at the leaves and a single cluster at the root. Dendrograms provide a graphical display of cluster analysis output. The dendrogram for all subjects’ data (Figure 3) shows the sentence clusters. Bifurcations that are more distant from the leaf nodes mean that the clusters are more dissimilar. For example, in Figure 3, the cluster of Sentences 1, 12, 3, and 15 indicates that many students who made errors on, say, Sentence 3 also tended to make errors on Sentences 1, 12 and 15. The primary branch at the root level indicates two quite distinct major clusters and three somewhat less dissimilar subclusters within the upper main branch (labelled 1–4 in Figure 3). We then looked at where the individual sentences lay on a scatter plot of PSI against Stick- iness; we noted that the dendogram clusters correspond to four distinct bands in Figure 4. 
 Figure 3. Dendrogram representation of cluster analysis outputs. Leaf node numbers correspond to Exercise 7.12 Sentences 1–20. Clusters labelled 1, 2, 3 and 4 correspond to bands in Figure 4. 
 The figure makes it clear that there is not a direct correspondence between our two measures of difficulty, although there is reasonable correlation in the case of many sentences (Spearman’s ρ = .61, p = .004). We single out for particular attention two situations in- volving outliers (and thus where our two measures of difficulty in some sense ‘conflict’): sentences with a high PSI but a low stickiness, and sentences with a low PSI but a high stickiness. 
 5 Hard to Get Right, But Easy to Fix.
 We can characterise sentences with a high PSI and a low average stickiness as being hard to get right—many students get them wrong first time—but easy to fix: once you know you’ve got it wrong, it’s easy to work out what the correct answer is. The most salient examples of this category in our data are Sentences 19 and 20, repeated here for convenience: 
 19. a is large just in case d is small. 20. a is large just in case e is. 
 
 Figure 4. Scatterplot of the difficulty measures PSI and Stickiness. Data point labels indicate Sentences 1–20 of Exercise 7.12 and the ‘bands’ correspond to the four clusters identified in Figure 3. 
 Both of these use the natural language expression just in case, which translates into FOL as the biconditional, ‘↔’. As we noted in [1], students find this expression particularly difficult, perhaps because it is so rarely used (at least with this interpretation) in natural language.3 The vast majority of students who get Sentence 19 wrong offer either of the following two translations, reflecting the common misunderstanding of just in case as a bare conditional: • Small(d) → Large(a) • Large(a) → Small(d) The analogous errors also occur very frequently for Sentence 20. In each case, we hypoth- esise that the students realise that the translation involves some kind of conditional; if they don’t get it right the first time, and incorrectly offer a conditional as in the cases above, the most obvious next alternative is the biconditional, which is the correct answer. In effect, there is a very small space of plausible alternative answers given the belief that some flavor of conditional is required, and therefore a relatively low likelihood of making a second error. This space of potential answers is further constrained by the simple nature of the sentences: they are amongst the shortest sentences in the exercise. Sentence 19 mentions only two objects (a and b) and two unary predicates (Large and Small). 
 In everyday language, the expression just in case is most often used as an approximate synonym for ‘as a precaution’. This is not what is meant when it is used by logicians or mathematicians, as is explicitly taught in the LPL textbook. 
 This suggests that stickiness is related to the extent to which each submitted (or re-submitted) sentence translation by the student reduces the space of plausible solutions. 
 6 Easy to Get Right, But Hard to Fix.
 We can characterise sentences with a low PSI and a high average stickiness as being easy to get right—most students get them right first time—but hard to fix: if you get it wrong, it’s hard to work out what the correct answer is. Sentence 8 represents a reasonable example of this phenomenon: 8. c is in back of a but in front of e. The implication of the difficulty profile of this example is that most students know how to translate but into FOL, but if a student doesn’t understand this, being told they are wrong (the current feedback provided by the Grade Grinder) is not of any particular help. The high average stickiness of this sentence suggests that students are at a loss as to what the correct answer might be, perhaps even trying random variations on their initial solution to see what works. In contrast to Sentence 19, discussed in the previous section, students are less likely to make errors on Sentence 8 and it is much stickier (Figure 2). That the PSI is low is probably explained by the fact that the logical connective that translates but is the relatively simple logical and (∧). This translation is introduced in an earlier chapter in LPL and may have been internalized by many students by the time they attempt this exercise. The surface structure of the NL sentence suggests many possible reasons to suppose that students who err in their first attempt find this sentence sticky. Sentence 8 mentions three objects (a, c, and e) and involves two binary relations (BackOf, FrontOf), and the NL sen- tence contains an elided reference to c which has to be made explicit in the FOL translation. Given the uninformative feedback from the Grade Grinder, we conjecture that students do not know which of these features they have misunderstood. The situation is probably compounded by the fact that this exercise appears in the chapter of the book devoted to conditionals, but does not require a conditional in its translation. 
 7 Conclusions.
 What should the implications of the analysis presented here be? We have endeavoured to demonstrate that a unidimensional estimation of difficulty based simply on how many students get an exercise right or wrong masks important variations in the kinds of problems students face. In the analysis presented here, we distinguish the proportion of students who get an exercise wrong as a measure from a measure of how easy it is for a student to correct a wrong answer. More broadly, we can identify four extremes that characterise individual problems: Hard to Get Wrong, Easy to Resolve: Such problems may be of limited pedagogical value, although they might serve to build a learner’s confidence. Easy to Get Wrong, Easy to Resolve: These might play a role in encouraging care or vigilance, and so might be appropriate for delivery to a careless student. Hard to Get Wrong, Hard to Resolve: These probably don’t belong in the curriculum, since they are likely to engender frustration in the student. Easy to Get Wrong, Hard to Resolve: These are the most challenging problems, perhaps best reserved only for those students who are on top of the curriculum. None of these extremes are ideal; what we really want to do is use exercises that are more or less balanced, perhaps with a bias towards difficulty in one or other dimension for par- ticular pedagogical purposes. As a student works through the LPL exercise set, we may be able to incorporate a measure of how they respond to exercises with different difficulty pro- files into a student model. Ideally, appropriate examples should be dynamically generated automatically in response to this measure as it is revealed. We would like to exploit our richer conception of logic exercise difficulty to enhance and enrich Grade Grinder’s feedback and to individualise LPL’s curriculum. To achieve this the Grade Grinder requires representations of the characteristics of sentences (such as number of predicates, number of constants, and arity). To this end we are currently developing knowledge representations of stimulus features: this task involves characterizing each LPL exercise in terms of its resources, represented as matrices of properties (for example, number of constants in NL or FOL sentences, number of predicates, types of predicates, their arity, number and types of connectives, and so on). Then we aim to derive, for each sentence, the space of plausible alternative answers based on the number of terms in a sen- tence, the number of predicates and their arities, etc. A large space of plausible alternative answers suggests prima facie a more difficult exercise. We will be able to validate the predictive difficulty measures by correlating them with PSI and stickiness, to determine whether these are all of the (and the only) factors playing into students’ experiences with the exercise, and which of the surface features are predictive of PSI and which of stickiness. The sentence characteristics (including the two difficulty measures and the stimulus fea- tures) could be encoded in a manner akin to q-matrices and used to represent students’ con- cept states at different stages of learning [2]. Stimulus feature analysis of LPL’s resources will also inform their decomposition into constituent sub-concepts and skills. Armed with this knowledge we should then be able to track how many times a student has encoun- tered each sub-concept to-date. We may also employ search algorithms and statistical techniques (for example, multivariate logistic regression) to build models of each student’s learning [5]. When the Grade Grinder detects that a student is manifesting a more-than- average number of errors, it can generate bespoke exercises for individual students using methods akin to those used in AI-based adaptive psychometric item generation [6]. Such approaches require, inter alia, rich representations of stimulus features, empirical data on item difficulty, and the application of item response theory (IRT) [7]. The ultimate aim is to generate exercises, judiciously adjusting the difficulty parameters and concepts they contain for each individual student.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Dimensions of Difficulty in Translating Natural Language into First Order Logic</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dave-barker-plummer"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dave-barker-plummer"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-cox"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-cox"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-dale"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-dale"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/196/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/dave-barker-plummer"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-cox"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-dale"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/197">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>A User-Driven and Data-Driven Approach for Supporting Teachers in Reflection and Adaptation of Adaptive Tutorials</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/197/authorlist"/>
		<swrc:abstract>It has been recognized that in order to drive Intelligent Tutoring Systems (ITSs) into mainstream use by the teaching community, it is essential to support teachers through the entire ITS process: Design, Development, Deployment, Reflection and Adaptation. Although research has been done on supporting teachers through design to deployment of ITSs, there is surprisingly little discussion about support for teachers’ Reflection - the ability to draw conclusions from ITS usage, and Adaptation - adapting the content to better meet the needs of students. We describe our work on developing analysis tools and methodologies that support reflection and adaptation by teachers. The work was done in the context of helping teachers understand student’s behavior in Adaptive Tutorials by post-analysis of the system’s data-logs. We used a hybrid solution – part of the data-mining effort is teacher driven and part is automated. We tested our approach by comparing the results of expert analysis of two Adaptive Tutorials with and without an automated Refinement Suggestion Tool, and found it to be a useful teacher’s aid. By using this tool, teachers act as ‘action researchers’, confirming or disproving their hypotheses about the best way to use ITS technology.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>A User-Driven and Data-Driven Approach for Supporting Teachers in Reflection and Adaptation of Adaptive Tutorials</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dror-ben-naim"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dror-ben-naim"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-bain"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-bain"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nadine-marcus"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nadine-marcus"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/197/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/dror-ben-naim"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-bain"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/nadine-marcus"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>How do Students Organize Personal Information Spaces?</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198/authorlist"/>
		<swrc:abstract>The purpose of this study is to empirically reveal strategies of students' organization of learning-related digital materials within an online personal information archive. Research population included 518 students who utilized the personal Web space allocated to them on the university servers for archiving information items, and data describing their directory hierarchies. Several variables for measuring folders size and depth were defined, and four of them were chosen as best representing different aspects of the user's archive structure. Then, as a result of cluster analysis of the students, four organization strategies emerged, refining the classical piling/filing classification: piling, one-folder filing, small-folders filing, and big-folder filing. Also, associations were found between the organization strategies and archive size, students' studies degree. A discussion of this study and further research is provided.</swrc:abstract>
		<led:body><![CDATA[ 1. On average, each student has 80.52 (SD=170.17) files and 13.58 (SD=45.33) directories. 
 Table 1. Descriptive statistics for the four describing variables.
 After clustering the students according to the four variables, we have calculated means and SD for each variable within each cluster; results are given in Table 2, where maximum and minimum values for each variable are bolded and italicized, accordingly. 
 Table 2. Means (SD) of the four variables by which the clusters were formed. 
 As might be seen from the table, Cluster 1 (n=141) is characterized by extreme values of two variables' means among clusters: Pile rate gets a maximum (0.97), and inner-pile rate gets a minimum (0.02). These results imply that in this cluster, most of the students' files are stored in the root directory (hence it is not surprising that the second largest folder is extremely small). These two extreme values of variables are typical for Piling organization strategy. In Cluster 2 (n=49), again the means of the same two variables as in Cluster 1 get to their extreme values, however in different direction. In this cluster, the mean of pile rate is minimal (0.09), and we may think that this is a non-piling strategy. However, the mean of inner-pile rate is relatively high (0.86), which indicates on the existence of a folder holding a large share of the archive. That means that the files were saved in one main folder out of the root directory – a strategy that we may call One-folder Filing. Cluster 3 (n=262) has minimum mean values for two variables: Files per folder and Largest folder, i.e., students in it have small folders on average (6.1), and their largest folder is also relatively small (14.52). This suggests that the cluster represents a Small- folders Filing organization strategy. In Cluster 4 (n=66), the means of the same two variables as in Cluster 3 take their extreme values: Both files per folder (23.1) and largest folder (71.62) are maximal. By examining the mean value of pile rate (0.13), it might be concluded that about 87% of their files are filed, with one folder containing about half of their files (0.48). Therefore, this cluster, which we call Big-folder Filing, describes a mixture of filing and piling. According to this analysis of the clusters, we present the following classification of personal information space organization strategies: Piling, One-folder Filing, Small- folders Filing, and Big-folder Filing. Table 3 shows the distribution of the four types in the research population. 
 Table 3. Personal Information Space Organization Strategies distribution. 
 For examining the association between the archive size and its organization strategy, mean values for archive size (total number of files) were compared between the clusters. Using Univariate ANOVA test, it was shown that the means are significantly different. As may be seen from Table 4, two strategies (Piling, One-folder Filing) have a small archive size on average (24.4 and 22.31, respectively), while the largest mean value for archive size (284.73) was found in the Big-folder Filing cluster. This indicates that larger archives are associated with strategies of filing into more than one directory. 
 Table 4. Archive size in the different clusters. 
 5 Discussion.
 The main purpose of this study was to empirically identify different types of personal information organization strategies, which are part of Personal Information Management (PIM), and to do so for a large population, using data mining methodologies. PIM is not only a coherent and integral part of the learning process in the digital era - it is a process through which students learn. Therefore, researching PIM in the context of learning is very important for having a broader understanding of the learning process. Applying data mining techniques for PIM research brings new and fascinating opportunities to this field, as was demonstrated in this study. Focusing on users' management of online personal archives, we were able to empirically identify four types of archiving strategies: a) Piling – most of the files are in the root directory; b) One-folder Filing – most of the files are located in one folder, under the root directory; c) Small Folders Filing – items are being divided into many relatively small folders (about 6 files per folder on average); d) Big-folder Filing – items are being divided into folders (about 23 files per folder on average) with about a half of them located in one big folder. These four types refine the classical Filing/Piling binary classification [14]. As the results suggest, students who tend to be Big-folder Filers, manage the largest archives and have relatively many files per folder on average. In order to construct a hierarchy of large coherent folders of different items related to a certain context (represented by each folder's name), students are required to a meaningful integration and generalization processes regarding the subject matter. Our analysis showed that more than half of the participating students were categorized as Small-folders Filers. As this strategy is characterized by the use of small folders, this might imply that there are relatively many near-empty folders. Empty folders might indicate on a pre-building strategy, as was previously observed in the context of students' PIM [8]. Having many empty folders might increase PIM complexity, as well as having big folders. The strategy of Big-folder Filing was found in this study as associated with large archives, supporting previous findings [11]. In the context of learning, increasing PIM complexity is of special interest as PIM activities require cognitive skills. Bloom's cognitive taxonomy for learning objectives [5] enables us to analyze the three main PIM activities – i.e., naming, sorting, and categorization – in the light of three different levels of the taxonomy's cognitive skills: knowledge, analysis, and synthesis, accordingly. Regarding the four personal organization strategies found in this study, we might suggest different levels of reflected activities. In Piling strategy, the students neither name, sort nor categorize any information items. In One Folder Filing strategy, the students name only few folders and don't sort or categorize at all. In Small Folders Filing, the students name folder and sort information items into them, however they only do little categorization (since they join only few items into each folder). Only in Big-folder Filing strategy, students name, sort and categorize many items into folders. As the results suggest, managing bigger archives requires a wider range of cognitive skills. Replicating the process described in this article over several points in time might enlighten issues regarding changes over time of PIM strategies and their related cognitive activities. PIM is subjective and idiosyncratic, and because PIM research mostly uses qualitative data collection from relatively small populations, it might seem that there are as many PIM variations as there are researched users [12]. However, using a large research population and data mining techniques, unexpected patterns might arise, suggesting similarities between groups of users, as was shown in this study. To promote the creation of large datasets, Chernov et al. [9] have suggested building a repository of PIM activity log files; this then would serve the PIM research community. Since it is likely that there will be problems obtaining participants' consent to trace their PIM activity over time, it might be easier to collect structural data reflecting accumulating activity.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>How do Students Organize Personal Information Spaces?</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hama-abu-kishk"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hama-abu-kishk"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ofer-bergman"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ofer-bergman"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/hama-abu-kishk"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/ofer-bergman"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/199">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Automatic Detection of Student Mental Models During Prior Knowledge Activation in MetaTutor</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/199/authorlist"/>
		<swrc:abstract>This paper presents several methods to automatically detecting students' mental models in MetaTutor, an intelligent tutoring system that teaches students self-regulatory processes during learning of complex science topics. In particular, we focus on detecting students' mental models based on student- generated paragraphs during prior knowledge activation, a self-regulatory process. We describe two major categories of methods and combine each method with various machine learning algorithms. A detailed comparison among the methods and across all algorithms is also provided. The evaluation of the proposed methods is performed by comparing the prediction of the methods with human judgments on a set of 309 prior knowledge activation paragraphs collected from previous experiments with MetaTutor on college students. According to our experiments, a content-based method with word-weighting and Bayes Nets algorithm is the most accurate.</swrc:abstract>
		<led:body><![CDATA[ 1. The paragraphs are reproduced as typed by students. Entire paragraphs are not shown due to space reasons. 
 Table 1.  Examples of PKA paragraphs for High (H) and Low (L) mental models (MM). 
 Given such a PKA paragraph, the task is to infer the student mental model. We work with three qualitative mental models: low, medium, and high. We view the task of detecting the student mental models as a standard classification problem. The general approach is to combine textual features with supervised machine learning algorithms to automatically derive classifiers from expert-annotated data. The parameters of the classifiers will be derived using six different algorithms: naive Bayes (NB), Bayes Nets (BNets), Support Vector Machines (SVM), Logistic Regression (LR), and two variants of decision trees (J48 and J48graft, an improved version of J48). These algorithms were chosen because of their diversity in terms of patterns in the data they are most suited for. For instance, naive Bayes are best for problems where independent assumptions can be made among the features describing the data. The assortment of the selected learning algorithms provides some diversity in terms of potential weighting and dependency patterns among the features used to model the task at hand, e.g. naïve Bayes assume total independence among features. In order to find a good method and algorithm for inferring student mental models based on PKA paragraphs, we have investigated two categories of methods and combined them with the above six machine learning algorithms. In one category of methods, called content-based, student-generated PKA paragraphs are automatically compared with various sources of knowledge describing the learning goal. The sources can be (1) a collection of pages that describe the goal, (2) a taxonomy that includes the major concepts related to the goal, or (3) ideal/expected paragraphs, written by human experts, describing the learning goal and its subgoals. The second category of methods, called word-weighting, maps student-articulated PKA paragraphs onto a set of features in which individual words act as features and the corresponding values are weights derived using distributional information of the words across a corpus of documents (in our case the PKA paragraphs). This latter method resembles traditional text classification models [14] in that it uses individual words as features (some classification models also use the position of the words in the documents). In addition to all the above methods, we also experimented with two baseline algorithms random guessing and uniform guessing, i.e. guessing all the time the dominant category in the training data. The rest of the paper is structured as follows. Background presents the mental models in MetaTutor and previous work on automatic student input assessment. The subsequent section, Methods, describes in detail the methods we proposed whereas Experimental Setup and Results presents performance figures, lessons learned, and also outlines plans for the future. The Conclusions section ends the paper. 
 2 Background.
 MetaTutor is an adaptive hypermedia learning environment that is designed to detect, model, trace, and foster students’ self-regulated learning about human body systems such as the circulatory, digestive, and nervous systems [5]. Theoretically, it is based on cognitive models of self-regulated learning [1, 17]. The underlying assumption of MetaTutor is that students should regulate key cognitive and metacognitive processes in order to learn about complex and challenging science topics. The design of MetaTutor is based on extensive research by Azevedo and colleagues’ showing that providing adaptive human scaffolding, that addresses both the content of the domain and the processes of self-regulated learning, enhances students’ learning about challenging science topics with hypermedia [2, 3, 4, 5, 10]. Overall, their research has identified key self-regulatory processes that are indicative of students’ learning about these complex science topics. More specifically, they include several processes related to planning (e.g., generating sub-goals), metacognitive monitoring processes (e.g., feeling of knowing, judgment of learning), learning strategies (coordinating information sources, summarization), and methods of handling task difficulties and demands (e.g., time and effort planning). 
 2.1 Mental Models.
 Mental models are mental representations that include the declarative, procedural, and inferential knowledge necessary to understand how a complex system functions. Mental models go beyond definitions and rote learning to include a deep understanding of the component processes of the system and the ability to make inferences about changes to the system. The acquisition of mental models of complex systems can be facilitated through presenting multiple representations of information such as text, pictures, and video in hypermedia learning environments [12]. Therefore, hypermedia environments, such as MetaTutor, with their flexibility in presenting multiple representations, have been suggested as ideal learning tools for fostering sophisticated mental models of complex systems [1, 8]. Detecting mental model shifts during learning is an important step in diagnosing ineffective learning processes and intervening by providing appropriate feedback. One method to detect students' initial mental model of a topic is to have them write a paragraph. Cognitively, this activity allows the learner to activate their prior knowledge of the topic (e.g., declarative, procedural, and inferential knowledge) and express it in writing so that it can be externalized and amenable to computational methods of analysis. A mental model can be categorized qualitatively, and depending on the current state (e.g., simple model vs. sophisticated model), is then used by the hypermedia system to provide the necessary instructional content and learning strategies (e.g., prompt to summarize, coordinate informational sources) to facilitate the student's conceptual shift to the next qualitative level of understanding. Along the way, students can be prompted to modify their initial paragraph and thereby demonstrate any subsequent qualitative changes to their initial understanding of the content. This qualitative augmentation is a key to an intelligent, adaptive hypermedia learning environment’s ability to accurately foster cognitive growth in learners. This process continues periodically throughout the learning session. 
 2.2 Mental Models Coding.
 Due to their qualitative nature, most researchers develop complex coding schemes to represent the underlying knowledge and most often use categorical classification systems to denote and represent students' mental models. For example, Chi and colleagues' early work [7] focused on 7 mental models of the circulatory system. Azevedo and colleagues [1] extended their mental models classification to 12 to accommodate the multiple representations embedded in their hypermedia learning environment. In this paper, we have re-categorized our existing 12 mental models of the circulatory system (see [10] for the details) into 3 categories of low-, intermediate, and high-mental models of the circulatory system. The rationale for choosing the 3-category mental models approach was to enhance the ability of determining students' mental models shifts during learning with MetaTutor and because the 12 mental models approach would have been too detailed of a grain size to yield reliable classifications and thus to accurately assess "smaller" qualitative shifts in students' models. 
 2.3 Previous Work on Evaluating Natural Language Student Input in.
 Intelligent Tutoring Systems and Automated Essay Grading Researchers who have developed tutorial dialogue systems in natural language have explored the accuracy of matching students' written input to a pre-selected stored answer: a question, solution to a problem, misconception, or other form of benchmark response. Examples of these systems are AutoTutor and Why-Atlas, which tutor students on Newtonian physics [9, 16], and the iSTART system, which helps students read text at deeper levels [13]. Systems such as these have typically relied on statistical representations, such as latent semantic analysis (LSA; [11]) and content word overlap metrics [13]. LSA has the advantage of representing texts based on latent concepts (the LSA space dimensions, usually 300-500) which are automatically derived from large collection of texts using singular value decomposition (SVD), a technique for dimensionality reduction. More recently, a lexico-syntactic approach, entailment evaluation [15], has been successfully used to meet the challenge of natural language understand and assessment in intelligent tutoring systems. The entailment approach has been primarily tested on short student inputs, namely individual sentences. Both LSA and the entailment approach pose some challenges for evaluating the PKA paragraphs we have to handle. LSA requires the construction of a LSA space based on a large collection of documents from the domain of interest, i.e. the circulatory system. Collecting such tests is a time consuming task. Also, LSA suffers from the text-length confound which means using it for handling paragraph-length texts would lead to high similarity scores, probably resulting in many false positives. The entailment approach has been designed for sentence-to-sentence relation and thus it is not trivial to extend it to handle paragraph- to-paragraph tasks as it requires the use of a syntactic parser which operates on one sentence at a time. We do plan to extend it to handle paragraph-to-paragraph textual relation detection using coreference resolution components that will link concepts across sentences for a paragraph-level meaning representation. For the time being, we opted instead for a set of methods that combine simple textual overlap features with machine learning algorithms to automatically infer student mental models. We take advantage of the goals and subgoals in MetaTutor when choosing the features to be used in our solution to the student mental model detection problem, as explained later. The problem of detecting student mental models from PKA paragraphs is related to the task of automated essay scoring (AES), i.e. automatically evaluating and scoring written texts. The purpose in AES is to improve time, cost, reliability and generalizability of the process of writing assessment. Dikli [19] gives a fairly comprehensive survey of AES systems. AES systems require training , i.e. human-scored written texts, and rely on form and content features to score written texts. They do not really understand the texts or emulating the human scoring process. One difference between AES and MM detection is that the length of the input is different. Usually, in AES essay-long texts, which are comprised of many paragraphs, are considered while in our task of MM detection we work with smaller, paragraph-length texts. AES systems use the multi-paragraph structure of essays as part of the scoring algorithm while in the MM detection problem this structural information is less important. The content-based components of the AES systems could be used for the MM detection task. Some of our proposed methods resemble some of the content-based methods employed in AES systems (see the word weighting in the vectorial representation used in E-rater, which is described in [19]). 
 3 Methods.
 All the methods we implemented, except the baselines, have two major steps. The first step consists of data processing and feature extraction. The details of this step are specific to each method and will be described later. During a second step, we used machine learning algorithms to induce various classifiers for categorizing PKA paragraphs into high, medium, and low mental models. We experimented with the six machine learning algorithms mentioned earlier. It is beyond the scope of this paper to discuss in detail these algorithms (see [14, 18] for details). We used the implementation of the algorithms from WEKA, a machine learning toolkit [18]. The algorithms were run with the default parameters, e.g. SVM was run with the polynomial kernel. There is a large parameter space for these learning algorithms and we plan to tweak these parameters in the future in order to further investigate their behavior for our problem. For this paper, the machine learning phase was used to check the effectiveness of the preprocessing phase and of the chosen set of features and methods. The performance of all the methods was evaluated using 10-fold cross validation. In k- fold cross-validation the available data set is split into k folds. Then, one fold is kept for testing and the other (k - 1) are used for training. This process is repeated for each fold resulting in k trials. The reported performance is then computed as the average of the individual trials' performances. When k = 10 we have 10-fold cross-validation. To further increase the confidence in the estimated values of the reported accuracy, we have run 10- fold cross-validation 10 times, each time with a different seed value, which is an input parameter to k-fold cross-validation evaluation. The seed value affects the way instances in the data set are selected for the individual folds. Thus, for each method and learning algorithm we compute 10 * 10 = 100 performance scores and then take the average. The advantage of running 10-fold cross-validation 10 times with different seed points is that each instance in the original data set is evaluated 10 times. By comparison, a 100-fold cross-validation would result in each instance being evaluated once. We also ran paired t- tests among different methods and learning algorithms in order to check if differences in performance are statistically significant. We report performance in terms of accuracy and kappa coefficient. Accuracy is the percentage of correct predictions out of all predictions. Kappa coefficient measures the level of agreement between predicted categories and expert-assigned categories while also accounting for chance agreement. 
 3.1 Content-based Methods.
 The methods in this category rely on the presence of key concepts related to the learning goal in the student-articulated paragraphs. The key concepts are specified in different ways for the three methods in this category and it is in this aspect that the methods differ. The key concepts are specified in the three methods using the following benchmarks, respectively: (1) expert-created domain taxonomy, (2) original pages of content, and (3) expert-generated ideal descriptions of the learning goal and its subgoals. For all three methods, 8 features are computed: one feature corresponding to the overall learning goal and one feature for each of the 7 subgoals. The value of each feature represents the percentage of words in the entire benchmark (for the feature corresponding to the overall learning goal) or parts of the benchmark corresponding to subgoals (for subgoal features) that are present in the student-generated PKA paragraphs. For instance, for the taxonomy-based method (tax in Table 2) a taxonomy of concepts is the benchmark. The overall goal, i.e. learn about the circulatory system, is the top node of the taxonomy (see Figure 1). The seven subgoals are the nodes in the ideal level in the Figure 1. The parts of the taxonomy benchmark corresponding to subgoals are the subtrees below the subgoals nodes in the taxonomy. We use nodes in these subtrees to compute the values corresponding to the 7 subgoal-related features. The advantage of the taxonomy-based method is its simplicity and small computational costs as the taxonomy only includes several dozen concepts. The trade-off is the expert associated costs to build the taxonomy. In MetaTutor, the taxonomy was needed for assessing and feedback during another self-regulation process, subgoal generation, and thus there is no extra effort to build the taxonomy specifically for mental model detection. 
 Figure 1. Partial Taxonomy of Topics in Circulatory System. 
 N-grams methods are very similar to the taxonomy-based method. Instead of using the taxonomy to identify key concepts relevant to the learning goal or subgoals, we used the subset of content pages related to the overall goal or subgoals, respectively. The values for the features are computed as the percentage of N-grams, i.e. sequences of N consecutive words, in the benchmark, or parts of it for subgoal features, that are present in the PKA paragraphs. In this method, it is necessary to know which page is relevant to which subgoal. An expert mapped each individual page onto each subgoal. Also, to generate the N-grams the pages and PKA paragraphs are pre-processed: stop words are eliminated and the remaining words are lowercased and stemmed. Stop words are very frequent words such as determiners, e.g. the. Stemming is the process of mapping all morphological variation of a word to its base form, e.g. hearts and heart are mapped to heart. We used both unigrams (uni) and bigrams (bi) to compute content overlap. We also experimented with a combined method in which both bigrams and unigrams are used (uni-bi). Bigrams have the advantage (over unigrams) of capturing some word order, i.e. syntactic information. The N-grams methods have the advantage of needing no extra structures, e.g. expert-built taxonomies, to generate the features. We simply used the original content pages about the circulatory system from Encarta, which are used in MetaTutor. On the other hand, there is need for an expert to specify which content page is relevant to which subgoal. The biggest disadvantage of the N-gram method is their use of too much content to compare against, e.g. bigrams from all the content pages for the overall goal feature, as opposed to a set of well-selected key concepts from a taxonomy as is the case with the taxonomy-based method. In the last method in this category, called expectation-based, we started by asking domain experts to generate ideal descriptions for each of the seven subgoals. These descriptions are short textual paragraphs comprising of 5-7 sentences. The collection of all paragraphs for the 7 subgoals is used to derive the eighth feature corresponding to the overall learning goal. The values of the features are generated using unigram and bigram overlap between the ideal paragraphs and the student PKA paragraphs. In this method (labeled ip - ideal paragraphs - in Table 2), there is no need for creating a crisp taxonomy of concepts and decide which concepts is directly related to which concept. The effort to create the ideal paragraphs is less compared to building a taxonomy for instance. 
 3.2 Word-weighting Methods.
 In this category of methods, we select from each paragraph all the words that have minimum 4 letters (when all words were used performance results were slightly worse), excluding the stop words. The selected words are then converted to lower case and stemmed. The resulting set of words is used to describe the paragraphs, i.e. they are the features. Each feature is weighted using tf-idf (term frequency-inverted document frequency), which captures the importance of the corresponding feature for a given paragraph. Inverted document frequency (idf) is computed as the inverse of document frequency, which is the number of documents a term occurs in from a collection of documents. In our case, document frequency is the number of prior knowledge- paragraphs a term occurs in. Term frequency, tf, is the number of occurrences of a term/word in a document, i.e. a PKA paragraph. As a result, a total of 1038 features are extracted and used to describe each instance in data set. Other weighting schemes, besides tf-idf, could be used but the tf-idf proved to be successful in a number of other applications [6] which is the reason we chose it. 
 4 Experimental Setup and Results.
 4.1 The Dataset.
 In this paper, we have experimented with an existing dataset consisting of 309 mental model essays collected from previous experiments by Azevedo and colleagues (based on [2, 3]). The dataset consisted of entries from senior high school students and non-biology college majors. These mental model essays were classified by two experts with extensive experience coding mental models. Each expert independently re-coded each mental model essay into one of the three categories and achieved an inter-rater reliability of .92 (i.e., 284/309 agreements) yielding the following new dataset for this paper: 139 low mental models, 70 intermediate mental models, and 100 high mental models. The coders included a nurse practitioner and a high school biology teacher. 
 4.2 Results.
 We report results for all combinations of methods and learning algorithms mentioned earlier. In Table 2, rows correspond to methods and columns to learning algorithms. An analysis of the results revealed that a tf-idf method combined with Bayes Nets leads to best overall results in terms of both accuracy and kappa values. The second best results were obtained using a combination of unigrams and/or bigrams with SVM or LR. Both SVM and LR are called function-based classifiers as they are both trying to identify a function that would best separate the data into appropriate classes, i.e. mental model types in our case. For the random baseline we obtained (accuracy = 31%, kappa = -0.06 - a kappa close to 0 means chance) based on averaging over 10 random runs while for the uniform baseline, i.e. predicting all the time the dominant class, which is the Low mental model class, we obtained (accuracy = 45%, kappa = 0). 
 Table 2.  Performance results as accuracy(%)/kappa values. 
 Based on a more careful analysis of the results in Table 2, we found that given a method the choice of the machine learning algorithm is important. Looking at the results within each group of methods one can notice the relative large range of the performance figures. For instance, the accuracy values for the tf-idf method vary most from 57.70% for naive Bayes to 76.31% for Bayes Nets. For Bayes Nets the Weka’s default K-2 search algorithm was used. This variability indicates that this method is more sensitive with respect to the choice of the machine learning algorithm. We call such methods less stable. One possible explanation for the variability of the tf-idf method could be its large number of features used (1038) relative to the number of instances (309). This is not unusual for text classification as, for instance, a typical naive Bayes method [14] uses not only all the words in the documents to be classified but also their positions leading to a very large number of features. The last three groups of methods in Table 2 also show variability but they seem more stable as the range of the values is somehow smaller. The most stable methods are the ideal paragraph-based methods and the unigram/bigram methods. As unigram/bigram methods provide better results than the paragraph-based methods we could say that the former offer the best of performance and stability across various machine learning schemes. We plan to conduct a study on the stability of the tf-idf method once more PKA paragraphs are available from future MetaTutor experiments. Given its best performance overall, if we can show that this method is also stable if more training data is available - as we suspect - it would be a very important finding. 
 5 Conclusions.
 We presented and evaluated several methods for detecting student mental models in the intelligent tutoring system MetaTutor. We have found that a tf-idf method combined with a Bayes Nets algorithm provides the best accuracy and kappa values. Bigram-based methods combined with Logistic Regression or Support Vector Machines provide competitive results. In addition, bigram-based methods seem to be less sensitive to the choice of the machine learning algorithm compared to the tf-idf method. It is believed that tf-idf methods would be more stable if more training data would be available. 
 Acknowledgments.
 This research was supported by funding from the National Science Foundation awarded to R. Azevedo (0133346, 0633918, and 0731828) and V. Rus (0836259). We thank Amy Witherspoon, Emily Siler, Michael Cox, and Ashley Fike for data preparation.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Automatic Detection of Student Mental Models During Prior Knowledge Activation in MetaTutor</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/vasile-rus"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/vasile-rus"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mihai-lintean"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mihai-lintean"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/roger-azevedo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/roger-azevedo"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/199/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/vasile-rus"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/mihai-lintean"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/roger-azevedo"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/200">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Predicting Student Grades in Learning Management Systems with Multiple Instance Genetic Programming</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/200/authorlist"/>
		<swrc:abstract>The ability to predict a student's performance could be useful in a great number of different ways associated with university-level learning. In this paper, a grammar guided genetic programming algorithm, G3P-MI, has been applied to predict if the student will fail or pass a certain course and identifies activities to promote learning in a positive or negative way from the perspective of Multiple Instance Learning (MIL). Computational experiments compare our proposal with the most popular techniques of MIL. Results show that G3P-MI achieves better performance with more accurate models and a better trade-off between such contradictory metrics as sensitivity and specificity. Moreover, it adds comprehensibility to the knowledge discovered and finds interesting relationships that correlate certain tasks and the time devoted to solving exercises with the final marks obtained in the course.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Predicting Student Grades in Learning Management Systems with Multiple Instance Genetic Programming</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/amelia-zafra"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/amelia-zafra"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/200/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/amelia-zafra"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/201">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Detecting Symptoms of Low Performance Using Production Rules</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/201/authorlist"/>
		<swrc:abstract>E-Learning systems offer students innovative and attractive ways of learning through augmentation or substitution of traditional lectures and exercises with online learning material. Such material can be accessed at any time from anywhere using different devices, and can be personalized according to the individual student’s needs, goals and knowledge. However, authoring and evaluation of this material remains a complex a task. While many researchers focus on the authoring support, not much has been done to facilitate the evaluation of e-Learning applications, which requires processing of the vast quantity of data generated by students. We address this problem by proposing an approach for detecting potential symptoms of low performance in e-Learning courses. It supports two main steps: generating the production rules of C4.5 algorithm and filtering the most representative rules, which could indicate low performance of students. In addition, the approach has been evaluated on the log files of student activity with two versions of a Web-based quiz system.</swrc:abstract>
		<led:body><![CDATA[ 1. Introduction.
 Modern information technologies have found their ways into the classrooms: new applications (learning management systems, virtual labs, etc.), new devices (PDAs, Smartphones), new protocols (SMS, Bluetooth) are used by teachers and students in real- life education [4]. These technologies have facilitated the wider adoption of online e- Learning systems in the last decade. Among other benefits of these systems, we can obtain more interactivity, reach learning experience, flexibility of access, etc. However, design and evaluation of e-Leaning systems yet remain complex tasks that require both time and expertise that many classroom teachers do not have. In most of cases instructors or course designers need to design his/her e-Learning course. Development of better technologies for authoring and evaluation of e-Learning systems is a very important research and practical problem, which becomes even more challenging, when the technology uses classroom instructors as its target audience. In this paper we try to address this problem by proposing an approach to evaluation of e- Learning systems based on the analysis of log data. Evaluation of e-Learning systems is a complex and time-consuming task. One of the challenges for an instructor evaluating an e-Learning system is lack of evidence from students, since he/she has access only to their interactions with the learning material and cannot observe their behaviors, or receive the feedback from them in a timely manner. Due to the fact that student’s behaviors are hidden inside their interactions, instructors should analyze them in order to assess the performance of students and evaluate the learning software. Furthermore, e-Learning systems generate vast quantity of data (log files). These data consist of records of students’ actions within the system. Since the log files are often very big, traditional data analysis tools and techniques are not always useful. Therefore, an alternative technology needs to be applied. We considered that data mining methodology is more adequate for this task, because it is a technology that blends traditional data analysis methods with sophisticated algorithms for processing large volumes of data in order to discover meaningful information [11]. Our work is focused on evaluating an e-Learning system by exploring its usage logs in order to find patterns that could indicate low performance. It is worth to mentioning that these patterns are called symptoms. An example of a symptom is a significantly higher number of failures for a given exercise than the average number of failures for this exercise. It is difficult for instructors to detect symptoms of this kind without the help of data mining methods. At the same time, it can be highly beneficial for an instructor to be aware that a certain student has problems with particular set of exercises, and hence be able to intervene. How do such symptoms occur? What factors can trigger them? The present work tries to answer these questions by providing a method for detecting symptoms of low student performance in the course. Naturally, not every symptom observed in the real data, is an indicator of variations in performance in the course. It is useful to identify the relevant symptoms causing drops in performance; it is also useful to distinguish the most important of them. For these purposes, we have developed a method helping to filter and rank relevant symptoms. The method is based on production rules of C4.5 algorithm. This paper is organized as follows. The next section presents a brief overview of related work. Section 3 describes the data set we have used in this study. The proposed approach, as well as the results of the evaluation are detailed in Section 4. Finally, the last section concludes the paper with discussion and plans for future work. 
 2. State of the Art.
 Over the last years data mining has become a very popular technology in many areas of information sciences including e-Learning. Romero and Ventura provide a comprehensive overview of data mining applications for education [8]. Large pool of student activity has been collected by many research facilities. A good example is the open data repository DataShop that stores over 110 datasets containing nearly 18 million student actions [5]. Many researchers recognize the potential of data mining methods to help diagnose problems in e-Learning applications. For example, Ueno proposes a method for detecting irregular learning processes using student response time [12]. The method uses Bayesian predictive model and parametric statistical tests to identify potential outliers. Our approach it is somewhat similar to Ueno’s work; however, it is based on the use of decision trees and the analysis of students’ answers to learning problems. While Ueno’s system aims to help students, the goal of our project is to help course designers to design and evaluate e-Learning courses. Another relevant work has been done by Meceron and Yacef [6]. They propose to use the cosine and lift as two alternative measures of interestingness for association rules instead of confidence and support. These alternative measures are very successful in filtering uninteresting association rules. The selection of the interesting or relevant rules is one of the foci of our work, but in our case we filter production rules instead of association rules. 
 3. Data Description.
 The input data for this project have been provided by the School of Information Sciences at University of Pittsburgh (Pittsburgh, USA). The data were collected through several semesters of students’ interaction with QuizGuide and QuizPACK systems within the framework of introductory programming course. Only the data in 2007 were selected for this work, since the students of different years are not comparable because the contents of the course are different by each year. Consequently, records corresponding to 55 students of seven different groups were selected. They took an adaptive course of “Introduction to C programming” generating 52734 interactions with the system. 
 3.1 QuizGuide and QuizPACK: providers of the data.
 QuizGuide is a Web-based service that provides personalized access to self-assessment quizzes for C programming language [9]. It does not serve the quizzes itself; instead it stays as a wrapper between the quiz provider and the student’s browser and augments the quizzes with adaptive navigation cues. To guide students to the right learning content QuizGuide exploits adaptive link annotation technique. Quizzes and questions in QuizGuide are annotated with adaptive icons. Every icon delivers to a student two kinds of information: his/her individual progress with the corresponding content, the relevance of the content to the student’s current learning goal. The goal information is calculated based on the course schedule and the relation of the quizzes to different topics in the course. The progress information is calculated based on the individual student’s history of correct and incorrect attempts for the corresponding question, as well as other questions covering the same domain concepts. The quizzes are generated and evaluated by QuizPACK system [10]. Every QuizPACK question consists of a simple C program that students need to evaluate and answer what will be the output of the program or what will be the final value of the target variable. The important feature of QuizPACK is question generation. When the question is delivered to a student, one of the numeric values in the question text is dynamically instantiated with a random value. Consequently the students can try the same question again and again with different correct answers. 
 3.2 Log Data Description.
 When a student answers a question in QuizGuide a new log entry is added to the database. These entries consist of several fields. For our analysis we augmented the log entries with information about domain concepts. Table 1 shows three instances of the enhanced data log we analyzed (52734 cases). 
 Table 1. Log entries of students’ interactions with QuizGuide. 
 The header of Table 1 contains the following attributes: • userId: id of the student in the system. • groupId: id of the group to which the student belongs. Six groups belong to different colleges and one group to “world group”. This last group is created for the students who did not belong to these colleges and follow the e-Learning course by free access. • result: outcome of the student’s attempt. Two values are possible: 0 (incorrect answer) and 1 (correct answer). • activity: name of the quiz or activity. • concept: name of the concept covered by the activity. • conceptParent: name of the concept parent. • session: id of the web session (a session combines students activity between login and logout). • success: verbal representation of the field “result” (result = 1 corresponds to success = “yes”; result = 0 corresponds to success = “no”). • The system also stores the time stamp when the student started the activity. It is important to point out that a student can perform the activity more than once. This situation can be observed in the first and second row of Table 1. For instance, the first row shows that the student uid_199, belongs to the group gid_190, obtained 0 (success = no) in the activity 2dimensional_array1. This table also provides information between the activities and concepts, e.g., this activity is related to the concept printf and the general concept of this activity is output. In addition, this row provides information about the session id, 21BC5. However, the second row exhibits that the same student solved the exercise successfully in another attempt. Thereby, interactions and cases are the same concepts, but students are not equivalent than cases since they can repeat the activities many times. 
 4. Analysis of the Data.
 The goal of this work is to analyze the log data and to find significant patterns of behavior, which can provide support for improving the teaching material. In this context, decision trees were selected as the data mining technique, since they produced good results in previous works [2]. In particular, the current work uses the C4.5 algorithm [7]. The next step on the analysis process was to define which attributes from the log files would be analyzed and which one used as the class variable. Considering the intention of finding activities that can present difficulties for a subset of the students, two attributes were selected: activity (name of the activity) and groupId (group id). The idea was to detect activities that were significantly more difficult for students of a given group, compared to the students of the other groups. These attributes were considered enough for the task at hand, without adding excessive complexity to the problem. The class variable is success representing the outcome (is given as yes or not depending on whether the student solved the activity successfully or not) of students’ attempts to answer QuizPACK question. Consequently, two classes compose the classification model, The space of the problem is 644∗ combinations, which together with the data size indicate that the data analysis is complex enough to apply the decision trees method. The distribution of training data is shown in Table 2. The first row shows the values of the success attribute (class variable) and the number of cases covered by them. In this case, proportions of the classes yes and no are rather similar (46% versus 54%). This feature is suitable for building decision trees, which benefit from the balanced classes. The next seven rows provide the values of groupId and the number of cases for each of these values. It is clear that the group gid_67 has the lowest number of cases (70), and group gid_190 holds most of them (38382). Even though the data sets of groups with fewer cases could be removed, we decided to keep them, since every set of cases is valuable. Moreover, as it is more substantial for this research the model might be able to represent every set of data than provides better predictions. Finally, the last row of the table 2 offers the names of activities available (46 activities) in the adaptive course “Introduction to C programming”. For example, the first three activities (2dimensional_array1, 2dimensional_array2 and 2dimensional_array3) range over exercises of coding arrays in C language. 
 Table 2. Distribution of training data. 
 The space of the problem is defined by the whole combinations of the attributes. In this case, activity variable has 46 different values, groupId has seven values, and two classes are possible. As a result, the space of this problem is 46*7*2 = 644. It is worth to mentioning that the space is a measure of the problem complexity. 
 The third step in the analysis was to use the algorithm C4.5 in order to obtain the decision tree. The pruning in the tree was deactivated, that is to say, the confidence factor (CF) is set to 100%. The reason of setting this CF is because overfitting does not represent a problem, since the resulting decision tree is not used to make predictions. Actually, the better model fits the training data, the better these data can be described by the model, which is the goal of this work. Another relevant parameter of the C4.5 algorithm consists of grouping attribute values. This option supported two concerns: insufficiency of data and information-gain ratio criterion. The first concern is that useful patterns of the data are not detected due to insufficiency of data; therefore grouping values can get enough data for detecting these patterns. The second concern is related to the performance of information-gain ratio is lower when the attribute has many values [7, Chapter 5]. This option produces better results when the data contain discrete attributes with many values. In the case of this study, the attribute groupId contains 7 different values and the attribute activity has 47 values. As there are discrete attributes with many values in the data, this work exploits grouping attribute values option. As a result, applying the algorithm C4.5 a decision tree with size of 168 is obtained. It is worth to bring out that the tree is composed by 113 leaves (54 leaves with class no and 59 with class yes) and 55 decision nodes. Subsequently, the decision tree was analyzed. As the objective of this research is to find difficulties leaves with value no are more interesting than others. The initial attempt was to use the key node method [1], but this method requires analyzing every path from the leaf to the top of the tree in order to find the relevant decision nodes. Therefore, analyzing 54 paths requires to analyze each decision node of each path. One of the main problems is that some paths could be irrelevant or redundant, and key node method does not ensure avoid redundant paths. For this reason, the next attempt was to utilize an alternative method the production rulesψ of C4.5. 
 Quinlan defines a production rule as left part --> right part [7, p. 8-12]. The left-hand side contains the conditions and the right-hand side is referred to the class. If the case satisfies all the conditions (conjunction of attribute-based tests), it is classified by the value on the right part. For example, the rule Rule 32: activity = variable2 --> class no [69.5%] indicates if a case contains the activity variable2 this case will be classified to class no with almost 70% of accuracy. 
 These rules are based on two assumptions: the rules are easier to understand and the rules avoid redundant data. Every node in the decision tree has a context established by the tests’ outcomes of the previous node. In this sense, a rule is easier to understand since the context is on the left side [7, Chapter 7]. The other assumption is related to the fact that the trees sometimes produce similar sub-trees. In that regard, the process of building the rules removes irrelevant conditions and avoids redundant rules. Applying the C4.5rules program to the data generated 20 rules. As in the case of decision trees only the rules in which the right side is no are selected (11 selected rules). The last step was to filter the rules in order to select the relevant rules. Following this idea two approaches are possible: from the point of used cases (i.e. the cases in which all the conditions of the rule are satisfied) and from the point of accuracy (i.e. the percentage of correctly classified cases). Taking into account data description, we have chosen the former approach. C4.5rules also provides a ranking of non redundant rules based on the error rate (table 3 contains this information). It is important to note that three redundant rules corresponding to no class are not including in this ranking. For this reason, Table 3 presents eight rules with no class (they are highlighted in the table). The column Rule shows the id rule, the next column indicates the number of conditions in the rule, the column Error is an estimation of number of cases classified incorrectly, the column Used provides the used cases, the column Wrong exhibits the number of cases classified incorrectly, and the last column indicates the value of the class. 
 Table 3.Ranking of rules. 
 As the filter criterion is based on used cases, the selected rules were ordered by this column. Another selection is needed because some rules are not enough representatives. This new sub-set of rules is obtained by using a lower band of used cases. Therefore, a rule is not enough representative if it is below this limit. Experiments with the data disclosed that the adequate lower band is calculated as 10% of sum of used cases of the rules. In this case, the sum of used cases is equal to 26445; thereby the lower band is 2645. Only three rules satisfied this threshold (rules 6, 23 and 12), hence they were selected. Table 4 demonstrates these rules. 
 Table 4.Representative rules. 
 Rule 6 indicates that most of the students from the groups 190 and 373 showed difficulties in the activities character_array2, printing2 and while2. This rule might indicate a symptom of low performance on these quizzes for these groups of students. The next rule demonstrates problems that the students from the groups 190, 441, 373 and 394 had when they where trying activities 2dimensional_array2, do2, array1 and array3. Hence, further attention should be paid to these groups and activities. Several questions can be asked. Are the exercises adequate? Were students presented with the necessary knowledge? Lastly, the rule 12 shows that students from group 190 experienced problems in the most of activities. This rule could indicate that the students in the group 190 did not have enough background knowledge to take the course “Introduction to C programming”. The previous analysis showed the whole process of achieving the representative symptoms, i.e. selected production rules. Therefore, the proposed method is summarized in the following lines: • Generate the production rules by using C4.5 algorithm. • Select the rules of the ranking table in which the right part indicates a “failure” in activities. In our case, a failure is indicated by value no of the class success. • Filter limit • Select the rules which cover a number of used cases greater than the filter limit. o After this selection is possible to obtain only one rule. In this case, it is advisable to select also the next closer rule to the filter limit. It is important to note that the information displayed in these last rules could be useful to instructors or course designers for enhancing their e-Learning courses. Thus, it would be a valuable help to show this information to them, since they could evaluate their courses better by finding the last symptoms. 
 5. Conclusions and Future Work.
 This work has presented full analysis of real data of an e-Learning course. In this particular case 52734 instances were analyzed by using decision trees and production rules. The objective of the analysis was to find symptoms that could indicate presence of low performance in courses provided by QuizGuide and QuizPACK systems, i.e., to find particular activities in which a given category of students showed difficulties providing feedback to the instructors. The analysis demonstrated that three rules indicate the presence of symptoms of drops in performance for several profiles of students. In fact, one of the rules exhibits that the group 190 showed difficulties in the course, since they failed the majority of the activities in the course. This information could be relevant for the instructor or designer of the e- Learning course, because he/she can improve it by adding new activities, and/or modifying existing activities or course structure. Furthermore, the other two rules showed that students who belong to groups 373, 441 and 394 presented problems in solving particular activities. This fact also indicates drops in the performance of the system for these groups of students. This information is also useful for the instructor or course designer, since he/she can modify these activities in order to improve the learning process for these particular students. It is worth noting that this analysis can be performed with other type of data, even for data of different contexts. However, experiments with other types of data showed that the filter limit of this work is less accurate than others like third quartile of used cases. Decision trees and production rules present some weaknesses. They are strongly related to the distribution of the data and proportion of classes. Therefore, small variations in the data could cause different conclusions. Consequently, the future work includes supporting these techniques with other Data Mining methods such as association rules. Other future line includes supporting the method described in this work in ASquare [3]. The goal of this tool is to provide a high abstraction level interface. Thereby, ASquare supplies more intelligible results to human beings; that is to say, a person without knowledge of Data Mining can understand the results. Finally, it is important to test and validate the proposed analysis with other types of data. In this sense, future work also includes to apply this analysis for data of different educational systems in order to improve the detection method. 
 Acknowledgements.
 This work has been funded by Spanish Ministry of Science and Education through the HADA project TIN2007-64718. We would like to thank the University of Pittsburgh for providing us the interaction data of their students. The authors would also like to thank to Leila Shafti who contributed in this work with her helpful comments.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Detecting Symptoms of Low Performance Using Production Rules</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-bravo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-bravo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/201/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-bravo"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Using Learning Decomposition and Bootstrapping with Randomization to Compare the Impact of Different Educational Interventions on Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202/authorlist"/>
		<swrc:abstract>A basic question of instructional interventions is how effective it is in promoting student learning. This paper presents a study to determine the relative efficacy of different instructional strategies by applying an educational data mining technique, learning decomposition.  We use logistic regression to determine how much learning is caused by different methods of teaching the same skill, relative to each other. We compare our results with a previous study, which used classical analysis techniques and reported no main effect. Our results show that there is a marginal difference, suggesting giving students scaffolding questions is less effective at promoting student learning than providing them delayed feedback. Our study utilizes learning decomposition, an easier and quicker approach of evaluating the quality of ITS interventions than experimental studies.  We also demonstrate the usage of computer-intensive approach, bootstrapping, for hypothesis testing in educational data mining area.</swrc:abstract>
		<led:body><![CDATA[ 1. Statistics of students’ performance on pretest. 
 Table 2. Statistics of students’ performance on posttest. 
 2.2 Approach.
 2.2.1 Introducing learning decomposition.
 Beck [2] introduced the idea of learning decomposition that extends the classic exponential learning curve by taking into account the heterogeneity of different learning opportunities for a single skill. The standard form of exponential learning curve can be seen in Equation 1. In this model, parameter A represents students’ performance on the first trial; e is the numerical constant (2.718); parameter b represents the learning rate of a skill, and t is the number of practice opportunities the learner has at the skill. 
 Equation 1. Standard exponential learning curve model. 
 Equation 2. Learning decomposition model. 
 The model as shown in Equation 1 does not differentiate different types of practice, but just counts up the total number of previous opportunities. In order to investigate the difference two types of practice (I and II), the learning opportunities are “decomposed” into two parts in the model in Equation 2 in which two new variables t1 and t2 are introduced in replace of t, and t = t1 + t21. t1 represents the number of previous practice opportunities at one type I; and t2 represents the number of previous opportunities of type II. The new parameter B characterizes the relative impact of type I trials compared to type II trials.  The estimated value of B indicates how many trials that one practice of type I is worth relative to that of type II. For example, a B value of 2 would mean that practice of type I is twice as valuable as one practice of type II, while a B value of .5 indicates a practice of type I is half as effective as a practice of type II. The basic idea of learning decomposition is to find an estimate of weight B that renders the best fitting learning curve. Equation 2 factors the learning opportunities into two types, but the decomposition technique can generalize to n types of trials by replacing t with B1*t1 + B2*t2 + … + tn. Thus, parameter Bi represents the impact of a type i trial relative to the “baseline” type n. 
 2.2.2 Decomposing learning opportunities.
 Now that we have described the model of learning decomposition, we want to “decompose” students’ learning opportunities in our data set in order to fit such a model. Various metrics can be used as an outcome measurement of student performance. For instance, Beck [4] chose to model student’s reading time since it is a continuous variable. Although one may argue for other indicators, e.g. students’ help requests and response times, we simply choose to use the correctness of student’s first attempt to a problem as an outcome measure of their performance. A “1” in the data indicates the student got a problem correctly on the first attempt, and thus proceeded to the next problem without getting any instructional assistance, while a “0” means he failed on the first try and received certain type of tutoring from the system, depending on which condition the student has been assigned into. When it comes to a nominal variable, in our case, dichotomous (0/1) response data, a logistic model should be used. Now learned performance, (i.e. performance in Equation 2), is reflected by odds ratio of success to failure. Equation 3 represents a logistic regression model for learning decomposition. Where α, γ are the new representation of students’ initial knowledge and their learning rates of a skill on the logistic scale. Now that we have determined our outcome variable and functional form of the model, all that remains is to decompose learning opportunities into components. We split student trials into four groups largely on the basis of experimental condition as below. Therefore, the number n is equal to 4 in this analysis. • hint_wrong_trial (th) indicates the number of prior wrong trials that a student in the hint condition had encountered • scaffold_wrong_trial (ts) counts the number of prior wrong trials that a student in the scaffold condition had made before. • delayed_wrong_trial (td) is similar to the other two variables but for students in the delayed condition. However, it is specially calculated such that the prior encounters will not increase until the student was presented the explanations for all the problems in order to address the fact that the learning actually happened at the moment when the explanations were shown. Note that by doing so we assume that simple exposure to the content does not cause learning. It is also worth pointing out that although the approach of learning decomposition itself does not require the administration of pretests and posttests, in this particular analysis, we do need the results of posttest to be able to detect the impact of explanations (in the delayed feedback condition) on student learning. • Others (to). Because what we really care about is the relative effectiveness of the different tutoring interventions during the experiment, we did not differentiate students’ practice trials on pretest, posttest and trials where they gave a correct answer to the experiment problem. Instead, all these trials are combined together into the group others. Actually, since the number trials on pretest and posttest are the same for all students, it is the correct trial on experiment problems that matter in this group. For those readers who are familiar with ASSISTments vocabulary, it is also worth pointing out that although in the experiment there are three versions of the experiment problems with different associated interventions, one for each condition, we created one unified problem ID for all the three versions, since the main questions are the same. 
 Table 3. Decomposed response data of student A. 
 Table 3 shows a sequence of time-ordered trials of a student who was assigned in the scaffolding condition. The student finishes all three sections, fails on one of the pretest problems, but learns to solve the problem during the experiment as suggested by a correct answer to the same problem in the posttest. The right part of Table 3 shows the corresponding data after the trials are decomposed into component parts. Since the student is in the scaffolding condition, all values in the hint_wrong_trial and delayed_wrong_trial are zero. He solves the first encountered experiment problem wrong (row 3), which cause an increase on the value of scaffold_wrong_trial from zero to 1 ( row 4). Again, he gets the third experiment problem wrong (row 5), and then the value of scaffold_wrong_trial increases from 1 to 2 in row 6. The value of trial for others just increases by one whenever a pretest problem, a posttest problem or a correct trial was encountered. For instance, the student answers the second encountered experiment problem correct (row 4), and thus the value of others increased by 1 (row 5). Limited by space, we only demonstrate the decomposition process for a student in the scaffold condition; the process for the hint condition would be identical.  For the delayed feedback condition, since the student would not see the feedback until after all of the experimental trials, it is necessary to model that differently. In the delayed condition, the number of delayed-wrong trials would stay as zero until it jumps to be 2 in row 7, since the student would have seen the two explanations after finishing the experimental questions. This problem requires a rather novel use of learning decomposition, and some care in accounting for when the learning opportunities actually occur. 
 2.3 Results.
 We fit the model shown in Equation 3 to the decomposed data in the statistics software package R (see www.r-project.org). To account for variance among students and items, student IDs and unified problem IDs are also introduced as factors. By taking this step we account the fact that student responses are not independent of each other, and properly compute statistical reliability and standard errors. Also, by fitting our model in this manner we do not suffer the scaling problems mentioned by [10] since all three conditions have the same intercept (i.e. A parameter).  After the model is fitted, it outputs estimated coefficients for every condition, as shown in Table 4. The result suggests that the delayed feedback, estimated coefficient being 0.720, is more effective at helping student learn the skill than the other two conditions, esp. the scaffolding condition for which the coefficient estimate is 0.633. In prior work with this experiment [14], the authors reported that they did not find any main effect. It is possible to use the estimated coefficients (B) and standard errors in Table 4 to perform a statistical z-test, as we did in [7]. However, there is a bit of serendipity: the first author was conducting some exploratory analyses using resampling to see how stable the parameter estimates really were. It appeared that there was little overlap between the estimates for the scaffold and delayed conditions. Therefore, we decide to test this approach formally using bootstrapping [5] and randomization tests [6]. 
 Table 4. Coefficients of logistic learning decomposition model. 
 Bootstrapping is a modern, computer-intensive, general purpose approach to statistical inference, falling within a broader class of resampling methods [5]. It involves the construction of a number of resamples of the observed dataset by random sampling with replacement from the original data set; and each resample is independent (conditioned on the original sample) and identically distributed. Although bootstrapping was developed as techniques for parameter estimation, it can be used for hypothesis testing as well. In general, first we make a null hypothesis. Then we draw repeated samples from the original data set under the condition that the null hypothesis is true, and then we reject the null hypothesis if the statistic computed from the observed dataset is unlikely under the null hypothesis, or otherwise retain the null. In this particular analysis, the hypothesis we would like to test is “The delayed feedback strategy promotes learning more or less effectively than the scaffold (or hint) strategy.” Correspondingly, the null hypothesis would be “There is no difference on learning promotion between the delayed and scaffolding strategies.” Specifically, we follow the following steps to test our hypothesis. Step 1: Decide on a metric to measure the relative effectiveness between delayed feedback and scaffolding strategies. For this example, we choose the difference between the estimated coefficients of Delayed_wrong_trial and Scaffold_wrong_trial. Step 2: Calculate the metric on the original data. The results in Table 3 provides B(Delayed_wrong_trial) – B(Scaffold_wrong_trial), equal to .087. Step 3:  Bootstrap the original data with randomization to construct samples where the null hypothesis is true Repeat N times { Repeat M times (M =  the number of students in our original data set) { Sample data of one student (with replacement) from the original data; Randomly allocate the student into one of the three conditions: delayed, scaffold, or hint by changing the “Condition” label of each data point Re-compute the number of prior trials for the student according to the newly assigned condition; } Train logistic learning decomposition model on the re-sampled data, and record B(Delayed_wrong_trial) – B(Scaffold_wrong_trial) ; } In our case, we pick the repeated times N to be 500, and M is 300 as there are 300 students in our data set. Step 4: Check how likely our original result is under the null hypothesis, and reject or retain the null hypothesis. After the bootstrapping process, we obtain a list of difference between Delayed_wrong_trial and Scaffold_wrong_trial, totally 501 cases including our original result. Then we rank the list descending, and found that the original result was at the 95 percentile, the 25th in the ranking order, which suggests that the probability of the original result has a probability of less than 5%.  Although it is tempting to think we have p < 0.05, this methodology is actually conducting a one-tailed test.  Thus, the two-tailed value is p = 0.1.  Therefore, we have a marginally reliable result that delayed feedback is better than scaffold + hint, and giving students delayed feedback seems causing more learning than requesting them to finish a series of scaffolding questions. To complete the story, we repeat the same process compare the other two pairs: delayed vs. hint conditions, and scaffold vs. hint conditions, but find that they are comparable to each other at helping students learning the math skill in ASSISTments. 
 3 Conclusion.
 This paper explored the research question of measuring the instructional effectiveness of different tutoring interventions, using the learning decomposition technique. We found that presenting students with delayed feedback works better than breaking problems into scaffolding questions. We also used bootstrapping with randomization to test the statistical reliability of the finding. Typically, there are two reasons for the usage of learning decomposition (or any educational data mining technique).  The first is repurposing a previous experiment’s data to answer a new question.  The second is using EDM techniques to “zoom in” and detecting subtle effects that previous approaches failed to report. Previous works on learning decomposition [3, 4, 16] have been focusing on the first reason, while in this paper we focus on the second reason through an item level analysis and bootstrapping. One open question is why bootstrapping plus randomization gives different results than the parametric method of using estimated coefficients and standard errors to derive an analytic p-value. We did a z-test using estimated coefficients and standard errors given in Table 4 and obtained p = .4. Typically computationally intensive techniques are less powerful than parametric ones, unless one or more of the parametric tests’ assumptions have been violated. We are not sure where the problem lies, but suggest caution in interpreting standard error terms from logistic regression models using learning decomposition. The contribution of the paper lies in three aspects. First, we found that there is a main effect in a randomized controlled study that delayed feedback tutoring strategy is more effective than giving students scaffolding questions in ASSISTments. While previous analysis using ANOVA failed to detect such an effect, we were able to do so by conducting an item level analysis using EDM techniques. Second, we showed how learning decomposition can be applied in the domain of mathematics to use observational data to estimate the effectiveness of different tutoring strategies. It provides evidence that the learning decomposition is not domain specific. This simple, low cost approach is generally applicable to a variety of ITS that focus on different domains for identifying variances in educational effectiveness of interventions. Also, our use of learning decomposition is novel in that we are careful to consider when various aspects of an intervention occur, and do not give credit for a learning opportunity that has not yet happened (the delayed-wrong condition). Third, the process described in this paper serves as a demonstration of how bootstrapping approach and randomization tests can be employed in the educational data mining field. 
 Acknowledgements.
 This research was made possible by the U.S. Department of Education, Institute of Education Science (IES) grants #R305K03140 and #R305A070440, the Office of Naval Research grant # N00014-03-1-0221, NSF CAREER award to Neil Heffernan, and the Spencer Foundation. All the opinions, findings, and conclusions expressed in this article are those of the authors, and do not reflect the views of any of the funders.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Using Learning Decomposition and Bootstrapping with Randomization to Compare the Impact of Different Educational Interventions on Learning</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mingyu-feng"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mingyu-feng"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/mingyu-feng"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/203">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Edu-mining for Book Recommendation for Pupils</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/203/authorlist"/>
		<swrc:abstract>This paper proposes a novel method for recommending books to pupils based on a framework called Edu-mining. One of the properties of the proposed method is that it uses only loan histories (pupil ID, book ID, date of loan) whereas the conventional methods require additional information such as taste information from a great number of users which is costly to obtain. To achieve this, the proposed method solves the book recommendation problem as a problem of loan date prediction, relying solely on loan histories. Experiments show that the proposed method achieves an accuracy of 60% and outperforms the method (weighted slope open collaborative filtering) used for comparison. In addition to the performance, the proposed method has the following two advantages: (i) it is inexpensive compared to the conventional methods and (ii) reading level is adjustable.</swrc:abstract>
		<led:body><![CDATA[ 1. These facts imply that Edu-mining has to solve the problems that arise from the differences between educational data and normal data in its own scheme. Second, it prefers simple and inexpensive techniques. It should be implemented at moderate cost since it mainly aims at the use in school. Also, the target users of Edu- mining are mainly teachers and/or students (including pupils). If the used techniques are simple, the target users are likely to use them easily. Besides, they may sometimes be able to give feedback on the techniques. Third and finally, whereas data/text mining aims at improving the quality of the mined knowledge, it is not necessarily the case in Edu-mining; its ultimate goal is to achieve good educational outcomes. In case of normal book recommendation, the ultimate goal is to find and recommend books that the user wishes to read (and probably to purchase). By contrast, in case of our task, this is not the ultimate goal; the ultimate goal is to facilitate their intellectual development by recommending proper books. This is the basic concept of Edu-mining. The next section describes the basic idea of the proposed method based on Edu-mining. 
 3 Basic Idea.
 So far, we have seen the basic concept of Edu-mining and its relation to book recommendation for pupils. This section describes the basic idea of the proposed method based on Edu-mining. In book recommendation for pupils, the peculiarity of the data is that taste information obtained from pupils may be unreliable as Section 1 describes. The proposed method overcomes the problem by not using taste information. Instead, it solves the book recommendation problem as a problem of loan date prediction. It uses a simple and intuitive way to predict loan dates. Before describing the basic idea of the proposed method, let us introduce a new loan date called absolute loan date. Loan date normally has the form of date, month, and grade of the pupil (e.g., 1st Sep. 1st grade). This form of loan date is not suitable for the calculation used in the proposed method as we will see below. So, absolute loan date is used instead of the normal loan date. Absolute loan date is a simple mapping of the normal loan date. The first day of the first grade is the base date and mapped to 0. Other loan dates are simply mapped to the corresponding absolute loan dates of which distance from the base date is given by the number of days from the first day of the first grade. For example, a month later from the first day is mapped to 30 (or 31), the first day of the second grade is mapped to 365, and so on. Figure 1 illustrates the mapping between normal loan dates and absolute loan dates. 
 Figure 1.  Mapping between normal loan date and absolute loan date. 
 Here, it is worthwhile to note that absolute loan dates roughly correspond to reading levels. Namely, first grade pupils tend to borrow books of low reading levels whereas upper grade pupils tend to borrow books of higher reading levels. This implies that if one can predict absolute loan dates, s/he can also estimate reading level. This is why reading level is adjustable in the recommendation of the proposed method. Now let us describe the basic idea of the proposed method. The proposed method solves the book recommendation problem as a problem of loan date prediction as already mentioned. This is equivalent to saying that the proposed method predicts absolute loan dates from loan histories.  Once it predicts absolute loan dates, it can easily recommend books to the target pupil because it knows when s/he will borrow the books s/he has not borrowed yet. Simply, it recommends books which are predicted to be borrowed at the day of the recommendation or near the day. Or, if one wishes to recommend a book of a higher reading level, it can recommend books which are predicted to be borrowed some days (say, a half year later) after the day of the recommendation; the opposite can also be done. To see how the proposed method predicts absolute loan dates, suppose that we have loan histories shown in Figure 2 where loan dates are expressed by absolute loan dates. Figure 2 shows, for example, that pupil A borrowed book A on the absolute date 365 (equivalently, the first day of the second grade). Further suppose that we are predicting the absolute loan date of book B for pupil A (the question mark in Figure 2 denotes that pupil A has not borrowed book B yet). If we look at the loan history of pupil B, we will notice that s/he borrowed book B 370 days after book A. Based on this, it is natural to predict the absolute loan date of book B for pupil A to be 370 ( 670 300) days after the loan date 365 of book A for pupil A, or equally 735 ( 365 670 300 ). Similarly, based on the loan history of pupil C, it is natural to predict the absolute loan date of book B for pupil A to be 725 ( 365 710 350 ). To obtain the final prediction, we take the average of the two absolute loan dates, that is, 735 725 /2 730 (equivalently, the first day of the third grade). It should be noted that adding the average of the differences between the loan dates to the loan date of the base book gives the same result. For instance, 730 = 365 + {(670 - 300) + (710 - 350)}/2. 
 Figure 2. Example of loan histories. 
 Although the loan histories in Figure 2 involves only three pupils and two books for illustration purpose, actual loan histories often involves far more pupils and books. Therefore, the average is taken over the relevant books and the relevant pupils in actual use. A rough definition of relevant pupils and relevant books is as follows (the next section will describe the strict definition). A relevant pupil is those who have borrowed the following two books: (a) one of the books the target pupil borrowed and (b) the book of which absolute loan history is to be predicted. A relevant book is the book that is borrowed by (i) the target pupil and (ii) one or more of the relevant pupils. This is the basic idea of how the proposed method predicts absolute loan dates from loan histories. The next section describes the prediction method in detail. 
 4 Proposed Method.
 To formalize the prediction method, we will use the symbol and to denote a pupil and a book, respectively, in the given loan histories. We will also use the symbol , to denote the absolute loan date when the pupil borrowed the book ; if the pupil has not borrowed the book yet, then , is set to 1. Now, let be the target pupil (target for book recommendation) and be the book of which absolute loan date is to be predicted. Then, a relevant pupil is those who have borrowed both and one of the books the target pupil borrowed. Thus, a set of relevant pupils is defined by 
 FORMULA_1.
 where denotes one of the books the target pupil borrowed. Also, a relevant book is a book that satisfies the following two conditions: (i) a book that the target pupil borrowed, and (ii) a book of which relevant pupil exists 1 . Using Equation (1), a set of relevant books is defined by 
 FORMULA_2.
 Using Equation (1) and Equation (2), absolute loan dates are predicted by 
 FORMULA_3.
 Here, corresponds to the simple prediction of absolute loan dates discussed in the basic idea in Section 2 (for instance, 365 670 300 ). The sums in the numerator are the total sum of the simple predictions over the relevant pupils and the relevant books; in the case of the same example, the sums correspond to 725 730. The denominator is the number of simple predictions. Hence, Equation (3) gives the average of the simple predictions. In case of meaning that the proposed method cannot predict the absolute loan date. Intuitively, books whose absolute loan date is given by Equation (3) are similar, in terms of the topic, to the books that the target pupil borrowed because the average is taken over the relevant books and relevant pupils; the average is taken over the relevant pupils who have borrowed some of the same books as the target pupil and over the books that the relevant pupils have borrowed. In other words, the book preferences of the target pupil are implicitly included in the prediction through the relevant pupils and the relevant books. Furthermore, the similarity of each relevant book is considered in Equation (3). This can be seen by noting that Equation (3) can be rewritten as 
 FORMULA_4.
 In the rewritten version of Equation (3), the base date , (the first term in the numerator) is weighed by the factor which denotes the number of pupils that borrowed both and . It is reasonable to think that the more pupils borrow two books, the more similar the two are, and in turn it is reasonable to give a higher weight to such a pair in the prediction. Equation (3) exactly does this. Also, it should be noted that the denominator can be regarded as the credibility of the prediction because it denotes the number of relevant pupils and relevant books involved in the prediction. The prediction is not reliable if it is made based on few relevant pupils and few relevant books. Considering this, predictions whose where denotes a certain threshold are discarded in the book recommendation. Once absolute loan dates are predicted for the books that the target pupil has not borrowed yet, the proposed method recommends books to the target pupil as follows. It recommends books which are predicted to be borrowed at the day of the recommendation or near the day; here ( 5 or 10, for example). Or, if a teacher wishes to recommend (or the target pupil wishes to read) books of a higher reading level, it recommends books which are predicted to be borrowed some days after the day of the recommendation. If one wishes the opposite, it recommends books which are predicted to be borrowed some days before the day of the recommendation. The amount of days can be chosen by an intuitive way to specify reading level. Recall that absolute loan date is simply the one to one mapping of normal loan date. If one sets the amount to 365 days after, it corresponds to specifying a one-grade-higher reading level. 
 5 Evaluation.
 For evaluation, we collected loan histories of pupils in an elementary school where the grades range from first to sixth. Table 1 shows the statistics on the loan histories. 
 Table 1.  Statistics on the loan histories used for evaluation. 
 In the evaluation, we conducted two experiments. In the first, we evaluate how accurately pil the proposed method can recommend books similar to the books that the target pu borrowed, which is described in 4.1. In the second, we evaluate the capability of the proposed method in estimating reading level, which is described in 4.2. 
 5.1 Experiment on Book Recommendation Accuracy.
 The experimental conditions and procedures are as follow. First, we randomly selected 10 target pupils (two for each grade, from first to fifth grade) from the loan histories; pupils in sixth grade ware not included in the experiment because of the limitation of the proposed method which will be discussed in Section 5. Second, we predicted absolute loan dates for the target pupils using the proposed method; the threshold , which was discussed in Section 3, was set to five. Third, we selected five most difficult books and five easiest books for each pupil according to the predicted loan date. Then, the 10 books were shown to two elementary school teachers together with the corresponding loan history. Finally, the two teachers separately rated each book as similar (to one or more of the books in the loan history in terms of its topic), not-related, or unknown referring to the corresponding loan history. The performance of the proposed method was measured by accuracy. Accuracy was defined by 
 FORMULA_5.
 For comparison, we implemented the weighted slope one collaborative filtering [2], which had been shown to be effective in item recommendation. To fully implement the weighted collaborative filtering, we need taste information for each book as described in Section 1. However, normal loan histories such as the ones used in this evaluation, do not contain taste information. For this reason, we implemented the weighted collaborative filtering with the loan histories in which an equal rating was given to all books. Doing so, it can recommend related books but cannot rank recommended books; all books are equally favored. So, 10 books were randomly chosen from the recommended books and shown to the two teachers for evaluation. The performance was measure by accuracy as in the proposed method. Table 2 shows the results. It shows that the proposed method achieves an accuracy of 0.600. This means that on average, six out of the 10 books recommended by the proposed method are related to the books that the target pupil borrowed. It seems to be not so difficult for teachers or even pupils to select related books from the recommended books which are actually related 60% of the time. 
 Table 2.  Evaluation on book recommendation accuracy. 
 Table 2 also shows that the proposed method outperforms the weighted slope one collaborative filtering. Indeed, the difference between the two is significant (normal approximation to the binomial test, p<0.01).  The performance of the weighted slope one collaborative filtering implies that its recommendation may confuse teachers and pupils because more than half of the recommended books are not relevant. 
 5.2 Experiment on Reading Level Estimation.
 The experimental conditions and procedures are as follow. First, we made five pairs of books by randomly selecting a book from the five most difficult books and a book from the five easiest books which the proposed method recommended to each target pupil (50 pairs in total). Second, we randomly labeled the two books in each pair as A and B. Third, four human raters (undergraduate students) separately determined which book in each pair was more difficult by referring to a book search system that retrieves book information including the title, the author(s), the number of pages, the picture(s) (if available), the reading level (if available), and the synopsis (if available). They separately gave each pair either 1, 1, or 0 meaning A is more difficult, B is more is difficult, and indistinguishable, respectively. Then, we merged the results. If the sum is equal to or greater than 3, then A is determined to be more difficult. Similarly, the sum is equal to or smaller than 3, then B is determined to be more difficult. If the sum is 2 or2, the first and second authors joined the four human raters in the evaluation giving newly the pair +1, -1, or 0. If the new sum is equal to or greater (smaller) than 3 ( 3), then A (B) is determined to be more difficult; otherwise, indistinguishable. Also, if the sum is between 1 and 1, the pair is determined to be indistinguishable. As the results, 34 out of 50 pairs were distinguishable in terms of the reading level. For the 34 pairs, the predictions of the proposed method agreed with the decisions of the human raters 62% of the time (21 Out of 34 pairs). Although the results show that the predictions of the proposed method roughly agree with the decisions of the human raters, the agreement is not as high as we expected. We will discuss the reason in the next section. 
 6 Discussion.
 The evaluation has shown that the proposed method is effective in recommending books related to the books that the target pupil borrowed. The reason is that the proposed method predicts absolute loan dates from the relevant books and the relevant pupils. The effects can be seen in the results of the recommendation. The proposed method is capable of recommending books in series as Table 3 shows. As underlined, the proposed method recommended Astronomical observation 1, 4, and 9 to the pupil who borrowed Astronomical observation 8. Information about books in series is useful for recommendation since teachers or book database systems do not necessarily have the information. More importantly, the results show that the proposed method is effective in recommending related books. For instance, it recommended Constellation observation 1, which is highly related to Astronomical observation 8, The wonder of the Earth, The birth of the great telescope Subaru, and Journey in the space. Table 4 shows another example. By contrast, the performance of the proposed method concerning reading level is not as high as we expected. As already described in the previous section, the differences in reading level were indistinguishable in 32% of the 50 pairs. For the rest, the predictions of the proposed method agreed with those of the human raters 62% of the time. One of the major reasons is that we used loan histories whose term is one year in the evaluation (or should we say we could only collect that amount?). This means that the difference in reading level is a one-grade higher or lower at most and often much less than one-grade. This explains why 32% of the 50 pairs were indistinguishable in terms of reading level. Considering this, the proposed method will improve in the reading level prediction with longer term loan histories. Another reason is related to the problem of evaluation. It is not so easy to accurately evaluate reading level. It was sometimes difficult for the human raters to determine which book was more difficult by only referring to the book search system. It is possible to take another way of evaluation, which will be our future work. 
 Table 3.  Example of book recommendation (books in series). 
 Table 4.  Example of book recommendation (related books). 
 This section has discussed the effectiveness of the proposed method in book recommendation. Here, it is also worthwhile to discuss the limitations of the proposed method. One of the limitations is that the proposed method is not effective in recommending books to pupils during the last days of school. It is often impossible to take the differences of absolute loan dates in Equation (3) because there are no pupils in higher grades. This is why we excluded pupils in sixth grade from the evaluation. Another limitation is that the proposed method is not capable of recommending books that have never been borrowed; other systems based collaborative filtering have the same limitation. By contrast, teachers or especially librarians can properly recommend such books. It requires other techniques to achieve this. 
 
 7 Conclusions This paper proposed a novel method for book recommendation based on Edu-mining. It has three advantages over the conventional methods: (i) it is inexpensive, (ii) it can recommend books related to the books that the target pupil borrowed, and (iii) reading level is adjustable. The evaluation reveals that the proposed method achieves an accuracy of 60% in recommending related books and outperforms the weighted slope open collaborative filtering. The evaluation also reveals that the reading level predicted by the proposed method roughly agrees with the reading level determined by human raters. For future work, we will investigate how the prediction of reading level can be evaluated more accurately. We will also investigate how other sources of information can be used to improve the proposed method.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Edu-mining for Book Recommendation for Pupils</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ryo-nagata"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ryo-nagata"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/keigo-takeda"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/keigo-takeda"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/koji-suda"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/koji-suda"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/junichi-kakegawa"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/junichi-kakegawa"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/koichiro-morihiro"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/koichiro-morihiro"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/203/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ryo-nagata"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/keigo-takeda"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/koji-suda"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/junichi-kakegawa"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/koichiro-morihiro"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/204">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Obtaining Rubric Weights For Assessments By More Than One Lecturer Using A Pairwise Learning Model</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/204/authorlist"/>
		<swrc:abstract>Specifying the criteria of a rubric to assess an activity, establishing the different quality levels of proficiency of development and defining weights for every criterion is not as easy as one a priori might think. Besides, the complexity of these tasks increases when they involve more than one lecturer. Reaching an agreement about the criteria and the levels of proficiency might be easier taking into account the abilities students must achieve according to the purpose of the subject. However, the disagreement about the weights of every criterion in an assessment rubric might easily appear. This paper focuses on the automatic weight adjustment for the criteria of a rubric. This fitting can be considered as a global perception that the whole group of lecturers have about the accuracy of solving an activity. Firstly, each lecturer makes a proposal of weights and then, from a set of pairs of students he/she globally expresses who of each pair has solved better the activity for which the rubric was designed. Secondly, an approach based on the pairwise learning is proposed in this work to obtain adequate weights for the criteria of a rubric. The system commits fewer errors than the lecturers and makes them improve and reconsider some aspects of the rubric.</swrc:abstract>
		<led:body><![CDATA[ 1. 
 4.2 Discussion of results.
 Five different experiments were compared for each activity. The first three consisted of checking in what extent the preferences of each of the three lecturers are coherent with the marks computed according to their own weights previously fixed. This is shown in the first three rows of Tables 2-5. The fourth experiment consisted of checking in what extent the preferences of all the lecturers together are coherent with the marks computed according to the weights obtained as the average of the weights of the three lecturers. This is shown in the fourth row of Tables 2-5. Finally, the fifth experiment consisted of checking in what extent the preferences of all the lecturers together are coherent with the marks computed according to the weights the learning process produces from the preference data. This is shown in the last row of Tables 2-5. Notice that the errors committed when the averages of the weights among the lecturers are considered are not necessary the averaged errors committed by each lecturer on their own. Besides, the number of preferences considered when the averages of the weights are computed and when the learning process is applied is the sum of the preferences of all the lecturers. Table 2 shows that lecturers commit some errors when they express their global impression with regard to their own weights of the criteria in Activity 1. Particularly, they disagree between 5% and 15% of the preferences, whereas the system is able to accurately reproduce a summary of all them (0% of error). Notice that using the averaged weights does not lead to an improvement. It seems that this activity presents great difficulties when defining a set of weights, since the weights of the system in general are quite different from those previously defined by the lecturers. 
 Table 2.  Weights of the lecturers, weights averaged and weights of the system for Activity 1. 
 In case of Activity 2 presented in Table 3, lecturers seem to agree among them about the weights, but there are slightly high differences between the marks of these weights and their own preferences, since they commit between 20% and 35% of errors. In this case the proposed system produces 8.33% of error against 20% if the average of weights is used. The differences between the weights produced by the system and those of the lecturer are useful for the lecturers as a feedback to make them think about the relevance of the criteria. 
 Table 3.  Weights of the lecturers, weights averaged and weights of the system for Activity 2. 
 The results of Activity 3 shown in Table 4 are quite similar to those of Activity 2. Again the system is able to engage the information of the lecturer team to reduce the error. 
 Table 4.  Weights of the lecturers, weights averaged and weights of the system for Activity 3. 
 Looking at Table 5 for Activity 4, criteria 8 and 9 is quite interesting. In this case lecturer 1 does not take into account criterion 8 and lecturer 2 does not take into account criterion 9, but lecturer 3 grants equal weight to both criteria. This is a conflictive case and the system according to the preferences of the lecturers agrees with lecturer 1 about the criteria 8 and with lecturer 3 about criteria 9. This proves that the system try to sum up the preferences of all lecturers, although it produces a bit more error than lecturer 1. 
 Table 5.  Weights of the lecturers, weight averaged and weights of the system for Activity 4. 
 In general, the weights produced by the system differ from those granted by the lecturer before. Let us notice that the percentage of errors committed by the learning system are considerable lower that the rest ways of considering the weights. This means that this system is able to quite accurately reproduce the whole preferences of the lecturers. This also means that lecturers are not perfect experts because their own way of setting weights are not so coherent with their own preferences. Hence, the weights produced by the system make lecturers check their own incoherencies in order to change all or some weights which leads to establish a more accurate rubric. In fact, it helps to reach a consensus of the assessment process to encourage transparency and avoiding discriminatory treatment. 
 Figure 3.  The averaged marks over the students when they are obtained from the weights averaged over the lecturers and from the weights the system grants. 
 Applying the weighs the system produces would benefit some students and damages others. But, the question is that if there would be a global benefit or damage. Figure 3 shows the averaged marks of each activity together with the dispersion with regard to the use of averaged weights and to the use of the weights yield by the system. At sight of Figure 3, one can observe that the mean and the deviation hardly vary between using average weights and the weights of the system. This allows concluding that the global benefit or damage will be the same. The advantage is that the marks of the students will be more accurately with regard to the global impression of the lecturer team. Notice that this process is internal among the lecturers and can be transparent for the students. Hence, it is not necessary to provide information to the student about the way of defining the weights. 
 5 Conclusions and Future Work.
 This work proposes a method based on preference learning to improve and adjust the weights granted to the criteria of an evaluation rubric according to the global impression of lecturers about pairs of activities solved by students when more than a lecturer is involved in the assessment process. The system proposed allows summing up the preferences of all the lecturers at the same time, and in fact, it reduces the errors between their own preferences and the original weights granted by every lecturer alone. Initially, lecturers give higher weight than the system yields from their preferences or vice versa. The tendency, unconsciously or not, of mixing criteria or taking into account other abilities such as transversal ones or those related to the attitude may be the cause of these disagreements. The results suggest lecturers must think about going more in depth into the design of the rubrics and about establishing more accurately the criteria and their relevance alone and together with their colleagues. Also, the weights the system grants benefit or damage the students the same with regard to consider the averages of the weights of all lecturers. A proposal for future work is to find out if either grouping or breaking down the criteria makes lecturers improve the design of the rubrics. 
 Acknowledgements.
 This research has been partially supported by the MICINN grants TIN2008-06247 and TIN2007-61273. The support of the University of Oviedo to the project entitled La minería de datos como mecanismo de ayuda para la toma de decisiones en la actividad docente dentro del marco del Espacio Europeo de Educación Superior is also gratefully acknowledged.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Obtaining Rubric Weights For Assessments By More Than One Lecturer Using A Pairwise Learning Model</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/j-r-quevedo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/j-r-quevedo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/e-montanes"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/e-montanes"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/204/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/j-r-quevedo"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/e-montanes"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Does Self-Discipline impact students’ knowledge and learning?</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205/authorlist"/>
		<swrc:abstract>In this study, we are interested to see the impact of self-discipline on students’ knowledge and learning.  Self-discipline can influence both learning rate as well as knowledge accumulation over time.  We used a Knowledge Tracing (KT) model to make inferences about students’ knowledge and learning.  Based on a widely used questionnaire, we measured students’ level of self-discipline. When we analyzed the relation of students’ self-discipline with their knowledge attributes, we found that high self-discipline students had significantly higher initial knowledge, but there is no consistent relationship of learning while using the tutor. Moreover, higher self- discipline students seemed more careful with respect to making careless mistakes.</swrc:abstract>
		<led:body><![CDATA[ 1. The questionnaire asked students for their gender. Twelve students gave an incorrect response. Suspecting them not being serious about the survey, we excluded those students from our study. 2. Some students might be randomly picking answers and therefore we checked for consistency in their answers. Among 12 questions in the survey, for 8 of them “Very much like me” implies low self-discipline (e.g. “I have a hard time breaking bad habits”), and for 4 of them, “Very much like me” implies high self-discipline (e.g. “I am good at resisting temptation”). For both types of questions we used the scoring system in Section 2.1.  If a student answered “Very much like me” for a question of the first type, he received -2 points. If he answered “Not like me at all” for a question of the second type, he received +2 points. The two responses consistently tell that he has low self-discipline. The sum is zero. But if he had answered “Very much like me” in the second type, the answers are not consistent and the sum of responses is -4. Similarly, if he had answered “Not like me at all” in both questions, that would be still inconsistent and sum would be +4. 3. For each student, we took average of points in both types of questions (since the groups are of unequal size) and summed the two averages and calculated the absolute value.  The sum value can range from 0 (completely consistent) to 4 (completely inconsistent). Based on the questionnaire composition and distribution of the sum from our data, we found 1.6 to be a reasonable cut point and dropped 11 students with sum greater than 1.6. 4. We selected two pairs of questions which are basically asking same trait in opposite ways. For example “I do certain things that are bad for me, if they are fun” and “I refuse things that are bad for me” state the same trait.  We cropped students who are saying “very much like me”/ “Mostly like me” or “not like me at all” in both questions. There were 19 such students among which 5 were already excluded from step 2. Finally, our dataset narrowed down to 134 students, with their 68285 log records. We excluded 10% of the records and 20% of the students. For each student, we had 12- dimensional vectors representing their responses corresponding to each survey question. We performed a factor analysis to reduce data dimensions and we used the strongest factor as the student’s self-discipline score. 
 2.3 Knowledge tracing model.
 We used knowledge tracing in Dynamic Bayesian Networks (DBN), see Figure 1, to make inferences about student knowledge based on his performance. 
 Figure 1. Knowledge tracing model: Dynamic Bayesian network. 
 Student performance is assumed to be a noisy reflection of student knowledge, mediated by two performance parameters guess and slip. The guess parameter represents the fact that the student may sometimes generate a correct response in spite of not knowing the correct skill. For example, some ASSISTment items are multiple choice, so even a student with no understanding of the question could generate a correct response for those. The slip parameter acknowledges that even students who understand a skill can make an occasional careless mistake [3]. The learning rate parameter estimates the probability that student learns new knowledge that he has not known before. We used Bayes Net Toolkit for Student Modeling (BNT-SM [4]), which inputs data and a compact XML specification of a Bayes net model to describe causal relationships among student knowledge and observed behavior. BNT-SM gives us knowledge parameters, prior knowledge and learning as well as performance parameters, guess and slip. 
 3 Results.
 3.1 Knowledge tracing model per skill.
 Based on self-discipline score, we divided students into three equal-sized groups having relatively high, medium and low self-discipline level. For each subgroup, we trained separate knowledge tracing models, and thus estimated knowledge and performance parameters that corresponded to each group.  We trained a knowledge tracing model for each of the 106 skills.  I.e. observe all the training data across all students for each skill and derive a set of parameters (Prior knowledge, learning, guess, slip) for each skill. Then, for each self-discipline subgroup, we calculated the median values across all the skills (see Table 1). We report median rather than mean values to avoid unnecessarily weighting outliers.  However, in accordance with standard convention, our statistical analyses are based on the means rather than medians. 
 Table 1: Knowledge and performance parameters for self-discipline groups. 
 From Table 1, we see that for prior knowledge, the high self-discipline students are statistically higher than the medium group, and the medium and low groups are statistically tied.  Meanwhile, high self-discipline students made more correct guesses and fewer slips relative to their lower self-discipline peers. A higher guess parameter should not be viewed as a bad thing.  Consider that guess means the ability to answer a question despite not having mastered the skill.  Consider two students with similar partial knowledge and one takes more care to figure the right answer while other quickly asks for help.  The model will treat this as a guess by the first student.  Such behavior seems related to self-discipline.  Similarly, students who are more careful and detail-oriented will make fewer slips (keep in mind that a “slip” is defined as making a mistake in spite of the skill being known).  The result shows that higher self-discipline students have more prior knowledge and they are more concerned and careful on their task. However, we received an inconsistent pattern in the learning parameter. The learning rate of the medium self-discipline group is higher than both the high and low groups. We were concerned with the possibility of overestimating the learning parameter in the medium group by giving the guess parameter less weight, while underestimating it in the high group by giving guess more weight. This concern is due to problems with estimating knowledge tracing parameters [8].  For example, a high “guess” parameter can result in students performing well, but allegedly having little knowledge.  Since student knowledge is not directly observable, it is hard to validate the parameter estimates and we are left trusting our model that two groups could perform equally well but one group knows less (see [8] for a fuller discussion of the problems of underdetermined models). To guard against this concern, we also plotted student performance as a function of practice opportunity so that we can see the cumulative effect of the knowledge and performance parameters in students’ future performance for each level of self-discipline. By using the four parameters of each subgroup and the knowledge tracing equations listed below, we computed the theoretical performance curves for each of them. Specifically, we initialize knowledge to be K0. After each practice opportunity, we update knowledge in formula I (below) as the new likelihood of the student knows the skill after the previous practice. Also we compute performance, the probability of the student will respond correctly in the current practice opportunity, by using formula II to combine the estimated knowledge with the slip and guess parameters. Intuitively, the probability of making correct response is dependent on student’s knowledge given that he does not slip and also on his probability to make right guess in absence of the knowledge. 
 Figure 2a.  Theoretic performance curve of three self-discipline groups. 
 Figure 2b.  Real performance curve of three self-discipline groups. 
 From the performance curve in Figure 2a, we see that the combined effects of the four model parameters result in higher self-discipline students performing better. The real performance curve in Figure 2b also showed a similar trend. One interesting observation is to examine the best-fit power curves for each group.  The high self-discipline students are performing more lawfully (i.e. higher R2) than those with low self-discipline, suggesting students with higher self discipline are more consistent. Simply looking at the learning parameters does not tell the whole story. High group students might be learning slower but they are better able to use their partial knowledge to perform better—at least that is what our model is suggesting. Based on all these findings, we built a causal model that unifies cognitive and non- cognitive aspects of our students. While knowledge parameters like prior knowledge and learning are cognitive attributes, the performance parameters, guess and slip are more related to non-cognitive attributes.  This model accounts for the results in Table 1, and suggests the performance parameters might be an interesting avenue of research in their own right (typically the knowledge parameters are of more interest). 
 3.2 Knowledge tracing model per student.
 While training a KT model per skill is the regular approach, it is also possible to instead train one model per student by observing his responses in all questions across skills. The model then estimates a set of parameters (prior knowledge, guess, slip and learning) for each student which represents his aggregate performance across all skills.  We then looked for a relationship between the student’s self-discipline score and his knowledge parameters (prior knowledge and learning). As seen in Table 2, self-discipline is positively correlated with student’s prior knowledge (K0), but again there is no statistically reliable correlation with the learning parameter. In the other words, students with higher self-discipline have more incoming knowledge than their lower self- discipline classmates. However, self-discipline seems not to contribute student’s ability to learn more in each learning opportunity within the tutor.  Perhaps higher self-discipline results in having more learning opportunities rather than learning more from each one? 
 Figure 3: Causal model of cognitive and non-cognitive attributes for academic performance. 
 Table 2: Correlation of self-discipline and knowledge parameters. 
 We also found an interesting observation that self-discipline is highly correlated with the number of problems solved. We were then confronted with two possibilities: either higher self-discipline students are more on task and solve more problems, or students with higher self-discipline have higher knowledge and so need less help and solve problems more quickly. When we did partial correlation within these three variables, as seen in Table 3, we found evidence for the latter possibility.  Once we account for prior knowledge, there is no relationship between self-discipline and number of problems solved. We built a causal model, Figure 4, based on the finding that the higher self-discipline students in fact solved more problems as they were equipped with more knowledge and, perhaps surprisingly, not because they were on task more.  The direct correlation between self-discipline and knowledge is 0.29, and between knowledge and number of problems solved is of 0.55.  The partial correlations are more interesting.  The partial correlation of self-discipline and number of problems, partialing out knowledge was only 0.11.  Thus, there is not a direct relation between the two.  The partial correlation of knowledge and number of problems solved, partialing out self-discipline is 0.52, i.e. the correlation is relatively unaffected.  Thus, knowledge appears to be the direct causal link for number of problems solved, and self-discipline is causally upstream of knowledge. 
 Table 3: Partial correlation of self-discipline, prior knowledge and # of problems solved. 
 Figure 4:  Causal model of self-discipline, knowledge and number of problems solved. 
 3.3 Model validation.
 KT model parameters can be sensitive to erroneous factors like wrong priors, insufficient data, etc. Therefore, we were curious to try some validation of our model parameters with external data. To validate our model, we used results from a pretest and posttest on the same set of students. The pretest consisted of a 33-item algebra quiz on the subset of knowledge components that we are using in our models. After a month, the students were presented with posttest with exactly the same questions as in the pre-test. The pretest was performed when the students started using the tutor, and the student’s score is used to indicate the amount of incoming knowledge before using ASSISTment.  Therefore it works as a standard against which to validate student prior knowledge (K0) that we estimated in our models. Also, we calculated students’ estimated performance after 8 practice opportunities (P8) as they practice 8 times on average for each skill during the one month period.  We estimate performance from prior knowledge, learn, guess and slip parameters as given by knowledge and performance equations mentioned in 3.1. The correlation of P8 with post- test can be a measure of validation of the other three parameters. There is strong positive correlation between the student pretest scores and model’s estimation of their prior knowledge. P8 and posttest scores are also reliably correlated even when we partial out initial knowledge (K0).  I.e. our performance measure is capturing student learning, not just the student’s overall level of knowledge.   In addition, 
 Figure 5 shows student learning between pretest and posttest. 
 The gains in Figure 5 are consistent with our KT model results. The high self- discipline group has higher incoming knowledge than both groups and their final performance is also highest. But when it comes to learning, the medium group appears to have the highest gain. So, we considered some possibilities for the explanation of lower learning in high group. One reason for lower learning in high group could be due to the fact that they already have high knowledge and it is harder to have more gain when we start from higher value.  For example going from 50 to 60 is easier than going from 80 to 90. 
 Table 4: Correlation of prior knowledge (K0) and P8 vs. pre and post-test respectively. 
 Figure 5:  Comparison of learning gain. 
 To test this possibility, we divided pre-test scores into three bins and performed an ANOVA.  We treated pretest and self-discipline as factors in our model since we did not necessarily expect a linear effect (as would be implied by treating them as covariates). Table 5 shows the estimated marginal means of gain score for each level of the factors. 
 Table 5: ANOVA analysis gains by pretest and self-discipline. 
 From the result in Table 5, we see that medium group has higher learning in all bins (i.e. they are learning faster no matter what their starting level in pre-test is). Therefore, it appears that the medium group indeed has higher learning and maybe having a balance of self-discipline and some spontaneity helps in having better learning gains. However, their lower incoming knowledge makes the idea of a higher learning rate difficult to reconcile. We choose to leave this as an open discussion for further experimentations in future. 
 4 Contributions.
 Psychosocial studies have been based on performance measures like report cards, GPA, income, college admission, etc. [1,8,9].  But, our fine grained model gives us the tools to measure their performance and also latent attributes like knowledge, learning, and even guess and slip.  We have found that the impact of self-discipline on students using computers is complex, and appears to influence knowledge and performance while using the tutor.  We have constructed a causal model of the impacts of cognitive and non- cognitive attributes on performance within an ITS, and showed that the variability in performance is not only dependent upon cognitive attributes, but also on other non- cognitive aspects like carefulness and self-discipline. We modified the regular approach to train KT model with data per skill and instead estimated per-student parameters.  Although a per-student model trained on prior users is not useful to ITS designers (since it does not apply to new students using the system!), performing parameter estimation at the individual level can open new ways to make different analyses with other individual characteristics. With this new approach, we were able to make correlations of students’ pre-test and post test with their knowledge and performance estimations, thus validating the model parameters. 
 5  Future work and conclusions.
 Our current method of estimating self-discipline relies upon a self-reported survey administered once. There can be problems of both over- and under-reporting. We could take advantage of the continuous data students generate, and construct a more robust estimate of self-discipline.  It may also be possible to consider self-discipline a latent construct, similar to what we do for knowledge in knowledge tracing, and simultaneously estimate both parameters.  Broadening the stream of ITS information to include observable measures like homework submission, attendance, usage of tutor, opinion of teachers and parents, etc. would make this possible. In conclusion, high self-discipline students have higher incoming knowledge, as substantiated from both KT model parameters and pre-test score. However, the impacts do not appear to be substantial, and tutor designers probably do not have to explicitly account for self-discipline. The higher self-discipline group makes better guesses and makes fewer slips, which implies that the higher self-discipline group is more careful and detail oriented. The cumulative effect of learning, slip and guess makes the performance of higher self-discipline students better than that of their peers. 
 Acknowledgements.
 We would like to thank all of the people associated with creating the ASSISTment system listed at www.ASSISTment.org.  We would also like to acknowledge funding from the National Science Foundation, the Fulbright Program for funding the second author and the US Department of Education and the Office of Naval Research for funding the other authors. All of the opinions expressed in this paper are those solely of the authors and not those of our funding organizations.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Does Self-Discipline impact students’ knowledge and learning?</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Back to the future: a non-automated method of constructing transfer models</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206/authorlist"/>
		<swrc:abstract>Representing domain knowledge is important for constructing educational software, and automated approaches have been proposed to construct and refine such models. In this paper, instead of applying automated and computationally intensive approaches, we simply start with existing hand- constructed transfer models at various levels of granularity and use them as a lens to examine student learning. Specifically, we are interested in seeing whether we can evaluate schools by examining the grain-size at which its students are best represented. Also, we are curious about whether different types of students are best represented by different transfer models. We found that better schools and stronger students are best represented by models with a fewer number of skills. Weaker students and schools are best represented, for our data, by models that allow no transfer of knowledge in between skills. Perhaps surprisingly, to accurately predict the level at which a student represents knowledge it is sufficient to know his standardized test score rather than indicators of socio economic status or his school.</swrc:abstract>
		<led:body><![CDATA[ 1. After the students had taken the state tests, the state released the items in that test, and our subject-matter expert tagged up these items in all the transfer models. The first column in Table 1 lists eight of the 106 skills in the WPI-106 model. For instance, equation-solving is associated with problems involving setting up an equation and solving it; while equation-concept is related to problems that have to do with equations in which students do not actually have to solve them. The two skills are nested inside of “Patterns, Relations and Algebra” in the third column which itself is one piece of the five skills that comprises the WPI-5 transfer model. The value of the fine grained model was shown in [14] by analyzing of data from over 1000 students’ two years usage of ASSISTment system. In [14], we presented evidence that, in general, the WPI-106 model did a better job at tracking students’ knowledge and, thus, made a more accurate prediction of their end-of-year exam scores than the coarser grained models. 
 Table 1. Hierarchical relationship among transfer models. 
 3.2 Approach.
 We have explained the nested hierarchical structure of our transfer models, and shown that the fine-grained model did the best overall at predicting student performance.  Now we will examine our results more closely to see how different transfer models fit different groups of students. 
 3.2.1 Data.
 The dataset we use was collected during 2004-2005 school year. It involves 495 8th- grade students (approximately 13 years old) from two middle schools who have used the ASSISTment system on at least 6 days, with an average of 9 days. The item-level MCAS test report is available for all students so that we are able to evaluate accuracy of our models at state test score prediction. Since the scaffolding questions show up only if the students answer the original question incorrectly, students who answer the original question correctly do not have a chance at scaffolding questions, and would only be credited for the original question in the data. In order to avoid this selection effect, we preprocess the data using a compensation strategy to mark all scaffolding questions correct if a student gets an original question correct. Also, because our transfer models allow multi-mapping (one question associated with multiple skills), we choose to use a simple credit-blame strategy where if a student succeeds in answering a question, we mark all associated skills as being correctly applied, while when a student answers a question incorrectly, we only blame the weakest skill of the student, i.e. the skill on which the student has shown worst performance. After preprocessing, the data set contains 147,624 data points, among which 45,135 come from original questions. On average, each student answers 91 original questions. It is worth pointing out that during our modeling process, student response on original questions and scaffolding questions are used in an equal manner and they have the same weight in evolution. The first portion of this research involves partitioning students into groups to determine if different groups of students have different patterns for learning math skills. Naturally, the 495 students can be separated by the schools they were in, with 312 from school F and 183 from school W. We also try to separate them by their performance level at the 2005 MCAS test. The high performing group includes the 128 students whose performance level is assessed by the state as “Advanced” or “Proficient”; the medium group includes the 154 students whose performance level is “Needs Improvement”, and the low performing group has the rest 213 students at performance level “Warning”. While these performance levels are somewhat specific to Massachusetts, they are at least criterion- referenced and much more general than numbers extracted from a student model or raw scores on a test (what qualifies as “Proficient” in Massachusetts is probably similar to “Proficient” in Macedonia). Our hypothesis is that students from a stronger school, or higher performing group, would show more transfer in their knowledge acquisition than those from a weaker school, or lower performing groups. Therefore, for the stronger students and schools the coarser grained model will better describe their learning and provide more accurate prediction of their MCAS test scores. 
 3.2.2 Modeling.
 In order to track individual student’s development of skills over time and make predictions, we choose to fit mixed-effects logistic regression models [8]. A mixed- effects model consists of both fixed effects, parameters corresponding to an entire population or repeatable levels of factors, and random effects, parameters corresponding to individual subject drawn randomly from a population. This approach takes into account the fact that responses of a student on multiple items are correlated. Moreover, the random effects allow the model to learn parameters for individual students separately. We use a logistic model because our dependent measure is dichotomous (0/1 for incorrect/correct). Regarding to the independent variables, for the fixed effects, we used a timing variable to represent the amount of time elapsed since the beginning of the school year, so that the model tracks the knowledge acquisition process longitudinally over time. Skills are included in the model as a factor to identify the skills associated with each response. Both the main effects of skills and an interaction term between the timing variable and skills are included in the model. Therefore, the model will learn an intercept (representing initial knowledge) and a slope (representing learning rate) for each skill separately. The timing variable is introduced as a random effect as well, in order to account for the learning rate variation of each individual student. The model is illustrated as below. To simplify the illustration, suppose TIME is the only covariate we care about in the model (skill can be introduced in a similar way). Thus, a 2-level representation of the model in terms of logit can be written as 
 FORMULA_1.
 Where ijp  is the probability that student i gives a correct answer at the jth opportunity of answering a question; TIMEij refers the jth opportunity when student i answered a question. In our data, it is a continuous value representing the number of months (assuming 30 days in a month) elapsed since the beginning of the school year. ii bb 10 , denote the two learning parameters for student i. ib0  represents the “intercept” or how good is the student’s initial knowledge; ib1 represents the “slope” that describes the change (i.e., learning) rate of student i. 10 ,ββ  are the fixed-effects and represent the “intercept” and “slope” of the whole population average change trajectory. ii vv 10 ,  are the random effects and represent the student-specific variance from the population mean. We fit the mixed-effects logistic regression models with R (http://www.r-project.org/) using the glmer() function in the lme4 package [3], using “logit” as a link function. For simplicity, assuming knowledge was changing linearly (in logistic space) over time. One model is fit for each school and each performing group separately. Given a student’s learning parameters on different skills, the skill-tagging of each MCAS question, and the exact test date of MCAS, we can calculate the probability of positive response from the student to each MCAS test question. Then we sum the probabilities up as the prediction of students’ MCAS scores. Two prediction evaluating functions are chosen, mean absolute difference (MAD), and mean difference (MD), as below. 
 FORMULA_2.
 where MCASi is the actual MCAS score of the ith student, and predictioni is the predicted score from our model. Both measures are used since MAD gives a good estimate the closeness of the prediction to actual scores while MD allows us to see if a certain model has been overestimating or underestimating. 
 3.3 Results and discussion.
 The results for both school F and school W are summarized in Table 2. As shown in Table 2, school F has a flat error line across all four different transfer models. The MAD for the WPI-39 model is the lowest, and yet a paired t-test that compares the absolute pair-wise differences of individual students among all models suggested that there is no reliable difference. However, for school W, the line tilts: the MAD of the WPI-39 model is reliably lower than those of the WPI-1 and WPI-5 models, indicating school W is better predicted by a finer grained model than by coarser grained models. Note that we are not able to fit the statistical model for school W with the WPI-106 transfer model (there is a technical glitch we do not understand and are investigating). We encounter the same problem later in the paper, which admittedly bring up some caveats in interpreting our results. The second part of Table 2 shows the values of MD for each model. The results indicate that both schools are optimized at the WPI-39 model. In general, student performance on the state test is overestimated by our models except that the WPI-106 model underestimates school F; and school W is even more overestimated than school F across known results from all the three models. As we know that, theoretically a one-skill model assumes perfect transfer. Since that is unlikely to happen, it would tend to overestimate student performance. And for a weaker school, perfect transfer is even more improbable. Thus, the overestimation would be greater since students are probably learning a collection of 106 unrelated skills. The tendency of overestimate decreases as the granularity of transfer models increases, and a very fine grained model such as the WPI-106 model that assumes no transfer or very low transfer may even underestimate when there is actually some level of knowledge transfer. We can see that in Table 2, the MD goes from negative to positive when we use the WPI-106 model for School F. Given these results, based on our hypothesis we would predict school F is the stronger school. An examination of both schools’ MCAS performance reports (for current achievement) and information on their Annual Year Progress (AYP, for changes in performance) confirms our prediction. 
 Table 2. Results for students grouped by schools. 
 As mentioned in section 1, a second validation approach is that instead of partitioning students by school, we can use their state assessment test score and partition them by math proficiency. If we see a trend for stronger students, it is reasonable to believe it applies to stronger schools. Therefore, as reported in section 3, we split all the 495 students into 3 groups based on their state test performance level, and fit a mixed-effects logistic regression model to each group separately for different transfer models. The values of MAD and MD are summarized in Table 3. We see a slight support with MAD: for the students at the high end, the WPI-39 does the best job at predicting their state test scores, reliably better than the other three models, while the WPI-106 model does reliably worse than the WPI-1 and WPI-5 models, suggesting there is certain amount of knowledge transfer happening with the high performing students. However, since we do not obtain results of the WPI-106 model for the other two groups, it is hard to draw a conclusion there. When it comes to the MD measure, we notice some support as well. Obviously, the advanced and proficient students have been underestimated by all models, and the amount of underestimation goes worst when the finest grained model, the WPI- 106 model, is applied. On the contrary, the medium and low performing students are all overestimated under all the models. Just as we hypothesize, the finer grained models overestimate less than the coarser grained models, and the better performing, stronger groups are less overestimated than the weaker groups. Therefore, weaker students are better represented by transfer models that are finer-grained. 
 Figure 1. Result of classifying in Weka. 
 3.4 A bottom-up aggregation approach.
 Rather than starting with an a priori disaggregation, we now focus on treating students as individuals and discovering commonalities among students who are best-fit with a particular transfer model. We have collected demographic data about several properties of a student, such as which school he/she goes to, ethnicity, gender, etc.  Finding out the relation among these properties and which transfer model best fits this student is our goal. Our plan is to bring together model-fitting information and student characteristics, and then use a machine learning classifier to determine the best-fit model. This bottom-up aggregation is a strong alternative to proposing and testing disaggregation, and will scale nicely as we get more descriptors for each student. For this purpose, we first re-fit models for all the students as one group2 and identify which model best fits each individual student. The best-fit model information is then combined with other properties of the student in a new data set. Specifically, the properties we use are: gender, free-lunch status (indicative of family income), special education status, ethnicity, and state test performance level. These properties are picked because they are easy to access, and all of them have meanings to researchers working with other populations in other locations. In comparison, properties such as the school a student attends are much less useful to those in other locations. Given the new data set, we built a J48 (C4.5 revision 8) decision tree in Weka 3.6 [15]. The constructed J48 pruned tree is show in Figure 1 that tells how the classifier uses the attributes to make a decision. The constructed tree is extremely simple with just 5 nodes. The WPI-1 model is overall the best fitting model for Advanced (A) and Proficient (P) students, and the WPI-106 is for “Needs improvement” (NI) or Warning (W) level students. The numbers in brackets after the leaf nodes indicate the number of instances assigned to that node, followed by how many of those instances is incorrectly classified as a result. In our case, the correct classification rates are relatively good for students at performance level of A, P, and W. Yet, for students at performance level of NI, even though the WPI-106 model is the best fit, it is not dominant with 76 out of 138 instances misclassified. It is encouraging that this simple decision tree can achieve a predictive accuracy of over 70% during stratified cross-validation. Although the decision tree only uses MCAS performance, it was provided with the variables described above but was unable to find a use for them. This result suggests the appropriate level of transfer model granularity really seems to depend on student knowledge, rather than on variables that may correlate with knowledge such as family wealth. Therefore, if tutor designers have students with rather different levels of knowledge, they might wish to use different levels of their skill hierarchy. 
 2 We had to reduce the number of students to 447 from 495 because of a memory limit of R. 
 This point does not contradict the use of evaluating interventions [10] and schools by model granularity: other properties certainly matter in how well knowledge transfers, but for our dataset they are not as predictive as the student’s knowledge. 
 4 Contributions, Future work, and Conclusions.
 The contribution of this paper lies in several aspects. First, automated techniques for revising transfer models for better knowledge representation have shown no huge improvements in accuracy but have addressed interesting scientific questions. Is there a way we can do interesting science on educational data sets and avoid the “irritating” automation step? Our answer is “yes,” if it is possible to build a hierarchy of transfer models with different granularity. Previous experience tells us that this is not a rare thing to have, and not very hard to think about. The hierarchy can be used for runtime benefit of intelligent tutoring systems such as the control of mastery learning or generation of feedback messages for students of various proficiency levels. It can also be used to evaluate schools and be validated via high stake test performance. Second, through the usage of a bottom-up aggregation approach, the problem is changed. Rather than trying to automate the model search, why don’t we automate seeing which student best fits which model? Third, we argue that hand-created transfer models and a bottom-up approach to aggregating students is a better use of human brains and computational power than approaches that focus search efforts on revising the domain model.  Better understanding what parts of the scientific enterprise can be best done by people and which are better done computationally is a major issue in EDM. A major open question of this work is whether just because a student is best modeled at a coarser grain size, shall we use such a model to drive tutorial instruction?  For example, even though strong students are best modeled by a single skill “Math,” it is not obvious how one would design hint messages in a system that only recognized one skill.  A hybrid approach would be to track student knowledge and drive mastery learning at a coarser grain size, but provide feedback using a finer-grained model. A second question is that, since student knowledge is changing over time, perhaps we should use different level models to represent a student at different points in his learning? In this paper, we start with existing hand-constructed transfer models at various levels of granularity, and use them as a lens to examine student learning. Specifically, we start by examining whether we can evaluate schools by determining the grain-size at which its students are best represented. We also examined what models best fit students at different levels of proficiency, and found some support for the idea of stronger students being better fit with coarser transfer models. The most interesting analysis was the bottom-up aggregation and using classification to find clusters of students who learn similarly. This analysis suggests transfer model granularity really seems to be about student knowledge. Finally, we argue that it is more productive to focus analytical effort on which students should use which transfer models rather than on automatically refining those models. 
 Acknowledgements.
 We thank Dr. Neil Heffernan for his help and insightful comments on this work. This research was made possible by the U.S. Department of Education, Institute of Education Science (IES) grants #R305K03140 and #R305A070440, the Office of Naval Research grant # N00014-03-1- 0221, NSF CAREER award to Neil Heffernan, and the Spencer Foundation. All the opinions, findings, and conclusions expressed in this article are those of the authors, and do not reflect the views of any of the funders.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Back to the future: a non-automated method of constructing transfer models</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mingyu-feng"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mingyu-feng"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/mingyu-feng"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/207">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Developing an Argument Learning Environment Using Agent-Based ITS (ALES)</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/207/authorlist"/>
		<swrc:abstract>This paper presents an agent-based educational environment to teach argument analysis (ALES). The idea is based on the Argumentation Interchange Format Ontology (AIF) using ”Walton Theory”. ALES uses different mining techniques to manage a highly structured arguments repertoire. This repertoire was designed, developed and implemented by us. Our aim is to extend our previous framework proposed in [3] in order to i) provide a learning environment that guides student during argument learning, ii) aid in improving the student’s argument skills, iii) refine students’ ability to debate and negotiate using critical thinking. The paper focuses on the environment development specifying the status of each of the constituent modules.</swrc:abstract>
		<led:body><![CDATA[ 1. ALES architecture. 
 2.1 The Domain Model.
 The domain model is represented in the form of the relational argument database (RADB), it has been developed and implemented by us, see [2, 3] for more details and discussions, which summon a huge number of arguments. These arguments were previously analyzed by experts based on Walton theory using the AIF ontology [6, 11]. The domain model can semantically be represented as a forest of a numerous directed free trees [7]. Each directed tree in the forest lays out a semantic representation for a specific argument analysis. The domain model representation is general enough to encapsulate multiple domains, it also enjoys the extendibility feature, where adding new schemes is permitted. Fig.2 describes the various building blocks concerned with the RADB, using screen shots of our implemented system, such that: (a) the table ”Scheme TBL” gathers the name and the index for different schemes, (b) the table ”Scheme Struct TBL” assembles the details of each scheme in ”Scheme TBL”, (c) the ”Data TBL” table contains the analysis of different arguments based on different scheme structure and preserves the constraints of the AIF ontology [6] (s.t. no information node(I-node) refines another I-node). The relation between those different basic tables is shown in Fig.3. 
 2.2 The Student Model.
 The student model stores details about student’s current problem-solving state and long term knowl- edge progress, that is essential for future student’s performance evaluations. The model considers personal information, pre-test evaluation, and performance history. Personal information contains personal data as name, ID, password, ..., etc. The pre-test evaluation permanently assesses the stu- dent’s argument analysis skills and follows the student progress through learning process. Finally, the performance history implicitly reflects how much the student has done and how well. 
 Fig. 2. The main tables in RADB. 
 Fig. 3. The relation between the main RADB’s tables. 
 2.3 Pedagogical Model.
 The pedagogical model is responsible for reasoning about the student behavior according to the student model, in order to: i) retrieve the most relevant results to both the subject of search and the students’ background, ii) expose the corresponding argument to the selected result, iii) guide the student analysis based on the pre-existing one. The pedagogical model as seen in Fig.1 consists of three main components: a parser, a classifier agent, and a teaching model. 
 2.3.1 Parser.
 The parser as shown in Fig.4(b) receives a statement S from the student. This statement is divided by the parser into tokens, and then the number of tokens is reduced. The tokens are reduced if they belong to a look up table containing a set of all unnecessary words like { a, an, the, he, have, is, him ..., etc }, otherwise it is added to the set of tokens to be sent to the classifier agent. Finally the final crucial set of words { w1 w2... wn } is sent to the classifier agent. The importance of the parser module lies in reducing the set of tokens into a set of significant keywords, which in turn will i) improve the results of the classifier where combinations of unnecessary words vanish, ii)reduce the number of iterations done by the classifier agent. The parser is already implemented as shown in Fig.4(a). 
 2.3.2 Classifier Agent.
 The classifier agent gathers and controls different mining techniques in order to classify the retrieved contexts based on student’s choices. The agent mines the RADB repository aiming to: (i) direct the search process towards hypotheses that are more relevant to student’s subject of search; classifying the analogous arguments in different ways based on students’ choice, seeking for the most relevant arguments to the subject of search. (ii) add flexibility to the retrieving process by offering different search techniques. The agent offers three search techniques: general search, priority search, and rule extraction search. 
 Fig. 4. The parser model.
 In the former, the general search classifies and retrieves the arguments based on the breadth first search technique. The priority search classifies the retrieved contexts based on the maximum support number using an adapted version of the AprioriTid[4] mining technique. In the latter, the rule extraction summarizes the retrieved arguments searching for hidden patterns that are most relevant to the subject of search, then this patterns are exposed in the form of rules. Each rule, for each retrieved argument, contains the affirmative ”+” and the negative ”-” parts relating to the final conclusion of that argument. Priority Search: The AprioriTid algorithm [4] has been implemented and embedded to the clas- sifier agent as ”Priority Search”. The Priority search aims to retrieve the most relevant arguments to the users’ subject of search and queuing them based on the maximum support number, such that the first queued argument is the one that has more itemsets[3] related to the subject of search. Although the AprioriTid algorithm has originally been devised to discover all significant association rules between items in large database transactions, the agent employs its mechanism in the priority search to generate different combinations between different itemsets [4, 3]. These combinations will then be used to classify the retrieved contexts and queued them in a descending order based on its support number. As a response to the priority search purpose, an adapted version of the AprioriTid mining algorithm has been applied. This adapted version, as seen in Fig. 5, considers the single itemset (1-itemset) size as well as the maximum support number usage, rather than k-itemset for k≥2 and the minimum support number ”minsup” mechanism. Fig. 5. An enhanced version of AprioriTid For more clarification, the priority search mines specific parts of the pre-existing arguments based on the users’ search criteria. This search criteria enables the student to seek the premises, conclu- sions or the critical questions lying in the different arguments. For example, suppose the student queries the RADB searching for all information related to ”Iraq war”. Simply, he may write ”the destructive war in Iraq” as the search statement and can choose the conclusion as the search criteria. 
 Fig. 6. The adapted AprioriTid mechanism. 
 In this case, the classifier agent receives the set of significant tokens {destructive, war, iraq} from the parser model. This set is considered as the single size itemset (1-itemset) C1={w1, w2, w3} that contains the most crucial set of words in the search statement. Then, the agent uses the adapted version of the AprioriTid algorithm to generate the different super itemsets C2≤k≤3, which are the different combinations between different tokens. So, the generated super itemsets, as seen in Fig.6, will be the 2-itemset C2={ w1w2, w1w3, w2w3 }, and the 3-itemset C3={ w1w2w3 }. Afterward, the different conclusions in the different arguments trees will be mined seeking for the most relevant set of arguments Ans={d1, d2, ..., dm } such that ∀ di∈D ∃ Ck∈{1,2,..,j}⊆ di . Finally, the results will be queued in a descending order and exposed in a list, where the student can choose the argu- ment name ”Argument 314” from the list to expose the associated context and analysis as in Fig. 10. General Search: The system uses the breadth first [14] search in order to seek the different ar- gument trees and retrieve the most relevant group. The revealed contexts are ordered based on the number of nodes ”nodes cardinality” that contain any keyword, in a way where the first context is the one which has more nodes related to the search statement. For example, suppose the user writes ”the destructive war in Iraq” as a search statement. The revealed contexts, as shown in Fig. 7, will be ordered based on the nodes’ cardinality. The breadth first search seeks each tree in our RADB, preserving the ancestor-descendant relation [7] by searching first the root, then the children in the same level and so on. Finally, if the user picks one of the resulted search arguments, the associated context and analysis are depicted as shown in Fig. 10. Rule Extraction Search: Rule extraction mining is a search technique in which argument trees are encountered to discover all hidden patterns ”embedded subtrees” [7] that coincide with the re- lation between some objects. These objects express a set of the most significant tokens of the user’s subject of search. Precisely, suppose the student wants to report some information about the relation between the ”USA war” and the ”weapons of mass destruction”. At the beginning, the user’s search statements are reduced to the most significant set of tokens by the parser [2][3][1]. Then, the differ- ent argument trees, pre-existing in the RADB repository, are mined in order to fetch these different tokens. Fig. 8(a) shows the analysis of an argument tree, where some enclosed nodes coincide with the student’s search statements, while Fig. 8(b) shows the revealed embedded subtree. 
 Fig. 7. The General search representation form. 
 Fig. 8. The tree Rule Extraction search. 
 Finally, each resulting subtree is expressed in the form of a rule as shown in Fig. 9, where ”+” indicates that this node is a support to the final conclusion whereas ”-” is a rebuttal node to the final conclusion. 
 2.3.3 Teaching Model.
 The teaching model monitors the student actions, guides the learning process and provides the appropriate feedback. However, In the mean time, it is still in the implementation phase. The model starts its role when the classifier agent sends the document Di selected by the student. The teaching model checks, according to the current student model, whether the student is in the learning or the assessing phase. If the student is in the learning phase, the document is presented associated with the corresponding analysis as the shown in Fig. 10. On the other hand, if the student is in the assessment phase, the student is able to do his own analysis, and the teaching model will guide him during analysis by providing personalized feedback whenever required. The feedback aims to guide the student and refine his analysis and intellectual skills. Two kinds of feedback are provided by the teaching model; partial argument negotiation and total argument negotiation. – Case of partial argument negotiation: In this case, the student starts analyzing the argu- ment context in the form of a tree in which the root holds the final conclusion of the issue of discussion. The teaching pedagogy used in this case provides partial hints at each node of the analysis tree. They are results of comparing the student’s current node analysis to the original one in the argument database. These hints are provided before allowing the student to proceed further in the analysis process; they aim to minimize the analysis error ratio, as much as possible, for the current analyzed node. 
 Fig. 9. The representation form of Rule Extraction search result. 
 Fig. 10. The representation of the selected argument. 
 Generally, the teaching model guides with the student via the partial hints at each node till the error of the current node is minimized to a specific ratio. After then, the student is able to move to the next analysis step (i.e., node). – Case of total argument negotiation: The total argument negotiation is similar to the partial argument negotiation. However, the teaching pedagogy is different in that it provides hints only at the end of the analysis process. In other words, after the student builds the full analysis tree for the selected context, the system interprets and evaluates the student’s analysis comparable to the pre-existing one and remarks the errors. Generally, in the assessing phase, the teaching model presents the transcript of the chosen argument associated with an empty tree skeleton and asks the student to start his own analysis. The student starts the analysis by copy and paste text passages from the transcript or enter free text into the nodes. The teaching model traces each node text and divides it into set of significant tokens, then interprets and evaluates the errors ratios comparable to the pre-existing analysis underlying in the RABD. Finally the model provides the feedback, partially or totally, based on the student choice and records the student’s errors for the current transcript, which in turn will be used, by the student model, to evaluate the performance and to follow progress of the student. 
 3 Illustrative Example.
 This example shows a complete run for the Total negotiation of the assessing phase. The system interactions are written in normal font. The student’s actions are in bold. My illustrations to some actions will be in capital letters. SUPPOSE THE STUDENT IN THE ASSESSING PHASE CHOOSING THE TOTAL FEEDBACK PROPERTY. THE SYSTEM WILL GIVE THE STUDENT THE ABILITY TO SELECT SPE- CIFIC SCHEME TO BE USED IN HIS ANALYSIS, AS SHOWN IN Fig. 11. User ”Argument from Verbal Classification Scheme”. 
 Fig. 11. The total negotiation assessment form. 
 THE WHOLE ARGUMENTS, THAT USE THE ”ARGUMENT FROM VERBAL CLASSIFICA- TION SCHEME” IN ITS ANALYSIS, WILL BE LISTED SUCH THAT THE PRIORITY IS TO THE CONTEXTS THAT HAVE NOT BEEN ACCESSED YET BY THE USER DURING THE LEARNING PHASE. System [argument 214, argument 1, argument 600]. User picks up one of the listed arguments, [argument 214] as example. SystemÀ presents the transcript of the chosen argument with an empty tree skeleton as in Fig. 11. User start the analysis by copy and paste text passages from the transcript or enter free text into the nodes then press advice. System divides each statement in each node into tokens, and compares these tokens with the ex- pert analysis for the same node. Then calculates and records the errors ratio for the whole nodes. System shows out a declarative report that describes the mistakes of each node separately. As SEEN IN Fig12 THE STUDENT ANALYSIS OF THE FINAL CONCLUSION NODE ”NODE0” IS PARTIALLY CORRECT AND THE STUDENT HAS BEEN ADVISED TOUSE THESEWORDS IN HIS ANALYSIS {SADDAM, REGION,...}. ALSO IN ”NODE3” WHICH IS ON OF THE CRIT- ICAL QUESTIONS, THE ANALYZED STATEMENT IS CORRECT HOWEVER THE TYPE OF THE NODE (SUPPORT OR ATTACH) IS WRONG. User press OK. AFTER THE USER FINISHES HIS ANALYSIS TO THE WHOLE CONTEXT, FILLING THE SUITABLE ANALYSIS FOR EACHNODE, THE SYSTEMWILL CALCULATES AND RECORDS THEWHOLE ARGUMENTANALYSIS RATIO FOR THATARGUMENT. THIS RATIOS RECORDS WILL BE USED LATER TO EVALUATE THE PROGRESS OF THAT STUDENT. 
 Fig. 12. The resulting report. 
 4 Related Work.
 Early, the field of AI and education was very interesting to most of the researchers, where many instructional systems have been developed to hone students argumentation skills. SCHOLAR and WHY[8] systems are examples for these trials. However, these systems were mainly designed to engage students in a Socratic dialog, which faces significant problems such as knowledge representations to develop a Socratic tutor[8]. This mainly occurred in complex domains like legal reasoning, control or preprocessing, and manipulate the natural language. Later, as a response to these difficulties, a number of argument mapping tools[18, 10, 17, 13] have been developed to foster debate among students about specific argument, using diagrams for argument representation. However, the data mining and artificial intelligence influence, which needed to guide the student to understand the relation between scientific theories and evidence, and refines his argument analysis ability, are missing in these tools. Recently, a number of mining weblogs[9] and case-based models[5] have been proposed to tackle the mining and the artificial influence problem. The mining weblogs is considered as a classification problem for the legal or informal reasoning considering law. Though, it mines the textual data that is intractable to be processed. On the other hand, the case-based argumentation systems, such as the CATO[5], use the case based reasoning method in order to reify the argument structure through tools for analyzing, retrieving, and comparing cases in terms of factors. Comparing CATO with our proposed application, both of them provides examples of specific issue to be studied by the different students, as well as evaluates students’ arguments comparable to the pre-existing one. Regarding to the search for arguments, both systems support students’ search for the existing database, and retrieve the most relevant argument. However, CATO limits the students’ search by a boolean combination of factors. Also, in the full-text retrieval search, one can retrieve documents, by matching phrases, which is not relevant to the search subject. On the other hand, ALES provides different search criteria to tackle this problem, as seen in section 2, using different mining techniques in order to: summon and provide a myriad of arguments at the student’s fingertips, retrieve the most relevant results to the subject of search, and organize the retrieved result such that the most relevant is the first rowed. Finally, I. Rahwan presents the ArgDf system [6, 11], through which users can create, manipu- late, and query arguments using different argumentation schemes. Comparing ArgDf system to our approach, both of them sustain creating new arguments based on existing argument schemes. In addition, the ArgDf system guides the user during the creation process based on the scheme struc- ture only, the user relies on his efforts and his background to analyze the argument. However, in our approach, the user is not only guided by the scheme structure but also by crucial hints devolved through mining techniques. Accordingly, the creation process is restricted by comparing the contrast- ing reconstruction of the user’s analysis and the pre-existing one. Such restriction helps in refining the user’s underlying classification. In the ArgDf system, searching existing arguments is revealed by specifying text in the premises or the conclusion, as well as the type of relationship between them. Then the user can choose to filter arguments based on a specific scheme. Whereas in our approach, searching the existing arguments is not only done by specifying text in the premises or the conclusion but also by providing different strategies based on different mining techniques in order to: refine the learning environment by adding more flexible interoperability, guarantee the retrieval of the most convenient hypotheses relevant to the subject of search, facilitate the search process by providing a different search criteria. 
 5 Conclusion and Future Work.
 In this paper we introduced an agent-based learning environment (ALES) to teach argument analysis. ALES extends the previous work done on building a highly structured argument repertoire (RADB) with managing tool [2, 3]. The main aim of developing this environment is to aid in improving the student’s argument skills. ALES serves as a new trend in teaching arguments. The proposed archi- tecture serves the educational process by allowing learning and assessing phases where personalized feedback is provided. ALES guides the student during argument learning, analysis, and preprocess- ing. In addition, ALES enjoys certain advantage over others, where a relevant and convenient result is assured to be obtained especially when the search statement is in this form: ”the destructive war in Iraq”. In the future, we intend to (i) integrate an NLP software to aid in polarity classification, in which the underlying RADB arguments are classified into affirmative and rebuttal lists to the issue of discussion, (ii) use the frequent tree mining techniques[7] in order to search for frequent patterns in different arguments, and (iii) consider the interaction between the student model and the pedagogical model, and how this is going to affect the abductive learning phase.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Developing an Argument Learning Environment Using Agent-Based ITS (ALES)</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/safia-abbas"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/safia-abbas"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hajime-sawamura"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hajime-sawamura"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/207/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/safia-abbas"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/hajime-sawamura"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/208">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/proceedings"/>
		<dc:title>Automatic Concept Relationships Discovery for an Adaptive E-course</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/208/authorlist"/>
		<swrc:abstract>To make learning process more effective, the educational systems deliver content adapted to specific user needs. Adequate personalization requires the domain of learning to be described explicitly in a particular detail, involving relationships between knowledge elements referred to as concepts. Manual creation of necessary annotations is in the case of larger courses a demanding task. In this paper we tackle a concept relationship discovery problem that is a step in adaptive e-course authoring process. We propose a method of automatic concept relationship discovery for an adaptive e-course. We present two approaches based on domain model graph analysis. We evaluate our method in the domain of programming.</swrc:abstract>
		<led:body><![CDATA[ 1.  E-course domain model. 
 The rest of the paper is structured as follows. In section 2 we discuss related work. Section 3 introduces the proposed method, while sections 4 and 5 provide its description in more detail. Section 6 provides evaluation details and the results of performed experiments. In section 7 we sum up our contribution and discuss future work. 
 2 Related work.
 The work related to concept relationship discovery in the area of adaptive e-learning is presented in [6]. Concept similarities are computed based on the concept domain attributes comparison. However, the meaning of concept in the terminology of Cristea is different from one described by Brusilovsky [1]. Although it contains domain attributes, it also holds textual representation. This should be considered an intentional description from the ontological point of view, but then the reusability of such concepts is arguable. We are not aware of any further evidence of (semi-)automatic concept relationship generation in the adaptive e-learning field. Finding relations between concepts is a subtask of ontology learning field [10]. The relations being created typically have taxonomic character (is-a). Considering text mining, related approaches mainly utilize natural language processing (NLP) techniques. Relations are induced based on linguistic analysis relying on preceding text annotation [2], incorporating formal concept analysis (FCA) [4] or using existing resources such as Wikipedia or WordNet [5]. The drawback of relationship discovery as an ontology learning task (in relation to e-course authoring) is its dependency on precise linguistic analysis. Most of the approaches rely on lexical or syntactical annotations, the presence of powerful POS taggers, existing domain ontologies, huge corpuses or other external semantic resources (e.g. WordNet). As mentioned above, this knowledge is often not available during an e-course authoring. The solution for teachers should involve unsupervised approaches to unburden them from additional work. This need we address in the method we propose. The task of structuring the concept space is also present in the area of the topic maps. In this field, the elementary units – topics – we can view analogical to concepts. Authors in [7] generate relations between topics by analyzing the HTML structure of Wikipedia documents. The results indicate that learning objects representation structure should be considered when structuring the domain space. Categorization methods are used in [9] where similar topics are discovered by latent semantic indexing (LSI) and K-means clustering. Unsupervised methods serve as guidance in topic ontology building. Similar approach is missing in the area of adaptive e-learning. Our method is based on statistical unsupervised text processing and graph analysis related to actual knowledge about the domain. We explore generated or existing concept associations in order to reveal hidden semantic relationships. We were motivated by good results of graph analysis employment achieved in the area of Web search. Our method does not depend on external semantic resources allowing to be used in various domain environments. However, additional semantic connections need not to be excluded. 
 3 Automatic concept relationship discovery.
 The concept relations discovery is one of several steps in the adaptive e-course authoring process. Prior to this step we assume a teacher has already created (eventually reused) learning objects, put them into reasonable structure (e.g., hierarchical), identified domain concepts and assigned them to learning objects. Concept extraction can also be done semi-automatically as we show later. Our goal is to utilize the actual knowledge and discover relationships between concepts. As we represent a domain using a graph model, we conduct a graph analysis. We employ two alternative graph algorithms suitable to this task. Before the algorithms are applied we perform learning objects preprocessing. Because we are interested in e-learning domain, we consider several specifics when dealing with the knowledge discovery. Learning objects are present mostly in a textual form. Thus we employ text mining techniques. We suppose the number of learning objects is known. This enables us to compute inverse document frequency when building term vector-based learning objects representations. Moreover, the learning objects we process are related to one domain area. This fact reduces the concept ambiguity problem unlike when processing heterogeneous sources. Learning objects are often mutually interconnected. We usually know the hierarchical relationships between learning objects in the course or references to other course parts may exist. Due to the specifics we are able to compose relatively accurate vector representation during the preprocessing. For example, we can employ bag-of-words model with tf-idf weights. Based on the weights we determine the degree of each concept’s relatedness to all learning objects. After the preprocessing step we apply concept relationship discovery method itself. Using selected graph analysis algorithm we (1) compute concept similarity scores and finally (2) create relationships between each concept and his top-k most similar neighbors. For graph analysis we employ two alternatives: spreading activation and PageRank-based approach. Both approaches are described in more detail in the following sections. In the second step, for each concept the most similar neighbors according to computed similarity are chosen. The top-k set we define as a set where the most similar neighbors sorted by score accumulate k% of all neighbor similarity values. For example, top-20 neighbors accumulate 20% of sum of all neighbors’ similarities. We typically set coefficient k from <10; 20>. Between each concept and its top-k neighbors we create relationships. The relations’ weights are normalized with regard to the whole domain model. 
 4 Spreading activation.
 The principle of the spreading activation approach is to consider the domain model to be a contextual network. Contextual network is network where several types of nodes exist. We recognize two node types: learning object nodes and concept nodes. In contextual networks, the spreading activation method is often used for similarity search [3]. The queried node is activated with energy E that spreads to neighbor nodes via incident edges. Final energy distribution in the graph determines the similarity of nodes. We use this principle to compute the degree of the concepts’ similarity. Basic steps of the algorithm can be described as follows: For each concept ci: 
 1. activate concept with initial energy E0, 2. spread activation to entire graph, 3. determine the degree of similarity to all concepts. 
 In step 2 the energy spreads from an activated node to all neighbor nodes proportionally according to outgoing relationships weight. Weights derived from learning object vector representation determine concept relatedness to different learning objects (refer to example depicted in Figure 2). The crucial step is step 3. After activation spreading finishes, each concept in the network is activated with energy according to its relatedness to concept ci. The degree of similarity between concept ci and its neighbor cj is computed as follows: 
 FORMULA_(1).
 where simsai,j is spreading activation similarity between concepts ci and cj, Ej is the energy of concept cj and sdi,j is the shortest distance between the concepts ci and cj in the contextual network graph. The purpose of the formula is to normalize the power-law distributed activation energy values. 
 Figure 2. Example of a contextual network with two types of nodes: concepts and learning objects with the simplified vector representation. After activation spreading finishes, each concept in the network is activated with energy depending on the initially activated concept “print” (accumulated activation is visualized by the concepts’ border width). 
 5 PageRank-based Analysis.
 The graph representation of the domain model forms the basis for the second variant of concept-to-concept similarity computation. This approach builds on the algorithms for estimating relative importance in networks [12]. Such algorithms are used to compute the quantitative measure of node similarity with respect to a given node (or set of nodes). We employ PageRank with Priors in particular, successfully used in categorization systems [8]. We modify Diedrich’s and Balke’s core idea to allow for the inclusion of weights between learning objects and concepts. The approach principle lies in the propagation of actual domain model topology characteristics into the explicit links between concepts. The algorithm consists of the following steps: 
 1. for each concept ci select concepts connected via exactly one learning object, 2. for concept ci and selected neighbors compute relation weight owij, 3. build a temporary domain graph where concepts are nodes and owi,j weights are assigned to edges, 4. for each concept ci select the most related co-occurring neighbors, 5. for each concept ci compute the PageRank scores biasing on the selected neighbors. 
 For owi,j computation in step 2 we use the following equation:.
 FORMULA_(2).
 where owi,j is the relation weight between concepts ci and cj, and wi,k is the weight assigned to the association between the concept ci and learning object lok. The idea is that weights are considered to be probabilities that the concept is related to learning object. The resulting probability of concepts’ similarity is the mean of individual probabilities. After building a graph in step 3, the most related co-occurring neighbors are determined in step 4. The most related neighbors we consider neighbors that accumulate top-k % of the sum of all neighbors’ similarity scores to a given concept. In step 5 we use the PageRank analysis algorithm (concepts and temporal links in between are analogical to the Web) to adjust the graph and compute the prestige of nodes. PageRank scores represent similarity simprpi,j towards a biased node set with regard to relations within the whole graph. The sorted set of all graph concepts is subject to top-k selection step in order to obtain only relevant relationships. 
 6 Evaluation in the programming learning domain.
 The proposed method we evaluated in the domain of programming learning. For an experiment we used Functional programming course being lectured at the Slovak University of Technology in Bratislava. The Functional programming course is a half-term course consisting of 70 learning objects on the functional programming paradigm and programming techniques in the Lisp language. The learning material is hierarchically organized into chapters and sections according to a textbook used in the course. Learning objects are represented using the DocBook markup language enabling easy processing. The method results we evaluated against manually constructed functional programming concept map. The course lecturer together with randomly chosen sample of 2007/08 course students were involved. Manual creation of concept map comprised the assignment of weighted values to concept relationships. As assigning continuous values from interval <0; 1> is non-trivial task, possible weight values were limited to set {0, 0.5, 1} implying: • 0 – concepts are not related to each other (no relation), • 0.5 – concepts are partially (maybe) related to each other (weak relation), • 1 – concepts are highly (certainly) related to each other (strong relation). There were 366 relationships created, 216 were weak relations while 150 were strong relations. In first step of experiment we preprocessed learning objects and composed their bag-of- words term vector representation. The frequency of the terms presented as domain keywords in the textbook index was boosted. We extracted concepts as most frequent terms and their normalized tf-idf values we used as weights for concept-to-learning object associations. Hereby we executed all necessary steps prior to concept relationship discovery. After preprocessing we separately applied both proposed method alternatives and obtained concept relationships. The relationships were compared to manually constructed reference concept map using well-known precision and recall measures and their harmonized mean, the F-measure. In order to gain more accurate evaluation, we extended the original recall measure to involve the manually constructed domain model relationship types: 
 FORMULA_(3).
 where R* is the extended recall measure, retrieved is the set of all relationships retrieved by the method, correctA is the set of manually created “strong” relationships with weight 1.0 and correctB is the set of manually created “weak” relationships with weight 0.5. The purpose of equation (3) is to take into consideration the fact that “weak” relationships need not necessarily be the part of a domain model. Table 1 sums up the results of the performed experiments. 
 Table 1.  Experimental results. The F* contains the harmonized mean using R* recall. 
 The results show that performance of PageRank-based approach is better than spreading activation. This result is evidence that PageRank-based analysis enables more precise processing of the underlying graph representation. The resulted F/F* measure we interpret as “completeness” of generated concept map. However, generated relationships not contained in the manually constructed concept space were all considered incorrect that should not reflect the reality. Though manual relationship creators made their best effort to match real-world relations, relationships retrieved automatically need not to be irrelevant. They might represent bindings, which were not explicitly realized even by the most concerned authors. As the proposed method goal is to serve as assistant to a teacher, the amount and accuracy of recommended relationships can be considered helpful. 
 7 Conclusions.
 In this paper we presented a method of automatic concept relationship discovery for an adaptive e-course. The method is applied as a step in the process of an adaptive e-course authoring and its goal is to help teacher and contribute to overall authoring automation. We proposed and evaluated two variants of the concept score similarity computation representing two approaches to domain model graph processing. We found that our PageRank-based variant achieves better results and identifies more correct relationships against manually constructed concept map. The main contribution of this paper is a novel approach to adaptive e-course authoring automation. No similar approaches relying on domain model processing and yielding similar results (F*-metrics 65.2%) are applied in the field of adaptive e-learning. The method we propose is independent “component” of a course authoring process. It has clearly defined interface and does not depend on preprocessing techniques. Prior to relationship discovery the concept extraction and weight assignment techniques may vary: various IR methods can be employed or manual annotations can be used. Both approaches may be eventually combined, which seems reasonable especially when considering social and collaborative aspects of e-course authoring. The method addresses the specifics of e-learning domain and does not rely on external resource presence like similar approaches do. Moreover, it is not dependent on the language of an e-course. As the results of evaluation seem promising, we currently work on more complex evaluation in a real-world environment to obtain even more objective feedback on discovered relationships accuracy and relevancy during functional programming learning. The further advantage of our method is that although the variants are targeted at the e-learning domain, they are not limited to it – the presented computations are also applicable to different environments. A similar situation with acute “metadata” need is on the Web. Concept maps constructed over the Web pages should in first step serve as backbone for development of richer semantic descriptions. Involvement of social annotations or folksonomies shifts our method applicability even further. 
 Acknowledgements.
 This work was partially supported by the Cultural and Educational Grant Agency of the Slovak Republic, grant No. KEGA 3/5187/07.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2009</swrc:year>
		<rdfs:label>Automatic Concept Relationships Discovery for an Adaptive E-course</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/marian-simko"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/marian-simko"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/maria-bielikova"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/maria-bielikova"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/208/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/marian-simko"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/maria-bielikova"/>
	</rdf:Description>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/ben-gurion-university">
		<rdfs:label>Ben-Gurion University</rdfs:label>
		<foaf:name>Ben-Gurion University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hama-abu-kishk"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/carnegie-learning-inc">
		<rdfs:label>Carnegie Learning Inc</rdfs:label>
		<foaf:name>Carnegie Learning Inc</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/steve-ritter"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/carnegie-learning-inc">
		<rdfs:label>Carnegie Learning Inc.</rdfs:label>
		<foaf:name>Carnegie Learning Inc.</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tristan-nixon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/annalies-vuong"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/brendon-towle"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-dickison"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/r-charles-murray"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/carnegie-learning-inc">
		<rdfs:label>Carnegie Learning Inc.</rdfs:label>
		<foaf:name>Carnegie Learning Inc.</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tristan-nixon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/annalies-vuong"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/brendon-towle"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-dickison"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/r-charles-murray"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university">
		<rdfs:label>Carnegie Mellon University</rdfs:label>
		<foaf:name>Carnegie Mellon University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ilya-m-goldin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vincent-aleven"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/bruce-m-mclaren"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-v-yudelson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/emma-brunskill"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/martina-a-rau"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jung-in-lee"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/leigh-ann-sudol-delyser"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kelly-rivers"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-scheines"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/derek-lomas"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jessica-kalka"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/angela-z-wagner"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/gail-w-kusbit"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-p-gonzalez-brenes"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-a-mclaughlin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/stephen-e-fancsali"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yanbo-xu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/geoff-gordon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nan-li"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/noboru-matsuda"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/william-w-cohen"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/weisi-duan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/s-isotani"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/m-munna"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/s-wu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/bao-hong-tan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/albert-t-corbett"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-m-fincham"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/john-r-anderson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/shawn-betts"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jennifer-l-ferris"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/turadg-aleahmad"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-kraut"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-shih"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hao-cen"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carolyn-p-rose"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-cui"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/adriana-mjb-de-carvalho"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/xiaonan-zhang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-valeri"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lili-wu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kyle-cunningham"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/alida-skogsholm"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/brett-leber"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology">
		<rdfs:label>Eindhoven University of Technology</rdfs:label>
		<foaf:name>Eindhoven University of Technology</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rafal-kocielnik"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/natalia-sidorova"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/n-trcka"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/p-de-bra"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/evgeny-knutov"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sicco-verwer"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/wil-van-der-aalst"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/gerben-w-dekker"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jan-m-vleeshouwers"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/toon-calders"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/hyogo-university-of-teacher-education">
		<rdfs:label>Hyogo University of Teacher Education</rdfs:label>
		<foaf:name>Hyogo University of Teacher Education</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/koji-suda"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/junichi-kakegawa"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/koichiro-morihiro"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/konan-university">
		<rdfs:label>Konan University</rdfs:label>
		<foaf:name>Konan University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ryo-nagata"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/keigo-takeda"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/macquarie-university">
		<rdfs:label>Macquarie University</rdfs:label>
		<foaf:name>Macquarie University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-dale"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/mcgill-university">
		<rdfs:label>McGill University</rdfs:label>
		<foaf:name>McGill University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/francois-bouchet"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/roger-azevedo"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/monash-university">
		<rdfs:label>Monash University</rdfs:label>
		<foaf:name>Monash University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-cox"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/niigata-university">
		<rdfs:label>Niigata University</rdfs:label>
		<foaf:name>Niigata University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/safia-abbas"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hajime-sawamura"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/oviedo-university">
		<rdfs:label>Oviedo University</rdfs:label>
		<foaf:name>Oviedo University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/j-r-quevedo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/e-montanes"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/purdue-university">
		<rdfs:label>Purdue University</rdfs:label>
		<foaf:name>Purdue University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/suleyman-cetintas"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/luo-si"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yan-ping-xin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/casey-hord"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/sheffield-university">
		<rdfs:label>Sheffield University</rdfs:label>
		<foaf:name>Sheffield University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ofer-bergman"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/stanford-university">
		<rdfs:label>Stanford University</rdfs:label>
		<foaf:name>Stanford University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dave-barker-plummer"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/min-chi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/worsley"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/blikstein"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/tel-aviv-university">
		<rdfs:label>Tel Aviv University</rdfs:label>
		<foaf:name>Tel Aviv University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ronit-azran"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/tutor-technologies">
		<rdfs:label>Tutor Technologies</rdfs:label>
		<foaf:name>Tutor Technologies</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-k-harris"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/uned">
		<rdfs:label>UNED</rdfs:label>
		<foaf:name>UNED</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/antonio-r-anaya"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jesus-g-boticario"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/universidad-autonoma-de-madrid">
		<rdfs:label>Universidad Autonoma de Madrid</rdfs:label>
		<foaf:name>Universidad Autonoma de Madrid</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-bravo"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/universidade-federal-de-alagoas">
		<rdfs:label>Universidade Federal de Alagoas</rdfs:label>
		<foaf:name>Universidade Federal de Alagoas</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/david-nadler-prata"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/evandro-db-costa"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-california">
		<rdfs:label>University of California</rdfs:label>
		<foaf:name>University of California</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-n-rafferty"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michelle-m-lamar"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-l-griffiths"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/stuart-russell"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba">
		<rdfs:label>University of Cordoba</rdfs:label>
		<foaf:name>University of Cordoba</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mi-lopez"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jm-luna"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mm-molina"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/r-pedraza-perez"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-raul-romero"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/enrique-garcia"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-de-castro"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/amelia-zafra"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-glasgow">
		<rdfs:label>University of Glasgow</rdfs:label>
		<foaf:name>University of Glasgow</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-granada">
		<rdfs:label>University of Granada</rdfs:label>
		<foaf:name>University of Granada</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/miguel-gea"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-memphis">
		<rdfs:label>University of Memphis</rdfs:label>
		<foaf:name>University of Memphis</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vasile-rus"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/cristian-moldovan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nobal-niraula"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/fazel-keshtkar"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/brent-morgan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carol-forsyth"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/philip-pavlik"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/zhiqiang-cai"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mae-lynn-germany"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sidney-d-mello"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/blair-lehman"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/whitney-cade"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-olney"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jonathan-wood"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mihai-lintean"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-new-south-wales">
		<rdfs:label>University of New South Wales</rdfs:label>
		<foaf:name>University of New South Wales</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dror-ben-naim"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-bain"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nadine-marcus"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-north-carolina-at-charlotte">
		<rdfs:label>University of North Carolina at Charlotte</rdfs:label>
		<foaf:name>University of North Carolina at Charlotte</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-john-eagle"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/matthew-w-johnson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/leena-joseph"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/b-mostafavi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/m-croy"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lorrie-lehman"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-ostrava">
		<rdfs:label>University of Ostrava</rdfs:label>
		<foaf:name>University of Ostrava</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lukas-zoubek"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michal-burda"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-technology">
		<rdfs:label>University of Technology</rdfs:label>
		<foaf:name>University of Technology</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/marian-simko"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/maria-bielikova"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-utah">
		<rdfs:label>University of Utah</rdfs:label>
		<foaf:name>University of Utah</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/cecily-heiner"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-l-zachary"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-washington">
		<rdfs:label>University of Washington</rdfs:label>
		<foaf:name>University of Washington</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tara-m-madhyastha"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/steven-tanimoto"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute">
		<rdfs:label>Worcester Polytechnic Institute</rdfs:label>
		<foaf:name>Worcester Polytechnic Institute</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/qing-yang-wang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/shubhendu-trivedi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yutao-wang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sujith-m-gowda"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-wixon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/aatish-salvi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jaclyn-ocumpaugh"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lisa-rossi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/gabor-n-sarkozy"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/c-heffernan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-b-goldstein"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/matt-bachmann"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/orlando-montalvo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-nakama"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mingyu-feng"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carolina-ruiz"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute">
		<rdfs:label>Worcester Polytechnic Institute</rdfs:label>
		<foaf:name>Worcester Polytechnic Institute</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/qing-yang-wang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/shubhendu-trivedi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yutao-wang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sujith-m-gowda"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-wixon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/aatish-salvi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jaclyn-ocumpaugh"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lisa-rossi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/gabor-n-sarkozy"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/c-heffernan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-b-goldstein"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/matt-bachmann"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/orlando-montalvo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-nakama"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mingyu-feng"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carolina-ruiz"/>
	</foaf:Organization>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hama-abu-kishk">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/ben-gurion-university"/>
		<rdfs:label>Hama Abu-kishk</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Israel"/>
		<foaf:firstName>Hama</foaf:firstName>
		<foaf:lastName>Abu-kishk</foaf:lastName>
		<foaf:mbox_sha1sum>1668fa129c9833d82b2d3d63e0a70f54ed7382d7</foaf:mbox_sha1sum>
		<foaf:name>Hama Abu-kishk</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/steve-ritter">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-learning-inc"/>
		<rdfs:label>Steve Ritter</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Steve</foaf:firstName>
		<foaf:lastName>Ritter</foaf:lastName>
		<foaf:mbox_sha1sum>a826897d0588b898ec859188112735d40e347468</foaf:mbox_sha1sum>
		<foaf:name>Steve Ritter</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/r-charles-murray">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-learning-inc"/>
		<rdfs:label>R. Charles Murray</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>R.</foaf:firstName>
		<foaf:lastName>Charles Murray</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>R. Charles Murray</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/brendon-towle">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-learning-inc"/>
		<rdfs:label>Brendon Towle</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Brendon</foaf:firstName>
		<foaf:lastName>Towle</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Brendon Towle</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tristan-nixon">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-learning-inc"/>
		<rdfs:label>Tristan Nixon</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Tristan</foaf:firstName>
		<foaf:lastName>Nixon</foaf:lastName>
		<foaf:mbox_sha1sum>4ce02dc4c74c55fc0e9a36dd54fc5a78aaf87c8c</foaf:mbox_sha1sum>
		<foaf:name>Tristan Nixon</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/daniel-dickison">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-learning-inc"/>
		<rdfs:label>Daniel Dickison</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Daniel</foaf:firstName>
		<foaf:lastName>Dickison</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Daniel Dickison</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Kenneth R. Koedinger</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Kenneth</foaf:firstName>
		<foaf:lastName>R. Koedinger</foaf:lastName>
		<foaf:mbox_sha1sum>0f1e313636d51d167f86b5b0daddf63f02ca3ca3</foaf:mbox_sha1sum>
		<foaf:name>Kenneth R. Koedinger</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/177"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/adriana-mjb-de-carvalho">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Adriana M.j.b. De Carvalho</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Adriana</foaf:firstName>
		<foaf:lastName>M.j.b. De Carvalho</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Adriana M.j.b. De Carvalho</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rebecca-nugent">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Rebecca Nugent</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Rebecca</foaf:firstName>
		<foaf:lastName>Nugent</foaf:lastName>
		<foaf:mbox_sha1sum>8e3f2493492992c603e53cd69fa4b57e54c2c860</foaf:mbox_sha1sum>
		<foaf:name>Rebecca Nugent</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rebecca-nugent">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Rebecca Nugent</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Rebecca</foaf:firstName>
		<foaf:lastName>Nugent</foaf:lastName>
		<foaf:mbox_sha1sum>8e3f2493492992c603e53cd69fa4b57e54c2c860</foaf:mbox_sha1sum>
		<foaf:name>Rebecca Nugent</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jack-mostow">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Jack Mostow</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jack</foaf:firstName>
		<foaf:lastName>Mostow</foaf:lastName>
		<foaf:mbox_sha1sum>cfcda98863a9ad325506c095b9e5e5f3e019de5d</foaf:mbox_sha1sum>
		<foaf:name>Jack Mostow</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/john-c-stamper">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>John C. Stamper</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>John</foaf:firstName>
		<foaf:lastName>C. Stamper</foaf:lastName>
		<foaf:mbox_sha1sum>bfaeaa7192e9815d69eb5153d89e3962b4bcd7c5</foaf:mbox_sha1sum>
		<foaf:name>John C. Stamper</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/181"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>P. Pavlik Jr</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>P.</foaf:firstName>
		<foaf:lastName>Pavlik Jr</foaf:lastName>
		<foaf:mbox_sha1sum>85528a9cf2d288d3dae150bfbc031ae708989272</foaf:mbox_sha1sum>
		<foaf:name>P. Pavlik Jr</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/177"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carolyn-p-rose">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Carolyn P. Rose</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Carolyn</foaf:firstName>
		<foaf:lastName>P. Rose</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Carolyn P. Rose</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hao-cen">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Hao Cen</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Hao</foaf:firstName>
		<foaf:lastName>Cen</foaf:lastName>
		<foaf:mbox_sha1sum>a37887427b172bd01b8bc0d04c1d0c8792541c27</foaf:mbox_sha1sum>
		<foaf:name>Hao Cen</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/177"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/yue-cui">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Yue Cui</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Yue</foaf:firstName>
		<foaf:lastName>Cui</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Yue Cui</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jan-m-vleeshouwers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Jan M. Vleeshouwers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Jan</foaf:firstName>
		<foaf:lastName>M. Vleeshouwers</foaf:lastName>
		<foaf:mbox_sha1sum>8c9dc3f1d84aa6cc9a97131e89b5a5fdc49bc2a0</foaf:mbox_sha1sum>
		<foaf:name>Jan M. Vleeshouwers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/195"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/wil-van-der-aalst">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Wil Van Der Aalst</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Wil</foaf:firstName>
		<foaf:lastName>Van Der Aalst</foaf:lastName>
		<foaf:mbox_sha1sum>1d738712d2a553353e9988c481a9351bf5f8736e</foaf:mbox_sha1sum>
		<foaf:name>Wil Van Der Aalst</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/p-de-bra">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>P. De Bra</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>P.</foaf:firstName>
		<foaf:lastName>De Bra</foaf:lastName>
		<foaf:mbox_sha1sum>5e6e0140af8fc08b5b9df45f327ebf20cf224f18</foaf:mbox_sha1sum>
		<foaf:name>P. De Bra</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Mykola Pechenizkiy</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Mykola</foaf:firstName>
		<foaf:lastName>Pechenizkiy</foaf:lastName>
		<foaf:mbox_sha1sum>92ec847241e20acbe9bd8be3fda6d568619228d8</foaf:mbox_sha1sum>
		<foaf:name>Mykola Pechenizkiy</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/195"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/gerben-w-dekker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Gerben W. Dekker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Gerben</foaf:firstName>
		<foaf:lastName>W. Dekker</foaf:lastName>
		<foaf:mbox_sha1sum>7c2215c5d5a2e5c76e45403d7bd2c0e2b9173e0d</foaf:mbox_sha1sum>
		<foaf:name>Gerben W. Dekker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/195"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/n-trcka">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>N. Trcka</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>N.</foaf:firstName>
		<foaf:lastName>Trcka</foaf:lastName>
		<foaf:mbox_sha1sum>9c0fea07301adfff81bfa6fce62c9fe11d507f0a</foaf:mbox_sha1sum>
		<foaf:name>N. Trcka</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Mykola Pechenizkiy</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Mykola</foaf:firstName>
		<foaf:lastName>Pechenizkiy</foaf:lastName>
		<foaf:mbox_sha1sum>92ec847241e20acbe9bd8be3fda6d568619228d8</foaf:mbox_sha1sum>
		<foaf:name>Mykola Pechenizkiy</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/195"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Ekaterina Vasilyeva</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Ekaterina</foaf:firstName>
		<foaf:lastName>Vasilyeva</foaf:lastName>
		<foaf:mbox_sha1sum>836e3a34e8066b0b9c2708abc07e75eab24445a8</foaf:mbox_sha1sum>
		<foaf:name>Ekaterina Vasilyeva</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/194"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/koji-suda">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/hyogo-university-of-teacher-education"/>
		<rdfs:label>Koji Suda</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Japan"/>
		<foaf:firstName>Koji</foaf:firstName>
		<foaf:lastName>Suda</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Koji Suda</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/203"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/junichi-kakegawa">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/hyogo-university-of-teacher-education"/>
		<rdfs:label>Junichi Kakegawa</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Japan"/>
		<foaf:firstName>Junichi</foaf:firstName>
		<foaf:lastName>Kakegawa</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Junichi Kakegawa</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/203"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/koichiro-morihiro">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/hyogo-university-of-teacher-education"/>
		<rdfs:label>Koichiro Morihiro</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Japan"/>
		<foaf:firstName>Koichiro</foaf:firstName>
		<foaf:lastName>Morihiro</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Koichiro Morihiro</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/203"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/keigo-takeda">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/konan-university"/>
		<rdfs:label>Keigo Takeda</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Japan"/>
		<foaf:firstName>Keigo</foaf:firstName>
		<foaf:lastName>Takeda</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Keigo Takeda</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/203"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ryo-nagata">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/konan-university"/>
		<rdfs:label>Ryo Nagata</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Japan"/>
		<foaf:firstName>Ryo</foaf:firstName>
		<foaf:lastName>Nagata</foaf:lastName>
		<foaf:mbox_sha1sum>55a14f89b41180ed8ee8575c3edb52b3246a4396</foaf:mbox_sha1sum>
		<foaf:name>Ryo Nagata</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/203"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/robert-dale">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/macquarie-university"/>
		<rdfs:label>Robert Dale</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Robert</foaf:firstName>
		<foaf:lastName>Dale</foaf:lastName>
		<foaf:mbox_sha1sum>0806623c0233ab2823e00dae31ccad4f126a36e8</foaf:mbox_sha1sum>
		<foaf:name>Robert Dale</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/196"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/roger-azevedo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/mcgill-university"/>
		<rdfs:label>Roger Azevedo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Roger</foaf:firstName>
		<foaf:lastName>Azevedo</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Roger Azevedo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/199"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/richard-cox">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/monash-university"/>
		<rdfs:label>Richard Cox</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Richard</foaf:firstName>
		<foaf:lastName>Cox</foaf:lastName>
		<foaf:mbox_sha1sum>8e764cd65a90ba7c5959f0f23ce51030ebe32985</foaf:mbox_sha1sum>
		<foaf:name>Richard Cox</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/196"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/safia-abbas">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/niigata-university"/>
		<rdfs:label>Safia Abbas</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/JAPAN"/>
		<foaf:firstName>Safia</foaf:firstName>
		<foaf:lastName>Abbas</foaf:lastName>
		<foaf:mbox_sha1sum>57c8301f458f586726549b0f1140bba0679041e3</foaf:mbox_sha1sum>
		<foaf:name>Safia Abbas</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/207"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hajime-sawamura">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/niigata-university"/>
		<rdfs:label>Hajime Sawamura</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/JAPAN"/>
		<foaf:firstName>Hajime</foaf:firstName>
		<foaf:lastName>Sawamura</foaf:lastName>
		<foaf:mbox_sha1sum>fa4e773c240d1d130f205667d3610a3d44305fd2</foaf:mbox_sha1sum>
		<foaf:name>Hajime Sawamura</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/207"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/e-montanes">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/oviedo-university"/>
		<rdfs:label>E. Montanes</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>E.</foaf:firstName>
		<foaf:lastName>Montanes</foaf:lastName>
		<foaf:mbox_sha1sum>fc8d13e0a5b74da3224f43be93e8e9474927b30e</foaf:mbox_sha1sum>
		<foaf:name>E. Montanes</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/204"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/j-r-quevedo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/oviedo-university"/>
		<rdfs:label>J. R. Quevedo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>J.</foaf:firstName>
		<foaf:lastName>R. Quevedo</foaf:lastName>
		<foaf:mbox_sha1sum>2941a3ae28870541dc7f49826896303167c0bb55</foaf:mbox_sha1sum>
		<foaf:name>J. R. Quevedo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/204"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/luo-si">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/purdue-university"/>
		<rdfs:label>Luo Si</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Luo</foaf:firstName>
		<foaf:lastName>Si</foaf:lastName>
		<foaf:mbox_sha1sum>f8cbdfe9bca35db57e1ce395902c1ff056971347</foaf:mbox_sha1sum>
		<foaf:name>Luo Si</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/186"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/yan-ping-xin">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/purdue-university"/>
		<rdfs:label>Yan Ping Xin</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Yan</foaf:firstName>
		<foaf:lastName>Ping Xin</foaf:lastName>
		<foaf:mbox_sha1sum>b2af0c30ac77ece88bc24665991be0569def2e45</foaf:mbox_sha1sum>
		<foaf:name>Yan Ping Xin</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/186"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/casey-hord">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/purdue-university"/>
		<rdfs:label>Casey Hord</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Casey</foaf:firstName>
		<foaf:lastName>Hord</foaf:lastName>
		<foaf:mbox_sha1sum>0cc2e89c779af1fba5a290243d409debdfa7febf</foaf:mbox_sha1sum>
		<foaf:name>Casey Hord</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/186"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/suleyman-cetintas">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/purdue-university"/>
		<rdfs:label>Suleyman Cetintas</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Suleyman</foaf:firstName>
		<foaf:lastName>Cetintas</foaf:lastName>
		<foaf:mbox_sha1sum>2c058f33349e381073fb242c30adbced3e0d4418</foaf:mbox_sha1sum>
		<foaf:name>Suleyman Cetintas</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/186"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ofer-bergman">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/sheffield-university"/>
		<rdfs:label>Ofer Bergman</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Ofer</foaf:firstName>
		<foaf:lastName>Bergman</foaf:lastName>
		<foaf:mbox_sha1sum>3e545ed7dd770f1d02d848859d64c4af20c9c5a7</foaf:mbox_sha1sum>
		<foaf:name>Ofer Bergman</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dave-barker-plummer">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/stanford-university"/>
		<rdfs:label>Dave Barker-plummer</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Dave</foaf:firstName>
		<foaf:lastName>Barker-plummer</foaf:lastName>
		<foaf:mbox_sha1sum>bafa346588af50729f95386f0a4f05a40570a924</foaf:mbox_sha1sum>
		<foaf:name>Dave Barker-plummer</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/196"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rafi-nachmias">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tel-aviv-university"/>
		<rdfs:label>Rafi Nachmias</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Israel"/>
		<foaf:firstName>Rafi</foaf:firstName>
		<foaf:lastName>Nachmias</foaf:lastName>
		<foaf:mbox_sha1sum>cea4310497b4026a3128713418a30f3d559cc461</foaf:mbox_sha1sum>
		<foaf:name>Rafi Nachmias</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/182"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rafi-nachmias">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tel-aviv-university"/>
		<rdfs:label>Rafi Nachmias</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Israel"/>
		<foaf:firstName>Rafi</foaf:firstName>
		<foaf:lastName>Nachmias</foaf:lastName>
		<foaf:mbox_sha1sum>cea4310497b4026a3128713418a30f3d559cc461</foaf:mbox_sha1sum>
		<foaf:name>Rafi Nachmias</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/182"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tel-aviv-university"/>
		<rdfs:label>Sharon Hardof-jaffe</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Israel"/>
		<foaf:firstName>Sharon</foaf:firstName>
		<foaf:lastName>Hardof-jaffe</foaf:lastName>
		<foaf:mbox_sha1sum>28023b0d370b7100ddc364f74ec8c1dc03d748f1</foaf:mbox_sha1sum>
		<foaf:name>Sharon Hardof-jaffe</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/thomas-k-harris">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tutor-technologies"/>
		<rdfs:label>Thomas K. Harris</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Thomas</foaf:firstName>
		<foaf:lastName>K. Harris</foaf:lastName>
		<foaf:mbox_sha1sum>95c63924327eaf0c97e7cc25e2ef08d4b3e33bbd</foaf:mbox_sha1sum>
		<foaf:name>Thomas K. Harris</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/190"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/antonio-r-anaya">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/uned"/>
		<rdfs:label>Antonio R. Anaya</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Antonio</foaf:firstName>
		<foaf:lastName>R. Anaya</foaf:lastName>
		<foaf:mbox_sha1sum>c0b90cada034d2145f7224f30bee9a63c4220233</foaf:mbox_sha1sum>
		<foaf:name>Antonio R. Anaya</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/185"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jesus-g-boticario">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/uned"/>
		<rdfs:label>Jesus G. Boticario</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Jesus</foaf:firstName>
		<foaf:lastName>G. Boticario</foaf:lastName>
		<foaf:mbox_sha1sum>7cda07d730eeb2d5f376cfd9f73171422c60999a</foaf:mbox_sha1sum>
		<foaf:name>Jesus G. Boticario</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/185"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/javier-bravo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidad-autonoma-de-madrid"/>
		<rdfs:label>Javier Bravo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Javier</foaf:firstName>
		<foaf:lastName>Bravo</foaf:lastName>
		<foaf:mbox_sha1sum>26cab71d17d2cc8d36e2272e5e5f4103dfe23fbe</foaf:mbox_sha1sum>
		<foaf:name>Javier Bravo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/201"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidad-autonoma-de-madrid"/>
		<rdfs:label>Alvaro Ortigosa</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Alvaro</foaf:firstName>
		<foaf:lastName>Ortigosa</foaf:lastName>
		<foaf:mbox_sha1sum>0c34f239fd95d91588d1ee7abae8ec4b50a9a802</foaf:mbox_sha1sum>
		<foaf:name>Alvaro Ortigosa</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/201"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/evandro-db-costa">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidade-federal-de-alagoas"/>
		<rdfs:label>Evandro D.b. Costa</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Brasil"/>
		<foaf:firstName>Evandro</foaf:firstName>
		<foaf:lastName>D.b. Costa</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Evandro D.b. Costa</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/david-nadler-prata">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidade-federal-de-alagoas"/>
		<rdfs:label>David Nadler Prata</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Brasil"/>
		<foaf:firstName>David</foaf:firstName>
		<foaf:lastName>Nadler Prata</foaf:lastName>
		<foaf:mbox_sha1sum>debe2eaf8668ae63915ec378947f836b3fc50828</foaf:mbox_sha1sum>
		<foaf:name>David Nadler Prata</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-california"/>
		<rdfs:label>Elizabeth Ayers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Elizabeth</foaf:firstName>
		<foaf:lastName>Ayers</foaf:lastName>
		<foaf:mbox_sha1sum>aae9cb2462a54bbcff7dc90d448896423efc64ab</foaf:mbox_sha1sum>
		<foaf:name>Elizabeth Ayers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-california"/>
		<rdfs:label>Elizabeth Ayers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Elizabeth</foaf:firstName>
		<foaf:lastName>Ayers</foaf:lastName>
		<foaf:mbox_sha1sum>aae9cb2462a54bbcff7dc90d448896423efc64ab</foaf:mbox_sha1sum>
		<foaf:name>Elizabeth Ayers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/amelia-zafra">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>Amelia Zafra</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Amelia</foaf:firstName>
		<foaf:lastName>Zafra</foaf:lastName>
		<foaf:mbox_sha1sum>21c7179b3c728d623ad21756ceff3b070c83209c</foaf:mbox_sha1sum>
		<foaf:name>Amelia Zafra</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/200"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carlos-de-castro">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>Carlos De Castro</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Carlos</foaf:firstName>
		<foaf:lastName>De Castro</foaf:lastName>
		<foaf:mbox_sha1sum>5b9d4162b367d2f66160df91001cef60cf915d39</foaf:mbox_sha1sum>
		<foaf:name>Carlos De Castro</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/enrique-garcia">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>Enrique Garcia</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Enrique</foaf:firstName>
		<foaf:lastName>Garcia</foaf:lastName>
		<foaf:mbox_sha1sum>389997610f3c6d3ec9b4334de90de4a99830a86e</foaf:mbox_sha1sum>
		<foaf:name>Enrique Garcia</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/s-ventura">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>S. Ventura</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>S.</foaf:firstName>
		<foaf:lastName>Ventura</foaf:lastName>
		<foaf:mbox_sha1sum>7cc996020e6d4f219b9c38c3863805c7682d4571</foaf:mbox_sha1sum>
		<foaf:name>S. Ventura</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/200"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/c-romero">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>C. Romero</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>C.</foaf:firstName>
		<foaf:lastName>Romero</foaf:lastName>
		<foaf:mbox_sha1sum>4fa368945fa5e56c76943d4dc36bc95f3b25899f</foaf:mbox_sha1sum>
		<foaf:name>C. Romero</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/s-ventura">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>S. Ventura</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>S.</foaf:firstName>
		<foaf:lastName>Ventura</foaf:lastName>
		<foaf:mbox_sha1sum>7cc996020e6d4f219b9c38c3863805c7682d4571</foaf:mbox_sha1sum>
		<foaf:name>S. Ventura</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/200"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nema-dean">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-glasgow"/>
		<rdfs:label>Nema Dean</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Nema</foaf:firstName>
		<foaf:lastName>Dean</foaf:lastName>
		<foaf:mbox_sha1sum>5847deff91aa9447d6418367c31eb9fdc80cc552</foaf:mbox_sha1sum>
		<foaf:name>Nema Dean</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nema-dean">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-glasgow"/>
		<rdfs:label>Nema Dean</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Nema</foaf:firstName>
		<foaf:lastName>Dean</foaf:lastName>
		<foaf:mbox_sha1sum>5847deff91aa9447d6418367c31eb9fdc80cc552</foaf:mbox_sha1sum>
		<foaf:name>Nema Dean</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/183"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/187"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/miguel-gea">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-granada"/>
		<rdfs:label>Miguel Gea</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Miguel</foaf:firstName>
		<foaf:lastName>Gea</foaf:lastName>
		<foaf:mbox_sha1sum>b0c9c405c29443a86f99e1fb72fd5dd0bf93151a</foaf:mbox_sha1sum>
		<foaf:name>Miguel Gea</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/191"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/vasile-rus">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Vasile Rus</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Vasile</foaf:firstName>
		<foaf:lastName>Rus</foaf:lastName>
		<foaf:mbox_sha1sum>2d0249738f36125405e9333b23035856b20db21c</foaf:mbox_sha1sum>
		<foaf:name>Vasile Rus</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/199"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mihai-lintean">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Mihai Lintean</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Mihai</foaf:firstName>
		<foaf:lastName>Lintean</foaf:lastName>
		<foaf:mbox_sha1sum>748543c31eb0a37f3e9ed33ea43fc1a63675b518</foaf:mbox_sha1sum>
		<foaf:name>Mihai Lintean</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/199"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/michael-bain">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-new-south-wales"/>
		<rdfs:label>Michael Bain</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Michael</foaf:firstName>
		<foaf:lastName>Bain</foaf:lastName>
		<foaf:mbox_sha1sum>04144e7d3d3f5ee9c8585a51b710bdd2b01d0afe</foaf:mbox_sha1sum>
		<foaf:name>Michael Bain</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/197"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nadine-marcus">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-new-south-wales"/>
		<rdfs:label>Nadine Marcus</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Nadine</foaf:firstName>
		<foaf:lastName>Marcus</foaf:lastName>
		<foaf:mbox_sha1sum>b900bfff8288d34c670324b0a8a3cb920bea65f4</foaf:mbox_sha1sum>
		<foaf:name>Nadine Marcus</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/197"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dror-ben-naim">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-new-south-wales"/>
		<rdfs:label>Dror Ben-naim</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Dror</foaf:firstName>
		<foaf:lastName>Ben-naim</foaf:lastName>
		<foaf:mbox_sha1sum>180f6ecec89fe979922737bbae29de1b896d2b45</foaf:mbox_sha1sum>
		<foaf:name>Dror Ben-naim</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/197"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tiffany-barnes">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-north-carolina-at-charlotte"/>
		<rdfs:label>Tiffany Barnes</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Tiffany</foaf:firstName>
		<foaf:lastName>Barnes</foaf:lastName>
		<foaf:mbox_sha1sum>daf05503be29d62c7a709507c606228daa499645</foaf:mbox_sha1sum>
		<foaf:name>Tiffany Barnes</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/181"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/lukas-zoubek">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-ostrava"/>
		<rdfs:label>Lukas Zoubek</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Czech_Republic"/>
		<foaf:firstName>Lukas</foaf:firstName>
		<foaf:lastName>Zoubek</foaf:lastName>
		<foaf:mbox_sha1sum>dfcb7c002c4c6e67fc81ff91ba889caa3e18cb38</foaf:mbox_sha1sum>
		<foaf:name>Lukas Zoubek</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/184"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/michal-burda">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-ostrava"/>
		<rdfs:label>Michal Burda</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Czech_Republic"/>
		<foaf:firstName>Michal</foaf:firstName>
		<foaf:lastName>Burda</foaf:lastName>
		<foaf:mbox_sha1sum>360775e1afa9155ba49f4a50b282d2a3f2a5589b</foaf:mbox_sha1sum>
		<foaf:name>Michal Burda</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/184"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/marian-simko">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-technology"/>
		<rdfs:label>Marian Simko</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Slovak"/>
		<foaf:firstName>Marian</foaf:firstName>
		<foaf:lastName>Simko</foaf:lastName>
		<foaf:mbox_sha1sum>8c956a8b4cb0258429917816c0b07ba3bfe3f92b</foaf:mbox_sha1sum>
		<foaf:name>Marian Simko</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/208"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/maria-bielikova">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-technology"/>
		<rdfs:label>Maria Bielikova</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Slovak"/>
		<foaf:firstName>Maria</foaf:firstName>
		<foaf:lastName>Bielikova</foaf:lastName>
		<foaf:mbox_sha1sum>4fbd4601ea9a749f8d8b3003d1b9f44f10aa1983</foaf:mbox_sha1sum>
		<foaf:name>Maria Bielikova</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/208"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/cecily-heiner">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-utah"/>
		<rdfs:label>Cecily Heiner</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Cecily</foaf:firstName>
		<foaf:lastName>Heiner</foaf:lastName>
		<foaf:mbox_sha1sum>885affae88e87e0520c1ad7be0e44b1c6cf64d8c</foaf:mbox_sha1sum>
		<foaf:name>Cecily Heiner</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/178"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-l-zachary">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-utah"/>
		<rdfs:label>Joseph L. Zachary</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>L. Zachary</foaf:lastName>
		<foaf:mbox_sha1sum>768fba9ec473619019ba809ac28c0739dfd5a5c0</foaf:mbox_sha1sum>
		<foaf:name>Joseph L. Zachary</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/178"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tara-m-madhyastha">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-washington"/>
		<rdfs:label>Tara M. Madhyastha</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Tara</foaf:firstName>
		<foaf:lastName>M. Madhyastha</foaf:lastName>
		<foaf:mbox_sha1sum>c283192e34aa3a93786c756e1d13a492c0d05bfa</foaf:mbox_sha1sum>
		<foaf:name>Tara M. Madhyastha</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/193"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/steven-tanimoto">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-washington"/>
		<rdfs:label>Steven Tanimoto</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Steven</foaf:firstName>
		<foaf:lastName>Tanimoto</foaf:lastName>
		<foaf:mbox_sha1sum>90bd30915bc21004a82e047419c2b0c2b8c0a219</foaf:mbox_sha1sum>
		<foaf:name>Steven Tanimoto</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/193"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Neil T. Heffernan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Neil</foaf:firstName>
		<foaf:lastName>T. Heffernan</foaf:lastName>
		<foaf:mbox_sha1sum>8c86318b5e87ca4e61bed8db77402ba0b24d7701</foaf:mbox_sha1sum>
		<foaf:name>Neil T. Heffernan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/192"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-e-beck">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Joseph E. Beck</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>E. Beck</foaf:lastName>
		<foaf:mbox_sha1sum>98757703f9ddfd60ced2fbeb80219aee5e11c1b6</foaf:mbox_sha1sum>
		<foaf:name>Joseph E. Beck</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Ryan S.j.d. Baker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ryan</foaf:firstName>
		<foaf:lastName>S.j.d. Baker</foaf:lastName>
		<foaf:mbox_sha1sum>188538b9d7ab9a3c2883dc5640511f67db9e3aab</foaf:mbox_sha1sum>
		<foaf:name>Ryan S.j.d. Baker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/180"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mingyu-feng">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Mingyu Feng</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Mingyu</foaf:firstName>
		<foaf:lastName>Feng</foaf:lastName>
		<foaf:mbox_sha1sum>1e94e64b29291795ef21d71ad0c9731a65fc09b8</foaf:mbox_sha1sum>
		<foaf:name>Mingyu Feng</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mingyu-feng">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Mingyu Feng</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Mingyu</foaf:firstName>
		<foaf:lastName>Feng</foaf:lastName>
		<foaf:mbox_sha1sum>1e94e64b29291795ef21d71ad0c9731a65fc09b8</foaf:mbox_sha1sum>
		<foaf:name>Mingyu Feng</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Ryan S.j.d. Baker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ryan</foaf:firstName>
		<foaf:lastName>S.j.d. Baker</foaf:lastName>
		<foaf:mbox_sha1sum>188538b9d7ab9a3c2883dc5640511f67db9e3aab</foaf:mbox_sha1sum>
		<foaf:name>Ryan S.j.d. Baker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/180"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/189"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/yue-gong">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Yue Gong</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Yue</foaf:firstName>
		<foaf:lastName>Gong</foaf:lastName>
		<foaf:mbox_sha1sum>080a8dbc1e3d9dbd79c27a1eeeb52fc943f6d801</foaf:mbox_sha1sum>
		<foaf:name>Yue Gong</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-e-beck">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Joseph E. Beck</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>E. Beck</foaf:lastName>
		<foaf:mbox_sha1sum>98757703f9ddfd60ced2fbeb80219aee5e11c1b6</foaf:mbox_sha1sum>
		<foaf:name>Joseph E. Beck</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-e-beck">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Joseph E. Beck</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>E. Beck</foaf:lastName>
		<foaf:mbox_sha1sum>98757703f9ddfd60ced2fbeb80219aee5e11c1b6</foaf:mbox_sha1sum>
		<foaf:name>Joseph E. Beck</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dovan-rai">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Dovan Rai</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Dovan</foaf:firstName>
		<foaf:lastName>Rai</foaf:lastName>
		<foaf:mbox_sha1sum>c4c50aefe737ad599086b7de8aa484a6efb0122b</foaf:mbox_sha1sum>
		<foaf:name>Dovan Rai</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Zachary A. Pardos</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Zachary</foaf:firstName>
		<foaf:lastName>A. Pardos</foaf:lastName>
		<foaf:mbox_sha1sum>08ca8bfb6323c3c3b0d306d519cc1ff39b2cfb2e</foaf:mbox_sha1sum>
		<foaf:name>Zachary A. Pardos</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/192"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Neil T. Heffernan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Neil</foaf:firstName>
		<foaf:lastName>T. Heffernan</foaf:lastName>
		<foaf:mbox_sha1sum>8c86318b5e87ca4e61bed8db77402ba0b24d7701</foaf:mbox_sha1sum>
		<foaf:name>Neil T. Heffernan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/192"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dovan-rai">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Dovan Rai</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Dovan</foaf:firstName>
		<foaf:lastName>Rai</foaf:lastName>
		<foaf:mbox_sha1sum>c4c50aefe737ad599086b7de8aa484a6efb0122b</foaf:mbox_sha1sum>
		<foaf:name>Dovan Rai</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-e-beck">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Joseph E. Beck</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>E. Beck</foaf:lastName>
		<foaf:mbox_sha1sum>98757703f9ddfd60ced2fbeb80219aee5e11c1b6</foaf:mbox_sha1sum>
		<foaf:name>Joseph E. Beck</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Arnon Hershkovitz</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Arnon</foaf:firstName>
		<foaf:lastName>Hershkovitz</foaf:lastName>
		<foaf:mbox_sha1sum>a63c9b6def54cbcc5d6415c35de988d400ab954e</foaf:mbox_sha1sum>
		<foaf:name>Arnon Hershkovitz</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/182"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-e-beck">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Joseph E. Beck</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>E. Beck</foaf:lastName>
		<foaf:mbox_sha1sum>98757703f9ddfd60ced2fbeb80219aee5e11c1b6</foaf:mbox_sha1sum>
		<foaf:name>Joseph E. Beck</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/188"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/206"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Neil T. Heffernan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Neil</foaf:firstName>
		<foaf:lastName>T. Heffernan</foaf:lastName>
		<foaf:mbox_sha1sum>8c86318b5e87ca4e61bed8db77402ba0b24d7701</foaf:mbox_sha1sum>
		<foaf:name>Neil T. Heffernan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/192"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/202"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/yue-gong">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Yue Gong</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Yue</foaf:firstName>
		<foaf:lastName>Gong</foaf:lastName>
		<foaf:mbox_sha1sum>080a8dbc1e3d9dbd79c27a1eeeb52fc943f6d801</foaf:mbox_sha1sum>
		<foaf:name>Yue Gong</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/179"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/205"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Arnon Hershkovitz</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Arnon</foaf:firstName>
		<foaf:lastName>Hershkovitz</foaf:lastName>
		<foaf:mbox_sha1sum>a63c9b6def54cbcc5d6415c35de988d400ab954e</foaf:mbox_sha1sum>
		<foaf:name>Arnon Hershkovitz</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/182"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2009/paper/198"/>
	</foaf:Person>
</rdf:RDF>
