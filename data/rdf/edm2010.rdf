<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:dcterms="http://purl.org/dc/terms/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:ical="http://www.w3.org/2002/12/cal/ical#"
    xmlns:swrc="http://swrc.ontoware.org/ontology#"
    xmlns:bibo="http://purl.org/ontology/bibo/"
    xmlns:swc="http://data.semanticweb.org/ns/swc/ontology#"
    xmlns:led="http://data.linkededucation.org/ns/linked-education.rdf#"
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" >
	<swc:ConferenceEvent rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010">
		<swc:completeGraph rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/complete"/>
		<swc:hasAcronym>EDM2010</swc:hasAcronym>
		<swc:hasRelatedDocument rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<rdfs:label>Educational Data Mining 2010</rdfs:label>	<ical:dtend rdf:datatype="http://www.w3.org/2001/XMLSchema#date">0000-00-00</ical:dtend>
		<ical:dtstart rdf:datatype="http://www.w3.org/2001/XMLSchema#date">0000-00-00</ical:dtstart><foaf:homepage rdf:resource=""/>
	</swc:ConferenceEvent>
	<swrc:Proceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings">
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/114"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/115"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/117"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/118"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/119"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/123"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/124"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/125"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/126"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/127"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/128"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/129"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/130"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/131"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/133"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/135"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/137"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/140"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/141"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/142"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/145"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/146"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/147"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/151"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/152"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/153"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/155"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/157"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/158"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/159"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/160"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/161"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/162"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/163"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/164"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/167"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/171"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/174"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/175"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/176"/>
		<swc:relatedToEvent rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010"/>
		<swrc:booktitle>Proceedings of Educational Data Mining, 2010</swrc:booktitle>
		<swrc:month></swrc:month>
		<swrc:series></swrc:series>
		<swrc:year>2010</swrc:year>
	</swrc:Proceedings>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>When Data Exploration and Data Mining meet while Analysing Usage Data of a Course</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[ 1.  Evolution of confidence, lift and cosine.
 Summing up, when ( )XP  is almost equal to ( )YXP , , data exploration is enough to infer the association rule YX → , there is no need to use a data mining algorithm for association rules extraction in such a case. 
 3    Conclusion and Future Work.
 We have used this result while analyzing the data of the course “Programming 1” in our university, see [1]. The associations found show that a group of students emerges that keep doing self-evaluation exercises during the semester. A future work is to continue conducting case studies with courses taught in different topics and designed in different ways to further enhance our catalogue of questions  that can be interesting for teachers, and investigating further connections between data exploration and data mining.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>When Data Exploration and Data Mining meet while Analysing Usage Data of a Course</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-kruger"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-kruger"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-wolf"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-wolf"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-kruger"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-wolf"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/114">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Analyzing Learning Styles using Behavioral Indicators in Web based Learning Environments</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/114/authorlist"/>
		<swrc:abstract>It is argued that the analysis of the learner’s generated log files during interactions with a learning environment is necessary to produce interpretative views of their activities. The analysis of these log files, or traces, provides "knowledge" about the activity we call indicators. Our work is related to this research field. We are particularly interested in automatically identifying learners’ learning styles from learning indicators. This concept, used in several Educational Hypermedia Systems (EHS) as a criterion for adaptation and tracking, belongs to a set of behaviors and strategies in how to manage and organize information. In this paper, we validate our approach of auto-detection of student's learning styles based on their navigation behavior using machine- learning classifiers.</swrc:abstract>
		<led:body><![CDATA[ 1.  LS Classification results.
 Through Table1, we notice that for the information processing LS’ attribute, all the classifiers learn the active style better than the reflective one, except for Neural Networks. This is due to the stronger presence of active learners than reflective ones. Concerning the understanding LS’ attribute, the global style was better learned by all classifiers than the sequential one for the same reason as the first attribute, where neural networks give the best total results. We observe that the total results are all over 50%. Thus, we can strengthen the hypothesis of the possibility to deduce information about learner preferences using simple navigational information that we can apply on any learning environment on the Web, without having to consider evaluation scores or the communication tool traces that allow us to give more details. We plan to continue the development of other indicators to improve the LS’ identification results.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Analyzing Learning Styles using Behavioral Indicators in Web based Learning Environments</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nabila-bousbia"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nabila-bousbia"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jean-marc-labat"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jean-marc-labat"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/amar-balla"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/amar-balla"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/issam-rebai"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/issam-rebai"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/114/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/nabila-bousbia"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jean-marc-labat"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/amar-balla"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/issam-rebai"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/115">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Inferring the Differential Student Model in a Probabilistic Domain using Abduction Inference in Bayesian Networks</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/115/authorlist"/>
		<swrc:abstract>In this paper we aim to estimate the differential student knowledge model in a probabilistic domain within an intelligent tutoring system. The suggested algorithm aims to estimate the actual student model through the student answers to questions requiring diagnosing skills. Updating and verification of the model are conducted based on the matching between the student and model answers. Two different approaches to updating namely coarse and refined model are suggested. Results suggest that the refined model, although takes more computational resources, provides a slightly better approximation of the student model.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Inferring the Differential Student Model in a Probabilistic Domain using Abduction Inference in Bayesian Networks</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nabila-khodeir"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nabila-khodeir"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nayer-wanas"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nayer-wanas"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nevin-darwish"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nevin-darwish"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nadia-hegazy"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nadia-hegazy"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/115/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/nabila-khodeir"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/nayer-wanas"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/nevin-darwish"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/nadia-hegazy"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Pinpointing Learning Moments; A finer grain P(J) model</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Pinpointing Learning Moments; A finer grain P(J) model</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-b-goldstein"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-b-goldstein"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-b-goldstein"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/117">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Examining Learner Control in a Structured Inquiry Cycle Using Process Mining</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/117/authorlist"/>
		<swrc:abstract>High potential variation in prior knowledge, metacognitive skills, and motivation within learner populations can prompt design strategies that combine explicit structuring and scaffolding with increased learner control.  We examine the use of such a strategy—a structured inquiry cycle—in a corpus of online modules (50) for adult informal learners using process mining.  We apply process analysis techniques previously demonstrated by others to formative assessment data from the modules.  We then use process modeling for mining module deliveries (N=5617) to investigate learner control within the inquiry cycle as a whole.   Our experience suggests roles for these techniques beyond assessing conformity, both for design reflection and in preparation for deeper inquiry on self-regulation.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Examining Learner Control in a Structured Inquiry Cycle Using Process Mining</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/larry-howard"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/larry-howard"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/julie-johnson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/julie-johnson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carin-neitzel"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carin-neitzel"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/117/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/larry-howard"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/julie-johnson"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/carin-neitzel"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/118">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Using Numeric Optimization To Refine Semantic User Model Integration Of Educational Systems</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/118/authorlist"/>
		<swrc:abstract>Nowadays, it is a common practice to use several educational systems in one domain. In this situation, each of the systems should be able to provide the best user modeling based on the integrated data about the user. However, differences in domain conceptualization complicate the ability of the systems to understand each other’s user models and necessitate the use of labor intensive and time consuming alignment procedures that require involvement of knowledge engineers. While the latter are best at detecting associative links between the user model items, they fail to reliably identify the strengths of these associations. In this paper, we are proposing a method to improve the user model mapping by using a numerical optimization procedure. Our results show that numerical weight optimization helped to decrease the amount of manual work and improved the target model accuracy.</swrc:abstract>
		<led:body><![CDATA[ 1. Model values are often exchanged as soon as they are produced by one of the systems. An example of the evidence-based integration is the work on integrating Ramapo Problets and QuizJET systems [3]. Both systems have internal domain ontologies of Java programming and the ontologies are different in granularity and focus of modeling. Integration of user data is done on the side of QuizJET. QuizJET uses the ontology concepts in the classical descriptive metadata sense, while Ramapo Problets uses the concepts-in-a-context paradigm, where each concept is accompanied by a descriptor of its situational application (e.g. for-loop vs. for- loop-executing-exactly-once). This leads to the situation when each of the Ramapo Problets’ concepts-in-context is related to a sizable weighted set of QuizJET concepts. 
 Figure 1 Evidence-based integration of two user models.
 The problem with all of the approaches above is that they heavily rely on time-consuming work on the part of experts to produce the mapping function. While experts are best for selecting the semantic relations between concepts of the domain models, the quality of their assignment of strengths to the inter-concept links is sub-optimal [6]. In our work, we are seeking to improve user model integration by utilizing an optimization procedure to either refine expert mapping weights or to produce them without human involvement, potentially altering the expert-suggested mapping links. 
 3 From Manual To Automated Student Model Mapping.
 3.1 Model Mapping For Evidence-Based Integration.
 In our previous work, we have dedicated a lot of attention to evidence-based integration of user models in the SQL domain. This work revolved around two systems: SQL Knot and SQL-Tutor. SQL KnoT [14] is an external-loop tutor serving parameterized problems testing students’ knowledge SQL. SQL KnoT represents domain model in the form of an ontology that has been developed by a team of human experts. Each problem is semi- automatically indexed with a set of ontology concepts. SQL-Tutor [11] is an ITS that presents problems to students and helps them to improve their knowledge of SQL. SQL- Tutor represents a domain model in the form of constraints. Constraints represent the fundamental principles of SQL and must be satisfied in any correct solution. Each SQL- Tutor problem has a set of relevant constraints. If the student solution violates these constraints, the solution is incorrect. The constraint set in SQL-Tutor contains about 700 constraints that assess the syntactic and semantic correctness of the solution. The gist of the user modeling approach in both SQL KnoT and SQL-Tutor is quite similar: they construct an overlay of the domain vocabulary of knowledge items (constraints or concepts); nevertheless, the fundamental differences between the two domain models make the alignment of their models quite challenging. The unique nature of SQL-Tutor constraints makes the use of the known automatic ontology mapping techniques impossible. A constraint is not directly related to a single concept or a sub-tree of the ontology; instead, it models the syntactic or semantic relations between various concepts. As a result, the mapping is not one-to-one, but many-to-many. 
 Figure 2 A fragment of constraints-to-concepts mapping with weights. 
 We charged a number of experts with the task of mapping the two domain representations [13]. The resulting mapping contained a set of constraint-concept relations with the relevance weights: small (1/3), medium (2/3), or large (3/3=1). A fragment of the mapping is shown in Figure 2. The formula for computing the concept knowledge scores is shown in Equation (1). Namely, sum of weights between the concept and satisfied constraints minus sum of weights between the concept and broken constraints, divided by the sum of weights between the concept and all activated constraints. 
 FORMULA_1.
 Both systems were deployed in an undergraduate database course at the School of Information Sciences, University of Pittsburgh during the Fall 2008 semester. Students had access to 300 SQL-Tutor problems and over 50 parameterized SQL KnoT problems. The student’s progress was stored in one single long-term user model. SQL-Tutor was responsible for nearly a quarter of the students’ problem-solving activity (SQL KnoT was responsible for the rest), despite that fact that SQL-Tutor was made available only in the middle of the semester and students were already familiar with SQL KnoT. 
 3.2 Problems Of Manual Mapping.
 Despite tangible progress in integrating the two SQL problem-solving tools, there are still open questions pertinent to this particular integration effort and to user model integration in general. First, merging the user models leaves the issue of the merger quality unanswered. The fusion of the user models does not automatically improve the quality of the combined model. Second, we relied on expert opinion to come up with the mapping. From the literature, we know that the experts can reliably identify mapping relations between items of two domains [8]. However, assigning an appropriate weight – even on a simpler categorical scale – often poses a problem [6]. The same person can change his/her opinion if the weighting procedure is repeated. This means that, even if all relations between concepts and constraints were identified correctly, the assigned weights could still be suboptimal; as a result, the quality of the mapped model would be poor. 
 3.3 Towards Automated Model Mapping.
 To answer these questions, we suggest using an optimization procedure to refine the expert-assigned constraint-to-concept weights. The idea is to employ the student logs of the source system (SQL-Tutor) to create and fine-tune a custom user model using least square fitting method with the mean squared error as the criterion. Then, using the same student logs, we utilize the experts' mapping to compute the mean squared error of the now mapped concept-based user model. Performing the search in the weight-space, we are minimizing the error of the target model, thus refining the mapping. Our hypothesis is that the suggested procedure would be able to, first, improve user model integration by optimizing expert weights. Second, starting with constant or random weights, produce the weights, without considering expert suggestion. Here, we expect the optimization results to be not as good as the one starting with expert mapping. And third, change the mapping links structure (by setting weights =0) for a more optimal one. 
 4 Experimental Study Of Automated Model Mapping.
 To verify the validity of the approach suggested, we conducted a set of experiments. First of all, to establish the baseline for mapping, we constructed the source constraint-based user model in such a way that modeling parameters minimize the modeling error. Second, the optimization was repeated. This time, the variables were the mapping weights and the objective function was the mean squared error of the mapped concept-based model. Logs of the three database courses offered at the University of Canterbury in the 2006- 2007 term were used for the experiment. We took the logs of the first course that contained 3544 transactions of 38 students. Each log entry contained user id, problem id, time the solution was submitted, solution correctness, list of confirmed constraints, and a list of broken constraints (if the solution is not correct). At the first stage of the experiment, we used user modeling approach that was different from the one deployed with SQL-Tutor, namely – Bayesian Knowledge Tracing (BKT) [5] – an established user modeling method in the area of intelligent tutoring systems. It the second stage, we used three sets of weights: supplied by the experts, equal constant weights, and the random weights. The details and the results of these procedures are given below. 
 4.1 Baseline Constraint-Based User Model.
 As we have mentioned before, Bayesian Knowledge Tracing (BKT) was used as the baseline user modeling approach in SQL-Tutor. BKT assumes a two-state model of knowledge items (often called skills or rules) of a particular learning domain. The knowledge item (KI) is either in a learned or unlearned state. While interacting with the system, knowledge of KI can transition from the unlearned to the learned state. Even if a KI is in the learned state, a student can make a mistake. As in the unlearned state, there is a chance student can guess correctly. For each of the modeled KIs, BKT model maintains four parameter estimates: p(L) – the probability that KI is in the learned state, the probability that KI is in the learned state prior to interacting with the tutor is p(L0); p(T) – the probability that the KI will transfer to the learned state on next time user practices it; p(S) – the probability the student will slip and apply the KI incorrectly even when it is in the learned state; and finally p(G) – the probability that the student will apply the KI correctly despite it being in the unlearned state. For details on BKT models, refer to [5] We adhered to the usual practice of BKT modeling and kept a unique set of parameters p(L0), p(T), p(S), and p(G) for each KI and had a separate running estimate of p(L) for each user-KI pair. Counter to the tradition, we didn’t assume conditional independence of KIs and obtained estimates for all parameters together. Reason being the special feature of constraint-based model: constraints always spanned several domain concepts. One other usual BKT practice is using only the first chance to apply the knowledge item per problem attempt. This rule is often used in the so-called inner-loop tutors that walk students through each step of the problem solving activity. SQL-Tutor is an outer-loop tutor: it gives feedback only when a complete problem solution has been submitted. It is common to see same constraint both satisfied and broken in the problem several times not knowing which came first. We ignored repetitions, and when the constraint was both satisfied and broken, set the value of p(L) to the arithmetic mean of p(L)+ (assuming correct response came first) and p(L)— (assuming incorrect response came first). 
 Table 1 Results of fitting source BKT model of SQL-Tutor constraints. 
 Out of about 700 constraints, we selected only those occurring in the user logs. In addition, we filtered out constraints that are always satisfied and never broken. This gave us 282 constraints with 4 parameters to estimate for each: a total of 1,128 parameters. Each constraint was treated as a separate KI. Mean squared error was used as an objective function of the optimization. To compute the BKT models, we used Matlab. Fitting was done using Matlab’s implementation of linear square fitting procedure using trust region reflective algorithm. The parameters of the resulting source BKT model of constraints are shown in Table 1. Model parameters given are weight-averaged over all participating constraints (the number of constraint occurrences in the log weights its contribution). 
 4.2 Mapping Weights Optimization.
 To optimize the model mapping weights, we modified the code used to construct the baseline constraint-based BKT model. User model parameters were held constant and mapping weights were used as variables. Objective function remained the same – mean squared error of modeling. The original constraint-based user model was updated as before, but for the computation of the user model error it was mapped to concept-based model using Equation (1). Since in the previous stage we reduced the number of participating constraints, the original number of 1,012 constraint-concept links/weights decreased to 576. Three sets of weights were used. The first set contained weights produced by experts: low, medium, or high (1/3, 2/3, and 1=3/3 respectively). In the second set, all weights were set to 0.5. In the third set, all weights were randomly chosen from 0.0 to 1.0. As in the case with BKT modeling of constraints, we did not assume conditional independence of sets of weights and all of the 576 weights were fit together. Matlab’s constrained nonlinear multivariable optimization procedure was used with an active-set algorithm. Table 2 presents the results of the optimization experiments. 
 Table 2 Results of weight mapping refinement.
 As we can see, under all three starting conditions the initial mean squared error was roughly the same: 45%-46%. However, only the first two conditions (originating from expert weights and from constant weights) lead to improved modeling error rate. Optimization of the expert-supplied weights ended with a 29% error rate, only 4% worse than the original constraint-based BKT model. The mapping originating with constant weights of 0.5, counter to our expectations, beat that figure by 2% and reached a 27%. Optimization of random weights did not lead to any changes. Although expert weights were closer to the found optimum (44 out of 576 weights were changed in the process of optimization and all of them were decreased) than constant weights (all weights were changed, 524 weights increased and 52 decreased), in both cases the optimization procedure arrived at similar results. In the process of optimization, 34 and 52 mapping links were removed in the expert and constant conditions respectively. Despite promising results, our solutions look like local optimums. Mean absolute difference of weights between expert-originated and constant-originated sets is 0.3581 and the number of removed links agreed upon is only 13. 
 5 Discussion.
 Our experiments have successfully shown the high potential of using numeric optimization to refine the expert’s alignment of two user models in general. In our case the mapping of the source constraint-based mode of SQL-Tutor to the concept-based model of SQL KnoT was significantly improved. The original 45% error of the mapped model was reduced to 29%; only 4% shy of the source model’s error. Taking into account principal differences between the two domain vocabularies mapped, the achieved result is impressive. The refinement of uniformly assigned constant weights, against our expectations, was able to achieve an even better 27% error. Although a 2% difference does not seem like a tangible one, what counts is that refining constant weights was comparable to the original constraint-based model. In the light of these results, in the future one could free experts from weight assignment entirely. The only thing left for experts to do is to set inter-model relations. The rest could be handled by the procedure we proposed. The optimization procedure also demonstrated the ability to not only change the weights, but also to drive them to zero as an indication of some of the model mapping links to be redundant. This could serve as an additional check of the mapping quality and as an aid to human experts in the by-hand iterative process of mapping. Despite its merits, there are several limitations to this work. First, mapping links could only be removed, not added as in some automated mapping approaches. A possible remedy for this could be changing the link selection procedure when merging the experts’ suggestions. Instead of using the relations agreed upon by the majority of experts, all suggested links could be considered and then the refinement procedure would do the necessary filtering. In general, additional expert verification of the refined mapping might be necessary. Computationally optimal mapping might not be pedagogically sound. Inappropriate mapping links might be emphasized by the use of maximal weights and important links could be removed. Second, we did not assume conditional independence of mapping weights and optimized all 576 weights at once. This posed a computation challenge for the optimization procedure. Also we used only a part of SQL-Tutor logs, one course worth of logs out of three available. A limited number of data points (3544) might have been a contributing factor for entrapment in the local minima of the objective function and possibly over-fitting. Mining large volumes of user data and enforcing conditional independence of the mapping weights could solve both of these issues. Third, the user model transformation formula inherently normalizes weights and scaling all mapping weights has no effect. For example, a set weights equal to {1, 2, 3} would be equivalent to {.1, .2, .3} or {.2, .4, .6}. However, optimization procedure treats scaled weights as completely different. Changing the transformation formula or the optimization procedure for the ones that don’t suffer from this phenomenon could further improve the mapping. And finally, both the proposed procedure and the refined weights it produced could possibly be sensitive to a particular user modeling approach employed (Bayesian Knowledge Tracing in our case). At this point we cannot confirm whether the obtained weights would remain optimal, if modeling formalisms are replaced with alternative ones. We plan to continue this work and further investigate the issue.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Using Numeric Optimization To Refine Semantic User Model Integration Of Educational Systems</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-v-yudelson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-v-yudelson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/peter-brusilovsky"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/peter-brusilovsky"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/antonija-mitrovic"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/antonija-mitrovic"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/moffat-mathews"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/moffat-mathews"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/118/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-v-yudelson"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/peter-brusilovsky"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/antonija-mitrovic"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/moffat-mathews"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/119">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Pundit: Intelligent Recommender of Courses</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/119/authorlist"/>
		<swrc:abstract>In this paper the authors describe Pundit, a course recommendation and search tool at Teachers College, Columbia University. The alpha prototype employs a novel combination of data retrieval and data mining approaches to recommend courses to users based on a match between their profiles and course contents. We utilize course management system and library e-reserves data to collect information about course content that is indexed and then matched with user profiles. In conclusion, we define an evaluation function that was used to determine the quality of recommendations.</swrc:abstract>
		<led:body><![CDATA[ 3   Evaluation.
 Our evaluation strategy links every user in the system with every course, s/he has taken at Teachers College. We compared this information with recommendations from Pundit to determine the number of intersections. Before doing this, we removed transcripts and/or other course related information from user profiles to eliminate any bias towards the final results. The data used for evaluating the Pundit methodology consisted of 45,713 course files, 3,403 courses, 35,215 student & faculty, 40 user profiles, 74 degree programs, and 247 department. Figure 1 below shows the results we obtained when 200 recommended courses for 40 faculty member profiles were compared against the courses faculty previously taught. 
 Figure.

 1. Courses Recommended Vs. Courses Taught.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Pundit: Intelligent Recommender of Courses</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ankit-ranka"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ankit-ranka"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/faisal-anwar"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/faisal-anwar"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hui-soo-chae"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hui-soo-chae"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/119/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ankit-ranka"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/faisal-anwar"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/hui-soo-chae"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>A Preliminary Investigation of Hierarchical Hidden Markov Models for Tutorial Planning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[ INTRODUCTION.
 For tutorial dialogue systems, selecting an appropriate dialogue move to support learners can significantly influence cognitive and affective outcomes. The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules derived from observing human tutors, but a data-driven model of strategy selection may increase the effectiveness of tutorial dialogue systems. Tutorial dialogue projects including CIRCSIM-TUTOR [1], ITSPOKE [2], and KSC-PAL [3] have utilized corpora to inform the behavior of a system. Our work builds on this line of research by directly learning a hierarchical hidden Markov model (HHMM) for predicting tutor dialogue acts within a corpus.  The corpus was collected during a human-human tutoring study in the domain of introductory computer science [4]. We annotated the dialogue moves with dialogue acts (Table 1).  The subtask structure and student problem-solving action correctness were also annotated manually. 
 Table.

 1. Dialogue act annotation scheme.
 We trained first-order Markov (bigram) models, HMMs, and HHMMs on the annotated sequences. In ten-fold (five-fold for the HHMMs due to data sparsity) cross-validation, the HHMM (partially depicted in Figure 1) predicted tutor dialogue acts with an average 57% accuracy, significantly higher than the 27% accuracy of bigram models (p<0.0001) and better than the 48% accuracy of HMMs without hierarchical structure (p<0.05).3 
 Figure 1. Subset of learned HHMM. 
 Because of HHMMs’ capacity for explicitly representing hidden dialogue structure and hierarchical task structure, they perform better than bigrams and HMMs for predicting tutor moves in our corpus. The models’ performance points to promising future work that includes utilizing additional lexical and syntactic features along with fixed student characteristics within a hierarchical hidden Markov modeling framework. More broadly, the results highlight the importance of considering task structure when modeling a complex domain such as those that often accompany task-oriented tutoring. Finally, a key direction for data-driven dialogue management is to learn unsupervised dialogue act and task classification models. 
 Acknowledgments. 
 This work is supported in part by the NC State Department of Computer Science and NSF grants CNS-0540523, REC-0632450, IIS-0812291, and a Graduate Research Fellowship. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent official views, opinions, or policy of the National Science Foundation.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>A Preliminary Investigation of Hierarchical Hidden Markov Models for Tutorial Planning</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kristy-elizabeth-boyer"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kristy-elizabeth-boyer"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-phillips"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-phillips"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/eun-young-ha"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/eun-young-ha"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-d-wallis"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-d-wallis"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mladen-a-vouk"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mladen-a-vouk"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/james-c-lester"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/james-c-lester"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/kristy-elizabeth-boyer"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-phillips"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/eun-young-ha"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-d-wallis"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/mladen-a-vouk"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/james-c-lester"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Towards EDM Framework for Personalization of Information Services in RPM Systems</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121/authorlist"/>
		<swrc:abstract>Remote Patient Management Systems (RPM), besides monitoring the health conditions of patients, provide them with different information services that currently are predefined and follow a one-size-fits-all paradigm to a large extent. In this work we focus on the problem of knowledge discovery and patient modeling by mining educational data, motivational and instructional feedback provided to patients within RPM system.</swrc:abstract>
		<led:body><![CDATA[ 1. 
 Figure 1.  The role of EDM in RPM (top) and motivating examples (bottom). 
 The preliminary results of our exploration study suggest that there is potential for EDM to facilitate data-driven patient modeling and motivate the shift from the one-size-fits-all approach currently employed in the development of RPM systems to personalization in providing educational materials, motivational support and informational content to their users. Our further work includes the many-sided analysis of the RPM usage database with the focus on the educational content and usage data. The particular focuses include subgroup discovery and identification of signatures describing well-doing home- monitored patients and those who require more assistance of the medical staff. 
 Acknowledgments. 
 This research is partly supported by KWR MIP, FP7 EU HeartCycle and NWO GAF projects.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Towards EDM Framework for Personalization of Information Services in RPM Systems</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksandra-tesanovic"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksandra-tesanovic"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/evgeny-knutov"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/evgeny-knutov"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sicco-verwer"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sicco-verwer"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/p-de-bra"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/p-de-bra"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksandra-tesanovic"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/evgeny-knutov"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/sicco-verwer"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/p-de-bra"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Mining Rare Association Rules from e-Learning Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122/authorlist"/>
		<swrc:abstract>Rare association rules are those that only appear infrequently even though they are highly associated with very specific data. In consequence, these rules can be very appropriate for using with educational datasets since they are usually imbalanced. In this paper, we explore the extraction of rare association rules when gathering student usage data from a Moodle system. This type of rule is more difficult to find when applying traditional data mining algorithms. Thus we show some relevant results obtained when comparing several frequent and rare association rule mining algorithms. We also offer some illustrative examples of the rules discovered in order to demonstrate both their performance and their usefulness in educational environments.</swrc:abstract>
		<led:body><![CDATA[ 1.  Rules in a database. 
 There are several different approaches to discover rare association rules. The simplest way is to directly apply the Apriori algorithm [2] by simply setting the minimum support threshold to a low value. However, this leads to a combinatorial explosion, which could produce a huge number of patterns, most of them frequent with only a small number of them actually rare. A different proposal, known as Apriori-Infrequent, involves the modification of the Apriori algorithm to use only the above-mentioned infrequent itemsets during rule generation. This simple change makes use of the maximum support measure, instead of the usual minimum support, to generate candidate itemsets, i.e., only items with a lower support than a given threshold are considered. Next, rules are yielded as generated by the Apriori algorithm. A totally different perspective consists of developing a new algorithm to tackle these new challenges. A first proposal is Apriori-Inverse [4], which can be seen as a more intricate variation of the traditional Apriori algorithm. It also uses the maximum support but proposes three different kinds of additions: fixed threshold, adaptive threshold and hill climbing. The main idea is that given a user-specified maximum support threshold, MaxSup, and a derived MinAbsSup value, a rule X is rare if Sup(X) < MaxSup and Sup(X)> MinAbsSup. A second proposal is the Apriori-Rare algorithm [11], also known as Arima, which is another variation of the Apriori approach. Arima is actually composed of two different algorithms: a naïve one, which relies on Apriori and hence enumerates all frequent itemsets; and MRG-Exp, which limits the considerations to frequent itemsets generators only. Finally, please notice that the first two approaches (Apriori-Frequent and Apriori-Infrequent) are taken to ensure that rare items are also considered during itemset generation, although the two latter approaches (Apriori- Inverse and Apriori-Rare) try to encourage low-support items to take part in candidate rule generation by imposing structural constraints. The algorithms aforementioned are the most important RARM proposals. Next, we will explore how these approaches can be applied over educational data in such a way that their usefulness in this research area is shown. 
 3 Experimentation and Results.
 In order to test the performance and usefulness of applying RARM to e-learning data, we have used student data gathered from the Moodle system to compare several ARM and RARM algorithms and show examples of discovered rules. 
 3.1 Experimentation.
 The experiments were performed using data from 230 students in 5 Moodle courses on computer science at the University of Córdoba. Moodle (http://moodle.org) is one of the most frequently used free Learning Content Management Systems (LCMS) and keeps detailed logs of all activities that students perform (e.g., assignments, forums and quizzes). This student usage data has been preprocessed in order to be transformed into a suitable format to be used by our data mining algorithms [10]. First, a summary table (see Table 1) has been created, which integrates the most important information about the activities and the final marks obtained by students in the courses. Notice that we have transformed all the continuous attributes into discrete attributes that can be treated as categorical attributes. Discretization allows the numerical data to be divided into categorical classes that are easier for the instructor to understand. 
 Table 1. Attributes used for each student instance. 
 Due to the way their values are distributed, the course and mark attributes are clearly imbalanced, i.e., they have one or many values with a very low percentage of appearance: - Course: From a total of 230 students, 80 took course 218 (34.78%), 66 students did course 94 (28.69%), 62 students did 110 (26.95%), 13 students took course 111 (5.65%) and 9 students took course 46 (3.91%). Thus, there are three predominant courses (C218, C94 and C110) and two minority courses (C111 and C46). - Mark: From among 230 students, 116 students PASS the final exam with a normal/medium score (50.43%), 87 students FAIL the exam (38.82%), 15 students obtain an EXCELLENT or very good/high score in the exam (6.52%) and 12 students were ABSENT from the exam (5.21%). So, there are two majority marks (PASS and FAIL) and two minority marks (EXCELLENT and ABSENT). A better view of such imbalanced value distribution for these two attributes (mark and course) can be seen graphically in Figure 2. 
 Figure 2. Value distribution for the attributes Mark and Course. The different colours on the right image correspond to the different Marks. 
 We performed a comparison between ARM and different RARM algorithms to discover rare class association rules [12] from the aforementioned data. A class association rule is a special subset of association rules with the consequent of the rule limited to a target class label (only one predefined item), whereas the left-hand may contain one or more attributes. It is represented as A → C, where A is the antecedent (in our case, the course and activity attributes) and C is the class (in our case, the mark attribute). This type of rule is more easily understood than general association rules, since it only comprises one element in the consequent and usually represents discovered knowledge at a high level of abstraction, and so can be used directly in the decision making process [8]. In the context of EDM, class association rules can be very useful for educational purposes, since they show any existing relationships between the activities that students perform using Moodle and their final exam marks. To obtain class association rules we need to filter the resulting rules from the ARM or RARM algorithms, so we only select those rules that have a single attribute (i.e., the mark attribute) in their consequent. We evaluated the four different Apriori proposals following the configuration parameters stated below: • Apriori-Frequent [2], setting the minimum support threshold at a very low value (0.05); • Apriori-Infrequent, setting the maximum support at 0.1; • Apriori-Inverse and Apriori-Rare, using the same support threshold set at 0.1. We also assigned the value 0.7 as the confidence threshold for all these algorithms. 
 3.2 Evaluation of results.
 Table 2 summarizes the results obtained from the four Apriori proposals, and shows the number of frequent and infrequent itemsets mined, the number of rules discovered, and their average support and confidence. 
 Table 2. Comparison of ARM and RARM proposals. 
 Notice that the Apriori-Frequent is the only algorithm that uses frequent itemsets. Therefore, it discovers the greatest number of rules (both frequent and rare) with the highest average support but not the highest confidence. This means that the instructor needs to search manually for the rare rules. On the other hand, Apriori-Infrequent mines the smallest number of infrequent itemsets. Though it discovers a great number of rare rules, most of them are redundant. Finally, Apriori-Inverse and Apriori-Rare behave in a very similar fashion and are the best at discovering rare association rules, since they use a higher number of infrequent items than Apriori-Infrequent and discover a lower number of rare rules. A lower number of rules is easier than a higher number of rules for the instructor to use and understand. Furthermore, the standard deviation shows that both Apriori-Inverse and Apriori-Rare tend to be very close to the average, so one expects to obtain rules that will not vary much from these values. 
 3.3 Examples of discovered rules.
 Due to the imbalanced nature of the data source, different versions of the conditional support were defined. Conditional support is a well-known measure for the processing of imbalanced data using class association rules [12]. Thus, three different measures are considered to evaluate rule support, as defined in continuation: • The traditional support of a rule A → C, with A as the antecedent and C as the consequent, is defined as N CAnCASup )()( ∩=→ , where n(A∩C) is the number of instances that matches both the antecedent and consequent, and N is the total number of instances. However, the support of rules that contain course and mark attributes (imbalanced attributes) must be defined as follows: o The conditional support with respect to the mark of a class association rule A → Mark, where Mark stands for the imbalanced attribute mark, and is defined as is the number of instances that matches both the antecedent and consequent and n(Mark) is the number of instances that matches the ”mark” attribute. o The conditional support with respect to the course of a class association rule MarkCourseA →∩ , where Course stands for the imbalanced attribute course and Mark for the class attribute, is defined as is the number of instances that matches the “course”  attribute. Next, there are some examples of rules that were mined using both the ARM and the different RARM algorithms. For each rule, we show the antecedent and the consequent constructed, as well as the support and confidence measures. Firstly, Table 3 shows some representative association rules mined using the Apriori-Frequent algorithm. A further description is detailed below. 
 Table 3. Rules extracted using the Apriori-Frequent algorithm. 
 As can be seen, all the rules discovered (not only the 6 rules shown in Table 3 but also the other 788 rules discovered) contain only frequent itemsets, such as mark=PASS (students who passed the exam), mark=FAIL (students who failed), course = 119 (students who took the course 119), course=218 and course=94. Secondly, we can see that these rules have low support (but not very low), a medium value in the two conditional supports and are of high confidence (but not very high). Finally, to explain the usefulness of these rules for the instructor, we are going to describe their meaning. Rule 1 shows that if students spend a lot of time in the forum (a high value) then they pass the final exam. It provides information to the instructor about how the forum has been a good activity for students with a confidence of 0.82. Rule 2 shows that if students have submitted and read messages to/from the forum, and they have passed quizzes, then they have passed the exam. The information provided is similar to the previous data but adds the quizzes as another determining factor in the final mark (as is logical). Rule 3 shows that students in course 110 who sent in many assignments then passed the final exam (rule 5 is the opposite version but for any course). So, the number of assignments is directly related to the final mark. Rule 4 and 5 show that if the total time in quizzes is low or the number of passed quizzes is low (and the course is 218), then students obtain a bad mark. So, quizzes are also directly related to the mark and can be used to detect in time students at risk of failing the final exam. Next, Table 4 shows some representative rare association rules obtained using the Apriori-Rare algorithm. Please notice that due to the Apriori-Inverse approach obtains almost the same set of rules, so we don’t present another analysis similar to the following table. A detailed description of this rule set is presented below. 
 Table 4. Rules extracted using the Apriori-Rare algorithm. 
 As can be seen, all the rules that are discovered (not only the 6 rules shown in Table 4 but also the other 44 rules discovered) contain only infrequent itemsets, such as mark=EXCELLENT (students who passed the exam with a very high score), mark=ABSENT (students who did not take the exam), course = 46 and course = 111 (students who did courses 46 and 111 respectively). We can see that these rules have a very low support, a very high confidence level (the maximum value) and also a high value for the conditional supports; indicating that they are rare/infrequent rules and their data is imbalanced with respect to the course and mark attributes. To explain the usefulness of these rules for the instructor, we are going to describe their meaning. Rule 1 shows that if students execute all the quizzes and pass them, then they obtain an excellent score in the final exam. It could be an expected rule that shows the instructor that quizzes can be used in order to predict very good student results. Rule 2 shows that if students spend a lot of time on assignments, they obtain an excellent score. This is the opposite of rule 5 in Table 3 Finally, we have also compared the values of the evaluation measures, shown in and so it proves again that the number of assignments is directly related to the final mark. Rule 3 shows that if students in course 46 send a lot of messages to the forum, they obtain an excellent score. The instructor can use this information to detect very good students in course 46 depending on the number of messages they send to the forum.  The last three rules are about students who have been absent for the exam. They show the instructor that if students do not spend time on assignments, forum participation and quizzes, then they do not take the exam. The instructor can detect this type of student in time to help him/her to take part in course activities and also do the final exam. 
 Table 3 and Table 4. Firstly, we can see that confidence values (Conf) are normally higher in Table 4 (rare association rules) than in Table 3 (frequent association rules). Secondly, the support values (Sup) of rare association rules are much lower in Table 4 than the support of frequent association rules in Table 3. Then, we can see that relative support values (SupC and SupM) of rare association rules are higher in Table 4 than the relative support of frequent ones in Table 3. It proves that rare association rules have high confidence levels, and although they have very low values of support with respect to all the data, these support values are high with respect to imbalanced attributes (as show the relative support measure). 
 4 Concluding remarks and future work.
 In this paper we have explored the use of RARM over educational data gathered from the Moodle system installed at the University of Córdoba. The use of this approach has shown to be an interesting research line in the context of EDM, since most real-world data are usually imbalanced. Rare-association rules are more difficult to mine using traditional data mining algorithms, since they do not usually consider class-imbalance and tend to be overwhelmed by the major class, leaving the minor class to be ignored. In fact, we have shown that the regular Apriori algorithm [2] (known as Apriori- Frequent) discovers a huge number of rules with frequent items. Hence we explored how some specific algorithms, such as Apriori-Inverse and Apriori-Rare, are better at discovering rare-association rules than other non-specific algorithms, such as Apriori- Frequent and Apriori-Infrequent. In fact, the set of rules discovered by Apriori-Rare are included into the set of rules discovered by Apriori-Inverse but they are included neither into the set of rules discovered by Apriori-Infrequent nor Apriori. Finally, we have shown how the rules discovered by RARM algorithms can help the instructor to detect infrequent student behavior/activities in an e-learning environment such as Moodle. In fact, we have evaluated the relation/influence between the on-line activities and the final mark obtained by the students. In the future, we would like to develop a new algorithm specifically to discover RARM using evolutionary algorithms, and to compare its performance and usefulness in e- learning data versus the previous algorithms. We also plan to explore the use of other different rule evaluation measures for rare association rule mining. 
 Acknowledgment.
 This research is supported by projects of the Regional Government of Andalucia and the Ministry of Science and Technology, P08-TIC-3720 and TIN2008-06681-C06-03 respectively, and FEDER funds.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Mining Rare Association Rules from e-Learning Data</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-raul-romero"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-raul-romero"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jm-luna"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jm-luna"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-raul-romero"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/jm-luna"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/123">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Mining Bodily Patterns of Affective Experience during Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/123/authorlist"/>
		<swrc:abstract>We investigated 28 learners’ postural patterns associated with naturally occurring episodes of boredom, flow/engagement, confusion, frustration, and delight during a tutoring session with AutoTutor, a dialogue- based intelligent tutoring system. Training and validation data were collected in a learning session with AutoTutor, after which the learners’ affective states (i.e., emotions) were rated by the learner, a peer, and two trained judges. An automated body pressure measurement system was used to capture the pressure exerted by the learner on the seat and back of a chair during the tutoring session. We extracted 16 posture-related features that focused on the pressure exerted along with the magnitude and direction of changes in pressure during emotional experiences. Binary logistic regression models yielded medium sized effects in discriminating the affective states from neutral. An analysis of the parameters of the models indicated that the affective states were manifested by three unique postural configurations and a general increase in movement (when compared to neutral).</swrc:abstract>
		<led:body><![CDATA[ 1. Summary of binary logistic regression analyses with posture features as predictors of each affective state from neutral for data sets based on affective judgments of the four judges. 
 Next, we consulted the numerical directions (i.e. signs, + and -) of the statistically significant coefficients of the logistic regression models in order to explore relationships between body position, movement, and affect. Although all 16 features were used as predictors for the logistic regression analyses, we focus on the average pressure and the change in pressure; these are the most interpretable features and can be theoretically aligned within the attentive-arousal framework. The logistic regression analyses were used to discriminate between each affective state versus neutral, hence, a statistically significant predictor implies that the feature is heightened (significant positive predictor) or suppressed (significant negative predictor) during the emotional experience when compared to neutral. For example the back average pressure change feature was a significant positive predictor for the boredom- neutral logistic regression, so the episodes of boredom were accompanied by an increase in movement on the back when compared to neutral. 
 Table 2. Significant predictors for the multiple regression models for emotions in each data set. 
 A number of relationships surface when one considers the significant predictors of the affective states in which at least two judges agreed. The requirement that two judges agree on the significance and direction of each predictor is motivated by a desire to establish a degree of convergent validity in exploring the posture-affect relationships. By requiring that the features need to be significant predictors of affect for at least half of the judges models ensures that, to some extent, they generalize across judges. Boredom. Our results suggested that during episodes of boredom, the learners leaned back and presumably disengaged from the learning environment (low attentiveness indicated by an increase in pressure on back and a significant decrease in pressure on the seat). Experiences of boredom were also accompanied by an increase in the rate of change of pressure exerted on the seat. Therefore, heightened arousal was associated with the boredom experience, presumably as learners mentally disengage from the tutor and divert their cognitive resources to fidget around and alleviate their ennui. Some may view the heightened arousal accompanying boredom to conflict with the preconceived notion of boredom in which a learner stretches out, lays back, and simply disengages. Our results suggest that the learner lays back, disengages, but is aroused. Furthermore, this pattern of increased arousal accompanying disengagement (or boredom) replicates a previous study by Mota and Picard [14]. They monitored activity related posture features and discovered that children fidget when they were bored while performing a learning task on a computer. Delight and Flow.  In contrast to boredom, learners experiencing the positive emotions of delight and flow demonstrate increased attentiveness towards the learning environment by leaning forward. Learners experiencing these emotions also demonstrate heightened arousal on the back of the chair – at least when compared to the neutral state. Confusion and Frustration. Similar to delight and flow, learners experiencing confusion and frustration tend to lean forward. However, it appears that during episodes of delight and flow learners lean forward at a steeper inclination than with confusion and frustration. We arrived at this conclusion because the increase in the pressure exerted on the seat of the chair was accompanied by a commensurate decrease in pressure exerted on the back of the chair for delight and flow. On the other hand, confusion was accompanied by a decrease in pressure exerted on the back of the chair without any accompanying increase on the seat. Similarly for frustration the increase in pressure on the seat was devoid of a notable (statistically significant) decrease on the back. We suspect that the pattern of body position with confusion and frustration indicates that learners are in an upright position when they experience these states, as opposed to the forward lean that seems to accompany experiences of delight and flow. Arousal. In addition to boredom, it also appears that experiences of delight, flow, confusion, and frustration are accompanied by significant arousal, either on the back or the seat of the pressure sensitive chair. The arousal that is affiliated with confusion and frustration occurs on the back while there is an increase in movement on the seat when the emotions of delight and flow are experienced. While the significance of the location of the arousal (back or seat) during experiences of these is unclear, what is important is that all affective experiences (including boredom) were accompanied by heightened arousal when compared to neutral. In summary, the experience of each affective state is accompanied by a significant increase in arousal, at least when compared to the neutral baseline. 
 General Discussion.
 We discovered relationships between body position, degree of movement, and learners’ affective states. Our findings are in line with an attentive-arousal or engagement-arousal framework (see section 2). With respect to the attentiveness dimension, it appears that there are three bodily configurations that are associated with the affective states. These include heightened attentiveness, which is manifested by a forward lean when the positive emotions of delight and flow are experienced. On the other hand, bored learners tend to lean back, presumably in a state of disengagement (boredom). States such as confusion and frustration occur when learners confront contradictions, anomalous events, obstacles to goals, salient contrasts, perturbations, surprises, equivalent alternatives, and other stimuli or experiences that fail to match expectations [28]. Learners are in a state of cognitive disequilibrium, with more heightened physiological arousal, and more intense thought. Our results suggest that the bodily corollary to the mental state of cognitive disequilibrium is an alert position, where the learner sits upright and pays attention. The single major finding with respect to the arousal dimension is that each of the affective experiences is accompanied by a significantly higher arousal when compared to a neutral baseline (i.e. no emotion). This finding has important theoretical implications because some of our colleagues view some of these emotions (i.e., flow, confusion, etc.) as cognitive states, whereas other researchers would classify them as either emotions or affect states. We have traditionally agreed with the latter group because we hypothesize that the single major discriminator of an affective state over a cognitive state is that the affective state is accompanied by enhanced physiological arousal (compared with neutral). Our results indicate that in most cases there is a significant increase in bodily movements (bodily arousal) during the experience of emotional episodes indicating that both cognitive and affective processes are at play. Therefore, it might be the case that the term cognitive-affective state is the most defensible position for mental states such as confusion, flow, and frustration. We acknowledge that the aforementioned relationships between body posture and affect ignore individual differences in affect expression. In ideal circumstances, from a statistical point of view, the landscape of postural configurations would be evenly distributed among the 28 different students. However, this claim is implausible and it is therefore important to contrast contributions of individual learners versus generalizable posture features in predicting affect. Our results focused on broad patterns observed across all learners and should be interpreted with a modicum of caution. We are currently addressing these concerns by building models that attempt to separate variance explained by individual student characteristics versus variance explained by posture features above and beyond individual differences.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Mining Bodily Patterns of Affective Experience during Learning</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sidney-d-mello"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sidney-d-mello"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/123/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/sidney-d-mello"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/124">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>An Annotations Approach to Peer Tutoring</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/124/authorlist"/>
		<swrc:abstract>In  this  paper  we  detail  a  preliminary  model  for  reasoning  about annotating learning objects and intelligently showing annotations to users who will benefit from them.  Student interactions with these annotations are recorded and this data is used to reason about the best combination of annotations and learning  objects  to  show  to  a  specific  student.  Motivating  examples  and algorithms  for  reasoning  about  annotations  are  presented.  The  proposed approach leverages the votes for and against an annotation by previous students, considering  whether  those  students  are  similar  or  dissimilar  to  the  current student,  in  order  to  determine  the  value  of  showing  this  annotation  to  the student.</swrc:abstract>
		<led:body><![CDATA[ 1.  This is worthwhile as the usefulness of an annotation may be discovered at a later date, and it is then given a chance to be promoted.  Conversely, if a well-regarded annotation is shown to be vacuous, the community has a chance to immediately begin decreasing its prominence. Consider the example of a clarifying annotation where a student made the connection, in a video that explains parameters, that procedure another name for a function.  After this annotation was given high ratings, a new article was added which explained functions and clearly presented the various terms such as routine, function, procedure or method. After having read this article, students begin finding the previously useful annotation redundant and it begins to receive negative ratings from current students.  The system is immediately responsive to this and each negative vote decreases the probability that this (once highly-regarded) annotation is shown to current students. 
 4  Validation of Approach.
 Our intention is to validate this work using simulated students. Let knowledge be defined as the known concepts in the domain under consideration (the course the ITS endeavours to educate the student on). k = { set of known concepts } After an interaction between student s and learning object l, there will be a set of relationship such that: if k s∈k l then k s=k s∪k newwith probablity p e.g. suppose learning object abc had the relationship: if k s∈ {B,J,U } then k s=k s∪ {M }with probablity 0 .25 This would imply that if a student using this learning object had attained concepts B, J and U, then upon completion of using this object he would have a 25% chance of attaining concept M. Let overall knowledge (K) be represented as a percentile, considered roughly analogous to the student's expected mark given their current understanding. K = Known Concepts  All Concepts  e.g. suppose an ITS had 26 concepts, each represented by a letter the alphabet.  Given a student who had obtained concepts B, J, M and U, their overall knowledge would be: 
 FORMULA_4.
 The goal of the system is to maximize the average K of student's using the system. 
 5  Annotation.
 An annotation will modify the relationships of a learning object in one of two ways. 1. Create a new relationship with a substitute, removed concept. 2. Increase or decrease the probability of attaining the new concept for an existing relationship. 
 5.1  Example.
 Student Amy annotates a chapter from a text book that was assigned to her by the system. Her annotation has the effect of creating a new relationship, based on the above example but instead of requiring an understanding of variables, constants, and functions it will now require an understanding of variables, constants and procedures (in order to understand the concept of recursion).  Students who use this new learning object with the annotation attached, have two “paths” to obtaining the concept of recursion. From a “real life” perspective, this could be viewed as Amy relating the learning object to an alternative background (in some way showing that functions are analogous to procedures and the annotation allows students with this alternative background to comprehend the chapter). Student Bob annotates a video about data structures.  His annotation (incorrectly claiming a B+ tree is a B tree written in C++) has the effect of adjusting the probability of an existing relationship.  This annotation makes it 10% less likely that students seeing the learning object with Bob's annotation will attain the new concept compared to student who experience the learning object without Bob's annotation.  Bob has confused students and prevented them from properly understanding what the video is trying to convey.  The system should stigmatize this annotation and prevent it from being shown. The system will run, and use the reputation and previous ratings to determine which annotations are shown to a student. 
 6  Related Work.
 Peer tutoring has been explored by a number of previous researchers.  Some work, such as [10] and the COMTELLA project of [11], have investigated annotation techniques such as folksonomies and user tagging.  While on the surface, this may seem similar to our work, there are important distinctions.  With tagging, the purpose it to have users categorize items in ways that are meaningful for them, with the goal of sidestepping many of the problems inherent with ontologies (as articulated in [8]).  In contrast, our approach endeavors to not just help students find an appropriate learning object, but to actually clarify that object and allow students to share insights with one another.  Other works, such as [9][10], have been more explicit about arranging peer-tutoring.  In their COPPER system, they arrange for students to practice conversations with one another, taking into account each student's level of proficiency, previous interactions and how they can best learn from one another.  While our approach is a far less intense interaction than peer-tutoring that reasons about groups and gives them task in order to learn from one another, our approach has the benefit of allowing asynchronous learning.  Students may be able to benefit from annotations left by students who are no longer even in the course. Other work [11][12] has been done considering text produced by learners, specifically the notes they take.  They used these notes and text retrieval techniques to implicitly derive information about the student and to build a profile and social network about them.  In contrast, we take an intensely pragmatic view of annotations and don't try to decipher the meaning.  Instead, our approach reasons directly about which annotation will help a student learn, and ignores the actually underlying content of the annotations. iHelp [2] is a project related to COMTELLA that involves reasoning about matching stakeholders (such as students, markers, tutorial assistants and instructors) in order to get the right information to the right person (both in public and private discussions).  In [3] the authors extend iHelp to explore the value of tools such as chat rooms where learners are automatically drawn when using learning objects, shared workspaces where multiple learners can edit the same source code while discussing it and visualization tools for indicating a particular student's degree of interaction with her classmates.  All of this done to encourage "learner collaboration in and around the artefacts of learning".  In contrast, our work seeks to provide repositories of useful information from past students, rather than provide tools to assist in the interactions between current students.  In many cases these “past students” may be a classmate who used the learning object the day before, while in others it might be a former student who has since graduated and left the school. The work of John Lee et al. on the Vicarious Learner project [7], investigates how to automatically identify worthwhile dialogs to show to subsequent students, by determining the “critical thinking ratio” of a dialog, generated using a content analysis mark-up scheme.  This ratio is determined from the positive and negative aspects within a discussion, with the assumption that discourse patterns provide signs of deeper levels of processing by learners and lead to a “community of enquiry” which benefits students. Dialogs with higher ratios could then be considered as valuable to show to new students. Our work differs from theirs in that we are interested in  messages that have been explicitly left for future students and tied to a particular part of the course, rather than data-mining past interactions between students.  Additionally, our approach is able to leverage similarities between students, in order to have a user-specific process for deciding which annotation should be shown.  It may be interesting to integrate Lee et al.'s automated analysis of the critical thinking of text, as a component of deciding whether an annotation should be shown to a student. 
 6.1  Incentives.
 A criticism may be leveled that students won't be interested in helping their classmates (or admitting their ignorance) and therefore won't leave annotations.  In the case that the intrinsic benefit aren't enticing to a group of students, various approaches, such as [5][6] could be used to encourage participation.  The authors' feeling is that such explicit systems for coercing participation will not be needed in many learning contexts. Learning could be considered an inherently social process that naturally develops as sharing information with other student.  Intrinsically this can be thought of as developing social capital by sharing information, leading to greater trust and respect within the community. 
 7  Conclusion and Next Steps.
 We have demonstrated in this work the foundation for an approach for allowing students to explicitly share insights from their educational experiences to similar students going through the same process.  Our next step will be to validate this approach, using simulated students following our approach, compared with both random and ideal provision of learning objects to students.  Following this, we intend to corroborate our results with a study on real students. There are various directions as well for extending our current model for reasoning about the annotations to be shown to students.  Currently, which annotation is shown to a student is sensitive to the reputation of the annotation and the similarity of the current student to those who have rated this particular annotation.  For future work, it is worthwhile to be modeling more extensively the student providing the annotation, assigning a greater weight to those annotations provided by students with greater learning proficiency.  It may also be useful to be examining the level of achievement of the student who provided the annotation, considering as more valuable those students who are at a higher level of learning. In our previous work [4], we presented an algorithm to determine which learning object to present to a student, based not only on their similarity to previous students but also on the extent to which those students benefited in their learning, when using that object.  The methods proposed in that model for capturing the gains in learning of students could form a useful starting point for determining how to incorporate the learning proficiency of students into our algorithms for determining which annotations to show. Another direction for future work is to examine alternative formulae for managing the votes for and against (beyond our current proposal to employ an arctan function). One possibility would be to examine alternative methods for converting the interval to a [0,1] range. Another direction would be to examine in more detail the statistical confidence between the votes that are being registered, as a method of determining the importance of the votes towards the decision of showing a particular annotation.  In addition, we are currently troubleshooting the proposed arctan function, as it is desirable to better incorporate the annotator's reputation and still maintain the desired [0,1] range. It is also worthwhile to be exploring more sophisticated metrics for determining the similarity between two students. Current research in collaborative filtering approaches for recommender systems has suggested more sophisticated techniques for making a recommendation, such as: statistical collaborative filtering, cluster models, and Bayesian networks[1].  We intend to examine whether these techniques may be applicable to our approach to annotations. Finally, it would be useful to consider our proposal for presenting annotations to learning objects together with other algorithms for determining which learning objects should be presented, as part of the overall tutoring of that student. Our own investigations into curriculum sequencing [4] and peer-based development of the corpus of learning objects [5] would be particularly relevant, here.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>An Annotations Approach to Peer Tutoring</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/john-champaign"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/john-champaign"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/robin-cohen"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/robin-cohen"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/124/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/john-champaign"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/robin-cohen"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/125">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Is Students' Activity in LMS Persistent?</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/125/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Is Students' Activity in LMS Persistent?</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/125/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/126">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Data Mining for Individualised Hints in eLearning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/126/authorlist"/>
		<swrc:abstract>In this paper we present a tool where both past and current student data is used live to generate hints for students who are completing programming exercises during a national programming online tutorial and competition. These hints can be links to notes that are relevant to the problem detected and can include pre-emptive hints to prevent future mistakes.  Data from the year 2008 was mined, using clustering, association rules and numerical analysis, to find common patterns affecting the learners’ performance that we could use as a basis for providing hints to the 2009 students. During its live operation in 2009, student data was mined each week to update the system as it was being used. The benefits of the hinting system were evaluated through a large-scale experiment with participants of the 2009 NCSS Challenge. We found that users who were provided with hints achieved higher average marks than those who were not and stayed engaged for longer with the site.</swrc:abstract>
		<led:body><![CDATA[ 1. Pre-emptive ones look similar, except they do not contain any associated topics to review. 
 Figure 1. Screenshot of a post-failure hint page.
 The topics to review hints link to specific sections of the notes that address these topics (e.g. string slicing in Figure 1). Each section of the notes and each question were tagged with the relevant Python topics, as per a lightweight ontology that we built for that purpose, therefore allowing for the notes and questions to be related to each other. The question hints link to other Challenge questions selected by the data mining. The hints only suggest notes and questions that all students already have access to. 
 4 Data mining for the hinting system.
 As mentioned earlier, we used data mining as a basis for the generation and triggering of relevant hints for each student. The data came from the past 2008 Challenge and the live 2009 Challenge, including the questions, the corresponding topic tags from the ontology and the students’ results and submissions. Data mining was carried out using clustering, association rule mining and simple numerical analysis, the results of which were used as various components in the final hinting system as indicated below. 
 Table 1. Generation methods for each part of the hint. 
 Table 2. Triggering mechanisms for hints. 
 4.1 Mining the 2008 data.
 We mined the Challenge data from the previous year (2008) with the overall aim to (i) try to extract useful information that could be used to generate hints for our 2009 Challenge students and (ii) experiment with some techniques such as clustering to select and fine- tune our methods before using them live in 2009. Throughout the five weeks of the 2008 Beginners Challenge, 16,814 submissions were gathered from 712 separate users. There were 25 questions in total (5 per week) that were available to the students, usually in increasing order of difficulty. Students could submit several times for the same question until successful. All these attempts were recorded, along with the mark eventually obtained by students in each question. 
 4.1.1 Clustering students.
 The aim was to group students based on their abilities. We used the K-Means clustering algorithm used in Tada-Ed [9]. For each student ID, we collated the following attributes for each of the 25 questions: whether the student attempted the question (nominal), whether the student eventually passed the question (nominal), and the marks gained for the question (numeric [0 or 5-10]). We also computed the average numbers of passed and failed questions, and the average number of submissions before the student passed a question. Clustering with these attributes produced three distinct groups, which we identified as being “strong”, “medium” and “weak” students. While this result was only relevant for the 2008 students, the effectiveness of clustering with these pre-processed attributes indicated that clustering was a viable technique for discriminating between students. 
 4.1.2 Clustering questions.
 We clustered the questions with two distinct aims: to find questions that were similar to each other and to group questions by difficulty. Our goal was to remind students of other related questions that may help them with the question they were considering at the time. We again used the K-means algorithm. The similarity-based clusters were extracted using the question metadata (topic tags). We found 5 clusters, as each of the 5 weeks of the Challenge introduced new topics. The difficulty-based clusters of questions were extracted based on the number of students who passed each question and the percentage of students who passed it that attempted it. Similarly to the clustering of students, we found three clusters, which we identified as “easy”, “medium” and “hard”. Table 3 shows the 2008 questions with performance statistics and their difficulty clusters. We found that the three groups were not grouped chronologically, but that several medium questions appeared earlier than the last of the easy questions, and that the hard questions were interspersed through the medium ones. This meant that clustering to generate difficulty levels was worthwhile, as a simple chronological ordering would not have worked. 
 Table 3. The 2008 questions and their clustered difficulty rankings. 
 4.1.3 Mining associations in topics.
 The aim was to find association rules that indicated which topics should be mastered before another question was attempted, so that the hints could suggest topics that students should review before moving onto a more complex one.  We initially aimed to generate the association rules by assigning scores for each student on the 46 tags used in the 2008 Challenge. If a student passed a question, they were given one point for each tag in the question; if they did not pass a question a point was taken away for each relevant tag. Once all the scores were calculated, positive scores were labelled as “passed” and zero or negative scores were labelled as “failed”. This method, however, was too coarse-grained. If a student failed a question tagged with a large number of tags but only had problems with one or two topics, they would be penalised for all the topics. We revised this to a more fine-grained method, and mined sequences of tags that students failed on. We ordered the students’ results chronologically, and kept an ordered sequence of the tags for each question they made an incorrect submission to. We then used these sequences to generate association rules. Originally, we set the support and confidence to 70%. However, this excluded many advanced topics as they occurred less often in the students’ sequences. This was because they were introduced in later weeks, and as such, fewer questions were tagged with them and fewer students attempted those questions. We therefore lowered the support and confidence to 20%, and used cosine, which has been shown to be a more appropriate evaluation metric for educational data [10]. We post- processed the rules generated by the aPriori algorithm [11] to discard rules with a cosine of less than 0.65  [10] and rules with topics out of the order in which they appeared in the notes. We only retained rules that had two topics in the antecedent and one in the consequent. We finally manually extracted the rules in which the three topics involved were related to one another to remove trivial rules. We ended up keeping 83 rules, two of which are shown in Table 4. The first rule means that students who struggle with basic arithmetic in Python and comparison operators also struggle with how to loop over a set of values, and the second one means that those who struggle with converting to integers and while loops also struggle with stopping after a number of iterations. 
 Table 4. Examples of association rules found.
 4.1.4 Numerical Analysis.
 Aside from the more complex data mining algorithms, we subjected the 2008 data to some simple numerical analysis to find frequencies and averages for certain aspects of the data. An important measure was to have an idea of the “give-up point”, that is, the number of wrong submissions a student made before he or she stopped attempting it. To do this we found, for each question, the total number of submissions made by students who never passed the question. We then averaged this over the number of students who had attempted but not passed that question. We then computed the mean of the averages already found, which was found to be 3.7. This was used in the final system as the point at which students were presented with post-failure hints; a student would only receive such hints after making their fourth incorrect submission to a question. 
 5 Experiment and results with 2009 Challenge.
 We tested our hinting system on participants of the 2009 NCSS Challenge. We ran the experiment using the live data being generated by the Challenge participants. We evaluated through a controlled experiment whether the hinting system had a positive effect on student performance, based on their marks and the ability clusters they were grouped into. 
 5.1 Experiment design.
 The Challenge ran for five weeks from mid August to late September 2009. There was an overnight period between one week’s questions being closed to submissions and the next week’s questions being released, which allowed us to carry out the data mining using the live system and current participant data and upload the new clusters for the next week. This also involved finalising the tags for the questions, and clustering both the questions and the students. All data mining was carried out as described in the previous section. 1303 participants registered for the 2009 NCSS Beginners Challenge. We took the first 1000 participants to enrol to be involved in our experiment, and provided them all with hints for the first week of the Challenge. At the end of the first week, we found any participants who had not yet made any submissions, and excluded them from our experiment due to inactivity, ending with a population of 584 students. At the end of week 1 where everyone received hints, we split the students into 2 equal-sized groups: a test group, which received hints, and a control group, which did not from week 2 to 5. All 584 students were clustered based on their week 1 results. We then split each cluster in half for our hinted and control groups, based on the schools the students were registered with so students from the same school were in the same group. At the end of each week, students were clustered according to abilities (as in 4.1.1), using the cumulative student data. Question clusters were also updated weekly. We discovered each cluster could be mapped to a specific topic when we clustered the 2008 questions. Since new topics were introduced each week, we increased the number of similarity clusters over the weeks. We created two clusters in week 1 and week 2, and then increased this weekly until there were five clusters in week 5. Unlike the 2008 questions, in which all the data on student performance per question was known and available for clustering, the data for the 2009 Challenge was being generated during the experiment. As such, we were required to estimate the difficulties for each of the new week’s questions instead of deriving them solely from the participant results. At the end of each week, we analysed the results data from that week and clustered the questions to assign their difficulty levels, then estimated and assigned a difficulty to the new questions for the next week. At the end of the next week we readjusted the difficultly level based on the results generated by the participants for those questions. While the difficulty levels sometimes needed readjustment, we were generally able to estimate the question difficulties accurately at the start of the week and could predict the difficulties that were generated at the end of the week by the clustering. 
 5.2 Results of 2009 clustering.
 The techniques and attributes of the clustering of questions and students was the same as in 2008. The final set of question clusters by similarity is presented in Table 5. These were the clusters as used for Week 5 of the 2009 Challenge. 
 Table 5. The 2009 questions clustered by similarity. 
 The data for the difficulty clustering of the questions is shown in Table 6, which also includes the clusters questions were assigned to at the start of week 5. The week 5 difficulty clusters (in italics) were estimated. 
 Table 6. The 2009 questions and their clustered difficulty rankings. 
 5.3 Evaluation.
 Firstly, we measured student performance based on their average overall marks. For each of the 584 students in the experiment, we calculated their average score out of 10 for the questions they made at least one submission to. We then calculated the mean of these scores over the hinted students, and the mean of the scores for the control group. The hinted group's mean score was 4.02 (sd = 2.78), while the control group had a mean score of 3.18 (sd = 2.71). This was a difference of 0.84, i.e. an increase of 26.4%, with a significance of p < 0.0006 using an Approximate Randomisation test [12]. We used this because the students’ marks were not normally distributed, making a t-test inappropriate. This indicates that the hints substantially helped students when solving problems and lead to a significantly higher level of correct submissions being submitted. 
 Table 7. The number of students who submitted at least one question per week. 
 We also investigated whether users who received hints were active on the site for longer than those in the control group. Table 7 shows the number of hinted and control group users who submitted an answer to at least one question per week. There were consistently more users in the hinted group who made submissions, meaning the hinted group of users had an overall higher level of participation over the five weeks of the Challenge. The hints therefore had a distinctly positive effect on students’ willingness to stay engaged with the course. To get insights on students’ satisfaction with the hinting system, we presented them with a survey at the end of the course. Students were asked to rate the relevance of the hints to the questions they were answering and the topics they had difficulty with by using a five- point Likert scale. 67% of students found the topics “relevant” or “somewhat relevant” and 90% of them found the questions “relevant” or “somewhat relevant”. Therefore, it is clear that as far as the users were concerned, the methods for choosing topics to present were effective. In addition, 71% of students stated they would like more hints. Overall, the survey responses showed that the students found the hints helpful. When asked to provide comments, many of the students emphatically stated that the hints had helped them with problem solving, with students giving extremely positive comments and requests for the hints to continue in future years of the Challenge. Furthermore, one student found that the hints helped her access the notes much more effectively, which was our overall aim for the system: “I found the tips more helpful, because when we are using the notes to solve the problem we really don’t know where to go and what to do or which formula to use. But after using the hint formula we know where to go and what to use for solving the problem. So I reckon that the hint boxes were a very smart way to access the notes that can help us to solve the problems." 
 6 Future work and conclusion.
 Our project aimed to integrate data mining into an e-learning system to generate dynamically tailored hints for users. These hints give users immediate help by directing them to parts of the notes and questions that are relevant to questions that they find difficult. We built this hinting system for the NCSS Challenge website, using association rule mining and clustering on the data produced by live users, to update the system as it was being used. We evaluated the hinting system through a large-scale experiment conducted with participants of the 2009 NCSS Challenge. In the future, we would like to compare the effectiveness of our dynamic hints with statically generated hints. We found that users who were provided with hints achieved a 26% higher average mark than those who were not provided with hints, with statistical significance of p < 0.0006. Furthermore, we found qualitative evidence through positive student feedback that the hinting system had greatly helped users. These results show that the use of data mining to provide hints as part of the system loop is extremely effective, and can be used to build intelligent systems with much less of the time and cost expenses associated with traditional ITSs.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Data Mining for Individualised Hints in eLearning</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-katrina-dominguez"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-katrina-dominguez"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/james-r-curran"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/james-r-curran"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/126/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-katrina-dominguez"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/james-r-curran"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/127">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Mining Students' Interaction Data from a System that Support Learning by Reflection</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/127/authorlist"/>
		<swrc:abstract>In this paper we utilising some popular educational data mining (EDM) methods to explore and mine educational data resulted from a system that supports reflection for learning called Reflect. Our objective is to study whether using the Reflect helps students learn better in the subject. First, we explore data by means of statistical analyses. Then, we used clustering and classification techniques to identify learning behaviours associated with positive or negative outcomes. The results suggest that there is a positive correlation between reflection and student performance characterised by the level of activities carried out in the Reflect and the exam mark. We claim that our approach has resulted in the identification of some behaviour that leads to the increase in the students’ performances.</swrc:abstract>
		<led:body><![CDATA[ 1. Students read a task and assess example solutions made available in the system. 2. Students provide their own solutions to solve the problem and 3. They self-assess those solutions compared to the criteria teachers defined for the task. 
 By performing these steps, the students had practised to self-assess themselves and their state of knowledge of a particular task. Thus, they carried out learning by reflecting to their learning experiences. 
 3 Subject and Data.
 3.1 Subjects.
 The subjects used in this research are the students who undertook the Software Construction I (SOFT2130/2830) course at School of Information Technology, the University of Sydney in the second semester of 2007. There were 175 students enrolled for the course, however only 156 (89.1%) students managed to completed it. Out of 156 students who completed the course, 109 (69.9%) students passed and 47 (30.1%) of them failed the course. 
 3.2 Data.
 The data used in this research are coming from two sources: (1) students' activities data gathered from Reflect system and (2) students assessment data gained from the lecture. In experiments both of data were used. Student activity data are gathered from Reflect database in the form of XML format. The data are organised in a hierarchical structure i.e. each task consists of a task name and a number of learning objectives. The numbers of learning objectives for each task are varying from one to five learning objectives. Moreover, a learning objective may appear in one or more task. The student assessment data are spread over several worksheets of Microsoft Excel format. These include lecture quiz marks, homework and labs marks (including practical exam marks on week 4, 8 and 12), quizzes marks, and the final exam mark of students who enrolled for Software Construction I topic for the year of 2007. These data are obtained from the lecturer who taught and organised the course. 
 4 Process of EDM: Results and Discussion.
 4.1 Data pre-processing.
 Data pre-processing tasks consist of cleaning the data from incomplete or inconsistence data and errors and also integrating and transforming the data to an appropriate format for mining. 
 Cleaning the Data. 
 After data extracted from the Reflect database in XML format, the next step is to clean the data from inconsistencies and errors before they are ready to be mined. We did the process of data cleansing manually by searching for incomplete data or errors and removed them from database. The errors may come from unintended users who did not enrol for the topic. These users include Reflect system administrator who used the system for testing and other students who did not enrol for the course but used the system for learning and practising their C programming skills. The error may also come from a user who had a double user logins. This could happen when a student changed their user login at some points during the semester. 
 Integrating and Transforming the Data. 
 Data integration is a process of integrating or merging the data from multiple tables or databases into a coherent data store [5]. Educational data mining processes often involve retrieving and analysing multiple data attributes that spread across several tables or databases. The data, therefore, needs to be summarised into a new summarisation table consisted all necessary attributes for mining. As mentioned earlier in Section 3.2, our data sources are coming from: the students' activity data gathered from the Reflect database and students assessment spreadsheet obtained from the course coordinator. To perform the data mining out of these data, we are required to merge necessary data attributes into a summary table. Table 1 shows the data attributes of this summary table. Data transformation may involve a number of techniques including smoothing i.e. to remove noise from the data; aggregation i.e. summary operation applied to the data; generalisation where the low level data are replaced by higher-level concepts and normalisation where the attributes data are scaled thus they fall within a small range i.e. 0.0 to 1.0. [5]. In regards to the Reflect data, we used a smoothing technique to remove a noise from a student with an excessive self assessment input. We also used aggregation and normalisation techniques to smooth our data. We used aggregation to summarise the weekly Homework/Lab marks and quiz marks into Total Homework/Lab and Total quizzes marks, while we used normalisation to normalise the attribute values of Total Lecture quizzes and Lecture quiz attendances.  Finally, we transformed our data into ARFF and csv for mining with Weka. 
 Table 1. List of data attributes in summary table. 
 Constructing Dataset for Experiments. 
 From the list of data attributes showed in Table 1, we constructed two sets of data: (1) a data set that consist only numerical data and percentages, and (2) a data set that consist of numerical data, percentages and categorical final exam mark. The first data set is used for statistical and clustering analyses while the second data set is mainly used for classification analyses. 
 4.2 Data Exploration.
 The statistical analyses are often providing a starting point for data analyses. Therefore we carried out a number of correlation analyses to study the relationships and to measure if one data attribute is significantly correlated to another  In addition, a statistical analysis can be used to determine which variable is best explain the differences between two or more groups, that is a variable that can distinguish one group from another. Our objective is to utilise statistical analyses to find the relationships and correlation analyses between: (1) lecture quizzes score and final exam mark, and (2) lecture quizzes attendances and final exam mark. 
 Discussion.
 Statistical analyses suggested that although there were positive correlations for both attending (Lec_qz_attd) and performing quizzes (Tot_Lec_qz) set by the teachers in the lectures and the final exam mark, these correlation are not considered high enough. The correlation score (r) between Lec_qz_attd and exam is only 0.462 (p<0.01) and correlation (r) between Tot_Lec_qz and exam mark is 0.413 (p<0.01). Meanwhile, another analyses suggested that correlation score between Total Howework/Lab (Tot_HWL) and exam is reasonably high (r=0.618; p<0.01). However, the correlation between Homework/Lab attendances (HWL_attd) and exam is not high enough (r=0.354; p<0.01). The results indicated that Homework/Lab performance is considered significantly important toward achieving a good mark in the final exam. However, this is not necessarily the case for lecture quiz and its attendances as their correlation with the exam mark are not significantly high. Nevertheless, the results showed positive correlations that indicated the importance of attending both and performing well in both lecture quizzes and Homework/Lab activities. 
 4.3 Clustering Students.
 Clustering is an unsupervised classification used for grouping objects into classes of similar objects [6]. A number of researchers have implemented clustering techniques for mining e-learning data with various purposes such as for finding groups of students who have similar learning characteristics, to encourage group-based collaborative learning and to provide incremental learner diagnosis [7]. Our work utilised a K-means clustering algorithms to cluster group of students based on their similar behaviour in using the Reflect system. We choose K-means clustering algorithm because it is considered as one of the most popular and mostly used clustering algorithm in broad data mining research community [8]. Another reason for choosing K- means is because it is available and ready to use in Weka system. Lecture quizzes versus final exam score We utilised both numerical data and a combination of numerical and nominal data set. Before running the K-means algorithm, the data set are transformed into comma separated value (csv) format. The reason is because, a csv file format is easy to use and it is one of file format acceptable in the Weka system. Here, our objectives are first, to distinguish stronger group of students from the weaker ones and then to study learning characteristics that differentiate each group. To achieve this objective, we utilised numerical data set with categorical exam mark. 
 Discussion.
 Clustering analyses are able to provide more detailed information about students beyond what simple statistical analyses may offer. Based on the clustering results, we are able to categorise students into several clusters based on their performances and attendances in the lecture quizzes. Each group is characterised by how often their attended the lecture quizzes and their performance on those quizzes. 
 Table 2. Student cluster using K-means (k=4).
 The results summarised in the Table 2, suggested that 33% of students who achieved higher exam mark had at least average high mark on lecture quizzes and its attendances. In the other hand, 28% students who achieved very unsatisfactory (very low) exam results had not attended enough lecture quizzes (very low attendances). These results highlighted the important of every lecture quiz toward the increasing of students' understanding to the topic. In other words, attending and performing tasks and exercises in the lecture quizzes may have a direct impact on student's knowledge of the topic and may affect their performance in the exam. This is maybe because attending and completing tasks and exercises in the ”exam-like” environment, such as in a lecture quiz, during the lectures may make the student familiar and well prepared with the type of question being asked in the final exam. Meanwhile, the students with poor class attendances would miss the opportunity to familiar themselves to the type of question that may appear in the exam. This result supports our hypothesis that “students who attended and have good marks on all lecture quizzes (week 2, 5 and 8) performed well in the final exam”. 
 4.4 Classifications.
 Classification is a supervised classification in which the labels of pre-classified patterns are identified. This pre-classified pattern is known as training data set. Within the training data set there is a class attribute that will be used to label a newly encountered, still unlabelled pattern [HanK06_Data_mining]. Our approach is to use a well known C4.5 (J48) algorithm from Weka data mining tools. The J48 algorithms would generate decision trees that might be used to extract classification rules. In our case, the objective is to classify students into different groups or branches with almost equals final exam mark. The model resulted from the experiments can be used to predict the final exam mark of new students. . The J48 decision tree algorithm required that the label in this case is exam mark, must be in the categorical or nominal form. We, therefore, grouped the exam mark into the following category: Fail if the exam mark is <40; Low if the exam mark >=40 and <55; Moderate if the exam mark >=55 and <70; Good if the exam mark >=70 and <80; and Very Good if the exam mark >=80. This categorisation is different to the categorisation of the final grade students received at the end of semester. The final grade which is determined by the course coordinator at the end of the semester integrated not only the final exam mark but also other assessment components including quizzes marks, homework/lab marks and assignments scores. The categorisation for the final exam mark is fixed by the course coordinator and agreed by students at the beginning of the course. Some rules were generated from classification analyses. These rules are summarised as follow: IF (Tot_qz <= 0.67) AND (n_task <= 6) THEN exam = Fail IF (Tot_qz <= 0.67) AND (n_task > 6) AND (HWL_attd <= 0.8) AND (Lec_qz_attd <= 0.33 ) THEN exam = Low IF (Tot_qz <= 0.67) AND (n_task > 6) AND (HWL_attd <= 0.8) AND (Lec_qz_attd > 0.33 ) THEN exam = Moderate IF (Tot_qz between 0.67 and 0.96) AND (n_lo <= 12) THEN exam = Low IF (Tot_qz between 0.67 and 0.96) AND (n_lo > 12) THEN exam = Moderate. IF (Tot_qz >= 0.96) THEN exam = Good 
 Discussion.
 The results suggested that the total quizzes mark (Tot_qz) was the main discriminator of the final marks followed by the number of task students done in the Reflect system. The rules generated by J48 algorithm revealed the characteristics of each group of students. For example, a student should at least complete six tasks or more in the Reflect system and achieved more that 67% of total quizzes mark to reduce the risk of fail in the final exam. Meanwhile a student who can achieved 96% of the total quizzes marks are directly classified as Good, meaning he/she will be among students who are likely to achieve a good mark in their final exam. The other groups of students are classified based on some other activities including homework/lab (HWL_attd) and lecture quiz attendances (Lec_qz_attd). As we can see from the results, there are set of critical separation point for each class, for example 67% or 96% of total quizzes, 6 or 12 tasks done in Reflect, 33% of lecture quiz attendances (Lec_qz_attd) and 80% of homework/lab attendances (HWL_attd). These separation points are set automatically by J48 classifier. Using J48 algorithm, our first experiment did not produce a good result. The accuracy level recorded for the first experiment is only 51.28%. This means that out of 156 data, only 80 instances were correctly classified into their classes. Other 76 instances were incorrectly labelled into the other classes. In the second experiments, the accuracy level increased although not much. The accuracy level for the classifier model became 56.41%. This means that out of 156 instances, 88 of them are correctly classified into their classes while remaining of 68 instances were still not correctly classify or labelled into their classes. This information would be very useful for a course coordinator for example the classifier can be used to predict the final exam marks of new students or to promote some types of activities to obtain higher marks. 
 5 Conclusions.
 We have demonstrated how EDM methods have been utilised to extract some valuable in formation from the Reflect data. We have discussed how the users of Reflect can perform self-assessment by following certain procedures as described in the section 2. These include to rate their understanding of each learning objectives related to the task, providing answers to the tasks, self assess their answers and sample solution against certain criteria defined by the teacher and comparing the discrepancies between their answers to the teacher's assessment. A number of issues have emerged during the study either related to the data or the interpretation of the results. First, the present study only used one semester data, hence the analyses is limited. For the further work, it would be more interesting if data mining is conducted for data that have been collected for many years. Secondly, the current data used was not designed in the first place to be suitable for mining. Therefore, the data are complex, contain noise, and heterogeneous. The results of EDM analyses suggested that the Reflect system helps users to self assess themselves and thus satisfied the aims indentified in the introduction of section that is students learnt more by using system that support learning by reflection such as the Reflect system.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Mining Students' Interaction Data from a System that Support Learning by Reflection</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rajibussalim"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rajibussalim"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/127/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/rajibussalim"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/128">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>An Analysis of the Differences in the Frequency of Students’ Disengagement in Urban, Rural, and Suburban High Schools</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/128/authorlist"/>
		<swrc:abstract>We study how student behaviors associated with disengagement differ between different school settings. Towards this, we investigate the variation in the frequency of off-task behavior, gaming the system, and carelessness in an urban school, a rural school, and a suburban school in the United States of America. This analysis is conducted by applying automated detectors of these behaviors to data from students using the same Cognitive Tutor educational software for high school Geometry, across an entire school year. We find that students in the urban school go off-task and are careless significantly more than students in the rural and suburban schools. Differences between schools in terms of gaming the system are less stable. These findings suggest that some of the differences in achievement by school type may stem from differences in engagement and problem behaviors.</swrc:abstract>
		<led:body><![CDATA[ 1.  School Population Demographics.
 There is evidence of considerable poverty in the urban school’s population, with a low median household income for the United States, and a high percentage of children under the poverty line. Similarly, the rural school has a low median household income for the United States, and a substantial percentage of children under the poverty line. The suburb has considerably less poverty, with a median household income over double the urban school’s median household income, and only 2.5% of children under the poverty line. Data from each school was obtained through the PSLC DataShop [16]. In each case, high school students took their Geometry courses using the same Cognitive Tutor for Geometry [7, 14, 15], and all data was collected in the PSLC DataShop. Data was collected for the entire school year, from August 2005 to May 2006. 434 students in the rural school used the software, 88 students in the suburban school used the software, and 34 students in the urban school used the software. The three schools each assigned the software to students who were neither in special needs nor gifted classes – the difference in the number of students using the software is solely based on school size, and how many teachers chose to use the software (in particular, the rural school is a large regional school, as is increasingly common in the USA in rural areas, whereas the urban and suburban schools serve smaller populations). In all schools, the software was used by groups of students in a computer laboratory, working individually at separate computers, at their own pace. Students in the rural school used the software an average of 9 hours, students in the suburban school used the software an average of 35 hours, and students in the urban school used the software an average of 51 hours. Hence it appears that the teachers in each school chose to have their students use the software in different amounts. This difference represents a selection bias in our data, but it is a difficult confound to resolve; for instance, restricting analysis to students who used the software above a time cutoff introduces a different selection bias. In particular, the difference in usage is a natural one, reflecting genuine implementation in each type of school. To address this selection bias stemming from teacher choice, we analyze the data in two ways – using all data (the more ecologically valid choice), and using a time-slice consisting of the 3rd-8th hours (minutes 120-480) of each student’s usage (this time-slice will not be as representative of the usage in each school, but avoids this confound). The 3rd-8th hours were selected, because the initial 2 hours likely represent interface learning, and therefore may not be representative of overall tutor use (and interface learning is likely to be dependent on prior experience with educational software, which is likely to be greater for wealthier students). In general, implementational differences between schools are likely to exist in year-long comparisons. In the long-term, this problem can probably be best addressed by conducting analyses of this nature across large numbers of schools, in order to average across implementational differences orthogonal to the type of school (though some implementational differences may be characteristic to certain types of schools – for instance, urban schools might use specific pieces of educational software more heavily due to having lower resources to provide a wide range of educational software in their classrooms). Each action in each data set was labeled using detectors of gaming the system, off-task behavior, and carelessness.  The gaming detector used was trained using data from students using a Cognitive Tutor for Algebra [5], using an age-similar population and an approach validated to generalize between students and between Cognitive Tutor lessons [4]. The off-task detector used was trained using data from students using a Cognitive Tutor for Middle School Mathematics. The off-task detector was validated to generalize to new students, and to function accurately in several Cognitive Tutor lessons [2]. Although the age range was moderately older in this study than in the original training data, off-task behavior is similar in nature within these populations – it involves ceasing to use the software for a significant period of time without seeking help (which can be detected in the log files by the behavior occurring before and after an idle pause). Carelessness was detected using the slip detector from [3], which was trained on data from Cognitive Tutor Geometry. This use of contextual slip is in line with theoretical work by Clements [9], who argues that making errors despite knowing the skills needed for successful performance should be considered evidence of carelessness. It is important, however, to note that contextual slip could potentially also be an indicator of shallow knowledge that does not apply to all items in the tutor, even if they are labeled as involving the same skill. 
 3 Results.
 In discussing results, we will first discuss our analyses conducted across the full year of tutor data, and then discuss the same analyses conducted across only a time-slice including the 3rd to 8th hours of tutor usage. 
 3.1 Analyses Across Full Data Set.
 Across the full data set, representing data collected during the entire school year, the pattern of off-task behavior was highly different between the three schools. Students in the suburban school were off-task an average of 15.4% of the time (past research in traditional classrooms has averaged 15-20% of time off-task [cf. 18, 19]). 
 Table 2.  Average incidence of each indicator per school. Parentheses give standard deviation. 
 Students in the rural school were off-task an average of 20.4% of the time. Students in the urban school were off-task an average of 34.1% of the time. Hence, students at the urban school were off-task 67% more than students at the rural school (a 1.0 SD difference), and over double as much as students at the suburban school (a 0.9 SD difference). The overall difference in off-task behavior between schools was statistically significant between schools, F(2,553)= 18.80, p<0.01. The model predicting time off-task by school predicted 6.4% of the variance in time off-task. The pairwise differences between schools were all statistically significant, using Tukey’s HSD to control for multiple comparisons. The frequency of gaming the system had smaller differences between the three schools, although there were still significant differences. Students in the suburban school gamed 6.9% of the time, students in the rural school gamed 6.6% of the time, and students in the urban school gamed 7.4% of the time. In other words, students in the urban school gamed only 13% more than students in the rural school (a 0.47 SD difference), and 9% more than students in the suburban school (a 0.16 SD difference). The overall difference in gaming the system between schools was statistically significant, F(2,553)= 3.12, p=0.05. The model predicting time spent gaming the system by school predicted 1.1% of the variance in gaming, considerably less than is predicted by individual differences between students or by the differences between tutor lessons [1]. According to Tukey’s HSD, the rural school had significantly less gaming than the urban school, but the other differences in gaming were not statistically significant. The pattern of carelessness was highly different between the three schools. Students in the suburban school had a probability of 0.32 of slipping despite knowing a skill, students in the rural school had a probability of 0.27 of slipping despite knowing a skill, and students in the urban school had a probability of 0.50 of slipping despite knowing a skill. The overall difference in slipping between schools was statistically significant, F(2,553)= 54.50, p<0.001. The model predicting slip probability by school predicted 16.5% of the variance in slip probability. The pairwise differences between schools were all statistically significant, using Tukey’s HSD to control for multiple comparisons. 
 3.2 Analyses Across Data From Hours 3-8.
 Within the restricted time-slice of data from hours 3-8, the differences in the frequency of off-task behavior were qualitatively similar to the analysis across the full data set, although the difference between the urban school and the other schools was smaller. 
 Table 3.  Average incidence of each indicator per school. Parentheses give standard deviation. 
 Students in the suburban school were off-task an average of 16.5% of the time, very similar to the 15.4% reported across all data. Students in the rural school were off-task an average of 21.0% of the time, very similar to the 20.4% reported across all data. However, students in the urban school were off-task an average of 25.8% of the time, substantially lower than the 34.1% reported across all data, a statistically significant difference, t(33)=-2.55, p=0.02, for a two-tailed paired t-test. This result suggests that off-task behavior increased during the year in the urban school. Nonetheless, even during this earlier time-slice, off-task behavior was higher in the urban school than the suburban school. The overall difference in off-task behavior between schools was statistically significant, F(2,484)= 3.01, p=0.05. The model predicting time off-task by school predicted 1.2% of the variance in time off-task. According to Tukey’s HSD, the urban school had significantly more off-task behavior than the suburban school, but the rural school was not significantly different from either of the other two schools. The pattern of gaming the system was highly different within the restricted time-slice of data from hours 3-8, as compared to the entire data set: Gaming the system was much rarer in the urban school. Students in the urban school gamed 4.7% of the time in the restricted time-slice, compared to 7.4% of the time in the full data set, a significant difference, t(33)=8.14, p<0.001, for a two-tailed paired t-test. Gaming was also less common in this time-slice in the other two schools, but to a much lower degree.  Students in the suburban school gamed 5.9% of the time, compared to 6.9% of the time in the full data set, which was not quite statistically significant, t(71)=1.51, p=0.13, for a two-tailed paired t-test. Students in the rural school gamed 6.4% of the time, compared to 6.6% of the time in the full data set. The overall difference in gaming the system between schools was statistically significant, F(2,484)= 4.09, p=0.02. The model predicting time spent gaming the system by school predicted 1.7% of the variance in gaming, considerably less than is predicted by individual differences between students or by the differences between tutor lessons [1]. According to Tukey’s HSD, the rural school had significantly more gaming than the urban school – the exact opposite of the result across the entire data set – but the other differences in gaming were not statistically significant. The pattern of carelessness retained the same ordering within the restricted time-slice of data from hours 3-8, and the entire data set, but the degree of carelessness was significantly higher for all three groups of students, t(71)=6.32, p<0.001 in the suburban school, t(374)=10.08, p<0.001 in the rural school, and t(33)=2.04, p=0.05 in the urban school. In all cases, a two-tailed paired t-test was used. The overall difference in slipping between schools was statistically significant, F(2,478)=29.78, p<0.001. The model predicting slip probability by school predicted 11.1% of the variance in slip probability. The pairwise differences between schools were again all statistically significant, using Tukey’s HSD to control for multiple comparisons. 
 4 Discussion and Conclusions.
 In this paper, we have analyzed the prevalence of three student behaviors associated with disengagement in urban, suburban, and rural classrooms in the USA: off-task behavior, gaming the system, and carelessness. These students used the exact same learning software for high school Geometry across the same school year. However, the students used the software for different amounts of time in each school, a common phenomena in real-world use of educational software, where usage decisions are made by teachers and school administrators, rather than researchers and curriculum developers. To address this difference, we compared between these schools in two fashions. First, we compared all data to get a fully ecologically valid comparison. Second, we compared within a time- slice consisting of each student’s 3rd to the 8th hours of usage in each school, in order to control for confounds stemming from differences in implementation between schools. The two versions of the analysis agreed that the urban school had more off-task behavior and carelessness than the suburban school and rural school. In terms of these behaviors, students in the rural and suburban schools were more similar to each other than either school was to the urban school. One interesting note is that carelessness dropped significantly more over the course of the school year in the suburban and rural schools than in the urban school, suggesting that some influence or factor caused the suburban and rural students to become more diligent during the school year, but that this influence or factor was significantly less relevant in the urban school. As both the rural school and the urban school had significant poverty, it appears that some aspect of these schools other than simply socio-economic status explains the higher frequency of off-task behavior and carelessness in the urban school. There are several potential hypotheses what other aspects may explain these behavioral differences, including differences in teacher expertise (which is often lower in urban schools [17]), differences in schools’ facilities, equipment (e.g. computers), and physical environment, and differences in students’ cultural backgrounds. Determining whether one of these factors explains the differences in off-task behavior and carelessness will be an important topic for future research. A contradictory finding between analyses was found for gaming the system. Across the whole data set and entire school year, gaming the system was most frequent in the urban school. However, within the 3rd to 8th hours of tutor usage, gaming the system was least frequent in the urban school. This suggests that students in the urban school gamed more, later in the year. This may just be an artifact of the lessons encountered, as tutor lesson predicts a substantial portion of the variance in gaming behavior in Cognitive Tutors [1]. However, it may also be that the novelty of Cognitive Tutors reduces gaming initially in American students. There is evidence for this possibility in the finding that gaming was lower in all schools during the 3rd-8th hours. The difference in gaming behavior between the early time-slice and the overall data set was more pronounced in the urban school, but this may be due to lower familiarity with educational technology in general, a finding worth investigating further. In future years, we plan to replicate these analyses with a larger number of schools in each of these settings, using the research presented here as a methodological template for that later research. Automated machine-learned detectors provide an essential tool for analysis of this sort, in this author’s opinion a better tool than existing alternatives. For example, it is not tractable to use observational, text replay annotation, or video methods at this sort of scale. [5] presents a use of text replay methods to analyze a single behavior among 58 students over an entire school year; though text replay methods are significantly faster than live observation or video coding methods, the coding needed for this analysis took over 200 hours. Utilizing text replays to annotate the 3 school sample used in this paper would have taken over 2000 hours, assuming a rate of observation equal to that in [5]. Video coding and field observation would have taken even longer. That said, it is worth noting that automated detectors have important challenges not present when using human labels. It is important to validate the generalizability of detectors across students, schools, and learning materials, a task which has been only partially completed for the detectors used in this paper, and which has received insufficient attention in the literature in general. Construct validity is also a key issue in the use of machine-learned detectors,  and is more a risk in detectors that are based on theoretically determined training labels (e.g. the model of carelessness), compared to detectors based on human judgments shown to have good inter-rater reliability (e.g. the detectors of off-task behavior and gaming the system). It is worth noting that automated detectors produced with a common alternative to machine learning, knowledge engineering, are likely to be prone to the same challenges to generalizability and construct validity as machine-learned detectors. Current practice with knowledge engineering often does not check detectors against human labels or across contexts, a potentially significant risk to using these models in discovery with models analyses. As research applying detectors across contexts goes forward, it has significant potential to support progress in studying the impact of school context. By further study of which school contexts – and what attributes of those contexts – are associated with greater frequencies of disengaged behavior, we may be able to better understand the differences in learning between different learning settings. This may in turn support education researchers and practitioners in designing curricula, learning software, and interventions tailored to different schools – a potentially key step towards developing educational software that is equally effective for all students, whether they are in urban schools, rural schools, suburban schools, or elsewhere. 
 Acknowledgements.
 This research was supported by NSF grant REC-043779 to “IERI: Learning-Oriented Dialogs in Cognitive Tutors: Toward a Scalable Solution to Performance Orientation”, and by the Pittsburgh Science of Learning Center (National Science Foundation) via grant “Toward a Decade of PSLC Research”, award number SBE-0836012.  We would also like to thank Adriana de Carvalho and Ma. Mercedes T. Rodrigo for valuable comments and suggestions, and the members of the Pittsburgh Science of Learning Center DataShop, Alida Skogsholm in particular, for their support and assistance.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>An Analysis of the Differences in the Frequency of Students’ Disengagement in Urban, Rural, and Suburban High Schools</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sujith-m-gowda"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sujith-m-gowda"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/128/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/sujith-m-gowda"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/129">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Using Educational Data Mining Methods to Study the Impact of Virtual Classroom in E-Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/129/authorlist"/>
		<swrc:abstract>In the past few years, Iranian universities have embarked to use e-learning tools and technologies to extend and improve their educational services. After a few years of conducting e-learning programs a debate took place within the executives and managers of the e-learning institutes concerning which activities are of the most influence on the learning progress of online students. This research is aimed to investigate the impact of a number of e-learning activities on the students’ learning development. The results show that participation in virtual classroom sessions has the most substantial impact on the students’ final grades. This paper presents the process of applying data mining methods to the web usage records of students’ activities in a virtual learning environment. The main idea is to rank the learning activities based on their importance in order to improve students’ performance by focusing on the most important ones.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Using Educational Data Mining Methods to Study the Impact of Virtual Classroom in E-Learning</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mohammad-hassan-falakmasir"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mohammad-hassan-falakmasir"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jafar-habibi"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jafar-habibi"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/129/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/mohammad-hassan-falakmasir"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jafar-habibi"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/130">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Can order of access to learning resources predict success ?</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/130/authorlist"/>
		<swrc:abstract>Learning management systems capture student’s interactions with the course contents in the form of event logs, including the order in which resources are accessed. We build on past research which indicates there are learning benefits if students determine their own ordering of use of learning materials. We report our exploration of sequential data mining that aims to help teachers determine whether some patterns of access to learning resources are predictive of performance, especially where this may signal the need for remediation. We report first explorations of the data in a graphics course and these indicate that sequence of resource access varied between the low, medium and high achieving student groups.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 The sequence in which learners should make use of learning materials is important in designing online courses. Learning management systems (LMS) provide one-size-fits-all solution where every student is presented with the same set of learning materials in one particular order. Previous research [1] indicates that the order in which the online learning materials are accessed may have an important relationship with student learning. Our research study explores the order in which resources students access as they solve set assessment tasks, such as tests, assignments and Exams. The presentation order of the problems (for instance easy questions followed by difficult ones) in these activities determines the sequence in which resources will be assessed in order to solve them. For instance, in a study by Pardos and Heffernan [2], the relationship between the sequence of problem order and learning in Intelligent Tutoring systems was explored using Bayesian methods. We explore the use of data mining techniques to analyse patterns of such access. While Pardos and Heffernan explored the relationship between problem order and performance, we analyse the order of resource usage and its links with learning. 
 Approach.
 The context of our work is a graphics course delivered in mixed mode. Each week students complete activities after reviewing the resources online. The resources consisted of a comprehensive tutorial guide and additional video and text based tutorial guides .Skills acquired during the weekly activities are tested using a mid-semester test and final exam. Student log data for file accesses during each week are extracted. Students were grouped, based on their achievement on the mid-semester test into high, medium and low achievement groups. The patterns within each group were analysed to identify distinctive sequences associated with each group. A key goal of the approach is to identify sequences that are more frequent among the weak students since such patterns might be used as an “early warning sign” by instructors. Our dataset comprises 66 students taking a Multimedia system course in Semester, 2010, using the Blackboard learning management system. As an indicator of initial knowledge, students completed an eighteen question self assessment questionnaire at the start of the semester. Event logs(1421) over two weeks were analysed to obtain the patterns. Preliminary results: Distinctive patterns of access were found for each group and patterns showed that high and medium groups frequently accessed the compressive tutorial in order to complete their tasks successfully.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Can order of access to learning resources predict success ?</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hema-soundranayagam"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hema-soundranayagam"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/130/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/hema-soundranayagam"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/131">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Peer Production of Online Learning Resources: A Social Network Analysis</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/131/authorlist"/>
		<swrc:abstract>This paper describes methods for collecting user activity data in a peer production educational system, the Instructional Architect (IA), and then takes a social network perspective in analyzing these data. In particular, rather than focusing on content produced, it focuses on the relationship between users (teachers), and how they can be analyzed to identify important users and like- minded user groups. Our analyses and results provide an example for how to select the most important factors in analyzing the dynamics of an online peer production community using social network analysis metrics, such as in-degree, out-degree, betweenness, clique, and community.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Peer Production of Online Learning Resources: A Social Network Analysis</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/131/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Online Curriculum Planning Behavior of Teachers</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132/authorlist"/>
		<swrc:abstract>Curriculum planning is perhaps one of the most important tasks teachers must perform before instruction.  While this task is facilitated by a wealth of existing online tools and resources, teachers are increasingly over- whelmed with finding, adapting and aligning relevant resources that support them in their planning.  Consequently, ripe research opportunities exist to study and understand online planning behavior in order to more generally characterize planning behavior.  In this paper, we introduce a web-based curriculum planning tool and study its use by middle and high school Earth science teachers.  We ex- amine the web analytics component of the tool and apply clustering algorithms to model and discover patterns of the use within the system.  Our initial results provide insights into the use of the tool over time and indicate teachers are en- gaging in behavior that show affinity for the use of interactive digital resources as well as social sharing behaviors.  These results show tremendous promise in developing teacher-centric analysis techniques to improve planning technologies and techniques to study online curriculum planning patterns.</swrc:abstract>
		<led:body><![CDATA[ 1. 
 Table 1 : Monthly data summary. 
 Table 2 : Cluster ranks and relative size (%). 
 4 Algorithms and Feature Analysis.
 Clustering algorithms are commonly used to find patterns within large data sets [2, 6, 7] and two clustering algorithms were chosen to study the initial data set. First, the K-means clustering algorithm was used. K-means is an unsupervised learning, iterative descent algorithm that partitions n data observations into K clusters. Each cluster is assigned a cen- troid and cluster membership is determined by minimizing the distance from each cluster member and the centroid. The second algorithm in the initial experiment was the Expec- tation-Maximization (EM) algorithm [4]. EM is a model-based iterative algorithm that examines data observations and represents each cluster as a probability distribution. Giv- en n data observations, EM maximizes the likelihood of the observed distributions by es- timating the means and standard deviations of each cluster. Neither algorithm is without flaws and our experimental results in section 6 show this. K- means primary weakness is that the number of clusters must be determined a priori. This weakness, however, is inherent in many partitional clustering algorithms and may require an experimentally selected n. Another weakness is that K-means is sensitive to outliers – data that are distant from the centroid may pull the centroid away from the real centroid in a given data set. Finally, it is difficult to understanding which feature contributes more to cluster membership, since every feature is assumed to have the same weights. EM‟s core weaknesses are the relative speed with which the clusters converge, and the possibil- ity of convergence at the boundary of a cluster. 
 5 Experiments.
 Two experiments were performed on the initial feature set with a few variations for com- parison. The first experiment was designed to run the K-means algorithm with n = 12 and the Euclidean distance function, referred to as K12. The n for this initial experiment was derived from the total number of sessions analyzed over the period (~1,400) divided by the number of users invited to participate (~120). This provides a baseline for comparison with the other algorithms. For comparison, the expectation maximization algorithm was chosen for the remainder of the experiments. EM was first chosen to automatically select n clusters using cross validation, referred to as EM*. This provided a baseline to compare the algorithm's performance against the K-means algorithm (K12). The last experiment was run using EM again with a fixed cluster size of 12, referred to as EM12. 
 6 Evaluation and Results.
 Table 2  shows each of the algorithms and the sizes of the largest clusters they produced. 
 EM* produced 10 clusters, the top 4 of which represent 84% of all the data. Similarly, for EM12, the top 5 largest clusters represent 84% of the data. Finally, K12 shows a similar trend, with the top 5 clusters representing 87% of the data. For the purposes of evaluation we consider the top 4 clusters in EM*, top 5 in EM12, and the top 6 clusters in K12, since the K12 distribution of clusters was more sparse. 
 Table 3 shows the features with the greatest means of the top clusters for each algorithm. The cluster labels represent UI features for example, A1 represents clicks on the Interac- tive Resources tab, A2 the Shared Stuff for This Concept tab, A6 for the Embedded As- sessments toggle element, A12 for the Images/Visuals tab, and so on.  The top features of the largest cluster in EM* (A1, A2 and A4) correspond to CCS UI tab clicks on Interac- tive Resources, Shared Stuff for This Concept and Shared Stuff for This Activity. This top cluster suggests a pattern of activity that is focused on both CCS-suggested interactive resources and shared resources that others have saved, which may indicate the impor- tance of  what others have saved as well the automatically generated interactive resource list. The  EM12 and K12 algorithms indicate very similar patterns. For example, EM12‟s largest cluster shows the exact same pattern as EM*. Similarly, K12 shows A1, A3 and A4 as its top features. Examining other clusters show cluster 4 of K12 and cluster 11 of EM12 share similar patterns over features A3, A6, A8 and A11. This pattern corresponds to clicks on Instructional Support Materials, Embedded Assessments, Answers and Teaching Tips system areas. Similarly, cluster 2 of K12 and cluster 1 of EM12 share sim- ilar features along A3, A7 and A14, which correspond to the Instructional Support Mate- rials and Activities tabs, suggesting time being spent on preparing or reviewing student activities and corresponding materials. 
 Table 3 : Top cluster features and their cluster membership. 
 Each cluster algorithm revealed data that was consistent with overall system use seen in the server logs, though the smaller cluster sizes show greater differentiation of features. That there was not complete agreement in cluster features or sizes, however, may indicate more experiments are required. 
 7 Related Work.
 Much of the work here has been influenced by the body of work in web use analytics, which break down into two categories : (1) content analytics and (2) usage analytics [12]. This work is focused on usage analytics. Broadly, use analytics aims at understanding the aggregate activity and use patterns of a website primarily using advanced server log anal- ysis. Such analytics often aim at understanding aspects of the site that are popular, con- tent that seems to be frequently accessed, times of frequent/infrequent use, etc. with the goal of developing a sense of where the site could be improved or enhanced for optimal performance, increased advertisement penetration or site content enhancement through recommender techniques [13]. Such use analytics are invaluable for developing site con- tent, but also useful in developing models of user behavior. Website session characteris- tics are commonly studied to determine how users are accessing the site and statistical techniques are used to determine tasks being performed within a website, revealing clus- ter usage patterns in the ways we have discussed here. Markov models have been used to derive the meaning of certain behaviors within a session by observing page transitions and their probabilities to develop behavioral models of use [5]. Work has also been done to connect page semantics to web usage, for example [9] use Probabilistic Latent Seman- tic Analysis to determine if the content and subsequent usage of a page implies an under- lying task. Finally, user interface event mining [8] aims at developing techniques to ex- ploit detailed user experience and interaction data. 
 8 Discussion.
 The initial experiments presented in this paper offer some insights into the planning be- havior of teachers online. However, two areas of improvement can be immediately dis- cussed : improved feature selection and expanded algorithm experiments and compari- son. The initial experimental feature set provides interesting insights into the behavior of teachers for the visual components selected in this initial observation. However, the flat- tened visual hierarchy of the CCS interface only provides a convenient way to discretize each visual element of the system without advancing the notion of the semantic structure of this hierarchy. For example, while it is clear that the Interactive Resources tab of the interface was widely used, there are substructures under that tab which also contain wide- ly used features. The current feature set is not capable of capturing this hierarchy or its implied semantic structure, though considering it might yield new insights into the se- mantics of the features commonly accessed by users. Further extensions to the feature set might also include adding link-to-link features, for example, exploring high frequency transitions might reveal unique relationships between UI features and functionality. The EM and K-means algorithms are commonly used in data mining, and while some clusters in K12 and EM12 had similar characteristics, all of the top clusters were not sim- ilar enough to say both algorithms were converging on exactly the same feature sets. This may underscore the differences in each algorithm or in the way they each treat the fea- tures. It may also reinforce the effects parameter sensitivity (e.g. n clusters) and feature selections have on the results. The focus of the next round of experiments will be to expe- riment further with EM and K-means parameters, and also to expand algorithm coverage to hierarchical-based algorithms. Such experimentation may also fit well with the seman- tic features already suggested and allow a comparison of the hierarchies that are produced from a semantic-based structure with the clusters already observed. As with all learning algorithms, it is challenging to determining if the experimental data would be predicted by and hold up to some gold standard or human expert evaluation. Determining if the discovered behaviors match the observed data in practice is difficult and further research is underway to study actual and reported system use through on-site observation and survey instruments, which should lead to a higher fidelity confirmation of the patterns discovered thus far.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Online Curriculum Planning Behavior of Teachers</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-e-maull"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-e-maull"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/manuel-gerardo-saldivar"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/manuel-gerardo-saldivar"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tamara-sumner"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tamara-sumner"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-e-maull"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/manuel-gerardo-saldivar"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/tamara-sumner"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/133">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>A Data Driven Approach to the Discovery of Better Cognitive Models</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/133/authorlist"/>
		<swrc:abstract>Cognitive models composed of knowledge components are an integral part of intelligent tutors and drive many of the instructional decisions that these systems make. Most of these models are designed by educators and subject experts. Today vast amounts of data, collected from many intelligent tutors, allow us to analyze and improve the current cognitive models through educational data mining.  In this research, we show how we identified, in the tutor data, potential improvements to existing cognitive models and then evaluated those improvements using statistical analysis and cross validation.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>A Data Driven Approach to the Discovery of Better Cognitive Models</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/133/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Where in the World? Demographic Patterns in Access Data1</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134/authorlist"/>
		<swrc:abstract>Standard webmetrics tools record the IP address of users’ computers, thereby providing fodder for analyses of their geographical location, and for understanding the impact of e-learning and teaching. In this paper, we describe how two web-based educational systems were engineered to collect geo- referenced data. This is followed by a description of joining these data with demographic and educational datasets for the United States, and mapping different datasets using geographic information system (GIS) techniques to visually display their relationships. We conclude with results from statistical analyses of these relationships that highlight areas of significance.</swrc:abstract>
		<led:body><![CDATA[ 1. U.S. map showing IA visits (darker dots indicate more visits) overlayed with median family income over 1 year. 
 We also examined statistical relationships between the demographic predictors and the number of site visits per location as reported by GA. Due to the high correlation between some of the predictors, only three out of the five were selected, which were population, number of school districts, and per capita income. A negative binomial regression (to handle the skewed data) showed that population, and per capita income were statistically significant predictors of site visits to both the IA and the ELRC, and number of school districts was also a significant predictor for the ELRC. We interpret these results to mean that online visitors to these sites came from, not surprisingly, more densely populated areas. In addition, the relationship with per capita income may be a function of the amount of resources (i.e., computing) available in the local schools and communities.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Where in the World? Demographic Patterns in Access Data1</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sherry-hsi"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sherry-hsi"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/christine-garrard"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/christine-garrard"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/sherry-hsi"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/christine-garrard"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/135">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Using Neural Imaging and Cognitive Modeling to Infer Mental States while Using an Intelligent Tutoring System</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/135/authorlist"/>
		<swrc:abstract>Functional magnetic resonance imaging (fMRI) data were collected while students worked with a tutoring system that taught an algebra isomorph. A cognitive model predicted the distribution of solution times from measures of problem complexity. Separately, a linear discriminant analysis used fMRI data to predict whether or not students were engaged in problem solving. A hidden Markov algorithm merged these two sources of information to predict the mental states of students during problem-solving episodes. The algorithm was trained on data from one day of interaction and tested with data from a later day. In terms of predicting what state a student was in during any 2 second period, the algorithm achieved 87% accuracy on the training data and 83% accuracy on the test data. Further, the prediction accuracy using combined cognitive model and fMRI signal showed superadditivity of accuracies when using either cognitive model or fMRI signal alone.</swrc:abstract>
		<led:body><![CDATA[ 1. Given that instruction must be made available in real time, inferences about mental state can only use data up to the current point in time. While inferences of mental state may become clearer after observing subsequent student behavior, these later data are unavailable for real-time prediction. 2. Model tracing algorithms are parameterized with pilot data and then used to predict the mental state of students in learning situations. Therefore, we trained our algorithm on one set of data and tested it on a later set. While many distinctions can be made about mental states during the tutor interactions, we focused on two basic distinctions as a first assessment of the feasibility of the approach. The first distinction involved identifying periods of time when students were engaged in mathematical problem solving and periods of time when they were not. The second, more refined, distinction involved identifying what problem they were solving when they were engaged and, further, where they were in the solution of that problem. While one might think only the latter goal would be of instructional interest, detecting when students are engaged or disengaged during algebraic problem solving is by no means unimportant. A number of immediate applications exist for accurate diagnosis of student engagement. For instance, there are often long periods when students do not perform any action with the computer. It would be useful to know whether the student was engaged in the mathematical problem solving during such periods or was off task. If the student was engaged in algebraic problem solving despite lack of explicit progress the tutor might volunteer help. On the other hand, if the student was not engaged, the tutoring system might nudge the student to go back on task. The research reported here used an experimental tutoring system described in Anderson (2) and Brunstein et al. (5) that teaches a complete curriculum for solving linear equations based on the classic algebra text of Foerster (8). 
 Figure 1  Interface for equation solving isomorph.  (a) The student starts out in a state with a data-flow equivalent of the equation x – 10 = 17. The student uses the mouse to select this equation and chooses the operation “Invert” from the menu. (b) A keypad comes up into which the student enters the result 17+10. (c) The transformation is complete. (d) The previous state (data-flow equivalent of x = 17+10) is repeated and the student selects 17+10 and chooses the operation “Evaluate”. (e) A keypad comes up into which the student will type 27. (f) The evaluation is complete. 
 The tutoring system has a minimalist design to facilitate experimental control and detailed data collection. It presents instruction, provides help when requested, and flags errors during problem solving. In addition to teaching linear equations to children, this system can be used to teach rules for transforming data-flow graphs that are isomorphic to linear equations. The data-flow system has been used to study learning with either children or adults and has the virtue of not interfering with instruction or knowledge of algebra. The experiment reported here uses this data-flow isomorph with an adult population. Figure 1 illustrates sequences of tutor interaction during a problem isomorphic to the simple linear equation x – 10 = 17. The interactions with the system are done with a mouse that selects parts of the problem to operate on, actions from a menu, and enters values from a displayed keypad. 
 2 Experiment.
 Twelve students went through a full curriculum based on the sections in the Foerster text for transforming and solving linear equations. The experiment spanned six days. On Day 0, students practiced evaluation and familiarized themselves with the interface. On Day 1, three critical sections were completed with functional magnetic resonance imaging (fMRI). On Days 2-4 more complex material was practiced outside of the fMRI scanner. On Day 5 the three critical sections (with new problems) were repeated, again in the fMRI scanner. Each section on Days 1 and 5 involved 3 blocks during which they would solve 4 to 8 problems from the section. Some of the problems involved a single transformation-evaluation pair as in Figure 1 and others involved 2 pairs (problems studied on Days 2-4 could involve many more operations). Periods of enforced off-task time were created by inserting a 1-back task (17) after both transformation and evaluation steps. A total of 104 imaging blocks were collected on Day 1 and 106 were collected on Day 5 from the same 12 students. Average time for completion of a block was 207 2- second scans with a range from 110 to 349 scans. The duration was determined both by the number and difficulty of the problems in a block and by the students’ speed. Students solved 654 problems on Day 1 and 664 on Day 5. 76% of the problems on both days were solved with a perfect sequence of clicks. Most of the errors appeared to reflect interface slips and calculation errors rather than misconceptions. Each problem involved one or more of the following types of intervals: 
 1.  Transformation (steps a-c in Figure 1): On Day 1 students averaged 8.2 scans with a standard deviation of 5.9 scans. On Day 5 the mean duration was 5.9 scans with a standard deviation of 4.1. 2.  1-back within a problem: This was controlled by the software and was always 6 scans. 3.  Evaluation (steps d-f in Figure 1): Students took a mean of 4.9 scans on Day 1 with a standard deviation of 3.6; they took 3.8 scans on Day 5 with a standard deviation of 2.7. 4.  Between Problem Transition: This involved 6 scans of 1-back, a variable interval determined by how long it took students to click a button saying they were done, and 2 scans of a fixation cross before the next problem. This averaged 9.1 scans with a standard deviation of 1.5 scans on both days. 
 In addition there were 2 scans of a fixation cross before the first problem in a block and a number of scans at the end which included a final 1-back but also a highly variable period of 6 to 62 scans before the scanner stopped. The mean of this end period was 11.0 scans and the standard deviation was 6.5 scans. The student-controlled intervals 1 and 3 show a considerable range, varying from a minimum of 1 scan to a maximum of 54 scans. Anderson (2) and Anderson et al. (3) describe a cognitive model that explains much of this variance. For the current purpose of showing how to integrate a cognitive model and fMRI data, the complexity of that model would distract from the basic points. Therefore, we instead adapt a keystroke model (6) based on the fact that cognitive complexity is often correlated with complexity in terms of physical actions. Such models can miss variability that is due to more complex factors, but counting physical actions is often a good predictor. We will use number of mouse clicks as our measure of complexity. As an example of the range in mouse clicks – it takes 15 clicks in the tutor interface to accomplish the following transformation 
 FORMULA_1.
 but only 5 clicks to accomplish the evaluation:.
 FORMULA_2.
 Transformation steps take longer than evaluation steps because they require more clicks (average 10.4 clicks versus 6.8). Figure 2 illustrates the systematic relationship that exists between mouse clicks required to accomplish an operation and the time that the operation took. The average scans per mouse click decreases from .77 scans on Day 1 to .57 on Day 5. On the other hand the average ratio shows little difference between transformations (.69 scans) and evaluations (.65 scans) and so Figure 2 is averaged over transformations and evaluations. As the figure illustrates, the number of scans for a given number of mouse clicks is approximately distributed as a log-normal distribution. Log-normal distributions estimated from Day 1 were part of the algorithm for identifying mental state. The only adjustment for Day 5 was to speed up the mean of the distribution by a constant 0.7 factor (based on Anderson (2), model in that volume figure 5.7) to reflect learning. Thus, the prediction for Day 5 is .77*.7 = .54 scans per click. 
 2.1 Imaging Data.
 Anderson et al. (3) describe an effort to relate fMRI activity in predefined brain regions to a cognitive model for this task. However, as with the latency data, the approach here makes minimal theoretical assumptions. We defined 408 regions of interest (ROIs), each approximately a cube with sides of 1.3 cm that cover the entire brain. For each scan for each region, we calculated the percent change in the fMRI signal for that scan from a baseline defined as the average magnitude of all the preceding scans in that block. We used this signal to identify On periods when a student was engaged in problem solving (evaluation and transformation in Figure 1) versus Off periods when the student was engaged in n-back or other beginning and ending activities. 
 Figure 2. (a) and (c): The relationship between number of clicks and duration of problem solving in terms of number of 2-sec scans. (b) and (d): Distributions of number of scans for different numbers of clicks and log-normal distributions fitted to these. 
 A linear discriminant analysis was trained on the group data from Day 1 to classify the pattern of activity in the 408 regions as reflecting an On scan or an Off scan.Figure 3a shows how accuracy of classifying a target scan varied with the distance between the target scan and the scan whose activity was used to predict it. It plots a d-prime measure (9), which is calculated from the z-transforms of hit and false alarm rates. So, for instance, using the activity 2 scans after the target scan, 91% of the 7761 Day 5 On scans were correctly categorized and 16% of 11835 Off scans were false alarmed yielding a d-prime of 2.34. Figure 3 shows that best prediction is obtained using activity 2 scans or 4 seconds after the target scan. Such a lag is to be expected given the 4-5 second delay in the hemodynamic response. The d-prime measure never goes down to zero reflecting the residual statistical structure in the data. 
 Figure 3. (a) Accuracy of classification as a function of the offset between the scan whose activity is being used and the scan whose state is being predicted. (b) Distribution of fMRI signal changes  for Day 1 and Day 5 On and Off scans using an offset of 2. All 408 regions are used. 
 While we will report on the results using a lag of 0, the main application will use the optimal lag 2 results – meaning it was 4 seconds behind the student.   Little loss occurs in d-prime going from training data to predicted data. The relatively large number of scans (21,826 on Day 1 and 19,596 on Day 5) avoids overfitting with even 408 regions. While our goal is to go from Day 1 to Day 5, the results are almost identical if we use Day 5 for training and Day 1 for testing. The weights estimated for the 408 regions can be normalized (to have a sum of squares of 1) and used to extract an aggregate signal from the brain. This is shown in Figure 3b for the On and Off scans on the two days. 
 2.2  Predicting Student State. 
 Predicting whether a student is engaged in problem solving is a long way from predicting what the student is actually thinking. As a first step to this we took up the challenge of determining which problem a student was working on in a block and where a student was in the problem. This amounts to predicting what equation the student is looking at. Figure 4.1 illustrates an example from a student working on a set of 5 equations. As the figure illustrates, each equation goes through 4 forms on the way to the solution: the first and third require transformation operations while the second and fourth require evaluation operations (see Figure 1). Adding in the 21 Off states between forms there are 41 states. Consider the task of predicting the student state on scan 200. Information available to the algorithm includes the 5 problems, the distributions of lengths for the various states, and that there are 41 states in all. The classifier additionally provides the probability that each of scans 1-200 came from an On state or an Off state. The algorithm must integrate this knowledge into a prediction about what state, from 1 to 41, the student is in at scan 200. A key concept is an interpretation. An interpretation assigns the m scans to some sequence of the states 1, 2, …, r with the constraint that this is a monotonic non- decreasing sequence beginning with 1. For example, assigning 10 scans each to the states 1 to 20 would be one interpretation of the first 200 scans in Figure 4.1. Using the naïve Bayes rule, the probability of any such interpretation, I, can be calculated as the product of prior probability determined by the interval lengths and the conditional probabilities of the fMRI signals given the assignment of scans to On and Off states: 
 FORMULA_3.
 The first term in the product is the prior probability and the product in the second term is the conditional probability. The terms pk(ak) in the prior probability are the probabilities that the kth interval is of length k and Sr(ar) is the probability the rth interval surviving at least as long as ar. These can be determined from Figure 2 for On intervals and from the experimental software for Off intervals. The second term contains p(fMRIj|I), which are the probabilities for the combined fMRI signal on scan j+2 given I’s assignment of scan j to an On or a Off state. The linear classifier determines these from normal distributions fitted to the curves in Figure 3b. Since the states are not directly observable and their durations are variable our model is technically a hidden semi-Markov process (16). To calculate the probability that a student is in state r on any scan m one needs to sum the probabilities of all interpretations of length m that end in state r. This can be efficiently calculated by a variation of the forward algorithm associated with hidden Markov models (HMMs, 19) . The predicted state is the highest probability state. The most common HMM algorithm is the Viterbi algorithm, a dynamic programming algorithm that requires knowing the end of the event sequence to constrain interpretations of the events. The algorithm we use is an extension of the forward algorithm associated with HMMs and does not require knowledge of the end of the event sequence. As such it can be used in real time and is simpler.  Figure 4.1 illustrates the performance of this algorithm on a block of problems solved by the first student. Figure 4.1a shows the 20 forms of the 5 equations. Starting in an Off state, going through 20 On states, and ending in an Off state, the student goes through 41 states. Figure 4.1b illustrates in maroon the scans on which the algorithm predicts that the student is engaged on a particular equation form. Predictions are incorrect on 19 of the 241 scans but never off by more than 1 state. In 18 of these cases it is one scan late in predicting the state change and in 1 case it is one scan too early. Going beyond showing 1 student during 1 block, Figure 4.2 shows the average performance over the 104 blocks on Day 1 and the 106 blocks on Day 5. 
 Figure 4. (4.1) An example of an experimental block and its interpretations. The sequence of equations is shown in column a. Columns b, c, and d compares attempts at predicting the states with both fMRI and model, just fMRI, or just model. On scans (when an equation is on the screen) are to the left and Off times (when no equation is on the screen) are to the right. (4.2) Performance, measured as the distance between the actual state and the predicted state, using both cognitive model and fMRI, just fMRI, or just a cognitive model on (a) Day and (b) Day 5. 
 Performance is measured in terms of the distance between the actual and predicted states in the linear sequence of states in a block. A difference of 0 indicates that the algorithm correctly predicted the state of the scan, negative values are predicting the state too early, and positive values are predicting the state too late. The performance of the algorithm is given in the curve labeled “Both”. On Day 1 it correctly identifies 86.6% of the 22138 scans and is within 1 state (usually meaning the same problem) on 94.4% of the scans. Since all parameters are estimated on Day 1, the performance on Day 5 represents true prediction: It correctly identifies 83.4% of the 19914 scans on Day 5 and is within 1 state on 92.5% of the scans. To provide some comparisons, Figure 4.2 shows how well the algorithm could do given only the simple behavioral model or only the fMRI signal. The fMRI-only algorithm ignores the information relating mouse clicks to duration and sets the probability of all lengths of intervals to be equal. In this case, the algorithm tends to keep assigning scans to the current state until a signal comes in that is more probable from the other state. This algorithm gets 43.9% of the Day 1 scans and 30.6% of the Day 5 scans. It is within 1 scan on 51.8% of the Day 1 scans and 37.3% of the Day 5 scans. 
 Figure 4.1c illustrates typical behavior -- it tends to miss pairs of states. This leads to the jagged functions in Figure 4.2 with rises for each even offset above 0. The model-only algorithm ignored the fMRI data and set the probability of all signals in all states to be equal. Figure 4.1d illustrates typical behavior. It starts out relatively in sync but becomes more and more off and erratic over time. It is correct on 21.9% of the Day 1 scans and 50.4% of the Day 2 scans. It is within 1 scan on 32.9% of the Day 1 scans and 56.9% of the Day 5 scans. The performances of the fMRI-only and model-only methods are quite dismal. Successful performance requires knowledge of the probabilities of both different interval lengths and different fMRI signals. 
 Conclusions.
 The current research attempted to hold true to two realities of tutor-based approaches to instruction. First, the model-tracing algorithm must be parameterized on the basis of pilot data and then be applied in a later situation. In the current work, the algorithm were parameterized with an early data set and tested on a later data set. Second, the model- tracing algorithm must provide actionable diagnosis in real time – it cannot wait until all the data are in before delivering its diagnosis. In our case, the algorithm provided diagnosis about the student’s mental state in almost real time with a 4 second lag. Knowledge tracing, which uses diagnosis of current student problem solving to choose later problems, does not have to act in real time and can wait until the end of the problem sequence to diagnose student states during the sequence. In this case one could also use the Viterbi algorithm for HMMs (19) that takes advantage of the knowledge of the end of the sequence to achieve higher accuracy.  On this data set the Viterbi algorithm is able to achieve 94.1% accuracy on Day 1 and 88.5% accuracy on Day 2.  Morever, prediction accuracy using both information sources was substantially greater than using either data source alone.  A Bayesian analysis can explain the basis of the apparent superadditivity of prediction accuracy when using the combined information sources. The odds of a scan being On given the model and the fMRI signal can be expressed: Odds(On | Model & Signal) = Odds(On | Model)* Likelihood-ratio(Signal| On & Model) If (a)  Likelihood-ratio(Signal| On & Model) = Likelihood-ratio(Signal| On) -- that is, the signal magnitude depends only on whether the state is On, (b)  Odds(On) = 1 -- that is, that On scans and Off scans are equally frequent, which is approximately true, and therefore Likelihood-ratio(Signal| On)  = Odds(On| Signal), then the equation above can be rewritten Odds(On | Model & Signal) = Odds(On | Model)* Odds(On| Signal), or by inverting the odds Odds(Off | Model & Signal) = Odds(Off| Model)* Odds(Off| Signal). 
 These two equations show there is a multiplicative relationship in the Odds(Correct Acceptance) and Odds(Correct Rejection). Increasing either the strength of the signal or the strength of the model multiplies the effectiveness of the other factor This experiment has shown that it is possible to combine brain imaging data with a cognitive model to provide a fairly accurate diagnosis of where a student is in episodes that last as long as 10 minutes. Moreover, prediction accuracy using both information sources was substantially greater than using either source alone. The performance in Figure 4.2 is by no means the highest level of performance that could be achieved. Performance depends on how narrow the distributions of state durations are (Figures 2b and 2d) and the degree of separation between the signals from different states (Figure 3b). The model leading to the distributions of state durations was deliberately simple, being informed only by number of clicks and a general learning decrease of .7 from Day 1 to Day 5. More sophisticated student models like those in the cognitive tutors would allow us to track specific students and their difficulties leading to much tighter distributions of state durations. On the data side, improvement in brain imaging interpretation would lead to greater separation of signals. Finally, other data like eye movements could provide additional features for a multivariate pattern analysis.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Using Neural Imaging and Cognitive Modeling to Infer Mental States while Using an Intelligent Tutoring System</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-m-fincham"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-m-fincham"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/john-r-anderson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/john-r-anderson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/shawn-betts"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/shawn-betts"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jennifer-l-ferris"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jennifer-l-ferris"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/135/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-m-fincham"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/john-r-anderson"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/shawn-betts"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/jennifer-l-ferris"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Using multiple Dirichlet distributions to improve parameter plausibility</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136/authorlist"/>
		<swrc:abstract>Predictive accuracy and parameter plausibility are two major desired aspects for a student modeling approach. Knowledge tracing, the most commonly used approach, suffers from local maxima and multiple global maxima. Prior work has shown that using Dirichlet priors improves model parameter plausibility. However, the assumption that all knowledge components are from a single Dirichlet distribution is questionable. To address this problem, this paper presents an approach to integrate multiple distributions and Dirichlet priors. We show that modeling groups of students separately based on their distributional similarities produces model parameters that provide a more plausible picture of student knowledge, even though the proposed solution did not improve the model’s predictive accuracy. We also show Dirichlet priors might be hurt by outliers and models with trimming work better.</swrc:abstract>
		<led:body><![CDATA[ 1.1 Knowledge tracing model.
 Corbett and Anderson style knowledge tracing (KT) [3] has been successfully used in many tutoring systems to estimate a student’s knowledge of a skill.  It is based on a 2- state hidden Markov model where the student performance is observable, whereas his knowledge is latent. There are two parameters slip and guess, which mediate student knowledge and student performance. These two parameters are called the performance parameters in the model. The guess parameter represents the fact that the student may sometimes generate a correct response in spite of not knowing the correct skill. The slip parameter acknowledges that even students who understand a skill can make an occasional careless mistake. 
 In addition to the two performance parameters, there are two learning parameters. The first is prior knowledge (K0), the likelihood the student knows the skill when he first uses the tutor. The second learning parameter is learning, the probability a student will acquire a skill as a result of an opportunity to practice it. Every skill to be tracked has these four parameters, slip, guess, K0, and learning, associated with it. 
 1.2 The problem and proposed solution.
 How to estimate the model parameters is an important issue. There are a variety of model fitting approaches. The Expectation Maximization (EM) algorithm is one of the most commonly used methods. It finds parameters that maximize the data likelihood (i.e. the probability of observing the student performance data). Compared to other model fitting approaches for KT, using EM to learn the parameters has been found to achieve the highest predictive accuracy [4].  However, it still suffers two major problems that are inherent in the KT model’s search space: local maxima and multiple global maxima [2,5]. Local maxima are common in many error surfaces. The issue is that the algorithm has to start with some initial value of each parameter, and its final parameter estimates are sensitive to those initial values. The second difficulty, multiple global maxima, is known as identifiability and means that for the same model, given the same data, there are multiple (differing) sets of parameter values that fit the data equally well. Based on statistical methods, there is no way to differentiate which set of parameters is preferable to the others. Consequently, we have to be more careful to select the parameters’ initial values when using EM to fit the model, as we want to neither be stuck with some local maxima, nor get unbelievable parameters which are meaningless for making scientific claims, even if those parameters make accurate predictions. In order to solve the problems, in the previous work [2], we proposed that, rather than using a single fixed value to initialize the conditional probability table when training a knowledge tracing model, it is possible to use Dirichlet priors to start the algorithm. Briefly speaking, we assumed each parameter’s values are drawn from Dirichlet distribution, which is specified by a pair of numbers (α, β).  The two numbers specify not only the most likely value for a parameter, but also the confidence in the estimate. The Dirichlet priors, which usually represent the researchers’ prior beliefs, provide a reasonable starting point and bias the model-fitting process, thus decreasing the probability of ending with an implausible value. Modeling all skills using the same set of Dirichlet priors assumes that all knowledge components are drawn from a single Dirichlet distribution. That is to say, knowledge components are assumed to have distributional similarities with each other in terms of all four attributes, prior knowledge, guess, slip and learning. Therefore, Dirichlet priors provide bias to all skills towards the mean of the distribution, especially to those abnormal outlier skills. In general, outliers could arise due to lack of sufficient observations. Specifically, with sparse data, the model is trained with few constraints from the evidence; thus although it achieves the highest predictive accuracy it could get, still generates implausible parameter estimates. In this situation, we argue that it is preferable to have parameters which are more similar to the other, better-estimated, skills. As shown in Figure 1, Skill A and Skill B are at the tail of the distribution.  By using Dirichlets, those outliers are biased towards the mean of the distribution. The hypothesis is that it is probably good that they are moved towards the center. 
 Figure 1 Dirichlet distribution with two outliers. 
 Figure 2  Dirichlet with more “outliers”. 
 1.3 The problem with a single Dirichlet distribution.
 Dirichlets has been shown to work well on positively biasing outliers [2,5]. However, a key question was overlooked:  are the outliers really outliers? Since the assumption of using Dirichlets is that skills in the domain are from a single distribution, those skills which are located further away from the mean are considered outliers. However, is it really true that all skills are from the same distribution? As shown in Figure 2, which has the same distribution as the one in Figure 1, if there are additional skills, with similar parameter estimates to Skills A and B, perhaps they are not really outliers. A plausible hypothesis is that they are from a cluster of skills which behave differently, i.e. they were not drawn from the same Dirichlet distribution as the other skills. If so, then moving them towards the mean may be inappropriate as they are better modeled as a separate distribution. 
 2 Methodology.
 2.1 Clustering.
 We used clustering to discover which skills should be modeled separately with their own distribution. In the current context, a skill cluster is considered a region in the knowledge tracing parameter space where the skills share similar patterns with respect to the four knowledge tracing parameters. For example, possibly a group of skills might be described as “not previously known (low K0), but easy to learn (high learning)”, or “hard to learn, but students have partial incoming knowledge”.  The intuition is that the skills within a group are spatially located close to each other in the parameter space. We used K-means clustering to identify the skill clusters. We did not  use any self- adaptive clustering variants to automatically determine the number of clusters. The reason for this is that it is hard to evaluate the appropriate number of clusters, as our goal is to find the clusters that will result in good predictive accuracy and parameter plausibility when modeled as Dirichlets.  We had no a priori reason to believe that an automated clustering approach would optimize our metrics.  Therefore, we used iteration until the number of clusters that works best on an unseen test set was found (i.e. we observed overfitting beginning to occur). 
 2.2 Trimming the data to improve Dirichlet parameter estimation. 
 There are several approaches to setting Dirichlet prior values. One approach is using knowledge of the domain [e.g. 5]. If someone knows how quickly students tend to master a skill or the likelihood of knowing a skill, that knowledge can be used to set the priors. One problem with this approach is that it is not necessarily replicable, as different domains, subjects, and experts may give different answers. Therefore, following the methods in [2], we automatically derived the Dirichlet priors from the data. It is important to note, however, that automatically calculated Dirichlets are susceptible to undue influence from outliers.  Similar to calculating the arithmetic mean, outliers can distort the parameter estimates. In order to address this problem, we trimmed outliers from the data using two different approaches. The first approach was value-oriented, in which we processed the four knowledge tracing parameters separately and trimmed out the largest and smallest 5% of the values. For example, the 0.001 learning rate of the Pythagorean Theorem skill was in the lowest 5% and thus removed, while the more believable 0.45 prior knowledge estimate was not. The second approach was skill-oriented, in which we calculated the relative distance between a skill’s parameters and its cluster’s centroid. Those skills furthest away from the centroid were considered outliers, 10% of which we trimmed from the data used to calculate the Dirichlet priors. 
 2.3 Training with multiple Dirichlet distributions. 
 To compare the parameter plausibility and predictive accuracy of the fixed, single- Dirichlet prior and the multiple Dirichlet prior models, we trained a KT model on each of them using the following approach: 
 TrainWithMultiDrichlets (model, data) 1    [prior knowledge, guess, slip, learning] := EM (model, data, fixed prior[]); 2    for k :=1 to n 3 if (k !=1)  clusters[] := K-means ([prior knowledge, guess, slip, learning], k); 4 else cluster[1] := [prior knowledge, guess, slip, learning]; 5 for i := 1 to k 6        for each dimension d from [prior knowledge, guess, slip, learning] k,i 7              Dirichlet priors[] (α, β) := CalculateDirichlets(d); 8           [prior knowledge, guess, slip, learning]’ k,i := EM (model, data in cluster k,i, Dirichlet priors[]); 
 We trained a knowledge tracing model for each skill using the same set of fixed priors for EM initialization. After finding each skill’s parameter set (prior knowledge, guess, slip, learning), we calculated the priors for a single Dirichlet distribution and reestimated the KT model. For multiple Dirichlet distributions, we classified the parameter sets into k clusters. For each cluster, we calculated its own Dirichlet priors, and then used those to initialize the EM algorithm and reestimated the models. We didn’t specify an upper bound on the number of clusters (i.e. the value of n), as the number of clusters should depend on the improvement of predictive accuracy and parameter plausibility rather than the statistical properties of the clusters. 
 2.4 Data.
 For this study, we used data from ASSISTment, a web-based math tutoring system. The data are from 345 twelve- through fourteen- year old 8th grade students in urban school districts of the Northeast United States.  They were from four classes, each of which only lasted one month. These data consisted of 92,180 log records of ASSISTment during Dec. 2008 to Apr. 2009. Performance records of each student were logged across time slices for 105 skills (e.g. area of polygons, Venn diagram, division, etc). We took 20% of the students as the unseen test subjects. Their performance records are our test data. 
 3 Results.
 We used BNT-SM [7] to apply the EM algorithm to estimate the KT model’s parameters. Following the above procedure in Section 2.3, we trained several models to fit the test dataset. We compared the models focusing on the model’s predictive accuracy and parameter plausibility. 
 3.1 Predictive Accuracy.
 We measured the models’ predictive accuracy on the unseen test data set using two metrics: AUC (Area Under ROC Curve) and R2. In Table 1, for the three models with Dirichlets, the Dirichlet priors are calculated based on the trimmed data (since that gave slightly better results). We see the AUC values don’t show any difference among the first four models. The values remain unchanged even we considered the possibility that skills come from multiple distributions (shown in the third and fourth rows). R2 also didn’t show any meaningful differences. Since Ritter et al. have found that predictive accuracy when using the cluster centers is not much worse than when each individual skill’s parameters are used, we decided to see if that result will replicate on our data set.  Therefore, we evaluated the models using the cluster centers to predict the test data. The results in the last two rows of Table 1 showed that AUC values are similar to their counterparts, but R2 values are lower (0.053 vs. 0.071 and 0.056 vs. 0.072), suggesting that compared to the predictions done by the models with parameters estimated for each skill, using generic cluster information to fit the data achieves less accurate, but possibly still acceptable, predictions. These results show that predictive accuracy is not improved by using Dirichlets even with multiple distributions. Related to the prior work [4] where we evaluated the predictive accuracy of the knowledge tracing model using a variety of model fitting approaches, and also the Performance Factor Analysis model [8], it seems that improving the model’s predictive accuracy on the unseen students is a very difficult task. 
 Table 1. Comparison of AUC and R2. 
 3.2 Parameter plausibility.
 In addition to using models for prediction, educational researchers also expect model parameters to be able to provide meaningful interpretations. Therefore, parameter plausibility is another important aspect for evaluating models. However, quantifying parameter plausibility or goodness is non-trivial due to the lack of gold standards. In our study, we used the two metrics we explored in [2]. For the first metric, we inspected the number of practice opportunities required to master each skill in the domain. We assume that skills in the curriculum are designed to neither be so easy to be mastered in three or fewer opportunities nor too hard as to take more than 50 opportunities. We define mastery as the same way as was done for the mastery learning criterion in the LISP tutor [9]: students have mastered a skill if their estimated knowledge is greater than 0.95. Based on students’ prior knowledge and learning parameters, we calculated the number of practice opportunities required until the predicted knowledge exceeds 0.95. Then, we compared the number of skills with unreliable values in both cases (fewer than 3 and more than 50). As seen in Table 2, the results might not be consistent in the two conditions. Fixed priors results in more skills with too fast mastery rate, whereas the other three models produce 5-6 more skills mastered too quickly. It is worth pointing out that the skills found to be slowly mastered by the fixed model is a subset of those found by the other three models. Furthermore, the skills with low mastery rates found by the three Dirichlet models have high overlap. One possibility is the skills really are learnt that slowly. For example, if the students lacked preparation, they are unlikely to learn just through an ITS.  All of the skills that required more than 50 opportunities to master were from the same distribution in the 2- distribution model.  That distribution with “unlearnable” skills has the parameter estimates of 0.5, 0.36, 0.22 and 0.08 for prior knowledge, guess, slip and learning, respectively. Compared to the learning rate of the other distribution, 0.36, the skills are captured as ones that students have difficulties to learn, thus the mastery rates are very slow. Interestingly, in the 3-distribution model, the “unlearneable” skills are from two distributions. One has higher prior knowledge, 0.62, but lower learning rates, 0.07. The other has lower prior knowledge, 0.39, but normal learning rate, 0.11. We know that both cases could result in a slow mastery progress. Therefore, although the numbers seems to suggest those skills are poorly-estimated, if there really are skills students don't learn, the models are better at finding them due to clustering. 
 Table 2. Comparison of extreme number of practice until mastery. 
 We also tried to evaluate parameter values directly by calculating the correlation between a skill’s estimated prior knowledge and the grade at which that skill was taught. We assumed that the earlier the students learned the skills, the higher their incoming knowledge would be. However, we found our data suffer a severe problem that most items require multiple skills to answer, especially skills learned in earlier grades. Consequently, it confounds the relationship between the estimated prior knowledge and the grade at which the corresponding skill was taught, thus this approach was not viable. Therefore, we still followed the technique in [2]: using external measurement to evaluate parameter plausibility. The students in our study had taken a 33-item algebra pre-test before using ASSISTment. Taking the pre-test as external measure of incoming knowledge, we calculated the correlation between the students’ prior knowledge estimated by the models and their pretest scores. In other to acquire the student’s K0 parameter, we used KT to model the students instead of skills (see [2] for details). 
 Table 3 shows four interesting results. The first and the most important one is that more Dirichlet distributions generally result in higher plausibility (shown in the second row). The correlation values of 0.88 and above are significantly higher than the baseline value 0.83 from the fixed prior model with p-values < 0.05.   In the 7-distribtuion model, the value drops to 0.83. It suggests classifying students in a fine-grained level provides the models more confidence about the distributions where the data are from, thus taking the extra information specified by the Dirichlet priors, the models converge at more believable points. The second result is that we found the evidence of Dirichlet is hurt by outliers. As seen in the first column, the Dirichlet model produces lower correlation (0.80 vs. 0.83) compared to the fixed prior model. However, the Dirichlet model with trimming equals the fixed prior model, indicating the necessity of trimming for Dirichlets. However, the advantage from trimming decreases as the number of cluster increases, until eventually the untrimmed Dirichlet has better performance.  Thus, the power from trimming is reduced as presumably the higher similarity of the students in a distribution reduced the problem of outliers. The third result is the hypothesis that there is an interaction effect between using more distributions and using Dirichlets. To confirm that higher plausibility is not simply an result of having additional distributions, we set each distribution’s mean values as the fixed priors to train the models (first row of Table 3). We see that fixed prior models performance is independent of the number of distributions (except for possible overfitting with 6 clusters).  Thus, the improvement from multiple Dirichlet distributions is not an artifact of multiple distributions necessarily resulting in better performance. The fourth result is shown at the last row of Table 3 where we used cluster centers to represent the individual student’s prior knowledge. This approach achieves surprisingly high plausibility. With more distributions, it even outperforms the fixed prior models in spite of requiring less computation. 
 Table 3 Comparison of correlation between prior knowledge and pretest, by number of clusters. 
 4 Contributions.
 This paper presents a new approach for strengthening the fundamental assumption of the usage of Dirichlet priors in order to improve the knowledge tracing model’s predictive accuracy and parameter plausibility. Although Dirichlets are a solution to the problem of parameter plausibility, the assumption that all skills are from a single distribution is troubling. Rather than modeling skills as a single homogenous group, we acknowledge that similar skills should be modeled similarly. We used clustering techniques to identify groups of similar skills, and then modeled those groups with their own, independent Dirichlet priors. In spite of using multiple Dirichlet distributions, we failed to find any improvement in predictive accuracy, which is consistent with the results in our previous work of investigating a single Dirichlet distribution. However, we confirmed that using distribution centers to fit the data isn’t much worse than using the skill’s individual parameter estimates [6]. For parameter plausibility of modeling skills, it appears using Dirichlets does not produce a more believable mastery rate, even when using multiple distributions. It is worth pointing out that if there really are skills that students don't learn, the Dirichlet approach is better at finding them. We also showed that using multiple Dirichlet distributions to model students results in high plausibility of the students’ knowledge parameters.  With multiple Dirichlet distributions (6 clusters), the correlation between the model’s parameter estimates and the external standards reaches 0.9. We also showed that using the cluster centers, rather than individual student estimates, generates plausible results too, but with less computational work. We found that Dirichlets are likely to be hurt by outliers, both with respect to predictive accuracy and parameter plausibility.  For predictive accuracy, the models with trimming perform comparable or even better than not using trimming.  For the student knowledge parameter plausibility, trimming resulted in stronger results, except when six clusters were used.  To understand this reversal requires additional experimentation. Finally, our intuition that modeling a distribution as a single Dirichlet could be hurt since the “outliers” are the skills which are drawn from a different distribution has been partially supported by the results. 
 5 Future work and Conclusions.
 There are several unsolved problems related to this work. First of all, predictive accuracy is strongly desired in most student modeling applications. We have tried various approaches to improve accuracy in the knowledge tracing framework. However, we have found that there are no quick wins [2, 4, 5, 8]. We think perhaps only relying on the KT model with the basic structure might not be sophisticated enough to account for the substantial variability in student problem-solving efforts.  One line of research is to consider integrating other useful information with KT, as it makes sense to be aware of other variables that might affect student performance such as question difficulty and student engagement.  By accounting for other sources of variance, it enables us to better estimate the student’s knowledge and (hopefully) consequently have a higher predictive accuracy and estimate more plausible parameters. Second, considering the existence of multiple distributions seems reasonable and using multiple Dirichlet distributions is found to be beneficial in improving parameter plausibility. Dirichlet priors work fine in parameter plausibility on the student models, but don’t have apparent benefit for skill models. It is an important task to understand how to overcome this issue, or even determine if it is a problem at all.  At present, we lack the strong domain-driven parameter plausibility metric that was used in the initial work with Dirichlets for reading [5]. Determining better metrics for the domain of mathematics, or even better domain-independent metrics is a high priority.  Human-generated Dirichlets might be a solution, as the single attempt [5] did result in more plausible parameters. Again, if we had more powerful parameter evaluation metrics we could better determine whether using human knowledge is a promising direction.  It is interesting to see the outcomes from using other techniques to identify the distributions, such as latent Dirichlet allocation (LDA [10]), which is a generative model that allows sets of observations to be explained by unobserved groups. In this context, skills can be considered from several unobserved groups and each of them can be represented by a Dirichlet distribution. Thus, LDA is a promising technique rather than using clustering. There is a limitation in this work. We took the benefit of looking at the test dataset for determining the number of clusters where the models result in the best performance. However, a better way that would be conducted in the future work is to use a tuning dataset besides the training and test datasets. This approach would enable us to tweak our models based on the models’ performances on the tuning data, and then validate our models on the test data. This paper has explored the idea of integrating multiple Dirichlet distributions with the knowledge tracing model. In terms of predictive accuracy, we failed to find any improvement contributed by the proposed technique.  This work provides some additional support that using the using cluster centers is a reasonable approach. We found that, with multiple Dirichlet distributions, student knowledge parameters achieved high plausibility, even when using cluster centers to represent student knowledge. We have also found Dirichlet priors could be hurt by outliers, and found that first trimming the data before Dirichlet parameter estimations usually gives better performance. 
 Acknowledgements.
 This research was made possible by the US Dept. of Education, Institute of Education Science, “Effective Mathematics Education Research” program grant #R305A070440, NSF CAREER award to Neil Heffernan, the Spencer Foundation, and a Weidenmeyer Fellowship from WPI.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Using multiple Dirichlet distributions to improve parameter plausibility</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/137">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>A Review of Student Churn in the Light of Theories on Business Relationships</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/137/authorlist"/>
		<swrc:abstract>The goal of this review is to use business theories in student retention research, which has so far been informed by economics, organizational behavior, psychology, sociology. Relationships in business networks are compared to these between students and universities, putting forth relevant characteristics for student churn/retention research. Theories regarding a taxonomy of customer churn, its determinants and consequences are also viewed in this context and implications for educational data mining (DM) are put forth. The relationship between entities is traditionally either following a strict hierarchical fiat (HF) – if the parties belong to the same organization –, or it’s essentially an arm’s length transaction (ALT) – if there is a market relationship between the entities. Recent business theories describe a more nuanced reality. In the light of wider changes in research, today’s corporations are found to be heterarchical; other authors speak of autonomous strategic initiatives that take place in corporations violating the principles of hierarchy. The relationships between companies are neither of ALT type. The works of Uppsala school [1] show that not only is a corporation essentially a network of units (as is elsewhere described both from multinational corporation (MNC) and its subsidiary perspective), but it is also embedded in a business network of its own. The picture is further complicated by the individualism-collectivism dimension. Big MNCs are comprised of internal markets (within one firm!) with an ongoing internal competition for world product mandates, centers-of-excellence, etc. between the subsidiary units; whereas the supply chain relationships that a firm belongs to, have been described as coevolving systems.  Concentrating our attention on the relationships in business networks, we see that two firms gradually increase their commitment, as they do business with each other. A process of learning about each other’s capabilities, needs and strategies takes place, as well as a formation of routines for undertaking transactions. Sides adapt to each other incrementally. Knowledge transfer is inherently present – with organizational learning taking place – the results being often tacit and intangible. The relationship between a student and the university varies on the HF-ALT dimension. A student can simply purchase single classes from the Open University; one may be a full time student with an opportunity to call the university his alma mater after graduation; within the university’s administrative framework, the studies can also in part be paid for by giving consults to one’s peers; during the post graduate studies becoming a teaching assistant and teaching simultaneously with the studies is even more common; and finally – it is a goal of universities to populate the ranks of its faculties with the best graduates, in which case the student would administratively become a part of the organization. Entering studenthood comprises of overcoming various entry barriers. The curriculum is substantially different from that of a high school and the university studies are qualitatively harder as well – as the amount of independent work is greater, the tempo in the classes faster and as in some universities general courses can be amongst the most difficult in the undergraduate curricula. At the same students have to learn scheduling, budgeting, develop their EQ and career. The steep entry barriers underline the commitment it takes to enter the university relationship. Therefore sunk costs are formed, which are reflected in the fact that student churn lessens considerably later on. The mutual commitment and adaption is evident in the following: the student will be able to pursue further career goals after graduation; the student becomes familiar with the university life, procedures and administrative system; the youth helps to keep the university abreast of times; the university assesses its employees based on student feedback. And perhaps most importantly – the students adapt to the university and their academic mentor’s profile. For both sides knowledge transfer and (organizational) learning ensue and as ties with the industry are created, so are the intangible assets.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>A Review of Student Churn in the Light of Theories on Business Relationships</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jaan-ubi"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jaan-ubi"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/innar-liiv"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/innar-liiv"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/137/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/jaan-ubi"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/innar-liiv"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Tracking Students’ Inquiry Paths through Student Transition Analysis</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[ INTRODUCTION.
 In the Science Assistment project (http://users.wpi.edu/~sci_assistments/) we provide students with rich and dynamic opportunities to engage in inquiry while researchers and teachers use performance data as assessment indices of scientific inquiry skills and domain knowledge. Activities in the Science Assistment Project follow a pedagogical model known as the inquiry cycle, namely, explore, hypothesize, experiment, and analyze. Figure 1 shows this cycle as a graph and the transitions that can be made between states. State “Comp” represents completion of the activity. 
 Figure.

 1. The Inquiry process as modeled by our activity. 
 Important to researchers and developers are ways to assess students’ inquiry process within microworlds, as well as a rigorous method for providing scaffolding to students who struggle with inquiry during the activity. The method presented here utilizes a Markov chain to track a student’s path through the various inquiry activities. The data were collected from 148 eighth grade students, ranging in age from 12- 14 years, from a public middle school in Central Massachusetts. Students engaged in inquiry using our learning environment including a microworld simulating the phase changes of water and a series of inquiry support tools for making a hypothesis, collecting data, and analyzing data. Based on the logging features of these microworlds, we track the path students take through the activity by counting the transitions students make and create a Markov model of their inquiry process (Figure 1). This potentially affords researchers a way of grouping students based on their path through the activity. The rationale for this is that some paths are more effective representing systematic inquiry [1] and, in turn, lead to positive learning gains while other inquiry paths do not. Using this Markov model, researchers hope to eventually be able to develop scaffolds for students whose path corresponds to less effective inquiry. However, in order to do this we need have models that can diagnose a students’ inquiry path. The current work seeks to develop such a student model. The first attempt at breaking up different groups of students used the scores students obtained on the multiple choice pretest: those in the top third on the pre-test, and those in the bottom third. The middle third was not used for this analysis. Based on these two groups of students, we generated two transition models. To validate these models we computed the log likelihood of each model given each student. A paired samples t-test on the log likelihoods of the intended models (the model generated from the group which the student belonged) versus the unintended models (a model generated from students from a different group) found the models to be distinct (t(98) = 3.385, p < .001). Using a sign test (log likelihood of intended model - log likelihood of unintended model) for each student, we found that the intended model had a higher log likelihood 68% of the time. Although this disaggregation was somewhat successful, 68% accuracy in terms of classification is not a great improvement over a baseline of 50% generated by random guessing. For our next attempt, rather than picking some measure and using that to disaggregate the models, we looked at the models themselves and attempted to disaggregate them by applying a K-means cluster analysis using Weka (version 3.6.2), which generated two  models each of which was based on a cluster. The major difference between the two models is that students in model 1 tended to return to the observe state of the inquiry cycle (see highlighted portion of Figure 1). The next steps going forward are to try to use our models along with other learner characteristics we have about our participants to predict what cluster the students fall into. We also need to find if the clusters fit into categories of high and low performance in the activity.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Tracking Students’ Inquiry Paths through Student Transition Analysis</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/matt-bachmann"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/matt-bachmann"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/matt-bachmann"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Using Text Replay Tagging to Produce Detectors of Systematic Experimentation Behavior Patterns</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139/authorlist"/>
		<swrc:abstract>We present machine-learned models that detect two forms of middle school students’ systematic data collection behavior, designing controlled experiments and testing the stated hypothesis, within a virtual phase change inquiry learning environment. To generate these models, we manually coded a proportion of the student activity sequence clips using “text replay tagging” of log files, an extension of the text replay method presented in Baker, Corbett and Wagner (2006). We found that feature sets based on cumulative attributes, attributes computed over all predecessor clips, yielded better detectors of CVS- compliant and hypothesis-testing behavior than more local representations of student behavior. Furthermore, our detectors classify behaviors well enough to use them in our learning environment to determine which students require scaffolding on these skills.</swrc:abstract>
		<led:body><![CDATA[ 1. Hypothesizing widget (left) and data collection panel (right) for the phase change microworld. 
 This learning environment has a moderate degree of learner control, less than in purely exploratory learning environments [1], but more than in classic model-tracing tutors [16] or constraint-based tutors [18]. Though our scaffolding restricts when students can switch inquiry phases, there is enough freedom such that students could approach these inquiry tasks in many ways. For example, a student could choose to specify only one hypothesis like, “If I change the container size so that increases, the melting point stays the same” (left side of Figure 1), and then test that single hypothesis. Alternately, they could generate several and test them all sequentially. While experimenting (right side of Figure 1), a student could set up and run as many different experiments as they desired, including repeating the same trial multiple times. A table tool was provided within the learning environment to display the results of the student’s previous experiments and to display their hypothesis list to determine which experiments to run next. As students engage in inquiry using our tools and microworld, they can exhibit several different inquiry behaviors. Students acting in a systematic manner [7] collect data by designing and running controlled experiments that test their hypotheses. Also, students acting systematically use the table tool and hypothesis viewer in order to reflect and plan for additional experiments. Students who are unsystematic, by contrast, may exhibit haphazard behaviors such as: constructing experiments that do not test their hypotheses, not collecting enough data to support or refute their hypotheses, not following CVS, running the same experimental setup multiple times, or failing to use the inquiry support tools to analyze their results and plan additional trials [10]. 
 3 Dataset.
 Participants were 148 eighth grade students, ranging in age from 12-14 years, from a public middle school in Central Massachusetts. These students used the phase change microworld as part of a broader study to determine if inquiry skills learned in one domain will transfer to inquiry skill in other domains [13]. Students engaged in authentic inquiry problems using the phase change and density microworlds within the Science Assistments learning environment.  Students were randomly assigned to one of two conditions that counterbalanced the order in which students engaged in a science domain: phase change followed by density vs. density followed by phase change. In this paper, we discuss detectors of systematic data collection for student actions within the phase change microworld only, as the version of the density microworld used lacked the hypothesizing scaffold used in the phase change microworld. In building these detectors, we look specifically at what students did in the “hypothesizing” and “experimenting” phases of inquiry. As part of the phase change activities, students attempted to complete four tasks using our interactive tools. Each of these students completed at least one data collection activity in the phase change environment (two other students did not use the microworld, and were excluded from analysis). As students solved these tasks, we recorded fine-grained actions within the inquiry support tools and microworlds. The set of actions logged included creating hypotheses, setting up experiments, showing or hiding support tools, running experiments, creating interpretations of data, and transitioning between inquiry activities (i.e. moving from hypothesizing to data collection). Each action’s type, current and previous values (where applicable – for instance, a variable’s value), and timestamp were recorded. In all, 27,257 student actions for phase change were logged. These served as the basis for generating text replay clips consisting of contiguous sequences of actions specific to experimenting. 
 4 Text Replay Tagging Methodology.
 In designing our text replays, it was necessary to use a coarser grain-size than in prior versions of this method (e.g. [4, 6]).  In particular, it was necessary to show significant periods of experimentation so that coders could precisely evaluate experimentation behavior relative to stated hypotheses. We decided our text replays should include actions from only the hypothesis and the experimenting phases. Another important issue was that trial run data from one hypothesis test could be used in another hypothesis test to make inferences about the hypothesis at hand (i.e. comparing a current trial to one conducted earlier). To compensate for this, we code using both the actions in testing the current hypothesis, and cumulative measures that include actions performed when testing previous hypotheses.  Hence, each tagged clip focuses on actions in the current part of the inquiry process, but may take into account the context of the cumulative section. 
 Figure 2. Text Replay Tagging Tool with an example student’s clip coded as running repeated trials, following CVS, and testing stated hypotheses. 
 To support coding in this fashion, a new tool for text replay tagging was developed in Ruby, shown in Figure 2. The start of the clip is triggered by a hypothesis variable change after the beginning of a new problem.  The tool displays all student actions (hypothesis and experiment) until the student transitions to the analysis stage. Subsequent clips include previous clips and any single new cycle which includes the Hypothesis and Experiment stage. A clip could be tagged with one of 9 tags corresponding to data collection behaviors: “Never Change Variables”, “Repeat Trials”, “Non-Interpretable Action Sequence”, “Indecisiveness”, “Used CVS”, “Tested Hypothesis”, “Used Table to Plan”, “Used Hypothesis Viewer to Plan”, “No Activity”, and one extra category for unclassifiable clips, “Bad Data”, for a total of 10 coding categories. Specifically for the analyses in this paper, we tagged a clip as “Used CVS” if the clip contained actions indicative of designing and running controlled experiments. “Tested Hypothesis” was chosen if the clip had actions indicating attempts to test the stated hypotheses, regardless of whether or not proper CVS procedure was used. 
 4.1 Clip Tagging Procedure.
 Two coders (the first and fourth authors) tagged the data collection clips using at least one of the ten tags. To ensure that a representative range of student clips were coded, we stratified our sample of the clips on condition, student, problem, and within-problem clip order (e.g. first clip, second clip, etc.) The corpus of hand-coded clips contained exactly one randomly selected clip from each problem each student encountered, resulting in 571 clips. Each coder tagged the first 50 clips; the remaining clips were split between the coders. For the 50 clips tagged by each coder, there was high overall tagging agreement, average κ=0.86. Of particular relevance to this study, there was also better agreement on the CVS and testing hypotheses tags, κ=.69 and κ=1.00 respectively, than has been seen for previous text replay approaches that led to successful behavior detectors (e.g. [4, 6]). 
 4.2 Feature Distillation.
 Features were extracted relevant to the 10 categories of behavior within the microworld. These included: all actions, total trial runs, incomplete trial runs, complete trial runs, pauses, data table display, hypothesis list display, field changes in hypothesis builder (left side of Figure 1), hypotheses made, and microworld variable changes. For each category, we traced the number of times the action occurred and the time taken for each action. For timing values, we also computed the minimum, maximum, standard deviation, mean and mode for each student and compared these values relative to all other students. We also included the number of pairwise trials where only one independent variable differed between them and a count for repeated trials, trials with the same independent variable selections. These last two had no time associated with them. We extracted feature values from student actions as follows. As stated in Section 2, student microworld activity was divided into tasks, each focusing on a specific independent variable. Also, within a task, the student could make and test several hypotheses.  For each of the categories, we extracted data for each hypothesis the student tested (local data), and across all hypotheses in the set (cumulative data). We did this because within each set, the data table accumulated the trial run data across hypotheses, enabling students to compare trial runs testing previous hypotheses with the runs made in the current hypothesis. 
 4.3 Machine Learning Algorithms.
 Machine-learned detectors of the two behavioral patterns of interest, CVS and hypothesis testing, were developed within RapidMiner 4.6 [17]. Detectors were built using J48 decision trees, with automated pruning to control for over-fitting, the same technique used in [4, 20]. Before running the decision tree algorithm, we filtered redundant features correlated at or above 0.6. Six-fold cross-validation was conducted at the student level (e.g. detectors are trained on five groups of students and tested on a sixth group of students). By cross-validating at this level, we increase confidence that detectors will be accurate for new groups of students. We assessed the classifiers using two metrics. First, we used A’ [15]. A' is the probability that if the detector is comparing two clips, one involving the category of interest (CVS or Hypothesis Testing) and one not involving that category, it will correctly identify which clip is which. A' is equivalent to both the area under the ROC curve in signal detection theory, and to W, the Wilcoxon statistic [15]. A model with an A' of 0.5 performs at chance, and a model with an A' of 1.0 performs perfectly. In these analyses, A’ was used at the level of clips, rather than students. Statistical tests for A’ are not presented in this paper. The most appropriate statistical test for A’ in data across students is to calculate A’ and standard error for each student for each model, compare using Z tests, and then aggregate across students using Stouffer’s method (cf. [3]) – however, the standard error formula for A’ [15] requires multiple examples from each category for each student, which is infeasible in the small samples obtained for each student in our text replay tagging. Another possible method, ignoring student-level differences to increase example counts, biases undesirably in favor of statistical significance. Second, we used Kappa (κ), which assesses whether the detector identifies is better than chance at identifying the correct action sequences as involving the category of interest. A Kappa of 0 indicates that the detector performs at chance, and a Kappa of 1 indicates that the detector performs perfectly. As Kappa looks only at the final label, whereas A’ looks at the classifier’s degree of confidence, A’ can be more sensitive to uncertainty in classification than Kappa. 
 5 Results.
 We constructed and tested detectors using our corpus of hand-coded clips. The CVS and hypothesis testing detectors were constructed from a combination of the subset of the first 50 clips that the two coders agreed on, and the remaining clips, tagged separately by the two coders. Of all clips, 31.2% were tagged as showing evidence of CVS and 34.4% were tagged as showing evidence of collecting data to test specified hypotheses. Detectors were generated for each behavior using J48 decision trees and two sets of attributes, cumulative and non-cumulative attributes. As a reminder, non-cumulative attributes were tallied over a single clip, irrespective of other clips, whereas cumulative attributes included data from earlier clips from the same problem. Thus, four different detectors were constructed. The CVS detector using cumulative attributes (A’=.85, κ=.47) appeared to perform better than the detector built with non-cumulative attributes (A’=.81, κ=.42). Likewise, the hypothesis testing detector built with cumulative attributes (A’=.86, κ=.46) scored higher on our metrics than the non-cumulative detector (A’=.84, κ=.44). We believe the detectors built from cumulative attributes perform better because students may perform actions within particular clips that, when taken in conjunction with actions from previous clips, represent a more complete picture of student behavior. For example, while analyzing results a student may realize they need to run one more experiment to correctly test their hypothesis (which would start a new clip). The human coders would correctly label this as CVS and testing a hypothesis in reference to the previous context, but values for noncumulative attributes would most likely indicate that the student was not systematic because these attributes’ values are not computed based on previous clips. 
 6 Discussion and Conclusions.
 The goal of this research was to develop machine-learned models that can automatically detect if a student is systematic in their inquiry, particularly in their data collection actions, using text replay tagging. This work showed that combining text replay clip tagging of low-level student actions and machine learning can lead to the successful development of behavior detectors in an ill-defined domain such as scientific experimentation. This work also presents a contribution to the text replay process since it is more efficient to code a clip with multiple tags. Our results were promising; using cumulative attributes, we can distinguish students who are successfully applying the Control of Variables Strategy (CVS) in the phase change environment from students not applying CVS 85% of the time and can distinguish students testing their hypotheses 86% of the time. Furthermore, the Kappa values indicate that each of these detectors are substantially better than chance. In other words, though these detectors are not perfect, they can be used to select students for scaffolding. Since they are not perfect, some students may receive help when they do not need it and vice versa. Hence, interventions used should be fail-soft, relatively non-harmful when given incorrectly. As such, we aim to use these detectors to determine which students will receive scaffolding. An important area of future work will be to improve our detectors’ A’ and Kappa. To this end, we plan to add lesson-wide attributes, learner attributes, and data on the other tags used to critique a clip. Lesson-wide attributes, such as task attempt number, that can benchmark a students’ experience within our environment may aid in predicting systematicity, in coordination with other features. Additionally, rather than treating learner characteristics, such as prior knowledge, as external predictors of systematicity, we could incorporate those measurements into the detectors themselves. Similar to computing average attribute differences between clips (i.e. computing the difference in number of trials run for the given clip and the average number of trials run for all clips), we could compute differences between students with similar learner characteristics. Similarly, rather than using systematicity to predict content knowledge, we could incorporate student prior knowledge of content and inquiry using our standardized-test style questions [13]. Another important area of future work will be to generalize and train our detectors across different microworlds (cf. [5]) to increase their applicability across middle school science learning. This approach also enables us to research the interactions between content knowledge and authentic inquiry performance within our learning environments. Being able to classify students as systematic according to different skills, e.g. testing hypotheses and CVS, will enable us to determine if skill proficiency in solving authentic inquiry problems will predict skill proficiency in solving standardized test-style inquiry questions. We can also determine the degree to which systematic behavior predicts robust content knowledge. Finally, by developing and generalizing detectors across domains, we can determine the degree to which authentic inquiry skill transfers between domains. As such, these models have considerable potential to enable future “discovery with models” analyses that can shed light on the relationship between a student’s mastery of systematic experimentation strategies and their domain learning. 
 Acknowledgements.
 This research is funded by the National Science Foundation (NSF-DRL#0733286) and the U.S. Department of Education (R305A090170). Any opinions expressed are those of the authors and do not necessarily reflect those of the funding agencies.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Using Text Replay Tagging to Produce Detectors of Systematic Experimentation Behavior Patterns</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/orlando-montalvo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/orlando-montalvo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-nakama"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-nakama"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/orlando-montalvo"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-nakama"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/140">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>AutoJoin:  Generalizing an Example into an EDM query</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/140/authorlist"/>
		<swrc:abstract>This paper describes an implemented method to generalize an example tutor interaction into a query to retrieve similarly related sets of events. It infers WHERE clauses to equate repeated values unlikely to match by accident. The Session Browser [1] shown in Figure 1 is an EDM tool to view data retrieved by querying a database of events logged by a tutor.  It displays retrieved events in a context tree of enclosing events, with a 1-line summary of the database record for each event. Figure 1:  Event context tree highlighting two events selected by the user to AutoJoin This brief example occurred in an activity to teach children to ask themselves questions about the text they read.  The first highlighted event summarizes a child’s multiple-choice response to a prompt to fill in the rest of a question about the text.  The second event describes the child’s spoken response to a prompt to speak the completed question aloud. Often we want a query to retrieve examples similar to a current example.  Complex queries are hard to construct, so we developed AutoJoin to generate them automatically. AutoJoin generalizes the two highlighted events into a query that finds “similar” cases, in this case a multiple choice step immediately followed by a free-form response step:</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>AutoJoin:  Generalizing an Example into an EDM query</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/bao-hong-tan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/bao-hong-tan"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/140/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/bao-hong-tan"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/141">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Assessing Reviewers’ Performance Based on Mining Problem Localization in Peer-Review Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/141/authorlist"/>
		<swrc:abstract>Current peer-review software lacks intelligence for responding to students’ reviewing performance. As an example of an additional intelligent assessment component to such software, we propose an evaluation system that generates assessment on reviewers’ reviewing skills regarding the issue of problem localization. We take a data mining approach, using standard supervised machine learning to build classifiers based on attributes extracted from peer-review data via Natural Language Processing techniques. Our work successfully shows it is feasible to provide intelligent support for peer-review systems to assess students’ reviewing performance fully automatically.</swrc:abstract>
		<led:body><![CDATA[ 1. From now on, feedback will be used to refer to the 1405 annotated feedback idea-units. 
 Table 1.  Descriptive statistics of annotations on history peer-review feedback data. 
 In addition to the feedback idea-units, we also have access to the collection of 24 essays to which the feedback refers. These essays provide domain knowledge, and are a self- contained resource that will assist us in mining features from the peer-review feedback data using statistical NLP techniques. 
 4 System and Features for Classification.
 Before diving into details of feature extraction and model learning, we would like to first provide an overview of our system, which takes the annotated feedback provided by a single reviewer, identifies target features sequentially for each piece of feedback, and generates assessment on the reviewer’s reviewing performance with respect to problem localization in general (as the flow suggests in Figure 1). 
 Figure 1.  System overview.
 4.1 System overview.
 The system described in Figure 1 can be viewed as an assessment module compatible with peer-review software such as SWoRD. The system consists of two binary classifier components for identifying problem localization at the feedback-level (part A in Figure 1), plus an aggregation component (part B in Figure 1), which propagates the prediction of problem localization from the feedback-level up to the reviewer-level. In pilot work, adding either annotated or predicted feedbackType into the feature set significantly improved the model’s performance on identifying problem localization at the feedback-level. Therefore, we decompose the task into two concatenated tasks. We first use supervised learning to train a classifier for identifying criticism feedback versus praise and summary feedback; then we use the same algorithm to train another classifier for identifying whether problems are localized (pLocalization = true) for a given criticism feedback. Note that although both feedbackType and pLocalization were annotated with three values (Table 1), we combined values to create two binary classification tasks. Because it is the criticism feedback that is actionable, and focused on in the next step (classification for pLocalization), we group the praise and summary feedback together as non-criticism. As a byproduct, this binary separation also results in a more balanced data set from the perspective of machine learning. Similarly, recall that all non-criticism feedback in the data set was labeled N/A for pLocalization; we group N/A with false (vs. true), which simplifies our model for handling noisy output of the feedbackType identifier (specifically, any non-criticism predicted as criticism and sent to the second component). Since our goal is to generate assessment of reviewing performance for each reviewer, we add another aggregation step into the system after the two components mentioned above, in which we make a binary decision on whether the reviewer provides enough problem localization in their reviews in general.  This decision is based on the predictions made by the two preceding components on problem localization for all the feedback submitted by that reviewer. Since localization at the feedback-level is relatively difficult even for humans (recall Kappa=0.69), we expect to provide more accurate (and hence useful) feedback at the aggregate level. 
 4.2 Criticism Identifier.
 To identify criticism feedback, we develop 3 groups of attributes that are automatically derived from the surface of sentences. Simple: This set simply contains two attributes, the wordCount and the feedbackOrder in the review. WordCount is the number of words in the feedback; feedbackOrder is the index of the feedback with respect to its original review before segmentation. Based on our brief exploration of the data, we hypothesize that negative feedback is more likely to be verbose than positive feedback, and there is a certain pattern in expressing opinions, thus the feedbackOrder is useful in detecting criticism feedback. Essay: There are four attributes in this group capturing the topic information contained in the feedback. To build a domain dictionary, first we preprocess the collection of 24 essays into bigrams (two adjacent words) and unigrams (single word). In particular, using NLTK3 we extract bigrams whose term frequency–inverse document frequency (TF-IDF) is above the average TF-IDF of all bigram-collocations to form the bigram-domain dictionary. Then we gather all unigrams that constitute the bigrams in the bigram-domain dictionary for building our unigram-domain dictionary. Our final dictionary contains 291 bigrams and 402 unigrams. To capture how much content of the feedback is related to the domain, we count bigrams in the feedback that also belong to the bigram domain dictionary, and create the attribute Collocation_d. Similarly we create Collouni_d based on unigrams. Besides the domain-topics shared by all essays in general, we also considered essay-topics, referring to terms that are more frequently used in one specific essay rather than all of them. For each piece of feedback, we compute its bigrams and unigrams, and then only count (Collocation_e and Collouni_e) those that also appear in the associated essay with above-average item frequency of that essay. These four counts are normalized with the length of feedback. Keyword: Due to the expensive computational cost for building models based on all words in the feedback corpus, we semi-automatically learned a set of Keywords (Table 2) which has categories based on the semantic and syntactic function of the words.4 We first manually created a list of words that are specified as signal words for annotating feedbackType and pLocalization in the coding manual; then we supplemented the list with the words selected by a decision tree model learned using a feature vector consisting of all words in the feedback (Bag-of-Words). As shown in Table 2, we generated nine attributes counting the number of words in the feedback that belongs to each tag category, respectively. 
 Table 2.  Keyword table.
 4.3 PLocalization Identifier.
 For a given piece of criticism feedback, we developed four groups of attributes to capture different perspectives of localized expressions. Regular Expression: Three simple regular expressions were employed to recognize common phrases of location (e.g., ―on page 5‖, ―the section about‖). If any regular expression is matched, the binary attribute regTag is true. Domain Lexicon: Intuitively, localized feedback tends to use more domain vocabulary. Using the domain dictionary that we generated in 4.2 to calculate ESSAY attributes, we counted unigram domain-topics (collouni_d) contained in each piece of feedback. Syntactic Features: Besides computing lexicon frequencies from the surface text, we also extracted information from the syntactic structure of the feedback sentences. We used MSTParser [6] to parse feedback sentences and hence generated the dependency structure of feedback sentences. Then we investigated whether there are any domain- topics between the subject and the object (SO_domain) in any sentence. We also counted demonstrative determiners (this, that, these and those) in the feedback (DET_CNT). Overlapping-window Features: The three types of attributes above are based on our intuition about localized expressions, while the following attributes are derived from an overlapping-window algorithm that was shown to be effective in a similar task -- identifying quotations from reference works in primary materials for digital libraries [7]. To match a possible citation in a reference work, it searches for the most likely referred window of words through all possible primary materials. We applied this algorithm for our purpose, and considered the length of the window (windowSize) plus the number of overlapped words in the window (overlapNum). 
 4.4 Example of Attribute Extraction.
 To illustrate how these attributes were extracted from the feedback, consider the following feedback as an example, which was coded as ―feedbackType=criticism‖, ―pLocalization=true‖: The section of the essay on African Americans needs more careful attention to the timing and reasons for the federal governments decision to stop protecting African American civil and political rights. This feedback has 31 words (wordCount =31) and its index in the review is 2 (feedbackOrder=2). It has 2 bigram domain-topics (―African American‖ 2), 9 unigram domain-topics (―African‖ 2, ―American‖, ―Americans‖, ―federal‖, ―governments‖, ―civil‖, ―political‖ and ―rights‖), and 2 bigram essay-topics (―African American‖ 2) plus 8 unigram essay-topics (same as the unigram domain-topics except the ―rights‖). These four numbers are then normalized by the count of words in this feedback. As for Keyword, it contains 1 SUG (―need‖) and 2 NEG (―more‖, ―careful‖). The regTag is true because one regular expression is matched with ―the section of‖; there is no demonstrative determiner, thus DET_CNT is zero; ―African Americans‖ is between the subject ―section‖ and the object ―attention‖, so SO_domain is true. 
 4.5 Aggregation.
 To finally generate an overall assessment of appropriate problem localization by each reviewer, the system aggregates the relevant predictions at the feedback level, calculating a pLocalization% score representing the reviewer’s overall performance, which is the percentage of criticism feedback whose pLocalization is ―true‖ submitted by that reviewer.  To classify reviewers into ―High‖ and ―Low‖ groups regarding overall reviewing performance, we compare their pLocalization% score against a threshold and make a binary decision. In this work, we used an intuitive threshold that is the pLocalization% of criticism feedback of all reviewers (the number of ―true‖ pLocalization criticism over the number of criticism feedback in the training data). This threshold performed best among several alternatives explored in a pilot study. 
 5 Experimental Setup.
 Though the system output is the assessment of reviewer’s reviewing performance, its error could be due to any of the predictions made in the three components. To better analyze the predictive power of our system, we first evaluate each component separately (Section 5.1), then combine them and test its performance in a fully automatic version (Section 5.2). We compare our result to a Majority Class baseline for all experiments. 
 5.1 Component Evaluation.
 The feedbackType experiment uses all 1405 feedback from 76 reviewers. We use the Decision Tree algorithm provided by WEKA5 and conduct 10-fold cross validation for evaluation. We chose the Decision Tree learning algorithm not only because it worked best in a preliminary study (compared with Naïve Bayes, Logistic Regression, and SVM), but also because the learned model (decision tree) is easier to interpret and provides clearer insights into how problem localization is recognized. Results are presented in Table 3 and explained in Section 6. Recall that pLocalization was only coded for criticism feedback (non-criticism feedback are directly coded as N/A), thus for the isolated evaluation of this component, we only use the 875 criticism feedbacks for training and testing. The learning algorithm and evaluation settings are the same as those used for feedbackType (Table 3, Section 6). 
 5.2 System Evaluation.
 Though there is no annotation of overall problem localization for each reviewer (pLocalization at reviewer-level) in the data set, we can generate this by aggregating the pLocalization (annotated values) at the feedback-level, as we described in section 4.5. Note that when generating binary labels (High and Low) for each reviewer, the aggregation is based on annotated pLocalization and the threshold is calculated with annotated labels. When predicting, the aggregation is based on all predicted values. Thus the threshold would be different correspondingly. When all components work together as a system, the pLocalization identifier receives the output of the feedbackType identifier. Therefore in the combined version, the pLocalization identifier was trained with the 1405 feedbacks, with one new attribute: predicted feedbackType. We again use the Decision Tree algorithm provided by WEKA for learning, while in this case we conduct leave-one-reviewer-out cross validation for evaluation. Results are presented in Table 4. 
 6 Results.
 Table 3 presents the experimental results of the performance of each component in isolation. With respect to the accuracy of our models, both significantly (p<0.05) outperform our baselines (79% vs. 62% and 78% vs. 53%) and their Kappa values are all greater than 0.5. Because we would like to provide further tutoring for the ―Low‖ group of reviewers in the future, we are more interested in precision and recall of predicting the ―Low‖ group. Thus we also analyze precision and recall for feedbackType=criticism and pLocalization=true, which are used to compute the pLocalization% scores.  As listed in Table 3, both models achieve precision higher than 0.8, while for identifying criticism feedback the model’s recall is even 0.86, though the pLocalization model has 0.73 for recall, which is still acceptable.  Since the Majority class always predicts feedbackType as criticism and pLocalization as true, its recall will always be 1, thus we don’t aim to beat the baseline for recall. Besides examining the quantitative results, we can also examine our results for qualitative characteristics. The learned model (decision tree) for pLocalization at the feedback-level is compact, using only 5 attributes (presented in [8]), and it suggests that domain-word counts plays an important role. The feedbackType model though more complicated, also relies on domain knowledge (Collocation_d and Collouni_d appear close to the root). When all the components work together, the overall system can successfully predict the ―Low‖ group of reviewers with both precision and recall higher than 0.8 (Table 4). Table 4 also presents the confusion matrix for details. Although system performance suffers from errors within each component, aggregation does alleviate it and maintains the overall performance comparable to the pLocalization model in isolation (M2 in Table 3). 
 Table 3.  Performance of identification of feedbackType and pLocalization at feedback-level. 
 Table 4. Performance of the overall system for identifying pLocalization at reviewer-level. 
 7 Conclusion and Future Work.
 In this paper, we proposed a novel system for generating automatic assessments of reviewing performance with respect to problem localization at the reviewer-level. As a preliminary study in this new area of automatically assessing reviewing performance, we have demonstrated the feasibility of detecting reviewers who have low problem localization in reviewing, which is a first step for enhancing peer feedback quality. From the perspective of data mining, we successfully mine features of problem localization patterns from  free form textual comments using statistical NLP techniques. Though we have started with simple methods and our classifiers are based on shallow NLP features, our system achieves comparatively high accuracy and precision for identifying reviewers who generally fail to provide localization information in their reviews. In the future, we hope to construct a more complete dictionary of domain vocabulary, which might provide us with a better result based on our observations from this work. To improve the generalization of our system, we would also like to use a data driven approach to generate the Keyword list fully automatically (Table 2). Clearly each component in our system could be a NLP research topic, so we plan to explore the use of more sophisticated models from the NLP community as we discussed in the related work. Finally, since our ultimate goal is to help students with reviewing, we would like to perform a follow-up study to further evaluate how helpful the assessment generated by our system is in term of improving problem localization in future peer-review exercises for our ―Low‖ reviewers.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Assessing Reviewers’ Performance Based on Mining Problem Localization in Peer-Review Data</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/wenting-xiong"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/wenting-xiong"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/diane-litman"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/diane-litman"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/christian-schunn"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/christian-schunn"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/141/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/wenting-xiong"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/diane-litman"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/christian-schunn"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/142">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Data Mining of both Right and Wrong Answers from a Mathematics and a Science M/C Test given Collectively to 11,228 Students from India [1] in years 4, 6 and 8</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/142/authorlist"/>
		<swrc:abstract>Our poster presentation illustrates how to include wrong answers in test analyses using Response Spectrum Evaluation (RSE) procedures to track answer patterns on an answer-by-answer basis. RSE is a statistical procedure adapted from the multinomial [3] that bypasses the linear dependency problem so that alternative (wrong) answers can be included in data-mining analyses. Thus, the study of the dynamics of learning events can be conducted on an answer-by-answer basis. Previous investigations [3] using this procedure have revealed:</swrc:abstract>
		<led:body><![CDATA[ 1. The selection of answers is the result of the way students interpret the test questions. 2. These interpretations are directly inferable from the answers selected (or presented). 3. Selection procedures involve a number of strategies that are characteristic of each student, providing diagnostic information that can inform teaching. 4. This information is of more value to teachers who focus upon teaching how to think and how to learn instead of reproducing course content. 5. Some students show systematic development similar to the sequence described by the clinical observations of Piaget [1], while others show deterioration in the reverse direction. 6. Some students systematically shift from the right answer on the easy questions to particular types of “wrong” answer when their ability breaks out of “all” or “nothing”  thinking (without considering other options) into more intellectually flexible mind- sets. RSE is the only procedure with this detection capability. 7. The focus upon the right answers in the psychology of test-taking reinforces closed- minded thinking on the part of students taking the test, meaning that if the objective of teaching is profound understanding, the focus upon “right” answers is  psychologically invalid. 8. The dynamics of learning revealed by the RSE procedures are non-linear and multichotomous, meaning that the use of total-correct scores to assess student performance is mathematically invalid because the normal distribution requires dichotomous data to be utilized. Our study considers two tests, mathematics and science, given to the same students at three school levels, years 4, 6 and 8. It presents three examples from each test, showing the patterns of answer selection transitions on these items and the interactions among them. In our study, we draw implications for using RSE to add important diagnostic and interpretive information for teachers. This information can often be derived by direct analysis of each answer to each item. The ways in which these behaviors aggregate, however, requires determining the associations among answers with all answers in the test (both right and wrong), requiring that the analysis bypasses linear dependency. We are grateful to Educational Initiatives Pvt. Ltd. (India) for sharing raw response data and questions from the ASSET test used in their study “Student Learning in Metros 2006.”]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Data Mining of both Right and Wrong Answers from a Mathematics and a Science M/C Test given Collectively to 11,228 Students from India [1] in years 4, 6 and 8</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/james-a-bernauer"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/james-a-bernauer"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jay-c-powell"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jay-c-powell"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/142/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/james-a-bernauer"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jay-c-powell"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Can We Get Better Assessment From A Tutoring System Compared to Traditional Paper Testing? Can We Have Our Cake (Better Assessment) and Eat It too (Student Learning During the Test)?</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143/authorlist"/>
		<swrc:abstract>Dynamic assessment (DA) has been advocated as an interactive approach to conduct assessments to students in the learning systems as it can differentiate student proficiency at a finer grained level. Sternberg and others have been pursuing an alternative to IQ tests. They proposed to give students tests to see how much assistance it takes a student to learn a topic; and to use as a measure of their learning gain. They referred to this as dynamic assessment. It was suggested that this assisting-while-testing procedure could be done well by computer. To researchers in the intelligent tutoring system community, it comes as no surprise that measuring how much assistance a student needs to complete a task successfully is probably a good indicator of this lack of knowledge. However, a cautionary note is that conducting DA takes more time than simply administering regular test items to students. In this paper, we report a study analyzing 40-minutes data of 1,392 students from two school years using educational data mining techniques. We compare two conditions: one contains only practice items without intervention while the other condition allows students to seek for help when they encounter difficulties. The result suggests that for the purpose of assessing student performance, it is more efficient for students to take DA than just having practice items.</swrc:abstract>
		<led:body><![CDATA[ 1   Introduction.
 In the past twenty years, much attention from the Intelligent Tutoring System (ITS) community has been paid to improve the quality of student learning while the topic of improving the quality of assessment has not been emphasized as much. However, student assessment is very important. In the US, state tests mandated by “No Child Left Behind” are causing many schools to give extra tests to see if they can group students together to get special help. Of course, giving tests for this practices is not meant to help students learn, but is mainly focused on being able to tell teachers and principals about who needs help on what. It would be great if intelligent tutoring systems could be used to do the tests, so that no time from instruction is “stolen” to do extra assessments. Many psychometricians would argue that let students learn while being tested will make the assessment harder since you are trying to measure a moving target. Can an ITS, if given the same amount of time, be a better assessor of students (while also of course providing the benefit of helping students learn during that time period.  Is it possible to have our cake (better assessment) and eat it too (also let student learn)? As an intelligent tutoring system adapts the educational interaction to the specific needs of the individual student, student modeling is an essential component in an ITS as well. The learning effectiveness depends heavily on the understanding of student knowledge, difficulties, and misconceptions. Yet, assessing students automatically, continuously and accurately without interfering with student learning is an appealing but also a challenging task. Dynamic assessment (DA, or sometimes called dynamic testing, Grigorenko & Sternberg, 1998) has been advocated as an interactive approach to conducting assessments to students in the learning systems as it can differentiate student proficiency at the finer grained level. Different from traditional assessment, DA uses the amount and nature of the assistance that students receive which is normally not available in traditional practice test situations as a way to judge the extent of student knowledge limitations. Even before the computer supported systems become popular, much work has been done on developing “testing metrics” for dynamic testing (Grigorenko & Sternberg, 1998; Sternberg & Grigorenko, 2001, 2002) to supplement accuracy data (wrong/right scores) from a single setting. Researchers have been interested in trying to get more assessment value by comparing traditional assessment (static testing; students getting an item marked wrong or even getting partial credit) with a measure that shows how much help they needed. Grigorenko and Sternberg (1998) reviewed relevant literature on this topic and expressed enthusiasm for the idea. Sternberg & Grigorenko (2001, 2002) argued that dynamic tests not only serve to enhance students’ learning of cognitive skills, but also provide more accurate measures of ability to learn than traditional static tests. Campione and colleagues (Bryant, Brown & Campione, 1983; Campione & Brown, 1985) took a graduated prompting procedure to compare traditional testing paradigms against a dynamic testing paradigm. In the dynamic testing paradigm, learners are offered increasingly more explicit prewritten hints in response to incorrect responses. In this study they wanted to predict learning gains between pretest and posttest. They found that student learning gains were not as well correlated (R = 0.45) with static ability score as with their “dynamic testing” (R = 0.60) score. They also suggested that this dynamic method could be effectively done by computer, but never pushed toward to conduct such studies using a computer system. Intelligent Tutoring Systems are perfect test beds for DA as they naturally lead students into a tutoring process to help students with the difficulties they have encountered. Traditional paper and pencil or even some online assessment usually focuses on students’ responses to test items and whether they are answered correctly or incorrectly. It ignores all other student behaviors during the test (e.g., response time). However, the most unique information from DA is information about the learner’s responsiveness to intervention (Fuches et al. 2007) in the tutoring system. There have been a few studies that pay attention to such unique information. For instance, recently Fuches and colleagues (Fuches et al., 2008) employed DA in predicting third graders' development of mathematical problem solving. We (Feng, Heffernan & Koedinger, 2006, 2009) have also taken advantage of a computer-based tutoring system (ASSISTments, www.assistment.org, Razzaq et al., 2005), to collect extensive information while students interact with the system. Our results showed that the assistance model that includes no assessment result on the main problems leads to significantly better predictions than the lean model that is based on the assessment results alone. This relative success of the assistance model over the lean model highlights the power of the assistance measures, which suggests not only is it possible to get reliable information during “teaching on the test”, but also data from the teaching process actually improves reliability. Although DA has been shown to be effective predicting student performance, yet there is a cautionary note about DA since students are allowed to request assistance: it generally takes longer for students to finish a test using the DA approach than using a traditional test. For instance, in Feng et al. (2009) we reported that we could do a better job predicting student state test score using DA than a contrast case, the traditional testing situation. However, there is a caveat that the DA condition has included more time than the contrast case, which seems unfair for the contrast case. Although this sort of contrast leaves out the instructional benefit (e.g., Razzaq & Heffernan, 2006, 2007; Feng, Heffernan, Beck & Koedinger, 2008) of the tutoring system and, moreover, may not be well received by teachers and students, whether or not the system using DA would yield a better prediction of state scores or learning is still worth of further research. In this paper, we report a study that aims to answer this question. 
 2   Methods.
 2.1   ASSISTments, the test bed.
 Fig.1. A screenshot showing student requested a hint for one scaffolding question in ASSISTments 
 Traditionally, the areas of testing (i.e. psychometrics) and instruction (i.e., math educational research and instructional technology research) were separated fields of research with their own goals. The ASSISTments system is an attempt to blend the positive features of both computer-based tutoring and benchmark testing. The online system presents math problems to students of approximately 13 to 16 years old in middle school or high school to solve. If a student gets an item (the main item) right, they will get a new item. If a student has trouble solving a problem, the system provides instructional assistance to lead the student through by breaking the problem into a few scaffolding steps (typically 3~5 per problem), or displaying hint messages on the screen (usually 2~4 per question), upon student request as shown in Fig.1. Although the system is web-based hence accessible in principle anywhere/anytime, students typically interact with the system during one class period in the schools’ computer labs every three or four weeks. As students interact with the system, time- stamped student answers and student actions are logged into the background database. The hypothesis is that ASSISTments can do a better job of assessing student knowledge limitations than practice tests or other online testing approaches by using the DA approach based on the data collected online. 
 2.2    Approach.
 Fundamentally, in order to find out whether DA was worth the time, we would want to run a study comparing the assessment value of the following two different conditions: • Static assessment condition (A): students were presented with one static (as opposed to dynamic) test item and were requested to submit an answer. Once they had done that, more static items followed. • Dynamic assessment condition (B): students were presented with one static test item followed by a DA portion where they could request help. Then the question was: Is condition B better, or at least as good considering the learning effect, at assessing students after we control the time? We could have conducted a randomized controlled experiment with the two conditions. But, since the logging system of ASSISTments had collected data with the information needed by DA, we chose to compare predictions made based on log data from 40 minutes of time across simulated conditions that were similar but not exactly the same as above: • Simulated static assessment condition (A'): 40 minutes of student work selected from existing log data on only main items • Dynamic assessment condition (B'): 40 minutes of work selected from existing log data on both main items and the scaffolding steps and hints Such a simulation study using educational data mining techniques not only saved time from setting up and carrying out classroom experiments, but also allowed us to compare the same student’s work in two different conditions, which naturally rules out the subject effect. There would be no threat to validity of the comparison as both A' and B' allow learning on the test so there was a general trend up that you would expect1 We chose to use student’s end of year state accountability test score as the measure of student achievement, and we used data from conditions A' and B' to predict state test scores and compare the predictive accuracy of the two conditions. . So we will not devote much attention to the learning value of these conditions. We will refer interested readers to our previous publications where there were evidences showing that the system led to better student learning (e.g. Razzaq & Heffernan, 2006, 2007; Feng, Heffernan, Beck & Koedinger, 2008). 
 2.3 Data.
 The first raw data set we considered came from the 2004 – 2005 school year, the first full year in which the ASSISTment system was used in classes in 2 middle schools in Massachusetts. 912 8th grade students’ logs were maintained in the system over the time period from September to May. Among these students, we were able to obtain complete data for 628. The data set contained online interaction data from the ASSISTment system and the results of 8th grade state tests taken in May 2005. Students whose state test scores were not available and those who had done less than 40 minutes of work were excluded. The second raw data set we used was from the 2005-2006 school year. We collected a full data set for 764 students from Worcester Public Schools, including the online data from ASSISTments and their 8th grade state test raw scores. We applied the same filter to exclude students who had not done enough work. In both years, the items involved in the data sets were given to students in a random order. For each of the two raw data sets, we prepared two data sets for analysis, one for simulated static assessment condition (A') and one for dynamic assessment condition (B'). The data for condition A' included student response data during the first 40 minutes of work on only main problems; all responses and other actions during the DA portion were ignored. On the contrary, the data for condition B included all the responses for main questions and scaffoldings, as well as hint requests. For instance, consider the following scenario: Chris spent one minute trying to answer a main question in ASSISTments but failed, and was forced into the tutoring session. Chris then spent four minutes working through the three scaffolding questions. Chris answered one scaffolding question correctly and requested hints for the other two. This scenario counted as 1 minute of static work among the 40 minutes of data we prepared for condition A' with a response to the main question being recorded as zero. Yet it counted as 5 minutes of dynamic work in the data for condition B', including 1 correct response to scaffolding, 2 incorrect responses to scaffolding and 2 hint requests. 
 2.4 Metrics.
 We followed our work in Feng, Heffernan & Koedinger (2006, 2009) of developing online metrics for dynamic testing that measures student accuracy, speed, attempts, and help-seeking behaviors. Simply, the metrics we picked were • Main_Percent_Correct – students’ percent correct on main questions, which we often referred to as the “static metric”. • Main_Count - the number of main items students completed. This measures students' attendance and how on-task they were. This measure also reflects students' knowledge since better students have a higher potential to finish more items in the same amount of time. This is especially true for condition B' where students’ work on scaffolding also counted as part of the 40 minute work. While in condition A', low performing kids could go through many items but give wrong answers since their time consumed during the tutoring session is disregarded. • Scaffold_Percent_Correct - students' percent correct on scaffolding questions. In addition to original items, students' performance on scaffolding questions was also a reasonable reflection of their knowledge. For instance, two students who get the same original item wrong may, in fact, have different knowledge levels and this may be reflected in that one may do better on scaffolding questions than the other. • Avg_Hint_Request - the average number of hint requests per question. • Avg_Attempt - the average number of attempts students made for each question. • Avg_Question_Time - on average, how long it takes for a student to answer a question, whether original or scaffolding, measured in seconds. The last five metrics are DA style metrics and were not measured in traditional tests. They indicate the amount of assistance students needed to finish problems and the amount of time they needed to finish the questions. Our hypothesis is that the last three metrics will be negatively correlated with students’ performance. Thereby, the more hints they request, the more attempts they make on a question and the longer they need to go through a question, the worse their performance. Among the above six metrics, condition A' used only the first one as predictor to simulate paper practice tests by scoring students either correct or incorrect on each main problem while condition B' used all the metrics. 
 2.5 Modeling.
 We ran stepwise linear regression to use the metrics described above to predict student state test scores. The same process was repeated on the second year’s data. For all the models, the dependent variable is the state test score but the independent variables differ. Specifically, for condition A', the independent variable of the simple linear regression model is Main_Percent_Correct; while for condition B', it changed to be a collection of metrics: Main_Percent_Correct, Main_Count, Scaffold_Percent_Correct, Avg_Hint_Request, Avg_Attempt, Avg_Question_Time. 
 2.6 Results.
 First, we noticed that in both years, students finished more test items in the 40 minutes in static condition than in dynamic condition, which is not surprising considering the tutoring portion in the DA condition. Particularly, in year 2004-2005, the average number of main items finished was 22 in the simulated static assessment condition while it was only 11 in the dynamic condition; in year 2005-2006, the number was 31 in the static condition but it was only 13 in the dynamic condition. Then, we examined the parameters and associated coefficients in the linear regression models of both conditions. 
 Table.

 1. Parameters of simple regression models for simulated static assessment condition (A'). 
 As shown in Table 2, the first three parameters entered the models were the same in both years (with the order changed a little bit). Scaffold_Percent_Correct was the most significant predictor in the first year while in the second year, it changed to be Main_Percent_Correct. Also, in the later year 2005-2006, Avg_Attempt was considered as a significant predictor while in the first year it was Avg_Hint. Yet, it was consistent with our hypothesis that more attempts or more hints on a question will end up with a lower estimated score. 
 Table 2. Parameters entered regression models of dynamic condition (B'). 
 Now that we had looked at the parameters in the regression models, we would examine which condition does a better job predicting state test score. The R square’s of all models were summarized in Table 3. Additionally, because the models in different conditions always had different numbers of parameters, we also chose to use Bayesian Information Criterion (BIC) to compare the generalization quality of the models. We applied the formula for linear regression models introduced by Raftery (1995, p135), which was different from what is typical used for calculating BIC but most convenient for linear regression models: 
 FORMULA_1.
 Table 3. Summary of models.
 As we can see from Table 3, in both years, the R square of the model from the dynamic condition was always higher than that of the simulated static condition. Raftery (1995) discussed a Bayesian model selection procedure, in which the author proposed the heuristic of a BIC difference of 10 was about the same as getting a p- value of 0.05. And the lower BIC indicated a better fitted model. Thereby, we can see, in both years, the dynamic assessment condition did a significantly better job at predicting state test scores than the control condition which is static. 
 2.7 Validation.
 Before jumping into the conclusion saying dynamic assessment is more efficient than just giving practice test items, we performed 5-fold cross validation on the 2004-2005 data set. For the testing data, we calculated mean absolute difference (MAD) as a measure of prediction accuracy, which was computed as the average of the absolute difference between students’ real state test scores and the predicted scores across all students included in the testing set. 
 Table 4. Results of cross validation.
 As illustrated in Table 4, out of the 5 folds, DA condition ended up with a lower MAD in 3 folds. On average, DA condition did a better job predicting state test scores in the testing set: The difference between MADs of the DA condition and simulated static condition was bigger in these 3 DA-winning folds, and it was much smaller in the other 2 folds (folds 3 &4). Even though, the results from two-sided paired t-test indicated none of the difference was statistically significant. Then we took a closer look to see whether the trained regression models of the DA condition were consistent across the 5 folds validation. We found out that the trained models were fairly stable. The four variables as shown in Table 2 (2004-2005 portion), Scaffold_Percent_Correct, Main_Percent_Correct, Main_Count, Avg_Hint entered all five trained models while no other variables have been selected. Scaffold_Percent_Correct was always the most significant predictor across all folds while the entering order of the other variables varied during the stepwise variable selection process. The associated coefficients of the selected variables differed across folds with variance ranging between 0.0 (Main_Count) and 2.4 (Scaffold_Percent_Correct). As the last step, we took the average of coefficients from the five trained models and applied the model on the full data set of year 2004-2005. The average model from the simulated static condition and the DA condition produced MAD of 9.01 and 8.7 respectively. The paired t-test suggested that there was a marginally significant difference (p=0.10) All in all, based on the results, we conclude that dynamic assessment is more efficient than just giving practice test items. So, not only that students are learning during DA but also DA can produce at least as accurate assessment of student math performance as traditional practice test, even limited by using the same amount of testing time. This is surprising as students in the dynamic assessment do few problems and yet we get better assessment results. Of course, DA has another major advantage in that kids are learning during the test and therefore are not wasting their time just testing, while the practices tests are not likely to lead to much learning. 
 3   Conclusion.
 Dynamic assessment (DA) has been advocated as an interactive approach to conducting assessments to students in the learning systems as it can differentiate student proficiency at the finer grained level. In this paper, we compare dynamic assessment against a tough contrast case where students are doing assessment all the time in order to evaluate efficiency and accuracy of dynamic assessment in a tutoring system. Contribution: The contribution of this paper lies in that it eliminates the cautionary note about dynamic assessment that says DA will always need a longer time to do as well at assessing students, which further validates the usage of tutoring systems for assessment. ITS researchers have showed the effectiveness of such systems at promoting learning (e.g. Koedinger et al., 1997). This paper adds to that fact and presents a nice result suggesting that maybe, students should take their tests in an ITS as well! General implication: Combining with our previous findings (Feng, Heffernan & Koedinger, 2006, 2009), this paper tells us that not only we can better assess students while teaching them, but also the assessment can be done efficiently. Our results are important because they provide evidence that reliable and efficient assessment and instructional assistance can be effectively blended. At the Race to The Top Assessment Competition public input meetings, experts advocated for computer-based state assessments and argued the tests should be taken more than once a year (U.S. Dept of Ed., 2009). The general implication from this series research suggests that such computer-based, continuous assessment systems are possible to build and that they can be quite accurate and efficient at helping schools get information on their students while allowing student learning at the same time. 
 Acknowledgements.
 We would like to acknowledge funding from the Institute of Education Sciences of US Department of Education, the National Science Foundation, the Office of Naval Research and the Spencer Foundation. All of the opinions expressed in this paper are those solely of the authors and not those of our funding organizations.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Can We Get Better Assessment From A Tutoring System Compared to Traditional Paper Testing? Can We Have Our Cake (Better Assessment) and Eat It too (Student Learning During the Test)?</rdfs:label>
		<dc:subject>dynamic assessment</dc:subject>
		<dc:subject>assessment in learning system</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/m-feng"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/m-feng"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/m-feng"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Representing Student Performance with Partial Credit</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[ 1.  Example of different Partial Credit.
 We conducted two analyses on 52,529 data records from 72 students in order to evaluate the predictive power of partial credit in comparison to traditional binary performance. The first analysis compares the predictive power of the mean value of all the previous performances. As a baseline we used a logistic regression model with binary performance as the dependent, and student id as an independent. We then compare using mean previous partial credit vs. mean previous binary performance as independents in the model. The results are shown in Table 2. The model with mean previous partial credit gets a 0.0019 better R-squared than the one with mean previous binary credit. The absolute numerical improvement is small, which indicates that it is difficult to substantially improve student modeling. On the other hand, because knowing mean previous binary performance only brings 0.0046 better R-squared over baseline, knowing mean previous partial credit brings a relative 41.3% better R-squared compared to it. 
 Table 2.  Comparison between partial credit and binary performance.
 In the second analysis, instead of the mean value of previous performances, we compare the predictive power of the trends of these two measures of performance. We choose data in which students have 5 opportunities in one skill and use the previous 4 performances to create a trend line to predict the value of the 5th performance. To smooth the result and generate a bounded prediction between 0 and 1, we also apply a logistic regression model to this prediction. The result is shown in the fourth and fifth column of Table 2. Using partial credit improves R-squared by about 0.002 compared to using binary performance. This result is consistent with analysis 1. 
 3 Conclusions and Future work.
 In this paper we present a naïve algorithm to assign partial credit given detailed student responses. Partial credit performance contains much more information than binary performance, which is currently used in almost all researchers in the educational data mining field. Evaluations show that partial credit improves student model fitting by only a small absolute value but a high relative value compared to the binary performance. One question we are interested in is how to refine the algorithm to better fit student data and infer student knowledge. Also, we are interested in finding out in which situations partial credit does better than binary performance, so we can use it as an efficient complement to all the current models that use only the binary performance.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Representing Student Performance with Partial Credit</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/yutao-wang"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/yutao-wang"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/yutao-wang"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/145">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>A Distillation Approach to Refining Learning Objects</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/145/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>A Distillation Approach to Refining Learning Objects</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/john-champaign"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/john-champaign"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/robin-cohen"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/robin-cohen"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/145/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/john-champaign"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/robin-cohen"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/146">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Effort-based Tutoring: An Empirical Approach to Intelligent Tutoring</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/146/authorlist"/>
		<swrc:abstract>We describe pedagogical and student modeling based on past student interactions with a tutoring system. We model student effort with an integrated view of student behaviors (e.g. timing and help requests in addition to modeling success at solving problems). We argue that methods based on this integrated and empirical view of student effort at individual items accurately represent the real way that students use tutoring systems. This integrated view helps to discern factors that affect student behavior beyond cognition (e.g., help misuse due to meta-cognitive of affective flaws).  We specify parameters to the pedagogical model in detail.</swrc:abstract>
		<led:body><![CDATA[ 1.1 Wayang Outpost: A Mathematics Tutoring System.
 Wayang Outpost is a software tutor that helps students learn to solve standardized- test type of questions, in particular for a math test called the Scholastic Aptitude Test, and other state-based exams taken at the end of high school in the USA. This multimedia tutoring system teaches students how to solve geometry, statistics and algebra problems of the type that commonly appear on standardized tests. To answer problems in the Wayang interface, students choose a solution from a list of multiple choice options, providing immediate feedback on students’ entries and offering hints that students can accept or reject. Students are encouraged to ask the tutor for hints that are displayed in a progression from general suggestions to bottom-out solution. In addition to this domain-based help, the tutor currently provides a variety of affective and meta-cognitive feedback, delivered by learning companions  designed to act like peers who care about a student's progress and offer support and advice [1][8]. Both decisions about content sequencing and characters response are based on a model of student effort, used to assess the degree of cognitive effort a student invests to develop a problem solution, described in the next sections. 
 2 Modeling and Acting Upon Student Effort.
 We start by estimating the expected behavior that a student should have on a problem based on three indicators of effort: 1) number of attempts to solve a problem; 2) number of hints requested for a problem; 3) time required to solve a problem. These are three orthogonal axes that help understand student effort. The only pre-processing done for this data set was to use data corresponding to “valid” student users (instead of test users), and discarding outliers just for the “time” variables. Figure 1 shows examples of problem solving behavior for nearly 600 students in one problem. This one problem may seem evidently too easy at first glance, as the majority of students made zero or few incorrect attempts, saw no hints, and solved the problem in less that 5 seconds. However, this is not the case. It is common to find problem-student interaction instances where students spend little time and effort. It is also common that students under-use the help in the system. We find it essential to take into account that this is the real way that students use the tutoring system, and we need to take into account what are likely student behaviors when considering how to adjust instruction and the presentation of the material to students. Note that the distributions are not normal, but more similar to Chi-Square distributions. 
 Figure 1. Distribution of attempts, hints and seconds in one problem. Expected and delta values. 
 The combination of mistakes, hints and time as shown in Figure 1 will allow to estimate higher-level scenarios of mastery or disengagement, see Table 1. For each of the hundreds of problems or practice items in an intelligent tutor, we compute the median (or the sample mean after discarding the top 10 percentile, which was a good approximation in our data and much easier to compute using SQL) and standard deviation for the whole population of students. This median or mean is considered the expected value, i.e. the expected number of incorrect attempts for a problem pi (E(Ii)) where i=1…N, and N=total practice items in the tutoring system. Expected hints seen is E(Hi) and time required to solve the problem is E(Ti). We also define two delta values for each E(Ii), E(Hi) and E(Ti), a total of six delta values (see Figure 1) for each problem pi, which represent a fraction  of the standard deviation, regulated by two parameters, θLOW and θHIGH in the interval [0,1]. For example, if θLOW=1/4  and θHIGH=1/2, then δIL=θLOWSD(Ii)= SD(Ii)/4  (a fourth of the standard deviation of Ii) and δIH= SD(Ii)θHIGH= SD(Ii)/2, half of the standard deviation of Ii. θLOW and θHIGH are the same for all problems in the system. These values help define what is “expected behavior” for a practice item within the tutoring system. Note that the notation for δ values has been simplified. 
 2.1 Pedagogical  Decisions based on Student Effort.
 The large benefit of an effort model based on different orthogonal axes of behavior (hints, time and correctness) is that it can help researchers discern between behaviors related to student engagement (affective) and behaviors related to help misuse (meta- cognitive or affective) in addition to behaviors related to cognitive mastery. Table 1 shows the estimations of most likely scenarios made by the pedagogical model in Wayang Outpost, and the pedagogical decisions made in terms of content difficulty, plus other pedagogical moves related to affective and meta-cognitive feedback. Note that disengagement (e.g. lines 3 and 5) produces a reduction in problem difficulty, based on the assumption that if a student is not working hard enough on the current problem, they probably won’t work hard on a similar or harder problem. However, the key intervention is that Learning Companions deemphasize the importance of immediate success. 
 Table 1. Empirical-based estimates of effort at the recently completed problem lead to adjusted problem difficulty and other affective and meta-cognitive feedback. 
 The retrieval of an increased difficulty item is based on a function Harder(H[1..n], γ) that returns a problem of higher difficulty; H is a sorted list of n practice items the student has not yet seen, all harder in difficulty than the one the student has just worked on; H[1] is the item of lowest difficulty, and H[n] is the item of highest difficulty, and γ is a natural number greater than zero. The problem returned by Harder is specified in Eq. 1. For example, Harder with γ=3 will return the problem at the 33rd percentile of items in list H[1..m]. 
 FORMULA_(1).
 Similarly, a problem of lesser difficulty is selected with function Easier(E[1..n], where E is a sorted list of problem items, all items are easier than the problem just seen by the student; E[1] is the item of lowest estimated difficulty, and E[n] is the item of highest difficulty. Eq. 2 shows Easier as a function of n and γ. Easier with γ=3 will return the item that at the 66th percentile of items in list E[1..n]. 
 FORMULA_(2).
 Both Easier and Harder work upon the assumption that there are easier or harder items to choose from. The next section addresses what happens when m=0 or n=0. 
 2.2 Progression through Knowledge Units.
 In Wayang Outpost, the curriculum is organized in a linear set of topics or knowledge units (KU), which is a classification of problems in sets of items that involve similar skills (e.g. polygon perimeter measurement problems). Pedagogical decisions about content sequencing are made at two levels: within a topic and between topics, skills or knowledge units. This section addresses between topic decisions. The criteria of “chunking” problems in knowledge units is based on the idea that similar problems should be seen close to each other, to maximize the transfer of what a student has learned, as the concepts are still in working memory to be applied to the next cognitive transfer task. Cognitive effort is then reduced, and the likelihood of applying a recently learned skill to the next task is enhanced. Each knowledge unit may be defined at a variety of levels, and is composed of a variety of problems involving a set of related skills. For instance, within the “Statistics” topic, a student may be presented with problems about finding the median of a set of numbers, or deciding whether the mean or median were larger, from a picture of a stem and leaf plot. While overlap of skills exists, not all problems within a topic involve the same skills, and their difficulties may vary to a large degree. Topics are arranged according to pre-requisites (problems presented in KU2 will not include skills introduced in KU3). When a topic begins students are presented an explanation of the kinds of problems that will follow, generally introduced aloud by pedagogical animated creatures. Sometimes this involves an example problem, accompanied by a worked-out solution via multimedia features. 
 Figure 2. Spiral curriculum in which Knowledge Units are ordered according to pre-requisites. 
 Table 2. Conditions for topic switching in Wayang Outpost. 
 A student progresses through these knowledge units depending on a variety of criteria, specified in Table 2 beyond cognitive mastery. For instance, condition 2.2 shows how a topic switch may be forced based on limitations of content --the system failed FKU times to find a problem of the difficulty it believes the student should get for the topic. If the pedagogical model suggests the student should increase problem difficulty, but there are no harder problems remaining, then a counter for the number of failures for the current topic is increased. Because failures < FKU an easier problem is provided instead. Another possibility is condition 2.3, where the teacher has allocated a specific amount of time for the student to study or review a certain topic. 
 2.3 Problem Difficulty Estimates.
 The pedagogical model must be able to estimate problem difficulty in order to assign problems for students in specific scenarios. We identify two faces of problem difficulty in intelligent tutors. From the perspective of a knowledge engineer, problems have objective difficulty (e.g., based on number of skills and steps involved in each problem). However, students may perceive each problem differently according to a student perceived difficulty (SPD). While objective problem difficulty should be similar to SPD, they are not necessarily the same. Proper estimation of problem difficulty is essential for this pedagogical model, and not possible to do with simple Item Response Theory because tutoring involves more dimensions (help, engagement) than testing (accuracy). We capture SPD from the three independent sources of evidence of students’ effort to solve a problem: 1) correctness in term of number of required attempts to solve a problem (random variable Ci); 2) amount of time spent in a problem (random variable Ti); 3) amount of help required or requested to solve the problem correctly (random variable Hi). We define problem difficulty di for a practice activity component i  in Eq. 3, as the mean of these three factors: attempts to solve, time and help needed. 
 FORMULA_(3).
 Where dci is the difficulty factor in terms of correctness, dti is the difficulty factor in terms of time, and dhi is the difficulty factor in terms of help needed. Alternatively, the three factors might be given a weight, to emphasize them differently. di, dci, dti and dhi are normalized values in the interval [0,1] and express SPD. Eq. 4, 5 and 6 show how each of the three difficulty factors are computed. 
 FORMULA_(4).
 FORMULA_(5).
 FORMULA_(6).
 dci (Eq. 4) is the expected value of Ii (number of incorrect attempts while trying to solve a problem pi) across all students who have seen that problem, divided by the maximum E(Ij) registered for any problem pj in the system (N=the total number of problems or practice activities in the system). Similarly, dti (Eq. 5) is the expected value of Ti (time spent on problem pi) and is also normalized. This expected time is the mean value after removing outliers, or median. dhi (Eq. 6) is the expected value of Hi (number of attempts for problem pi) divided by the maximum E(Hj) registered. 
 2.4 Accuracy of Item Difficulty Estimations.
 We computed SPD estimates using a data set of 591 high school students who used Wayang Outpost tutoring software over past years, from 2003 until 2005. The tutors employed a variety of problem selectors during those years, with some percentage of students using a random problem selector. Validating that student perceived difficulty estimates were reasonable seemed essential. The first reason is that the difficulties play a crucial role in the adaptive behavior of the tutor, and inappropriate difficulties would make the system behave in undesired ways (e.g. providing a harder problem when the student clearly needs an easier one). The second reason is that it is just too likely that the student perceived difficulty estimates are biased, because student behavior is contingent to the problem selector in place at the moment the data on problem performance was collected. Unless the raw data comes from a random selection of problems, student behavior and thus the data collected will be biased in some direction. This will make problems look easier or harder than they truly are. We devised a variety of methods to assess the correctness of our estimation of perceived student difficulty, and implemented three of them. All of these are based on the following axiom: “Pairs of Similar Problems Should have Similar Problem Difficulty Estimates”. In other words, if two problems are very similar, the perceived differences in their difficulty should approach zero. We subsequently drew a subset of 60 mathematics problems (p1 to p60) from our tutoring system. These sixty problems are special because may be divided into 30 pairs of problems, where each pi , with i=1…30, is extremely similar to p30+i. In this domain of geometry problems, similar problems involved similar showing graphics with slightly different angles, or measurements. For example, same problems with a rotated figure (and different operands). Similar problems involve the application of the same skills the same amount of times. We call these highly similar pairs and now describe four criteria used to verify that these pairs are similar in their difficulty estimates. 
 2.4.1 Criteria 1: Correlations. We tested that such pairs had similar difficulty estimates with a simple Pearson correlation, which is the most familiar measure of dependence between two quantities. It is obtained by dividing the covariance of the two variables by the product of their standard deviations. A Pearson correlation determined that pairs of problems were significantly correlated (N=30, p<0.000, R=.823), thus this test is passed. 
 2.4.2 Criteria 2: Mean Squared Error. Another criteria used was that the difference in objective difficulty between highly similar problems should be smaller than the difference in difficulty between either of these problems and any other problem in the system that is not as similar –other problems will involve different skills, or different total amount of applications of the same skills. While it may be coincidental that a problem foreign to the pair might have a very similar difficulty to either problem in the pair, this should not be the general case. The distance between the difficulty of a problem pi and its highly similar problem pair p30+i should be smaller than the mean distance between one of the problems in the pair and the remaining problems in the set. A more common jargon when talking about differences due to error is the mean squared error. Eq. 7 rephrases the above in terms of squared differences, where N=total number of pairs=30. 
 FORMULA_(7).
 If we can show that this inequality holds in general for problems, we have some evidence that our system is doing a reasonable job at estimating difficulties. We computed the 30 square differences, and their corresponding mean squared differences as specified in Eq. 7. The result was that the inequality holds for 29 of the thirty cases, which is a 97% success rate. A paired-samples t-test for the two inequality terms in Eq. 6 revealed that these two sides of Eq. 7 are significantly different t(29)=7.35, p<.000. The second test is then passed. 
 2.4.3 Criteria 3: Human Expertise. While pairs of highly similar problems should have similar student perceived difficulty levels, they don’t necessarily need to have exactly the same difficulty (i.e. the difference in their difficulty levels will not be exactly zero. In other words, is a small number. While it would be hard to determine the true value of epsilon for each problem pair, an expert human eye (e.g. a teacher or tutor) could probably make good predictions about whether. This kind of expert knowledge can help us establish that the latter problem should be harder for a student to solve than the former one. Other restrictions may have to do with operand size, involvement of decimals or negative numbers, or a small extra step. We managed to establish such restrictions for 21 of the 30 pairs of problems we considered, the other 9 were just too similar to each other. Such restrictions (true positives or true negatives) were correctly guessed in 14 of the 21 cases (67%), and a Chi-Square test revealed this is significantly better than chance (Pearson Chi-Square=5.25, p=.022). Thus, the third test is passed. 
 2.4.4 Criteria 4: Convergence. Ideally, the difference between highly similar pairs of problems would converge to as more data arrives to the logs, even if different problem selectors are in place at different moments. This test is still ongoing. 
 2.5 Evaluation of Effectiveness  of Effort-Based Pedagogical Model. 
 While we may be satisfied that difficulty of items are reasonably estimated, we need also to show that the adaptive mechanism underlying the pedagogical model makes a difference to student learning. A study was carried out in the 2003-2004 academic year with 60 students to evaluate the effectiveness of the adaptive sequencing of problems, compared to a random selection of problems within a topic (no learning companions or affective feedback). Both the experimental and the control conditions implemented topic switching based on one parameter only, NKU, so that the “topic switch” criterion was set to a fixed maximum number of problems per topic. This was established so that all students were exposed to the same number of problems in each topic. MKU, FKU and TKU were then ignored. The main difference between conditions was the problem selection mechanism within the topic. For the experimental condition, it adjusted problem difficulty as described in previous sections, with the following parameters: γ=2; θLOW=0; θHIGH=0; this made the changes in problem difficulty quite  marked.  Control condition students received random problems within each topic. Students were randomly assigned to either the Effort-based Adaptive Problem Selection condition, or the Random Problem Selection Condition. Students used the Wayang Tutoring System for 4 class periods, completing a 10-item math test before starting and a similar posttest the last day. The test consisted of items drawn from the SAT (Scholastic Aptitude Test) and released by the College Board. The two tests were counterbalanced –half of students received pretest A, and half pretest B, and the tests were reversed for students at posttest time. We measured the total number of correct items achieved in the test, and the accuracy at items (correct/test items attempted) as a measure of performance, see Table 3. We obtained full pre and posttest data for 56 students, 23 in the experimental adaptive condition, and 33 in the control condition.  Table 3 shows the mean and standard deviation of pretest and posttest scores for the pretest and the posttest. Mean achievement in the posttest increased and standard deviations reduced for both groups. However, mean improvement was higher for the experimental adaptive problem selection group (Figure 3). 
 Figure 3. Pre to Posttest Improvement with Effort-based Pedagogical Model compared to a Random Problem Selector Within the Topic. 
 Table 3. Pretest and Posttest Scores in Math Test.
 This difference is significant (ANCOVA for posttest score with pretest score as a covariate, group effect F(55,1)=8.4, p=.006). The group receiving adaptive effort-based pedagogical decisions about problem difficulty improved more than did the group receiving random problem selection control condition. We conclude that adaptive problem selection is better than random. 
 3 Summary.
 This paper presented a novel approach to the development of smart learning environments, based on empirical measures of student effort at individual items. It described a pedagogical model that uses empirical estimates of problem difficulty, specifying parameters that regulate behavior within knowledge units (γ, θLOW and θHIGH) and between knowledge units (MKU, FKU , TKU, NKU). Knowledge Units may be defined at different levels of abstraction, thus addressing restrictions of content. This allows for replication in other ILEs, even in ill-defined domains or in small ILEs that are trying to encode smart decisions about practice items or activity selection. We have described criteria for evaluating that estimates of problem difficulty are not too biased to the problem selector in place at the time of data collection. Last, we have shown that this effort-based pedagogical model leads to improved learning compared to uninformed random decisions within a topic or knowledge unit.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Effort-based Tutoring: An Empirical Approach to Intelligent Tutoring</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ivon-arroyo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ivon-arroyo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hasmik-mehranian"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hasmik-mehranian"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/beverly-park-woolf"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/beverly-park-woolf"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/146/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/ivon-arroyo"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/hasmik-mehranian"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/beverly-park-woolf"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/147">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Multiple Test Forms Construction based on Bees Algorithm</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/147/authorlist"/>
		<swrc:abstract>This paper proposes a new construction method of multiple test forms that applies a Bees Algorithm and a parallel computation technique to improve the computational costs of the traditional methods.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Multiple Test Forms Construction based on Bees Algorithm</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/pokpong-songmuang"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/pokpong-songmuang"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/maomi-ueno"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/maomi-ueno"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/147/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/pokpong-songmuang"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/maomi-ueno"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Analysis of Productive Learning Behaviors in a Structured Inquiry Cycle Using Hidden Markov Models</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148/authorlist"/>
		<swrc:abstract>This paper demonstrates the generality of the hidden Markov model approach for exploratory sequence analysis by applying the methodology to study students’ learning behaviors in a new domain, i.e., an asynchronous, online environment that promotes an explicit inquiry cycle while permitting a great deal of learner control. Our analysis demonstrates that the high-performing students have more linear learning behaviors, and that their behaviors remain consistent across different study modules. We also compare our approach to a process mining approach, and suggest how they may complement one another.</swrc:abstract>
		<led:body><![CDATA[ 1.  The Adapted STAR Legacy Cycle.
 While the canonical inquiry cycle supported by the STAR Legacy interface has students progressing in a linear fashion from Challenge to Wrap-up, the system is also designed to promote the idea of learner control [4, 11]. In other words, students are given complete freedom to navigate through the cycle to accomplish their learning tasks. We should note that it is precisely this freedom that allows for various behavior patterns to emerge as the students use the system; this makes the activity sequences collected from the system particularly amenable to our exploratory data analysis methods. 
 3 Experimental Study.
 The users of the system are primarily adults who take the course to meet certification requirements or for recommended workplace training. A few students took the course for other reasons, but this number was insignificant. In all, the preliminary dataset which we retrieved consists of 6,298 learners covering 50 learning modules in 10 independent- study courses. Each module represents roughly 4 to 8 hours of self-study, and each independent-study course contains 3 to 7 different learning modules. The students could pass the pretest for a module and avoid the training. Otherwise they went through the training and took the summative assessment at the end (outside of the training system). Both the pretest and the summative assessment questions were randomly generated subset from a shared pool of questions. It is important to note that learners who successfully completed every section (pretest or summative assessment) were immediately awarded the course certification. In fact, around 18% of the students successfully prequalified during the pretest, and skipped the training modules. Hence, the students who used the system were necessarily those who did not have the requisite prior knowledge to pass the test the first time. In our previous analyses, we created groups based on how the students improved from pre to posttest by using the system, and made distinction between a low (pretest performance) and low (posttest performance) group, low and high group, and high and high group [2]. However, in this study, as discussed above, all students who use the system universally belong in the “low prior knowledge” category. Therefore, our grouping uses only the posttest scores. For this study, we split the scores into three uniform groups (after determining that the data was unimodal) to define our low, middle and high performance groups. At the two ends of the middle group, there were a number of scores that were close to the low and high group scores respectively, so we ignore the middle group, and focus our analysis on the differences between the low and the high groups. Using these performance groupings, we examine ways to characterize the behaviors of the different students, and determine, for example, whether the high-performing students have learning behaviors that are distinct from the low performers. In our previous domain, we have found that the higher-performing students exhibit patterns of behaviors that correspond to good learning strategies (e.g., explore one’s understanding of a topic by asking relevant follow-up queries and checking to see if the explanations for the answer are meaningful) [7]. For our current domain, cyber security, we direct our attention to studying how students transition between the different phases of the STAR Legacy cycle. In particular, we are interested in determining whether the path students take when following the cycle is in any way related to the students’ performance. For example, one may argue that the students who follow the cycle in a linear fashion (the implicit model of inquiry learning in STAR Legacy) would perform better in their learning tasks than students who make numerous jumps between the phases. An alternate hypothesis may state that students who make multiple forward and backward jumps in the cycle do so because they have formulated their own strategy for learning the content material, and hence would perform better. Our behavior analysis will focus on the transitions students make as they go through the cycle and their context. Howard, et al. [9] have defined six different types of transitions: linear (L), jumping (J), retrying (R), searching (S), transitioning (T), and backtracking (B). Linear transitions comprise directed navigation steps in the cycle, and may be performed by clicking the “Next Up” navigation button appearing at the end of each activity or resource. Jumping transitions imply the use of Resources and Assessments in a non-linear way through the use of descriptive menus. Retrying transitions occur in the Assessment phase, and represent the activity of re-attempting a specific question. Searching transitions comprise of actions, where the students search the materials in either the Resources or the Assessments phase. Transitioning represents the students moving to and from the course module menu. Finally, Backtracking transitions indicate that the student has previously seen the destination activity, resource, or menu. We hypothesize that students in the different performance groups will employ different learning behavior patterns (and likely, different strategies) when using the system. Further, students’ behavior patterns will evolve as they study different modules. Therefore, hypothesis 1 states that there will be a marked difference between the two performance groups, especially in their use of linear transition behaviors as they learn using the STAR Legacy cycle. Hypothesis 2 states that the students’ behaviors will evolve as they use the system, and that this can be seen when comparing early and late module behaviors. We will use two metrics to assess the difference between the models: (1) the definition and interpretation of the HMM states; and (2) the stationary probabilities, which represent the proportion of time spent in a state relative to the other states of the HMM. We will describe these metrics in more detail in the following section. 
 4 Methods.
 Before the students’ trace data can be analyzed to generate HMMs, necessary preprocessing steps have to be executed on the raw activity sequences. As stated before, the focus of our analysis is to interpret students’ behaviors as states and the transitions between states. Thus, the objective of the preprocessing is to extract and reduce the activity sequences to just the transition and the context associated with the transition. Because clear distinctions were made between the different types of transitions, all of the preprocessing can be done automatically. At the end of the preprocessing phase, we have activity sequences consisting of series of transition-context pairs (e.g. “AR-Linear; RR- Linear; RR-Retrying”, where A represents the Assessment phase, and R represents the Resources phase). 
 4.1 The HMM Procedure.
 Deriving a HMM from students’ activity sequences should result in a model that identifies frequently occurring sequence patterns, which are then interpreted as student behaviors. The HMMs are so named because the derived states are hidden, i.e., they cannot be directly observed. Instead, one provides meaning or interpretation to the states by studying the activities associated with the state. These activities imply behaviors, and the collection of states may imply one or more behavior patterns. A first step in model generation is to initialize the parameters that define the states of the HMM and the possible state transitions. Starting from an initial model description, expectation-maximization techniques are applied iteratively until the model parameters converge [6, 10]. The estimation process is quite sensitive to the initial state description, and bad initializations may lead to generating suboptimal models. In past work [6, 8], we have used a conceptual clustering algorithm to generate good parameters for the initial state. Another unknown in the model derivation process is the “best” number of states that define the HMM. The metric that we use to assess the model fit is the Bayesian Information Criterion (BIC) measure, which takes into account both the log likelihood of the model (i.e., how likely the model is given the data) and the number of states in the derived model (i.e., how complex the model is) to find the model that strikes the best balance between high likelihood and low complexity [8]. The full procedure for deriving HMMs from sequence data is outlined in our previous EDM paper [6]. The HMMs offer us a state-based aggregated interpretation of the students’ activity sequences, and we can analyze them in various ways. A first step is to assign meanings or labels to the states of the model. For example, we may find that some types of activities tend to cohere in certain states, which would provide clues to determine what behaviors these states represent. Also, we can examine the stationary probabilities to get a sense of what the more relevant (i.e., more frequently occurring) states are. The stationary probability, as we define here, is the relative proportion of activities that belongs to a certain state (i.e., the stationary probability of a state A is the proportion of occurrences of State A among all states that occur in a sequence of length n iterations generated by the model; n is typically the average number of activities in the input sequences). For example, a state having a 20% stationary probability implies that 20% of students’ activities during the session are related to the behavior(s) that state represents. We can also study the transitions between the states to see how students transition between different behaviors. In addition, the structures of the models themselves can be studied to see if certain patterns appear (e.g., cycles), and these patterns can then be interpreted in terms of students’ learning behaviors. 
 4.2 The Experimental Datasets.
 The HMMs were run using two different datasets. For the first dataset, we investigated the first module that the students worked on. The second dataset was a module that the students worked on toward the end of their study for that course. Table 1 lists the number of students in the early and late modules, and the average posttest scores for these modules. 
 Table 1.  Modules Used.
 5 Results.
 Examples of the HMM structures generated are shown in Figs. 2 and 3. All four models (Low-Early; Late-Early; High-Early; and High-Late) had 5 or 6 states. The main difference among the models was more in the activities associated with each state, rather than the state transition behavior. We discuss one of the derived models in some detail to familiarize the reader with the HMM structure. Fig. 2 depicts the model for the high performance group in the early module. Table 2 complements the model by listing the major composition of activities for each state. The first two letters denote the phases involved in the transition (e.g. “AR” is the transition from Assessments to Resources), and the letter after the hyphen indicates the transition type (e.g. “L” is linear). We see that the students begin from state 1, and moving from Overview to Thoughts is the only activity associated with this state. Fig. 1 confirms that this corresponds to the initial part of the STAR Legacy cycle, so we label this state as the Start State. From state 1, the students move directly to state 2, where they proceed to thoughts (82% of the time) or jump to the resources (14% of the time) (see Table 2). Since this corresponds to the start of the problem solving, we call this the Initiation State. Then, the students move to state 3, where the students perform multiple activities, but the two primary ones are proceeding linearly through the resources (56%) and retrying questions in the resources (13%). The dominant activities plus the others lead us to label this state as the Assessment State. 
 Other analyses have determined that students spend significant portion of the time working on assessments (can be thought of as self-assessments), so not surprisingly our model indicates a self-loop with high likelihood (90%) for this state. When students exit the Assessment State, the HMM indicates moves to state 4 (likelihood of 7%) or to state 5 (likelihood of 3%). State 4 is composed mainly of linear transitions involving the wrap- up phase (47%) and retrying assessments (35%). For this reason, we call this the Wrap-up State. States 5 and 6 are composed mainly of backtracking transitions. In particular, we note that state 5 contains a significant portion of backtracking through resources and assessments (44%). Hence, we call state 5 a RA (Resources-Assessment) Backtracking State. Meanwhile, state 6 is composed significantly of activities involving wrap-up, overview, or thoughts (48%). Hence, we call state 6 a HL (High-Level) Backtracking State. 
 Figure 2.  Early Module – High Performance Group.
 Figure 3.Early Module – Low Performance Group.
 Table 2. Distribution of Major Activities (those with probabilities greater than 5%). 
 Figure 3 shows the corresponding HMM for the low performing group working on the early module. This model has 5 states. Upon close observation, we see that the reduction in states is due to the Wrap-up State and the HL Backtracking State being merged into a single state (State 3). The other states that we have identified all appear in this model, albeit with a somewhat different compositions of activities, which we will be one of the key issues in our investigation. The identified states are summarized in Table 3. Using these interpreted states, our first level of analysis would be to see if there are differences in the proportion of time that the students spend in each of the states. The computed stationary probabilities are summarized in Table 4. It is clear that the High- Early and the High-Late models are almost identical. Thus we may conclude that the high-performing students’ learning behaviors remained the same as they studied the different modules. On the other hand, the low performers’ behaviors changed significantly from the early to the late module, primarily in the amount of time they spent on wrap up, as well as the amount of backtracking between resources and assessments. The mixed state that was observed early also went away in their learning activities in the late module. One may conclude that with time, the low performers started behaving more like the high performers, and one may conjecture that this was because the system was helping the low performers become better learners. This needs to be investigated further. In terms of hypothesis 2, Table 4 implies that the low-performer data supports hypothesis 2, but that the high-performer data does not. One may conclude that this is reasonable, because the high performers have figured out how to do well, and do not need to modify their behaviors. 
 Table 3.  Descriptions of States.
 Table 4.  Stationary Probabilities.
 Another interesting difference revealed by the models is how the proportions of activities significantly differ among the different performance groups. In particular, there is much higher incidence of linear transitions in the Initiation State and the Assessment State among the higher performing groups. These results are summarized in Table 5. 
 6 Discussion and Comparisons.
 Using the results from the hidden Markov model analysis, we sought to investigate how the high-performing students transitioned through the different phases in the system in contrast with the low-performing students. In particular, we were interested in the students’ relative tendencies to follow the explicit, canonical model presented by the system, given the great degree of freedom that they were also provided. Confirming our first hypothesis, we found that the high-performing students moved more linearly from the Challenge to the Initial Thoughts phase, and also that they moved more linearly through the Assessment phase. We also found that the high-performing students spent markedly more time in the Wrap-upstate, while the low-performing students engaged much more in backtracking through previous assessment questions and resource items. While these results are promising, we should note that the results likely stem from the high-performing students having a better understanding of the domain, and, therefore, showing better abilities to plan their learning and assessment tasks. On the other hand, the low performers seem to flounder more, which manifest as backtracking actions. 
 Table 5. Proportions of Linear Transitions.
 Our results are complemented by our colleagues’ findings from using process mining techniques to examine similar problems [5]. This approach uses specifically structured models to investigate individual problems at a greater detail. For example, our colleagues provide a Petri Net Model for how the learners navigate through the STAR Legacy cycle. Such a model provides precise numbers of students who follow the sequential path and those who deviate from it at any point. A result of interest to us is the finding that most students follow the canonical model at first, but start to diverge as they use the system. Of particular concern is that most of the divergence stems from the students’ avoidance of the Thoughts phase in the later modules; the Thoughts phase figures prominently into the cycle as it is designed to create “cognitive dissonance” among the learners [4]. Complemented with our finding that higher-performing students are more likely to proceed linearly and not skip the Thoughts phase. It might be worth investigating why the students grow to avoid the Thoughts phase, and whether this avoidance actually leads to a drop in performance. Also, the direction of the causality should be explored: does proceeding linearly lead to higher performance, or does higher cognitive ability lead to proceeding linearly? Our hypothesis is that learners face increased cognitive load for the metacognitive tasks, such as strategic planning, and that following the structural scaffolds in the environment, i.e. progressing linearly, would help them manage the cognitive load, and hence perform better. 
 7 Conclusions and Future Work.
 In this paper, we have shown that the hidden Markov model approach is general, and can be applied to performing exploratory data analysis in new domains. The analysis implies that there are differences in transitionary state behavior between the high and the low- performing students. Namely, the results indicate that the high-performing students move through the model more linearly and spend less time backtracking than the low- performing students. Our results, combined with our previous work, show the general nature of the hidden Markov model approach for exploratory sequence analysis. Like speech synthesis applications [8], HMMs may prove to be a valuable tool for analyzing learning behaviors in a variety of domains. We have also shown how exploratory data analysis can be complemented by process mining analysis to help examine problems in greater detail. In particular, we imagine that interesting and inconclusive results from exploratory data analysis may be investigated using process mining to yield more definite findings. We believe that the combination of these approaches will lead to many promising avenues to extend our research in the future.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Analysis of Productive Learning Behaviors in a Structured Inquiry Cycle Using Hidden Markov Models</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hogyeong-jeong"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hogyeong-jeong"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/gautam-biswas"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/gautam-biswas"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/julie-johnson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/julie-johnson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/larry-howard"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/larry-howard"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/hogyeong-jeong"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/gautam-biswas"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/julie-johnson"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/larry-howard"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Process Mining to Support Students' Collaborative Writing</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149/authorlist"/>
		<swrc:abstract>Writing, particularly collaborative writing is a commonly needed skill. Investigating how ideas and concepts are developed during the process of writing can be used to improve not only the quality of the written documents but more importantly the writing skills of those involved. In this paper, process mining is used to analyze the process that groups of writers follow, and how the process correlates to the quality and semantic features of the final product. Particularly, we developed heuristics to extract the semantic nature of text changes during writing. These semantic changes were then used to identify writing activities in writing processes. We conducted a pilot study using documents collected from groups of undergraduate students writing collaboratively in order to evaluate the proposed heuristics and illustrate the applicability of process mining techniques in analyzing the process of writing.</swrc:abstract>
		<led:body><![CDATA[ 1. Introduction.
 Nowadays, Computer-Supported Collaborative Learning, particularly Collaborative Writing (CW) is widely used in education. Students often use computers to take notes during lectures and write essays for their assignments. Thanks to the availability of the Internet, students increasingly write collaboratively by sharing their documents in a number of ways. This has led to increased research on how to support students’ writing on computers. Our research motivation is to investigate ways to support these collaborative writing activities by providing feedback to students during the collaborative writing process. We have built a prototype which, based on the text written by students, can automatically perform analysis on text changes in order to discover types of semantic changes and identify writing stages. In order to develop a tool supporting CW, understanding how ideas and concepts are developed during writing process is essential. The process of writing consists of steps of writing activities. These steps of writing activities can be considered as the sequence patterns comprising of both time events and the semantics of changes made during those steps. Therefore, process mining, which focuses on extracting process-related knowledge from event logs recorded by an information system, can be used to extract sequence patterns of writing activities that lead to quality outcomes. In this paper, a process mining technique is used to detect semantic changes in writing activities in order to gain insight on how student write their documents. Our work used a taxonomy of writing activities, proposed by Lowry et al. [4]. In addition, we used a model developed by Boiarsky [2] for analyzing semantic changes in the writing process. 
 Although process mining techniques have been successfully applied to extract process- related knowledge from event logs recorded by information systems [3] for two decades, the techniques have only recently applied to educational data. For example, in Pecheniskiy et al [8], the authors utilized process mining tools to analyze data from online multiple choice examinations and demonstrated the use of process discovery and analysis techniques. They were interested in individual students' activities of answering online multiple-choice questions during assessment, not in activities where students write and edit texts collaboratively. In addition, there has been abundant research for improving the support of quality writing in education such as automatic scoring of essays [10], visualization [6], and document clustering [1]. However, these approaches, unlike ours, focus on the final product, not on the writing process itself. The remainder of the paper is organized as follows. In Section 2, the framework for supporting CW is presented. The heuristics for extracting semantic changes and writing activities during writing process are proposed in Section 3. A pilot study mining student writing processes is discussed in Section 4. Finally, Section 5 concludes and outlines and our future work planned in this area. 
 2. WriteProc.
 The framework, WriteProc for exploring collaborative writing processes was introduced in [12]. It integrates a front-end writing tool, Google Docs, which not only supports collaborative writing activities, but also stores all revisions of documents created, shared, and written by groups of authors. WriteProc uses two process and text mining tools, ProM [9] and TML [13]. ProM provides a way to extract knowledge about writers' activities and collaboration. In [12], process mining was used to extract only patterns of students' interaction and collaboration for peer review. In this paper, the process analysis is used for identifying sequence patterns of writing activities that lead to a positive outcome and indicate patterns that may lead to problems. TML analyzes text changes between individual revisions of documents. This analysis can provide semantic meaning of changes in order to gain insight into how writers develop ideas and concepts during their writing process. Full details about WriteProc can be found in [12]. In our pilot study, WriteProc retrieves some revisions from Google Docs of all the documents written by the students, using Google Document Data API. Google Docs automatically saves documents every few seconds. Authors can also deliberately save their documents. In Google Docs, every saving command (either auto or committed) produces one revision for each document. WriteProc does not retrieve all revisions of the documents, as many of them do not contain any changes. For a particular document, it first obtains the revision history (log) containing information, such as revision number, timestamp of auto and committed saving, and author id. The revision history is then analyzed to identify writing sessions for individual writers. From this analysis, for each session WriteProc downloads a revision after each minute of writing. These downloaded revisions are then pre-processed to extract and index paragraphs and sentences for further analysis. Using predefined heuristics (defined in the next section), WriteProc performs a text-based analysis on the indexed revisions of each document which extracts semantic meaning of text changes and identifies writing activities. Sequences of writing activities are then analyzed using process mining. In the next section, we propose the heuristics and explain how they will be used to detect writing activities. 
 Table 1. Heuristics for determining collaborative writing activities: Brainstorming (B), Outlining (O), Drafting (D), Revising (R), and Editing (E) based on text change operation (C1 – C8), text structure (S1 – S2), and functions (F1 – F3). 
 Abbreviation: An operation is allowed (Y) or not allowed (N), List (L), Structured List (SL), Sections and Paragraphs (Sec. & Par.), Number of Sentences (S), Number of Paragraphs (P), Changing/Fluctuating (F), Constant(C), Improving (I), and Not applicable (N/A). 
 3. Heuristics for determining collaborative writing activities.
 Writing activities in collaborative writing can be categorized into 6 common activities: brainstorming, outlining, drafting, reviewing, revising, and editing. The definition of these activities is described in [4]. It is important to note that in general these six activities do not occur in a linear sequence. In a document writing process, we consider reviewing activities made not only by the writers (owners) of the document, but also by instructors or editors or peers who read and annotate the document for content, grammar, and style improvements. In this work, we concentrate on automatically identifying the five collaborative writing activities: brainstorming (B), outlining (O), drafting (D), revising (R), and editing (E). In order to identify the five collaborative writing activities in the writing process of a particular document, basic heuristics are proposed. Particularly, our heuristics are based on text changes, text structures, topic changes and cohesion improvement in the document from one revision to another. The heuristics utilized in our analysis are presented in Table 1. Each writing activity can be identified using text change operations (C1 to C8), text structures (S1and S2), and functions (F1 to F3), which are explained below. 
 3.1 Text structures.
 The writing activities can be determined by the structure of the written texts (S1) and the number of sentences and paragraphs (S2). During brainstorming, authors normally write in bullet-point lists consisting of single words or phrasal words (compound nouns). Consequently, the number of paragraphs (the number of lines) is approximately equal to the number of sentences (the number of words or items). Although during an outlining phase the number of paragraphs and sentences are still the same, the text structure is more organized into sections and subsections. When writers start drafting their documents, number of sentences and paragraphs change dramatically. During this phase, the number of sentences is expected to be higher than the number of paragraphs. This is also truth for revising and editing phases. 
 3.2 Text change operations.
 Eight types of text change operations (C1 – C8) were used in our heuristics. These text change operations were based on the revision change functions proposed by Boiarsky [2]. Writers use the text change operations in their writing activities for different purposes in order to produce final pieces of writing. We developed basic assumptions for our pilot study as following: • During brainstorming, writers can reorder, adding, or deleting items of lists of brainstorming ideas. They can also format the lists, alter the whole items of the lists, or change some items. Similarly, during outlining, writers can add, delete, reorder, format, and change some or the whole sections of their organized list. • During drafting, revising and editing, text change operations become more complicated. Drafting activities start when the structure of the written text changes from bullet-point or structured lists to paragraphs. In other words, alteration of form (C7) usually indicates the start of drafting activities (as depicted by N** in the table). During drafting, information is added and removed all the time. Therefore, expansion of information (C5), deletion of information (C6), and micro-structure change (C8) imply drafting activities. • However, if C5, C6 and C8 happen after reviewing activities, we consider them as revising activities (as noted by N* in the table). Particularly, peer review was incorporated in our pilot study. We assumed that after getting feedback from their peers, students may add, delete, and alter texts in their documents. Also students may completely erase the whole written text and rewrite the text from scratch after getting feedback from their peer review. This operation is C7. • In addition, common revising activities are reordering (C2), consolidating (C3), and distributing texts (C4). These changes occur after writers start drafting and reoccur many times in the writing process. Our assumption is that during drafting writers may stop writing and revise their own written texts in order to improve the cohesion and effectively convey information and ideas to readers. • For simplicity, all surface change operations (C1) including formatting, spelling and punctuation corrections are consider to be editing activities. Similar to C2 - C4, editing activities are considered to be common and reoccur many times in the writing process. 
 3.3 Number of words.
 The ratios between the number of words of two consecutive revisions are computed (F1). The ratio was used in conjunction with topic overlap and cohesion measurement discussed below to determined writing activities. 
 3.4 Topic overlap.
 Our heuristics also used a topic overlap measurement (F2). We analyze the change in topics (concepts) for two consecutive revisions of documents. Our intuition is that when people write about something, they usually repeat the subject (topics) to keep readers’ attention. By identifying concepts and comparing them between two consecutive revisions of pieces of writing, we gain information on how writers develop their idea and concept during writing process. Intuitively during drafting and revising, topics overlap (F2) changes dramatically. However, during editing F2 should be constant. The method for computing the topic overlap (F2) is described in Section 4.1. 
 3.5 Cohesion Changes.
 Another measurement used in our heuristics to detect writing activities is the cohesion of the text. We measure cohesion of each individual revision of documents. Particularly, we calculate the distance between consecutive sentences and paragraphs in the written texts in order to gain an insight on how paragraphs and the whole texts have been developed. Our assumption is that during a drafting phase, the cohesion of the written text fluctuates a lot. After authors revise the text, the cohesion of the text is usually improved. There should be no change in the cohesion of the written text during editing phase. We use the Latent Semantic Analysis (LSA) technique to measure the cohesion of the text. In particular, for each revision of documents we compute average sentence and paragraph similarities using LSA for single documents as described in [14] and compare the result with the former revision of the same documents in order to determine if there is an improvement in cohesion for these two revisions. 
 4. Pilot study.
 As a way of evaluating the proposed heuristics and illustrating how process mining can be used to analyze writing activities, we conducted a pilot study to investigate writing processes of students in the course of E-business Analysis and Design, conducted during the first semester of 2009 at the University of Sydney.  In this course, students were organized in groups of two and asked to write Project Specification Documents (PSD) of between 1,500 and 2,000 words (equivalent to 4-5 pages) for their e-business projects. They were required to write their PSD on Google Docs and share them with the course instructor. The course also used peer review in which each PSD was reviewed by other two students who were members of different groups. After getting feedback from their peers, students could revise and improve their documents if necessary before submitting the final version one week later. In addition, the marks of the final submissions of the PSD (as presented in [12]) together with a very good understanding of the quality of each group through the semester was used to correlate behaviour patterns to quality outcomes. In particular, to be able to give insight into how students wrote their own documents, we performed a process diagnostic to give a broad overview of students' writing activities. 
 4.1 Pre-processing.
 In this section, we describe the document pre-processing method used in our study. We analyzed 21 documents in this study. As discussed in Subsection 3.4, LSA was used for measuring the changes in cohesion in the written text. The pre-processing step for LSA involved the extraction of the terms from all concerned revisions of the documents. First, each revision of documents was split into paragraphs using a simple matching to the newline character. Then, each paragraph was divided into sentences using Sun’s standard Java text library. After that, each sentence, paragraph and the whole text were indexed using Apache’s Lucene, which performed the tokenization, stemming, and stop word removal. We used Porter’s stemmer algorithm (Snowball analyzer integrated in Lucene) for stemming words. Next for each revision, a term-document matrix was created. Term frequency (TF) and document frequency (DF) were selected as weight terms. We discarded terms that only appear once in each revision of documents. The space of term- document matrix was reduced using Singular Value Decomposition (SVD).  We adopted the method of Villalon et al. [14] to reduce the dimension of the LSA space to 75% of the total number of sentences. In order to compute the topic overlap discussed in Subsection 3.3, we first extracted topics from each revision of documents. Our approach in extracting topics from each revision of documents was based on Lingo clustering algorithm developed by Osinski et al. [7]. Especially, we extracted frequent phrases from each revision (we use the assumption and definition of frequent phrase discussed in detail in [7]). Next, by using the reduced term-document matrix calculated for LSA above, for each revision we discovered any existing latent structure of diverse topics in a particular revision. After discovering topics of each revision of documents, we compared topics of two consecutive revisions to calculate the topic overlap between the two revisions. As a baseline measure, we selected a simplistic word overlap measures. This measure was used for sentence- level similarity such as in the work of [5]. The final step in our data preparation was to create a writing activity log of all documents in the format used by a process mining tool like ProM. First, for each revision (except the first revision) we compare it to the former revision and obtain the types of text change operations between the two using a file comparison utility like diff tool. Then, we applied the proposed heuristics using the obtained types of text changes, the results of LSA cohesion and topic overlap calculated above. In conjunction with timestamp and user identification obtained from the revision history discussed in Section 2, we can create an event log of writing activities, in which process analysis can be performed. 
 4.2 Writing process analysis.
 After preprocessing, the resulting event log consisted of 8,233 events in total. Each process case represented one document. The mean number of events per document was 392, with a minimum of 77 events per document and a maximum of 705 events per document. There were 6 different types of events corresponding to 6 types of writing activities. We performed process analysis in order to gain insight into how students wrote their documents. The Dotted Chart Analysis utility of ProM [9] was used to analyze students' writing activities. The dotted chart was similar to a Gantt chart [11], showing the spread of events over time by plotting a dot for each event in the log. Figure 1 illustrated the output of the dotted chart analysis of students writing their PSD documents. All instances (one per document) were sorted by start time. In the figure, points represented writing activities occurring at certain time. There were six writing activities as mentioned above. They were represented with different colors and shapes: white circles denoted brainstorming, brown circles were outlining, black triangles represented drafting, black squares depicted reviewing, brown squares were revising, and white diamonds denoted editing activities. 
 Figure 1. Dotted chart of 21 groups of students writing collaboratively (from ProM tool). White circles denoted brainstorming, brown circles were outlining, black triangles were drafting, black squares were reviewing, brown squares were revising, and white diamonds were editing activities. 
 From the figure, we observed that most students started their writing approximately two weeks before the due date for the submission of peer review (27th March 2009). Exceptionally, there existed 6 groups starting their writing activities quite early. Group 19 (received the final mark of 9/10) and 21(10)1 started their outlining and brainstorming activities very early in the first week of the semester. Group 13(9), 18(9), 23(8), and 24(8) also commenced their writing tasks early in the second week of the semester. Therefore, we noticed that students who performed brainstorming and outlining started their writing quite early and received quite good marks. In addition, as we expected, there were many writing activities during a week before the due date for peer review submission. Interestingly, there were 5 groups commencing their writing tasks few days before the due date. They were Group 9 (10), 15 (9), 22 (9), 25 (9), and 26 (9). These groups also received quite high marks for their writing, although they started writing quite late. Actually students of these groups did not use Google docs to perform outlining and brainstorming. They also started their writing using other word processing applications such as MS Word because all of them commenced their writing on Google Docs with substantial amount of texts (containing sections and paragraphs). During the one-week of peer review, we expected to have no writing activities since students were waiting for the feedback from peer review. However, Group 22, who just started their writing on Google Docs, performed some activities during this time. We checked the revisions of the document of Group 22 and found that there were substantial text changes performed by these activities. At the end they received a good mark of 9/10. Furthermore, after getting feedback from their peer review (3rd April 2009), students started revising and editing their documents before the final submission (10th April 2009). We observed that Group 16(9), 18(9), and 24(7) started working on their documents soon after getting feedback. They were among top groups in the class. In addition, we were naturally interested in finding out more about writing activities of each group and the path each group was following in the process of writing. ProM provides a Performance Sequence Analysis (PSA) plug-in to find the most frequent paths in the event log [3]. Figure 2 illustrates a sequential diagram of students' writing activities in our pilot study. The patterns were ordered by the number of groups generating them. From the analysis above, we learned that not all groups of students performed their brainstorming and outlining before actually drafting their documents. The PSA also confirmed this. In addition, from the PSA we checked each individual group’s activities in order to determine which groups did not conduct brainstorming and/or outlining for their writing tasks. From Figure 2, there were seven distinct patterns of activities. Pattern 0 and 5 indicated groups that started drafting without brainstorming and outlining. Pattern 0 was originated from 8 groups: 2, 9, 13, 14, 15, 22, 25, and 28. For Pattern 5, there was only one group: 26. Pattern 1, 3 and 4 involved all activities except brainstorming. There were 7 groups belonged to Pattern 1: 1, 4, 7, 10, 16, 17, and 27. For Pattern 3 and 4, each of them was generated by only one group namely 19 and 23, respectively. Clearly, more than half of the class did not conduct outlining and one third of the class did not bother performing brainstorming (at least on Google Docs) before drafting. We discussed this matter with the course instructor who was aware that most students performed their writing plans offline. Finally, there were 3 groups whose their writing activities generating Pattern 2 and 6: 18, 21 (for Pattern 2), and 27 (for Pattern 6). These groups planed their writing tasks with brainstorming and outlining. Consequently, all of them received high marks. The process model of all 21 documents was discovered by using the Heuristic miner algorithm [15] with default threshold parameters (implemented in ProM). Figure 3 depicts a transition diagram of the model. The numbers in the boxes indicate the frequencies of the writing activities. The decimal numbers along the arcs show the probabilities of transitions between two activities and the natural numbers present the number of times this order of activities occur. In addition, we also extracted the process model of each individual group in order to gain an insight on how the group conducting its writing. 
 Figure 2. Sequence patterns of 21 groups of students writing collaboratively. The patterns are ordered by the number of groups generating them (from ProM tool). 
 Figure 3. A transition diagram of the process model for 21 documents. The natural numbers refer to frequencies (of events and transitions), and the decimal numbers represent probability of the transitions. 
 5. Conclusion and future work.
 The work presented in this paper is a work in progress. The pilot study described in the previous section provides fundamental work for us to develop basic heuristics to extract semantic meaning on text changes and determine writing activities. Based on the heuristics, we were able to analyze student's writing activities using a process mining and discover 7 patterns on writing activities of 21 groups of students. However, correlated with final assessment, we could not distinguish clearly the better from the weaker groups. This preliminary work gives us direction for the next step of our work. In the future, the discovered patterns, process snapshots provided by performance sequence and dotted chart analysis can be used for providing feedback to students during their writing so that they are aware of their writing activities and can coordinate effectively. One way to improve our understanding of what writing processes lead to better outcome is to improve our heuristics. In our current work, the surface change operation indicates only changes in spelling, number, punctuation, and format. We did not include grammatical correction in the current work yet. In addition, one of text change operations proposed by Boiarsky is the improvement in vocabulary [2]. We did not detect the improvement in vocabulary in our current analysis. These two text change operations will be cooperated in the heuristic in the future. In addition, we already measure the change in topics (concepts) which represent word repetition. Although word repetition is common in writing, good writers usually utilize synonymy and pronouns to avoid annoying repetition. This issue was not considered in this paper and will be cooperated in the future work. 
 Acknowledgements.
 This work has been funded by Australian Research Council DP0986873. We would like to thank the many people involving in the development of ProM and TML.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Process Mining to Support Students' Collaborative Writing</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/vilaythong-southavilay"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/vilaythong-southavilay"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/vilaythong-southavilay"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Navigating the parameter space of Bayesian Knowledge Tracing models: Visualizations of the convergence of the Expectation Maximization algorithm</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150/authorlist"/>
		<swrc:abstract>Bayesian Knowledge Tracing (KT) models are employed by the cognitive tutors in order to determine student knowledge based on four parameters: learn rate, prior, guess and slip. A commonly used algorithm for learning these parameter values from data is the Expectation Maximization (EM) algorithm. Past work, however, has suggested that with four free parameters the standard KT model is prone to converging to erroneous degenerate states depending on the initial values of these four parameters. In this work we simulate data from a model with known parameter values and then run a grid search over the parameter initialization space of KT to map out which initial values lead to erroneous learned parameters. Through analysis of convergence and error surface visualizations we found that the initial parameter values leading to a degenerate state are not scattered randomly throughput the parameter space but instead exist on a surface with predictable boundaries. A recently introduced extension to KT that individualizes the prior parameter is also explored and compared to standard KT with regard to parameter convergence. We found that the individualization model has unique properties which allow it to avoid the local maxima problem.</swrc:abstract>
		<led:body><![CDATA[ 1.1 Expectation Maximization algorithm.
 The Expectation Maximization (EM) algorithm is a commonly used algorithm used for learning the parameters of a model from data. EM can learn parameters from incomplete data as well as from a model with unobserved nodes such as the KT model. In the cognitive tutors, EM is used to learn the KT prior, learn rate, guess and slip parameters for each skill, or production rule. One requirement of the EM parameter learning procedure is that initial values for the parameters be specified. With each iteration the EM algorithm will try to find parameters that improve fit to the data by maximizing the log likelihood function, a measure of model fit. There are two conditions that determine when EM stops its search and returns learned parameter results: 1) if the specified maximum number of iterations is exceeded or 2) if the difference in log likelihood between iterations is less than a specified threshold. Meeting condition 2, given a low enough threshold, is indicative of algorithm parameter convergence, however, given a low enough threshold, EM will continue to try to maximize log likelihood, learning the parameters to a greater precision. In our work we use a threshold value of 1e-4, which is the default for the software package used, and a maximum iteration count of 15. The max iteration value used is lower than typical, however, we found that in the average case our EM runs did not exceed more than 7 iterations before reaching the convergence threshold. The value of 15 was chosen to limit the maximum computation time since our methodology requires that EM be run thousands of times in order to achieve our goal. 
 1.2 Past work in the area of KT parameter learning.
 Beck & Chang [3] explained that multiple sets of KT parameters could lead to identical predictions of student performance. One set of parameters was described as the plausible set, or the set that was in line with the authors’ knowledge of the domain. The other set was described as the degenerate set, or the set with implausible values such as values that specify that a student is more likely to get a question wrong if they know the skill. The author’s proposed solution was to use a Dirichlet distribution to constrain the values of the parameters based on knowledge of the domain. Corbett & Anderson’s [1] approach to the problem of implausible learned parameters was to impose a maximum value that the learned parameters could reach, such as a maximum guess limit of 0.30 which was used in Corbett & Anderson’s original parameter fitting code. This method of constraining parameters is still being employed by researchers such as Baker et al. [4] and Ritter et al [5] in their more recent models. Alternatives to EM for fitting parameters were explored by Pavlik et al. [5], such as using unpublished code by Baker to brute force parameters that minimize an error function. Pavlik also introduced an alternative to KT, named PFA [5] and reported an increase in performance compared to the KT results. Gong, Beck and Heffernan [6] however are in the process of challenging PFA by using KT with EM which they report provides improved prediction performance over PFA with their dataset. While past works have made strides in learning plausible parameters they lack the benefit of knowing the true model parameters of their data. Because of this, none of past work has been able to report the accuracy of their learned parameters. One of the contributions of our work is to provide a closer look at the behavior and accuracy of EM in fitting KT models by using synthesized data that comes from a known set of parameter values. This enables us to analyze the learned parameters in terms of exact error instead of just plausibility. To our knowledge this is something that has not been previously attempted. 
 2 Methodology.
 Our methodology involves first synthesizing response data from a model with a known set of parameter values. After creating the synthesized dataset we can then train a KT model with EM using different initial parameter values and then measure how far from the true values the learned values are. This section describes the details of this procedure. 
 2.1 Synthesized dataset procedure.
 To synthesize a dataset with known parameter values we run a simulation to generate student responses based on those known ground truth parameter values. These values will later be compared to the values that EM learns from the synthesized data. To generate the synthetic student data we defined a KT model using functions from MATLAB’s Bayes Net Toolbox (BNT) [7]. We set the known parameters of the KT model based on the mean values learned across skills in a web based math tutor called ASSISTments [9]. These values which represent the ground truth parameters are shown in Table 1. 
 Table 1. Ground truth parameters used for student simulation.
 Since knowledge is modeled dichotomously, as either learned or unlearned, the prior represents the Bayesian network’s confidence that a student is in the learned state. The simulation procedure makes the assumption that confidence of prior knowledge is evenly distributed. 100 users and four question opportunities are simulated, representing a problem set of length four. Each doubling of the number of users also doubles the EM computation time. We found that 100 users were sufficient to achieve parameter convergence with the simulated data. Figure 1 shows pseudo code of the simulation. 
 Figure 1. Pseudo code for generating synthetic student data from known KT parameter values 
 Student responses are generated probabilistically based on the parameter values. For instance, the Bayesian network will roll a die to determine if a student is in the learned state based on the student’s prior and the learn rate. The network will then again role a die based on guess and slip and learned state to determine if the student answers a question correct or incorrect at that opportunity. After the simulation procedure is finished, the end result is a datafile consisting of 100 rows, one for each user, and five columns; user id followed by the four incorrect/correct responses for each user. 
 2.2 Analysis procedure.
 With the dataset now generated, the next step was to start EM at different initial parameter values and observe how far the learned values were from the true values. A feature of BNT is the ability to specify which parameters are fixed and which EM should try to learn. In order to gain some intuition on the behavior of EM we decided to start simple by fixing the prior and learn rate parameters to their true values and focusing on learning the guess and slip parameters only. An example of one EM run and calculation of mean absolute error is shown in the table below. 
 Table 3. Example run of EM learning the Guess and Slip parameters of the KT model 
 The true prior parameter value was set to the mean of the simulated priors (In our simulated dataset of 100 the mean prior was 0.49). Having only two free parameters allows us to represent the parameter space in a two dimensional graph with guess representing the X axis value and slip representing the Y axis value. After this exploration of the 2D guess/slip space we will explore the more complex four free parameter space. 
 2.2.1 Grid search mapping of the EM initial parameter convergence space 
 One of the research questions we wanted to answer was if the initial EM values leading to a degenerate state are scattered randomly throughout the parameter space or if they exist within a defined surface or boundary. If the degenerate initial values are scattered randomly through the space then EM may not be a reliable method for fitting KT models. If the degenerate states are confined to a predictable boundary then true parameter convergence can be achieved by restricting initial parameter values to within a certain boundary. In order to map out the convergence of each initial parameter we iterated over the entire initial guess/slip parameter space with a 0.02 interval. Figure 2 shows how this grid search exploration of the space was conducted. 
 Figure 3. Output of the grid search procedure exploring the initial EM guess/slip parameter space of KT 
 We started with an initial guess and slip of 0 and ran EM to learn the guess and slip values of our synthesized dataset. When an EM run finished, either because it reached the convergence threshold or the maximum iteration, it returned the learned guess and slip values as well as the log likelihood fit to the data of the initial parameters and the learned parameters (represented by LLstart and LLend in the figure). We calculated the mean absolute error between the learned and true values using the formula in Table 3. We then increased the initial slip value by 0.02 and ran EM again and repeated this procedure for every guess and slip value from 0 to 1 with an interval of 0.02. 
 3 Results.
 The analysis procedure produced an error and log likelihood value for each guess/slip pair in the parameter space. This allowed for visualization of the parameter space using Guessinitial as the X coordinate, Slipinitial as the Y coordinate and either log likelihood or mean absolute error as the error function. 
 3.1 Tracing EM iterations across the KT log likelihood space.
 The calculation of error is made possible only by knowing the true parameters that generated the synthesized dataset. EM does not have access to these true parameters but instead must use log likelihood to guide its search. In order to view the model fit surface and how EM traverses across it from a variety of initial positions, we set the Z-coordinate (background color) to the LLstart value and logged the parameter values learned at each iteration step of EM. We overlaid a plot of these EM iteration step points on the graph of model fit. This combined graph is shown below in figure 4 which depicts the nature of EM’s convergence with KT. For the EM iteration plot we tracked the convergence of EM starting positions in 0.10 intervals to reduce clutter instead of 0.02 intervals which were used to created the model fit plot. No EM runs reached their iteration max for this visualization. Starting values of 0 or 1 (on the borders of the graph) do not converge from the borders because of how BNT fixes parameters with 0 or 1 as their initial value. 
 Figure 4. Model fit and EM iteration convergence graph of Bayesian Knowledge Tracing. Small white dots represent parameter starting values. Green dots represent the parameter values at each EM iteration. The red dots represent the resulting learned parameter values and the large white dot is ground truth. The background color is the log likelihood (LLstart) of the parameter space. Dark blue represent better fit. 
 This visualization depicts the multiple global maxima problem of Knowledge Tracing. There are two distinct regions of best fit (dark blue); one existing in the lower left quadrant which contains the true parameter values (indicated by the white “ground truth” dot), the other existing in the upper right quadrant representing the degenerate learned values. We can observe that all the green dots lie within one of the two global maxima regions, indicating that EM makes a jump to an area of good fit after the first iteration. The graph shows that there are two primary points that EM converges to with this dataset; one centered around guess/slip = 0.15/0.10, the other around 0.89/0.76. We can also observe that initial parameter values that satisfy the equation: guess + slip <= 1, such as guess/slip = 0.90/0.10 and 0.50/0.50, successfully converge to the true parameter area while initial values that satisfy: guess + slip > 1, converge to the degenerate point. 
 3.2 KT convergence when learning all four parameters.
 For the full four parameter case we iterated through initial values of the prior, learn rate, guess and slip parameters from 0 to 1 with a 0.05 interval. This totaled 194,481 EM runs (21^4) to traverse the entire parameter space. For each set of initial positions we logged the converged learned parameter values. In order to evaluate this data we looked at the distribution of converged values for each parameter across all EM runs. 
 Figure 5. Histograms showing the distribution of learned parameter values for each of the four Knowledge 
 Tracing parameters. The first row shows the parameter distributions across all the EM runs. The second row shows the parameter distributions for the EM runs where initial guess and slip summed to less than 1. The first row of histograms in Figure 5 shows the distribution of learned parameter values across all EM runs. Generally, we can observe that all parameters have multiple points of convergence; however, each histogram shows a clear single or bi-modal distribution. The prior and learn rate appear to be the parameters that are easiest to learn since the majority of EM runs lead to values near their true values. The guess and slip histograms exhibit more of the bi-modal behavior seen in the two parameter case, with points of convergence at opposite ends of the parameter space. In the two parameter case, initial guess and slip values that summed to less than one converged towards the ground truth coordinate. To see if this trend generalized with four free parameters we generated another set of histograms but only included EM runs where the initial guess and slip parameters summed to less than one. These histograms are shown in the second row. 
 3.3 Evaluating an extension to KT called the Prior Per Student model 
 We evaluated a model [9], recently introduced by the authors, that allows for individualization of the prior parameter. By only modeling a single prior, Knowledge tracing makes the assumption that all students have the same level of knowledge of a particular skill before using the tutor. The Prior Per Student (PPS) model challenges that assumption by allowing each student to have a separate prior while keeping the learn, guess and slip as parameters of the skill. The individualization is modeled completely within a Bayesian model and is accomplished with the addition of just a single node, representing student id, and a single arc, connecting the student node to the first opportunity knowledge node. We evaluated this model using the two-parameter case, where guess and slip are learned and learn rate and prior are fixed to their true values. 
 Figure 6. EM convergence graphs of the Prior Per Student (PPS) model (left) and KT model (right). 
 Results are shown with ground truth datasets with guess/slip of 0.14/0.09, 0.50/0.50 and 0.60/0.10 The KT models, in the right column of figure 6, all show multiple points of convergence with only one of the points near the ground truth coordinate (white dot). Unlike KT, the PPS models, in the left column, have a single point of convergence regardless of the starting position and that single point is near the ground truth value. The red lines in two of the PPS models indicate that the maximum iteration count was reached. In the 0.14/0.09 model it appears that PPS with starting parameters in the upper right region were converging towards the true values but hit the max iteration count before arriving. The PPS model was shown [9] to provide improved prediction over standard knowledge tracing with real world datasets. The visualizations shown in figure 6 suggest that this improved prediction accuracy is likely due in part to the PPS model’s improved parameter learning accuracy from a wider variety of initial parameter locations. In the case of the PPS models show above there were as many prior parameters as there were students and these parameters were all set to the values that were generated for each simulated student as seen in the line “KTmodel.prior = prior(user)” in figure 1. Accurately inferring many initial prior values would be difficult in practice; however, a heuristic is described in Pardos et al [9] that seeds each individual prior based on the student’s first response. Applying this same heuristic to our synthesized dataset with ground truth guess/slip values of 0.14/0.09 we found that all points converged to the true parameter location without EM reaching its maximum iteration count. This performance suggests that single point convergence to the true parameters is possible with the PPS model without the benefit of individual student prior knowledge estimates. A more detailed description and analysis of this technique is in work that is in preparation. 
 4 Discussion and Future Work.
 An argument can be made that if two sets of parameters fit the data equally well then it makes no difference if the parameters used are the true parameters. This is true when prediction of responses is the only goal. However, when inferences about knowledge and learning are being made, parameter plausibility and accuracy is crucial. It is therefore important to understand how our student models and fitting procedures behave if we are to draw valid conclusions from them. In this work we have depicted how KT exhibits multi-modal convergence properties due to its multi-modal log likelihood parameter space. We demonstrated that with our simulated dataset, choosing initial guess and slip values that summed to less than one allowed for convergence towards the ground truth values in the two parameter case and in the four parameter case, applying this same rule resulted in a convergence distribution with a single mode close to the ground truth value. Lastly, we found that use of the Prior Per Student model eliminated the multiple maxima dilemma in the two parameter case for our synthesized datasets and use of a prior seeding heuristic for PPS resulted in performance comparable to having perfect knowledge of the individual prior confidence probabilities. This research raises a number of questions such as how KT models behave with a different assumption about the distribution of prior knowledge. What is the effect of increased number of students or increased number of question responses per student on parameter learning accuracy? How does PPS converge with four parameters and what does the model fit parameter convergence space of real world datasets look like? These are questions that are still left to be explored by the EDM community. 
 Acknowledgements.
 We would like to thank all of the people associated with creating ASSISTments listed at www.ASSISTments.org. We would also like to acknowledge funding from the US Department of Education, the National Science Foundation, the Office of Naval Research and the Spencer Foundation. All of the opinions expressed in this paper are those of the authors and do not necessarily reflect the views of our funders.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Navigating the parameter space of Bayesian Knowledge Tracing models: Visualizations of the convergence of the Expectation Maximization algorithm</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/151">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Off Topic Conversation in Expert Tutoring: Waste of Time or Learning Opportunity?</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/151/authorlist"/>
		<swrc:abstract>While many aspects of tutoring have been identified and studied, off topic conversation has been largely ignored. In this paper, off topic conversation during 50 hours of one-to-one expert tutoring sessions was analyzed. Two distinct methodologies (Dialogue Move occurrence and LIWC analysis) were used to determine the anatomy of off topic conversation. Both analyses revealed that the expected social talk occurred, but pedagogically-relevant talk emerged as well. These occurrences may reflect the discussion of more global pedagogical strategies. These findings suggest that off topic conversation may serve a useful purpose in tutoring and that further investigation is warranted.</swrc:abstract>
		<led:body><![CDATA[ 1. Comparison of tutor dialogue moves within off topic modes and base rate. 
 As was expected, student and tutor off topic dialogue moves along with other socially- focused dialogue moves occurred more frequently during the non-pedagogical modes [8]. Consistent with this expectation, problem solving and other pedagogically-focused dialogue moves occur outside of non-pedagogical modes. In particular, the absence of tutors asking questions, students answering questions, and tutors giving feedback demonstrates that non-pedagogical modes are truly a separate, distinct time of the tutoring session. 
 Table 2. Comparison of student dialogue moves within off topic modes and base rate. 
 Those occurrences which break from these general patterns are of particular interest. The only pedagogical move to occur more frequently in non-pedagogical modes was preview. This could indicate that non-pedagogical modes are being used as a transition between new topics or problems. Preview could potentially be occurring within Introduction, which can be thought of as a preview to the entire tutoring session. For student dialogue moves, the higher occurrence of metacomment shows that non-pedagogical modes contain discussions of the student’s knowledge. Many metacomments are delivered in response to tutor comprehension-gauging questions (i.e., “Do you understand?” “Okay?”). The fact that comprehension-gauging questions are not significantly occurring during non-pedagogical modes suggests that student knowledge and comprehension is being discussed in a different context. However, the strongest occurrences by far were still tutor (d = 2.45) and student (d = 1.79) off topic dialogue moves. Given that these dialogue moves serve as a catchall for any topic outside of the tutoring topic (e.g., algebra), it is difficult to truly determine what occurs during non-pedagogical modes. While it was casually observed that these dialogue moves ranged from after school activities to study strategies, the exact proportion of each is currently unknown. There are two options for expanding our analysis of off topic dialogue moves. One is to create a new coding scheme that makes finer distinctions in off-topic. The other option, which we discuss next, is to use a text analysis tool to look for text-level features that might show what’s going on inside off topic. 
 3.2 LIWC analysis of off topic modes.
 The comparisons between the Off Topic and Scaffolding modes along LIWC dimensions were done using a series of paired t-tests. The means (values in % of words), standard deviations, t-values and effect sizes (Cohen’s d) of each comparison can be found in 
 Table 3. The analysis was conducted on both tutor (T) and student (S) contributions during the tutoring sessions. These same comparisons were also made using the Wilcoxon’s signed-rank test, as a normal distribution of scores cannot be assumed. However, those results very closely mirror the results of the paired t-test, and so only the paired t-test comparisons are presented here. A Bonferroni correction was not used in this analysis; as this is exploratory research that will be used to orient future research, the authors felt that a conservative correction would result in a loss of critical, if minor, information. In sum, the results here seem to indicate that every significant category difference favored the Off Topic mode, with the exception of when students use ACHIEVEMENT and FUTURE words. 
 Table 3. Occurrence of LIWC category words.
 In general, it may be said that there is more to off topic conversation than simple socializing and “time wasting.” Instead, there is a complex dynamic within the Off Topic mode, where the tutor and student are achieving a balance of work-related discussion and subtle socializing. Though off topic conversation seems to be an unlikely place for WORK words to arise, the Off Topic mode contains significantly more WORK words than does Scaffolding. At first, this seems counterintuitive; however, this difference may be due to the way in which work is talked about in these two different modes. In Scaffolding, work may not be discussed on a superficial level, as this is where work is actually performed. Off Topic may be a place to discuss work on a superficial level, without content. The authors of LIWC list examples of WORK words being things like “class” and “graduate,” so perhaps Off Topic conversation is a place where the student and tutor talk generally about schoolwork and homework. This could be supported by the significantly elevated use of HOME words in Off Topic as opposed to Scaffolding. Perhaps HOME is being discussed in the context of homework. Word categories that would hint at a more social use of HOME words, like FAMILY and FRIEND words, do not significantly differ from Off Topic to Scaffolding. In fact, those two categories make up less than 1% of the words in both modes. This suggests that HOME words are not being used in a social context, but rather, in work-related discussion. This is similar to the results of [8]; in that work, tutors engaged in “social talk” with their students, which involved discussing learning strategies. Our tutors may be doing something similar during their “social talk” (Off Topic). It may be the case that expert tutors use off topic conversation to discuss more general studying strategies. This may also explain why TENTATIVE words and NONFLUENCIES occur significantly more in Off Topic than they do in Scaffolding on the part of the tutor. Rather than overtly stating problem solving and study suggestions, the tutor may use TENTATIVE words and NONFLUENCIES to lessen the face threatening nature of these suggestions. In one tutoring session, the tutor tells the student that, although the teacher assigned the odd problems for homework, she should work additional problems to get more practice. While this can be portrayed as a mere suggestion, it is alluding to the student’s deficient abilities and need for further practice. Suggesting additional work like this may induce the tutor to use more words like “maybe,” “perhaps,” and “umm”, given that barking orders is unlikely to lead to the completion of this additional work. Although the LIWC results do suggest the presence of work-related discussion, there is an undeniable socio-emotional factor involved in off topic conversation. Generally, the Off Topic mode contains more POSITIVE EMOTION words than does Scaffolding; this aligns with work by [16], which found that the emotion “happiness” was much more likely to occur with tutor and student off topic conversation than other portions of the tutoring session. These off topic conversations, then, may be used as a sort of short “break” from the tutoring material that restores positive emotion and builds rapport between the tutor and student. This positive emotion and rapport building may act as a buffer against some of the direct, negative feedback that expert tutors give [17]. However, other affective LIWC categories like AFFECTIVE PROCESSES, NEGATIVE EMOTION words, and ANXIETY are not used in significantly different amounts between Off Topic and Scaffolding. This may be indicative of students’ greater comfort in discussing positive emotions, as their negative emotions are likely tied to past and current academic struggles and failures. However, it may instead reflect that the purpose of Off Topic is not to discuss the emotional state of the student during learning. Which may also mean that off topic conversation does not necessarily include “pep talks”, as [8] suggest. In addition, tutors do not use a larger amount of ACHIEVEMENT words Off Topic, suggesting that they are not trying to overtly bolster students’ feelings of confidence. Instead, rapport seems to be built in more subtle ways, such as by using more SOCIAL PROCESSES words like “we” and “us”, and perhaps by using higher-level strategies of rapport building like humor and solidarity statements. 
 4 Conclusion.
 These two methodologies seem to converge and support our initial casual observations that off topic conversation is more than simply social talk or irrelevant ramblings. Off topic does not seem to be simply an “other” category. We feel that the evidence supports claims that off topic dialogue may serve motivational uses, to discuss more global pedagogy or study skills [8], build rapport [11], and in certain cases serve as a much needed mental break from tutoring. Exploratory studies often are limited by the use of a single methodology. This study benefits from the use of two distinct approaches to investigating off topic conversation in tutoring. One approach utilized pre-existing coding schemes to determine whether the activities generally assigned to pedagogical conversation are occurring to any degree during off topic conversation. The second approach brings in a new and different analysis of the dialogue. This approach allows for a more objective look at the data, whereas results from the pre-existing coding scheme could be critiqued as simply an artifact of our coding methodology. Given that both approaches showed off topic conversation to be complex and multidimensional, this convergence gives support to further exploration. These results have given an improved depiction of what occurs during off topic conversation; however, we are still only able to speculate on its anatomy. Future research will reveal the true role and importance of off topic conversation in tutoring. These findings have afforded a framework to begin future, more directed investigations. First, they have allowed us to determine whether further exploration is even worthwhile. These exploratory findings suggest that further investigation is, in fact, warranted. Second, the findings here can be used as the basis for future coding schemes. Whether a manual coding scheme or an automated methodology, such as probabilistic topic models [19], is employed, either can be used to determine the proportion of off topic conversation that is dedicated to global pedagogy, building rapport, social topics, and possibly even irrelevant ramblings. Through the accurate depiction of off topic conversation, its most advantageous features can be applied to the building of ITSs. While implementing off topic conversation into ITSs under the current conception of discussing social topics seems peculiar, incorporating those pedagogical and rapport- building dimensions of off topic conversation would be a useful addition. So while an ITS may never form a deep, meaningful social bond with a student, it could help to increase learning in a broader scope than simply the present topic. ITSs such as MetaTutor already incorporate strategies similar to our proposed global pedagogy during learning [20-22]. By incorporating the global pedagogy of expert tutors as well as more local pedagogical strategies, ITSs can give greater aid to struggling students in over many disciplines.  Further analysis will be needed to know the exact nature of off topic conversation and its potential usefulness in building ITS systems. 
 Acknowledgements.
 This research was supported by a grant awarded by the U. S. Office of Naval Research (N00014-05-1-0241) and the Institute of Education Sciences (R305A080594). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Office of Naval Research or the Institute of Education Sciences.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Off Topic Conversation in Expert Tutoring: Waste of Time or Learning Opportunity?</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/blair-lehman"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/blair-lehman"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/whitney-cade"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/whitney-cade"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-olney"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-olney"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/151/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/blair-lehman"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/whitney-cade"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-olney"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/152">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Skill Set Profile Clustering: The Empty K-Means Algorithm with Automatic Specification of Starting Cluster Centers</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/152/authorlist"/>
		<swrc:abstract>While students’ skill set profiles can be estimated with formal cognitive diagnosis models [8], their computational complexity makes simpler proxy skill es- timates attractive [1, 4, 6]. These estimates can be clustered to generate groups of similar students. Often hierarchical agglomerative clustering or k-means clustering is utilized, requiring, for K skills, the specification of 2K clusters. The number of skill set profiles/clusters can quickly become computationally intractable. Moreover, not all profiles may be present in the population. We present a flexible version of k- means that allows for empty clusters. We also specify a method to determine efficient starting centers based on the Q-matrix. Combining the two substantially improves the clustering results and allows for analysis of data sets previously thought impossible.</swrc:abstract>
		<led:body><![CDATA[ 1. Set the 2K starting cluster centers mg appropriately in the K-dim hyper-cube (Sec. 4). 2. Create the cluster assignment vector A by assigning each Bi to the closest mg. 3. For all g, if no Bi is assigned to mg, i.e. FORMULA_4 4. Alternate between 2) and 3) until the cluster assignment vector A does not change. 
 This algorithm continues to minimize the WC criterion with each step; the empty clusters make no contribution to the criterion value. We discuss the choice of appropriate starting centers in Section 4. Our k-means variation allows for empty clusters or fewer clusters than originally requested. This flexibility removes the constraint that there be one cluster per skill set profile. Early work in this area has relied heavily on small examples with K = 2, 3, 4 skills. With the advent of online tutoring systems and end-of-year assessment exams, the number of skills has grown considerably. It is not uncommon to be interested in K = 10, 15, 20, etc. For K = 10, say, we would have 210 = 1024 different skill set profiles. In practice, it would be extremely uncommon to see a sample with all 1024 different subgroups. Moreover, the large number of profiles computationally prohibits clustering of samples where N < 2K. Our k-means variation allows for the identification of the clusters/profiles that we do have; any computational constraints (e.g. memory, storage) are limited and are a characteristic of the operating system/platform and not of the algorithm. 
 4 Choosing Starting Centers.
 It is well-known than k-means can be dependent on the set of starting centers [9]. Given our goal of identification of the true skill set profiles in the population, a natural set of starting centers might be the hypercube corners αi = {αi1, αi2, ..., αiK} where αik ∈ {0, 1}. If students map closely to their profile corners, k-means should locate the groups affiliated with the corners very quickly. However, even if all profiles are present, the students may not be near their profile corner due to attenuation of our skill estimates in the presence of multiple skill items. Below are two possible Q matrices for J = 24 items. In Q1, items 1-8 only require skill 1, items 9-16 only skill 2, and items 17-24 only skill 3 (all single skill items). In Q2, the first 12 items are single skill; the remaining items require multiple skills. If a student’s true skill set profile is {0, 1, 0}, (s)he should miss items 1-8, 17-24 in Q1 but receive a Bi2 of 1. In Q2, (s)he should miss items 1-4, 9-24 which correspondingly drops Bi2 from 1313 to. Similarly, a student with profile {1, 0, 1} will have Bi1 = Bi3 = 1 for Q1 but see a drop in capability from 1313 to 13 using Q2. (Analogous drops are seen in sum scores.) These attenuated estimates are not reflective of the true profiles. 
 4.1 Generating Response Data.
 To illustrate, we generate response data for N = 250 students for both Q-matrices from the deterministic inputs, noisy “and” gate (DINA) model, a common educational research conjunctive cognitive diagnosis model [8]. The DINA item response form is 
 FORMULA_5.
 The slip parameter s j is P(yi j = 0 | ηi j = 1); the guess parameter g j is P(yi j = 1 | ηi j = 0). Similar to the capability matrix (and the sum score), if student i is missing any skills required for item j, P(yi j = 1) decreases due to the conjunctive assumption. Prior to simulating the yi j, we fix the skills to be of equal medium difficulty with an inter-skill correlation of either 0 or 0.25 and generate true skill set profiles αi for each student. (In our work thus far, only a perfect inter-skill correlation has a non-negligible effect on the results.) These choices spread students among the 2K true skill set profiles. We randomly draw our slip and guess parameters (s j ∼ Unif(0, 0.30); g j ∼ Unif(0, 0.15)). Given the true skill set profiles and slip/guess parameters, we then generate the student response matrix Y and estimate their corresponding capabilities. 
 Figure 1a below contains the capabilities estimated from the Q1 matrix, numbered by their true profile (slightly jittered for visualization purposes). The absence of multiple skills allows the mapping of the students to (near) their profile corners. Figure 1b contains the capabilities estimated via the Q2 matrix, also jittered, numbered by the true profile. The presence of multiple skills has pulled the non-{1, 1, 1} profiles toward the profile {0, 0, 0}. Using the hypercube corners as the starting centers for empty k-means in the second data set will make it more difficult to find the true groups. In fact, if there are no students within a corner’s octant (0.5 as the cutoff), that profile will not be found. When multiple skill items are included, the hypercube corners are no longer representative of the true profiles. We would expect their locations to be attenuated as well. Given the Q matrix, we map the true skill set profiles to their corresponding rescaled locations in the hypercube. 
 Figure 1: a) Bi for Q1; b) Bi for Q2; Starting centers indicated with X’s. 
 4.2 Rescaling the Starting Centers.
 Let αp be the possible true skill set profiles where p = 1, 2, ..., 2K (e.g. {0, 0}, {1, 0}, {0, 1}, {1, 1} for K = 2). Let Ap j =∏Kk=1 αq jkpk . Then Ap j indicates whether or not a student with true skill set profile p has all the skills necessary to answer item j. If yes, Ap j = 1, 0 otherwise. Our starting centers C∗p are then, for k = 1, 2, ..K, 
 FORMULA_5.
 The numerator is counting the number of items with skill k that the skill set profile p could answer. The denominator is the number of items requiring skill k. (Note that∑Jj=1 q jk = Jk. If we were using sum scores, we would not scale C∗pk by the denominator.) If the Q matrix contains only single skill items, the starting centers return to the hypercube corners αi. In our example, the starting centers for Q2 would be, (as indicated by X’s in Figure 1b): (0, 0, 0); (4/13, 0, 0); (0, 4/13, 0); (0, 0, 4/13); (7/13, 7/13, 0); (7/13, 0, 7/13); (0, 7/13, 7/13); (1, 1, 1). These values are representative of the true profile locations given the Q matrix if all stu- dents answered items according to their true profiles. They are derived with respect to the conjunctive assumption made by the capability matrix (and the sum score). In practice, we would expect students to slip up or make some lucky guesses; however, setting the starting centers to these rescaled profile locations will allow the empty k-means (or even traditional k-means) to easily find the groups. With respect to missing profiles, we still use the full set of C∗p as our starting centers and allow the algorithm to discard the unnecessary ones. Note that Ap j is similar in form to ηi j in the DINA model. Although they serve a similar function, our approach is not unique to clustering DINA-generated data. The capability score (and the sum score) are reasonable estimates for any conjunctive CDM. As we will see in Section 5, we can similarly rescale the centers for use with other CDMs. 
 4.3 Performance.
 After calculating the corresponding B matrix, we cluster the students using hierarchical clustering (complete linkage) and traditional k-means, both asking for 23 = 8 clusters. We then re-cluster with traditional k-means and the empty k-means variation using the rescaled starting centers. (Note that the symmetry of the rescaled starting centers is a direct result of the balanced Q matrix; an unbalanced Q matrix will give asymmetric starting centers.) To gauge performance, we calculate percent correct as the correct classification rate based on the best one-to-one mapping of clusters to true skill set profiles. We also quantify the clusters’ agreement to the true profiles using the Adjusted Rand Index (ARI), a common measure of agreement between two partitions [7]. Under random partitioning, the expected ARI value is zero. Larger values indicate better agreement; the maximum value is one. 
 Table 1: Comparing Clustering Methods with the True Skill Set Profiles via % Correct, ARIs. 
 All methods performed well; the rescaled starting centers resulted in the highest percents correct and ARIs. Our k-means variation (correctly) found 8 clusters. In order to assess the performance when not all possible skill set profiles are present, we then removed the three smallest profiles {(0, 0, 1); (0, 1, 1); (1, 0, 1)} (which is the most favorable situation for the other methods) and re-clustered. 
 Table 2: Comparing Clustering Methods with a Subset of the True Skill Set Profiles via % Correct, ARIs. 
 Again, all methods performed fairly well. Random starting centers for k-means showed a decrease in performance when clustering a subset of the profiles. Traditional k-means returned an error when using the rescaled starting centers since the initial cluster assignment returned empty clusters (as expected). Our k-means variation, however, found five clusters and had almost perfect agreement with the true skill set profiles. Even if we knew the true number of clusters (5), it is not a guarantee of superior performance. The five-cluster complete linkage solution was 93.5% percent correct with an ARI of 0.937. The traditional k-means (5 random centers) was 80.5% correct with an ARI of 0.679. Even when using only the five rescaled starting centers corresponding to the present profiles, the traditional k- means performance was comparable (97.6%, ARI = 0.946) to using our k-means variation which used the rescaled centers but required only an upper bound on the number of clusters. 
 5 Simulations.
 We explore the performance of our approach using two conjunctive CDMs while varying N, J, and K. For each simulation, the Q-matrix is randomly generated with a parameter dictating the percentage of single skill questions. We initially cluster all generated students and then remove a random number of profiles and re-cluster (the notation “—” corresponds to errors in standard k-means). We first simulate from the DINA model (Section 4.1). 
 Table 3: Performance with DINA-generated Responses: % Correct (ARIs). 
 In all cases, the k-means variation with attenuated starting centers outperforms the other methods (via ARIs). We also noted in our simulations (not all presented here) that increas- ing the percentage of multiple skill items decreases the other methods’ performance while our k-means variation remains fairly steady. Moreover, in “classroom” size data sets, this variation identified the profiles present while other methods unnecessarily split the clusters. We also present results using responses generated from the noisy input, deterministic output “and” gate (NIDA) model, another common conjunctive CDM. The item response form is 
 FORMULA_5.
 where sk, gk are slip, guess parameters indexed on skill (rather than item); see [8] for further details. Responses are similarly generated; the results are comparable. 
 Table 4: Performance with NIDA-generated Responses: % Correct (ARIs). 
 6 Conclusions.
 The modified k-means algorithm presented here, called “empty k-means”, allows a more flexible approach to clustering for use in applications such as skill set profile clustering where the true number of clusters is not known, but may be bounded. It allows the user to specify a maximum number of possible clusters which removes the need to make a subjec- tive decision on the number of clusters. We define our attenuated starting cluster centers by the Q-matrix (giving us the hypercube corners in the case of all single skill items). As seen in the simulated results, in cases where all natural clusters were present, such starting val- ues gave superior clustering results (compared with both k-means with random starts and hierarchical clustering). In cases where some natural clusters were not present, the empty k-means algorithm with the defined starting values again had superior performance, while commonly traditional k-means would report an error due to empty clusters. Other appli- cations might fit this framework as well. For example, compositional data on the simplex would have natural cluster centers on the corners of hyper-triangle. Empty k-means could also be used to investigate both the validity of theorized cluster centers and the believed number of clusters. Further exploration of this approach is ongoing.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Skill Set Profile Clustering: The Empty K-Means Algorithm with Automatic Specification of Starting Cluster Centers</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/152/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/153">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Data Reduction Methods Applied to Understanding Complex Learning Hypotheses</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/153/authorlist"/>
		<swrc:abstract>Modern learning science researchers are facing a flood of data as it becomes easier and easier to collect multiple streams of information from students before, during, and after learning experiments. While oftentimes these experiments do experimentally manipulate specific variables to improve responses on a posttest, these experiments are also interested in how the many related student factors explain who responds to the treatment and why. This poster introduces a recent experiment and explains how the data were analyzed using a combination of exploratory factor analysis (using SPSS) and exploratory structural equation modeling (using Tetrad) to partially refute a theoretical hypothesis and reveal a new explanation for further testing.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Data Reduction Methods Applied to Understanding Complex Learning Hypotheses</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/153/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Identifying High-Level Student Behavior Using Sequence-based Motif Discovery</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154/authorlist"/>
		<swrc:abstract>We describe a data mining technique for the discovery of student behavior patterns while using a tutoring system. Student actions are logged during tutor sessions.  The actions are categorized, binned and symbolized.  The resulting symbols are arranged sequentially, and examined by a motif discovery algorithm to detect repetitive patterns, or motifs, that describe frequent tutor events.  These motifs are examined and categorized as student behaviors.  The categorized motifs can be used in real-time detection of student behaviors in the tutor system.</swrc:abstract>
		<led:body><![CDATA[ 1.2 sec, (j) 1.2 to 2.9 sec, (k) greater than 2.9 sec. First, there are two categorical bins, skip and solve on first attempt. These are each determined from an indicator in the log data for that problem. Skipping a problem implies only that students never clicked on a correct answer; they could have worked on the problem and then given up, or immediately skipped to the next problem with only a quick look.  Solved  on first attempt indicates correctly solving the problem. If neither of the first two bins are indicated in the logs, then the secOther metric measures the mean time for all attempts after the first. The divisions of 1.2 sec and 2.9 sec for the latter three bins were obtained using the mean and one standard deviation above the mean for all tutor usage; (i) less than 1.2 seconds would indicate guessing, (j) would indicate normal attempts, and (k) would indicate a long time between attempts. numIncorrect – (o, p, q) - Each problem has four or five possible answer choices, that we divide into three groups: (o)  zero incorrect attempts, indicates either solved on first attempt, skipped problem, or last hint solves problem (defined by the other metrics); (p) indicates choosing the correct answer in the second or third attempt, and (q) obtaining the answer by default in a four answer problem or possibly guessing when there is five answer problem. Some of the bins have dependencies that effect motif discovery and analysis.  For example, last hint solves (c) precludes solved on first (h) and skipped (g); solved on first (h) requires zero incorrect attempts (o).  In addition, by binning skip (g) in the secOther group, the timing of incorrect attempts is lost when the problem is skipped. 479 student strings representing 3762 total problems were constructed and concatenated into a 15048 character input string for motif discovery.  The first 160 characters of the string (40 words/problems, separated at problems for clarity) are: 
 afkq bfho cekp bfho aeho cekq bfiq bfkq bfip aeho aeho aeip aeho aeip afip cekp aeho afip cfho aeho cfho bfkp bekp bekp aeho aekp beho bekp aeho cfho bfkq aekp ceho cfkp bfjp aeip bfkp aeho afip afho 
 In the first problem, afkq indicates no hints used, greater than 30 seconds to first attempt, over 2.9 mean seconds in other attempts, and most or all choices were made to find the solution.  In the next problem, coded bfho, the student asks for one or two hints, greater than 30 seconds to first attempt, then solves the problem on first attempt.  The sequence based motif discovery algorithm searches the input string in step two of the process. A detailed description follows. 
 5 Word-skipping sequence-based motif discovery algorithm.
 Our sequence-based motif discovery algorithm is a modification of the PROJECTION algorithm [10].  It is similar to the Chiu et al. [5] projection algorithm, except that our sliding window moves per word rather than per character. The PROJECTION algorithm is an efficient way to find planted strings in a long sequence of characters, and our modification to the algorithm allows us to apply it in a multivariate fashion where each character of the word represents a variable. In order to illustrate the algorithm, we first present an example, and then present a formal description of the algorithm. 
 5.1 Example word skipping projection.
 Take as input a string words, four characters each, with no separation.  Construct a matrix S as a 40 character sliding window that slides 4 characters (1 word) per row. The below 6 x 40 character matrix (Figure 1a) represents the first 64 characters or 16 words. Randomly select 10 columns in the S matrix as the projection highlighted below. Project the selected columns to a new matrix (Figure 1b). If there is a string match between any pair of rows, then add a collision to the collision matrix (Figure 1c). The highlighted rows (3 and 4) match and thus the 3rd row and the 4th column has a collision. 
 5.2 Random projection multivariate motif discovery algorithm.
 Function Name: wordMotif Inputs: word sequence (t), motif size in characters (n), word size (w), projection length (p), number of iterations (m), max character distance (d), number of motifs to find (c). Outputs: c motif lists with start index and strings of length n for each motif example. 
 Figure 1. Steps of word skipping projection. (a) the S matrix of 40 character substrings highlighted with a random projection (b) the projected matrix with 2 matching substrings (c) the resulting collision added to the collision matrix. 
 wordMotif(t,n,w,p,m,d,c).
 1. Construct S matrix of size (t/w * n) by sliding an n sized window w characters at a time. 2. Create collision matrix using project(S,p,m) 3. Compare pair of examples (A,B) with highest collision value 4. If (A,B) do not overlap and are within a hamming distance of d characters, then test them against other members of the S matrix, adding all members that are within hamming distance d to the motif set, and removing the collision values from the collision matrix. 5. Repeat 3 and 4 until c motifs are found or the collision matrix is exhausted. 6. Return the lists of up to c motifs 
 project(S,p,m).
 1. let k be the number of rows in S 2. construct an empty k x k collision matrix 3. repeat m times a. make a k x p matrix based on a random mask of p columns of the S matrix b. add a collision for each pair whose string is equivalent. 4. Return collision matrix 
 The algorithm outputs a matrix of indices into the string, with a column for each discovered motif. For this study we found thirty motifs. The first two indices of the first 10 motifs are shown in Table 1. 
 Table 1. The first 2 indices of the first 10 motifs. 
 Each value in the matrix is an index to the first character of the motif. We can index into the original data to determine the symbols in each of these motifs. The first instance of the motif (in row 1) is representative of the pattern, and the other instances (in row 2, etc.) will be identical or within 10 characters of both the first and the second instance.  Table 2 shows the first instance of the first ten motifs. 
 Table 2. Ten of the motifs The index of the first character is followed by the 40 character motif with a separation each word to see problem characteristics. 
 6 Discovered motifs.
 In the third step, we examine and analyze the discovered motifs to determine the high level behavior that each motif discovered. We group motifs with similar behaviors. For this study we used the algorithm to detect thirty motifs.  Chiu et al. [5] suggest a need to eliminate degenerate motifs, which are motifs that have no informational content, such as a sequence of repeated characters. In the case of the tutor data, repeated words are not degenerate because they inform us that the student is repeating a particular behavior. Repetition of undesired behavior is an important feature to capture, so we do not remove such motifs as degenerate. In the 30 discovered motifs, there were a number of repeated motifs. This is because a number of motifs were essentially straight, i.e. repetitions of a motif word, so there are cases of motifs that have essentially the same structure; these would overlap with each other, so they are considered distinct patterns.  By grouping these exact match motifs, and also by comparing the different motifs by eye, we grouped the 30 motifs into 7 distinct meaning groups coded g, f, F, k, r, z, n: Game-like (g). adgo (10), adip (10), or adiq (10) – Student is not reading the problems and either skipping or making quick guesses. Frustration (guess) (f). adiq (1) adgo (9) – Guessing to find solution, then skipping the next 9 problems.  This could be an indication of frustration. Frustration (hints) (F). cdgo (1) adgo (9) – Using the hints to find solution, then skipping the next 9 problems.  This also could indicate frustration, and was grouped together with the previous motif. Not challenged (k). a[ef]ho (10) – Solving the problem on first attempt, not using hints, not guessing.  This student is using the tutor appropriately, but not being challenged. Too difficult (r). ceho (10) - student is taking time to read the problem, then using hints to find the answer.  These students could be working but the material is too difficult for them to solve the problem themselves. Skipping (z). adgo (5) aeho (5)  – Student skips 5 problems, then solves 5 normally on first attempt.  Taking time to read all problems, then answering or skipping depending on whether the answer is known. On-task(n) aeiq  aeho  aeho  aekp  aeho  aeiq  aeho  aeip  aeho  aeip - This is the most complex motif found and seems to indicate “on task learning” tutor usage; the student is always reading the problem and making a good first attempt, with a mixture of solving on first attempt (aeho), solving after some attempts (aekp, aeip) and guessing (aeiq).  The student is not using the hints nor skipping. With these groups coded as a single character, we can look a the progress of one student by converting the string of words to a string of motif groups for quicker interpretation.  For example, in the too difficult (r) grouping, these problems show the student is taking 5 to 30 seconds before taking any action.  However, if the student initially chose the wrong answer, subsequent attempts were quick guesses or gaming hints.  In contrast, in the game-like (g) grouping the student was not reading the problem, simply skipping, or making quick guesses. In the frustration (guess) (f) grouping the student was skipping after an attempt.  This could just be an indication of skipping problems, or possibly the error in the first attempt triggering a frustration response to skip the next 9 problems. And the on-task(n) grouping appears to be a mixture of responses for on task tutor use. 
 7 Evaluating student interaction.
 The final step in the process is to apply the 7 meaning groups to individual students. To do this we can convert each student string into a student motif string. This is done by scanning the student string and replacing each problem string with a dash (-) until a motif is detected; at this point, the problem string is replaced with the meaning group code associated with the motif. The meaning group code is only placed at the final problem of each motif (the last four characters). The conversion is shown below for two students. Using the meaning group code, we generated the student number followed by patterns for each student representing their tutor interaction. We examine two students below. 
 For student 4362 the not challenged (k) motif indicates solving of problems on the first attempt without using hints.  (The (-) patterns are those which did not match any motif.)  The behavior is a lagging indicator, representing the current and previous 9 problem. Student 4363 started by gaming the tutor, game-like (g) either skipping or guessing quickly to find answers.  This is followed by not challenged (k), a string problems solved on first attempt followed by more tutor gaming.  The too difficult (r) string of r’s are where the student began using the hint facility but in a manner to find answers. After about 20 of these too difficult problems he/she returned to skipping or guessing. This conversion process illustrates how our discovered motifs can be used in a real-time application. A tutor can detect these patterns and respond based on the meaning group in order to have a more personalized interaction with the student.  With student 4362 a tutor could increase problem difficulty.  For student 4363, a tutor could intervene at problem 15 where gaming was detected, perhaps by introducing the hint system, or directing the student to a teaching video. 
 8 Discussion and future work.
 This paper describes a novel method for determining student behavior without linking the behavior to performance outcomes. We have shown a case study where a number of meaningful behaviors were discovered using a combination of hand chosen features and automatic pattern discovery. The features that have been found can be used to detect student behaviors so that the tutor can react in real time. However, these outcome of our study needs to be verified in future work. A number of future directions are discussed below. We will validate that the discovered motifs accurately represent student behavior by implementing them in the tutor. Upon motif detection, the corresponding meaning group will be verified by a person in real time. The student or a teacher observer will verify the meaning group by responding to a query during the tutoring session, e.g.. “Are you skipping problems because...” These responses will be compared with the predicted behaviors for validation. We will study automatic data categorization as modifications to our process. The data binning is the most user intensive part of this process. Finding methods to automate it would allow for broader use of these methods. We will compare a number of different motif window sizes in order to understand the time scale of problem behavior patterns. The value used in this paper, ten problems, is sufficient to describe behavior, and it yielded a manageable number of informative motifs. However, other window levels may yield motifs of different quality and quantity.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Identifying High-Level Student Behavior Using Sequence-based Motif Discovery</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/david-h-shanabrook"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/david-h-shanabrook"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/david-g-cooper"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/david-g-cooper"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/beverly-park-woolf"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/beverly-park-woolf"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ivon-arroyo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ivon-arroyo"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/david-h-shanabrook"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/david-g-cooper"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/beverly-park-woolf"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/ivon-arroyo"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/155">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Mining information from tutor data to improve pedagogical content knowledge</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/155/authorlist"/>
		<swrc:abstract>School-based intelligent tutoring systems present a unique, largely untapped, teacher- development opportunity. Research on teacher content knowledge has shown that effective teachers develop a specialised 'pedagogical content knowledge' for teaching mathematics. School- based tutors provide a rich record of the student learning process. This record can be mined for insights that contribute to teacher’s ‘pedagogical content knowledge’. By situating this knowledge in the context of an active teaching-learning process occurring in the school, these insights contribute to a continuous data-driven teacher development exercise.</swrc:abstract>
		<led:body><![CDATA[ 1. Introduction.
 School-based intelligent tutoring systems are designed to assist teachers in meeting learning objectives by providing personalised learning opportunities to students. Such systems embody expert pedagogical knowledge as well as student models which are used to guide students through content so as to maximise learning. Mindspark®, developed by Educational Initiatives, is a mathematics tutoring system being used as a part of the school curriculum by nearly 7000 students in more than 13 schools across India. While most tutoring systems aim to provide comprehensive learning opportunities within the tutoring environment, a school-based tutoring system presents a unique, largely untapped teacher-development opportunity. Research on teacher content knowledge in the past 3 decades has brought to the table and refined the notion of ‘pedagogical content knowledge’ [1] and provided empirical measures of the subject knowledge that is required for teaching[2]. Insights mined from tutoring systems can be used to inform teachers about the nature of the learning process students go through in specific content domains and thus provide a source of continuous teacher development. 
 2. Data.
 The Mindspark® system consists of a sequence of specially designed learning units (clusters), which contain finely graded questions on concepts that make up the topic. Students learn through feedback and by going through specific remedial clusters, which help address specific problems. The questions are MCQs, ‘fill-in-the-blank’ type, multiple-select or interactive. Student response, the time taken in making the response, the time spent reading the feedback, the no. of repetitions of each learning unit and other data is recorded in a database. 
 3.1 Detecting misconceptions and common errors.
 The Mindspark system helps teachers identify specific misconceptions or learning gaps prevailing at the grade or school level. Mindspark consists of distractor-driven MCQs that are capable of trapping common errors. 
 Figure 1 – Example of a common error/misconception highlighted and a table showing the use of standard deviation of wrong answer % as a sorting criterion. The correct answer is marked in green, common wrong answers in red/pink. 
 A measure that directly picks up questions where a large proportion of students have been drawn to a particular wrong answer (thus indicating a common error pattern) is the standard deviation of the % of students choosing the various wrong answers. A higher SD identifies clustering of response in particular options. Tutor data from across grades can then be used to inform teachers about how misconceptions evolve with age and ability. 
 3.2 Identifying ‘speed-breakers’ in learning.
 The Mindspark learning units contain very finely graded learning units based on conceptual progression as determined by curriculum and content experts.. However, students sometimes treat closely related question-types very differently and such a situation often represents a ‘kink’ in the curriculum and a mismatch between student understanding and teacher’s perception of how difficult different related tasks are. Such potential kinks in the curriculum represent specific points where student learning may be derailed, both in the tutor environment as well as in the classroom. 
 3.3 Identifying and gaining from difficult learning situations.
 Students using the Mindspark system occasionally get trapped in difficult learning situations. They make no progress and repeatedly fail to clear particular learning units despite feedback. By using correlation-based measures to classify student trajectories, we provide useful information to teachers on the student’s experience and make it possible for these situations to be fruitfully utilized to systematically improve teacher’s pedagogical content knowledge. 
 4. Conclusion.
 School-based intelligent tutoring systems provide a rich record of the student learning process. This record can be mined for insights that contribute to teacher’s ‘pedagogical content knowledge’. By situating this knowledge in the context of an active teaching-learning process occurring in the school, these insights contribute to a continuous data-driven teacher development exercise.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Mining information from tutor data to improve pedagogical content knowledge</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/suchismita-srinivas"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/suchismita-srinivas"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/muntaquim-bagadia"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/muntaquim-bagadia"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/anupriya-gupta"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/anupriya-gupta"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/155/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/suchismita-srinivas"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/muntaquim-bagadia"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/anupriya-gupta"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>A Data Model to Ease Analysis and Mining of Educational Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156/authorlist"/>
		<swrc:abstract>Learning  software  is  not  designed  for  data  analysis  and  mining. Because  usage  data  is  not  stored  in  a  systematic  way,  its  thorough  analysis requires long and tedious preprocessing. In this contribution we first present a data model to structure data stored by Learning Management Systems (LMS). Then we give an overview of the system architecture that performs the structure/ export functionality and of its implementation for the Moodle LMS. Finally, we show  first results using this data model for analysing usage data.</swrc:abstract>
		<led:body><![CDATA[ 1.1   Background and Related Works.
 In our own university and in most universities at least in Germany, LMS are used both in distance education and face to face teaching. They are becoming a must-have in distance education [18]. They are more and more used in face to face teaching as they make the administration of a course much easier for teachers, and provide a handy way to cater for special needs. Experienced teachers can handle better a diverse classroom as supplementary learning materials can easily be made available through them for example. LMS provide a virtual place where students can find learning materials, study guides as well as an overview of their results, and where they can communicate with teachers and with other students. The functionality of an LMS can be divided in three main parts: Management of learning resources, management of users, and communication between users [18]. However statistics and reports are usually basic. To illustrate this last point, we list below a few questions that reporting facilities of most of the currently used LMS cannot handle: 
 1.How many students have never viewed Learning Resource A? 2.If students do well on activity B, do they also do well on activity C? 3.If students solve exercise D, do they also solve exercise E? 4.What is the average mark on quiz G got by students who have viewed resource F? 5.Which courses use a lot of Audio Learning Resources? 
 To manage properly learners, especially distance learners, it is quite important to gain a good overview of their learning behaviours. To manage properly degrees, it is important to harmonize the different modules offered in a degree. The aim of our work is to complement LMS in all aspects dealing with data analysis and mining. Many works to analyse and mine data stored by LMS have been undertaken, see [17, 1] for some overview. To the best of our knowledge, all these works do some ad hoc preprocessing of the data.  They do not aim at proposing some data model for analysis and mining that could be shared by all LMS independently of their internal structure. The work undertaken in [16] bears similarity with our work in the sense that it considers all data stored by an institution in higher education and unifies it into a model to explore behaviours of students. Another work that bears similarities to our work is PSLC Datashop [9], an open repository of educational data. The analysis tools offered in Datashop are more suited for educational software such as intelligent tutoring systems. Our work is not concerned with discovering or sharing learning objects as other works such as [19, 7] do. It is concerned with describing and structuring the interactions of users with learning objects as stored in LMS. Our vocabulary to describe these interactions is partly borrowed from the vocabulary adopted by the LMS Moodle [14], as Moodle is used by a large community worldwide. The part related to the interactions of users with quizzes contains elements that the IMS specification [8] also contains, though it is much simpler that [8]. This paper is organised as follows. The following section introduces our data model. The third section gives an overview of the export tool that exports the data stored by an LMS into this data model. Section 4 presents a case study in analysing data stored in a specific course using our tool. Last section concludes this paper. 
 2    The Data Model.
 The data model we present is quite close to a fact constellation schema [4]. It contains three kinds of tables: tables to describe objects found in LMS, these tables can be seen as dimension tables; tables to describe interactions with learning objects, these tables can be seen as fact tables; and third, association tables to describe associations between objects. The choice of having a table per object comes from the observation that most LMS have a limited set of objects that teachers are used to handle. Adopting this same set should make it easier for teachers to analyse usage of resources by students. 
 2.1   The schema.
 Our data model makes several general assumptions that we expose now. First we  assume that an LMS contains users and courses. Users can enroll or sign in courses and sign off courses.  Users can have roles like “lecturer”, “administrator”, “tutor”, “student” and so on. A user may have different roles in different courses. For example a user  can be a tutor in the course “Introduction to Programming” and a student in the course “Early American History”. An LMS may contain groups that are associated to courses. Students enroll in those groups. An LMS contains forums, wikis, resources and quizzes. We call a quiz any kind of assignment, exercise or test a lecturer may wish to give to students. Forums, wikis, resources and quizzes are associated to courses. Thus a resource for example, can be used in several courses. A quiz may contain one or more questions that are also contained in the LMS. Questions are associated to quizzes and a given question can be associated to several quizzes. These general assumptions cover the particular case of LMS where resources, forums, wikis or quizzes exist only inside a given course. In this particular case an association table contains only one tuple. We assume that an LMS logs or stores interactions of users. For any given interaction, the LMS stores the identification of the user, of the course, of the resource, forum, wiki, quiz, as well as the timestamp, the nature of the interaction (“view”, “modification”, “creation”, “attempt”, “submit” and so on), the marks and the contribution when relevant. We present now in more details the tables that are the most useful for our case study, see Figure 1. For the current full set of tables, we refer to [11, 12]. Every table contains an element id, which is the key or identifier of the tuple. The five tables below describe objects usually found in LMS. 
 Table user:  This table describes users registered in the LMS. The elements firstaccess and  lastaccess are the times and dates when a user first and last accessed any kind of learning object, such as a resource, a quiz etc. in the LMS. The elements  lastlogin and currentlogin are  the  times  and  dates  a  user  logged  in  the  LMS  for  the  last  time, respectively currently. Note that a user can log in without accessing any learning object. 
 Table course: This table describes courses existing in the LMS. We assume that a course exists for a given period of time. The element timecreated is the date and time the course has been created, usually by the administrator.  The element startdate is the time and date the course is supposed to start, this time is usually fixed by the lecturer in charge. The element enrolstart is the date users are allowed to enroll in this course, and the element enrolend is  the  date  users  can  not  enroll  anymore  in  the  course.  The  element timemodified is the time and date this course has been last modified. The elements  title and shortname are self explanatory. 
 Table quiz: This table describes quizzes existing in the LMS. As already mentioned, we call a quiz any kind of assignment or test a lecturer gives to students. The element qtype is the  type of the quiz. It can take values such as “assignment”, “SCORM” and so on, according to the different kinds of quizzes an LMS makes available. The elements  qid combined with  type make up the identification of a quiz. A quiz  may contain one or more questions, see table question. The element title is a title the lecturer in charge gives to this quiz. The elements timeopen and timeclose refer to the dates and times students are allowed to answer the quiz, while the element timecreated is the date and time the quiz has been created and the element timemodified is the  date and time the quiz has been last modified. 
 Figure 1.  Snapshot of the relational schema.
 Table question: This table describes questions that make up quizzes. The element title is the title of the question, while the element text is the actual text of the problem to solve. The  element  type is  a  category like  “multiple-choice”,  “true-false”  etc.  The  elements timecreated and timemodified are as described in the table quiz. 
 Table resource: This table describes resources available in the LMS that lecturers may use in courses. The element  type describes the type of the resource like “file”,  “uri”, “directory”, “audio”, “picture” and so on. The elements timecreated and timemodified are as for quiz. The element title is the title of this resource like “transparencies01”. 
 The three following tables describe interactions with learning objects. They are the facts that are stored while users use objects of an LMS. 
 Table  quiz_log: This  table  describes  the  information  that  a  LMS stores  when  users interact with quizzes. The element user is the id (the key) of the user who interacted. The element course  is the id (the key) of the course in which the interaction took place. The elements qid and qtype refer to the quiz that was tackled.  The element grade is the mark obtained in the quiz. The element  timestamp gives the date and time of the interaction. The element action gives the kind of action that took place. An action can be “view”, in that case the user simply looked at the quiz,  “attempt”, in that case the user attempted the quiz,  “submit”, in that case the user attempted and finished the quiz, “modify” if the quiz has been modified etc. . 
 Table question_log: This table describes the information that a LMS stores when users interact with a question of a quiz, and contains all elements already included in the table quiz_log. The element penalty gives the penalty marks given in that interaction. If a quiz is run in adaptive mode then a student is allowed to try again the question after a wrong answer. In this case one may want to impose a penalty for each wrong answer to be subtracted  from  the  final  mark  for  the  question.  The  amount  of  penalty  is  chosen individually  for  each  question  when  setting  up  or  editing  the  question.  The  element raw_grade gives the raw mark obtained in that interaction. The element grade gives the marks for that question in that interaction when penalty has been taken into account. It includes also an element  question, the id of the question that was tackled, the elements type, which can take values like “multiple choice”, “true/false” and the element answers, the actual answer or answers, when several answers are allowed, given by the user in the interaction. 
 Table resource_log: This table describes the information that a LMS stores when users interact with resources. This table contains elements that are similar to the ones of table quiz_log. Finally our data model contains a number of association tables to associate objects with each other, see [11,12]. 
 3    System Architecture.
 Figure 2 presents an overview of the system architecture. The central part of the system is the abstract class “ExtractAndMap” that describes and partly implements functionalities concerning data extraction from an LMS and data generation for the data model. To create the present data model with an LMS, what is needed is to implement the abstract extract  methods according to the features of the LMS. The concrete save function can be inherited as is. The system contains an implementation for Moodle. It is implemented in Java, uses the Database Mysql [15] and the persistence framework Hibernate [6]. 
 Figure 2.  System architecture.
 4    First Results.
 We have used our system to analyze the course “Introductory Programming with Java” taught in face to face teaching to first semester students enrolled in the degree “Computer Science and Media” at the Beuth University of Applied Sciences, Berlin, in winter semester 2009/2010. In that semester 65 students were enrolled in this course. The teaching of this course is supported by the use of the Learning Management System Moodle in which mandatory as well as additional resources are uploaded for students. A list of 8 exercises belongs to the mandatory resources. Students have to solve these 8 exercises to get a mark for the practical part of the course. The lecturer has additionally offered gradually in the semester 7 self-evaluation exercises on key concepts of Java programming like methods, arrays, statements etc.. These exercises are not compulsory. Solving them is left to the sole discretion of the students. The lecturer is interested to know whether students have used these self-evaluation exercises and, most importantly, whether solving or attempting them could have a positive impact on the marks obtained in the final exam. Figure 3, obtained by simple queries, gives an overview of how students have used these exercises. For each exercise the column on the left means view, the column in the middle means attempt and the column in the right means close attempt. Note that view means that students have clicked on the resource, attempt means that they have submitted a solution and close attempt means that they have finished the exercise. 
 Figure 3.  Access to self-evaluation exercises.
 One notices a pattern that we have already observed in other courses regarding optional self-evaluation exercises [13, 11]: As the semester progresses always less students make use of them. We are interested in investigating whether a dedicated group of students emerges that keep doing the exercises during the semester. Therefore we want to know whether associations such as “if students complete exercise 2, they complete exercise 1” or ,  “if students complete exercise 3, they complete exercise 2”, and so on, hold. For that we have used the method exposed in [10]. Indeed, the answer is positive as Table 1 shows. The association rule 2→1 means “if students complete exercise 2, they complete exercise 1”. All rules have a rather high confidence and are rated as interesting both by lift and cosine. We recall that confidence is a number between 0 and 1 (highest is 1), that lift rates a rule as interesting if its value is above 1, and that cosine rates a rule as interesting if its value is above 0.66. Support gives the proportion of the data involved in the rule. 
 Table 1.  Association rules “if students attempt exercise x, they also attempt exercise x-1” 
 To investigate whether solving these self-evaluation exercises has a positive impact on the marks in the final exam cannot be achieved by correlation or regression analysis as not so many students have solved them. That means for many students we would have missing data, since 45 students have written the final exam. We have simply queried our data and looked at the average mark on each group of students. The result is given in Table 2. 
 Table 2.  Completing self-evaluation exercises and marks in the exam. 
 The line General is the minimum, maximum, mean and standard deviation obtained taking all students who have taken part in the final exam. The last column gives the mean for students who have not solved any exercise. The line Ex1 gives similar results restricting the population to students who have completed the first self-evaluation exercise. The last column gives the mean for students who have not solved the first self- evaluation exercise. And so on till Ex7. One notices that the highest average and smallest standard deviation in the final exam is obtained in the group of students who have completed exercise 7 (11.67 and 1.41 respectively), while smallest average is obtained in the group that has not solved any exercise (7). Given the results of the association rules, students who have completed exercise 7 have most probably completed all optional self- evaluation exercises. These results may speak for a positive impact on the final mark of the self-evaluation exercises. However our small population prevents of making any strong conclusion since statistical tests to check the significance of the difference in the average, like t-test, usually require a sample of size 30 or more. The line for exercise 4 looks different and requires more investigation. 
 5    Conclusion and Future Work.
 In this paper we have presented a data model to structure and export the data that most LMS usually store in scattered places into an homogeneous schema. The first aim of our data model is to automate and alleviate the preprocessing that is needed to explore, analyse and mine these data. We have designed an architecture of the tool that does the actual structure/export functionality, and  implemented it for the LMS Moodle. Finally we have used our tool to analyse the data stored in the course “Programming 1” in our university. We have focused our analysis on the optional self-evaluation exercises. The analysis shows that, as the semester progresses, less students solve them. It shows also that a group emerges that keeps solving them and that reaches slightly better marks in the final exam. Another aim of this data model is to couple loosely an LMS and the analysis of the data it stores.  If the persistence functionality of an LMS is changed, only the structure/export tool needs to be changed, not the analysis tools linked to the data model. Note that the implementation of the module that performs the export functionality for a concrete LMS has to be programmed with particular consideration regarding performance as the data stored inside an institution can be huge. However this programming happens only once. As noticed in [5] on-line learning is likely to grow, and so is the use of LMS. As pointed out in [1] the number of works tackling data stored by LMS is increasing. We hope that this work will help boost results and best practices in that area. We have used this data model mainly to explore thoroughly how students use learning resources in a course. Results of such an exploration are enlightening for teachers and are necessary to conduct a better informed data mining afterwards. Will this model be robust enough to answer any pedagogical question using data mining techniques? It  depends on who is asking. Our data model contains all interactions that users perform with any object of the LMS along with the timestamp. Therefore a whole range of pedagogical questions related to navigation, performance prediction, activity of students for instance should be treatable. We begin to notice recurring questions that users of LMS are interested in. A future work is to continue enhancing and structuring those questions, so that we try out our data model further. We aim also at not restricting our data model to LMS, but consider other learning software as well. Our next step in that direction is to consider learning portals. We are also working on a graphical user interface for users to query and mine the data model in an intuitive way. This interface should work as an adaptive front-end for the user, the real analysis work will be done by connecting suitable queries and mining tool already available. Our work is open for the community and will be released soon on [12]. 
 Acknowledgment.
 We thank warmly our colleague Prof. Dr. Ripphausen-Lippa for her cooperation in analyzing the data of her course “Introductory Programming with Java”.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>A Data Model to Ease Analysis and Mining of Educational Data</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-kruger"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-kruger"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-wolf"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-wolf"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-kruger"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-wolf"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/157">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Using Topic Models to Bridge Coding Schemes of Differing Granularity</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/157/authorlist"/>
		<swrc:abstract>While Intelligent Tutoring Systems (ITSs) are often informed by the data extracted from tutoring corpora, coding schemes can be time consuming to implement. Therefore, an automatic classifier may make for quicker classifications. Dialogue from expert tutoring sessions were analyzed using a topic model to investigate how topics mapped on to pre-existing coding schemes of different granularities. These topics were then used to predict the classification of words into moves and modes. Ultimately, it was found that a decision tree algorithm outperformed several other algorithms in this classification task. Improvements to the classifier are discussed.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Using Topic Models to Bridge Coding Schemes of Differing Granularity</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/whitney-cade"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/whitney-cade"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-olney"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-olney"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/157/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/whitney-cade"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-olney"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/158">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Automatic Rating of User-Generated Math Solutions</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/158/authorlist"/>
		<swrc:abstract>Intelligent tutoring systems adapt to users’ cognitive factors, but typically not to affective or conative factors. Crowd-sourcing may be a way to create materials that engage a wide range of users along these differences. We build on our earlier work in crowd-sourcing worked example solutions and offer a data mining method for automatically rating the crowd-sourced examples to determine which are worthy of presenting to students. We find that with 64 examples available, the trained model on average exceeded the agreement of human experts. This suggests the possibility for unvetted worked solutions to be automatically rated and classified for use in a learning context.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Automatic Rating of User-Generated Math Solutions</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/turadg-aleahmad"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/turadg-aleahmad"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/vincent-aleven"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/vincent-aleven"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-kraut"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-kraut"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/158/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/turadg-aleahmad"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/vincent-aleven"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-kraut"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/159">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Clustering Student Learning Activity Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/159/authorlist"/>
		<swrc:abstract>We show a variety of ways to cluster student activity datasets using different clustering and subspace clustering algorithms. Our results suggest that each algorithm has its own strength and weakness, and can be used to find clusters of different properties.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Clustering Student Learning Activity Data</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/haiyun-bian"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/haiyun-bian"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/159/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/haiyun-bian"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/160">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Modeling Learning Trajectories with Epistemic Network Analysis: A Simulation-based Investigation of a Novel Analytic Method for Epistemic Games</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/160/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[ INTRODUCTION.
 Epistemic games have been developed in recent years to help players develop domain- specific expertise that characterizes how professionals in a particular domain reason, communicate, and act [1]. For example, learners may learn what it is like to think and act like journalists, artists, business managers, or engineers by using digital learning technologies to solve realistic complex performance tasks. This is accomplished by designing the game in such a way that completing it mimics the core experiences that learners outside the gaming environment would have in a professional practicum in the field. As one might expect, traditional measurement models with latent variables designed for traditional large-scale assessments struggle to jointly accommodate the complexities of the data that arise from these games. Thus, there are currently no off-the- shelf statistical models that can be applied directly to epistemic games to satisfy the desired scaling and reporting purposes; alternative modeling approaches grounded in non-parametric methods appear to be more promising in this regard. 
 In this poster, we report on a comprehensive simulation study for investigating one candidate method that has recently been proposed in the literature called epistemic network analysis (ENA) [3]. The method is purely descriptive at this point and has been applied to real data collected in several different epistemic games. However, it has not been thoroughly investigated using simulation studies that use conditions representing a wide variety of realistic game-play scenarios. In our work we specifically investigate the sensitivity of different ENA statistics to capturing the different learning trajectories of players who play different types of epistemic games. 
 In order to simulate data we used principles from modern latent variable models, specifically models in item response theory (IRT) and diagnostic classification models (DCMs) [2]. In these models, contributions of learner and task characteristics to response probabilities are statistically separated by specifying separable parameters for each. The following table provides an overview of the simulation design for our study. Note that we are specifically investigating various ENA outcome statistics on different metrics. For example, we are using the raw ENA statistics as well as the percentage overlap of empirical confidence bands, computed using the 100 replications, across the entire game play of ENA statistics. The latter approach in particular provides us with a non- parametric approach for sorting learners according to their learning trajectory profiles that can be adapted for real-data analyses. The simulation design further helps us to quantify the relative influence of different design factors on the variation in the raw ENA statistics or secondary derived statistics such as percentage overlap of ENA statistics. For example, the following table shows how most of the variation in one global statistic, the weighted density, is accounted for by the similarity of the underlying learning trajectories independent of the different game conditions as captured by the task parameters. The research is currently ongoing with the goal of having completed results submitted for publication by the end of the summer.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Modeling Learning Trajectories with Epistemic Network Analysis: A Simulation-based Investigation of a Novel Analytic Method for Epistemic Games</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-a-rupp"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-a-rupp"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/shauna-j-sweet"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/shauna-j-sweet"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/younyoung-choi"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/younyoung-choi"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/160/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-a-rupp"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/shauna-j-sweet"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/younyoung-choi"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/161">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Analysis of a causal modeling approach: a case study with an educational intervention</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/161/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Analysis of a causal modeling approach: a case study with an educational intervention</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/161/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/162">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Using a Bayesian Knowledge Base for Hint Selection on Domain Specific Problems</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/162/authorlist"/>
		<swrc:abstract>A Bayesian Knowledge Base is a generalization of traditional Bayesian Networks where nodes or groups of nodes have independence. In this paper we describe a method of generating a Bayesian Knowledge Base from a corpus of student problem attempt data in order to automatically generate hints for new students. We further show that using problem attempt data from systems used to teach propositional logic we could successfully use the created Bayesian Knowledge Base to solve other problems. Finally, we compare this method to our previous work using Markov Decision Processes to generate hints.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Using a Bayesian Knowledge Base for Hint Selection on Domain Specific Problems</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/m-croy"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/m-croy"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/162/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/m-croy"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/163">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>On the Faithfulness of Simulated Student Performance Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/163/authorlist"/>
		<swrc:abstract>The validation of models for skills assessment is often con- ducted by using simulated students because their skills mastery can be predefined. Student performance data is generated according to the pre- defined skills and models are trained over this data. The accuracy of model skill predictions can thereafter be verified by comparing the pre- defined skills with the predicted ones. We investigate the faithfulness of different methods for generating simulated data by comparing the pre- dictive performance of a Bayesian student model over real vs. simulated data for which the parameters are set to reflect those of the real data as closely as possible. A similar performance suggests that the simulated data is more faithful to the real data than for a dissimilar performace. The results of our simulations show that the latent trait model (IRT) is a relatively good candidate to simulate student performance data, and that simple methods that solely replicate mean and standard deviation distri- butions can fail drastically to reflect the characteristics of real data.</swrc:abstract>
		<led:body><![CDATA[ 1. Obtain indirect and independent measures of skill mastery. Many studies rely on an independent source to estimate skill mastery and match the model prediction with this independent source. For example, Vomlel [9] asked experts to determine if a student mastered a set of skill in fraction algebra based on their answers to a test. The test data was used for training a Bayesian Network model and the prediction of the model was matched against the experts’ judgment. 2. Match predictions over observed items only. Another approach consists in using solely the predicted outcome of observable items that can be directly matched to real data. No attempt is made at estimating skill mastery, and instead the approach relies on the assumption that hidden skills are correctly assessed if observed performance is accurately predicted. 3. Generate simulated data. The approach we investigate here consists in generating student performance data according to a predefined model for which skill mastery is defined for each student. This approach is commonly used in psychometric research where latent response models are validated against simulated data (see for eg. [4]). The approach has also been used for cognitive modeling within a number of studies and over different models such as the DINA [1] and a the Bayesian Network approach [7], to name but a few examples. The obvious advantage of having predefined skills with simulated data is, however, plagued by the issue that the underling skill model may not reflect the reality. The models can be over simplistic, or they can misrepresent the relationships between skills and performance, and among skills themselves. We investigate this issue by using four models of skills to generate simulated student data. We look at how close are the performances of a student model trained over real and simulated data, while ensuring that the simulated data reflects as closely as possible the characteristics of the real data. The student model for this study is a Bayesian approach to cognitive modeling, POKS [5]. The first data generation model is one of the simplest possible and it serves as a baseline. The probability of item outcome (generally defined as a success or a failure to a test item question, or to an exercise) is a function of the expected values from marginal probabilities of item success rate and student scores. The second data generation model relies on a Q-Matrix that defines the links between items and skills. The matrix is used to assign skill outcome probabilities, from which a data sample can be generated. A third approach is based on a standard approach in Monte Carlo simulations in which sample data is generated by a technique that preserves the correlations among variables (among items in our case). The fourth approach is based on latent trait modeling (IRT—Item Response Theory) [2]. A number of studies on generating simulated student data have been conducted for the latent trait (IRT) approach [10][3][6], but they were all done within the IRT framework, using the same underlying latent trait models both for simulating the data and for measuring the predictive accuracy of the student model constructed from this data. On the contrary, the current study uses a Bayesian approach as the student model and a makes comparison of widely different approaches in addition to IRT. We explain each of the simulated data generation approach in greater details below before moving to the experiments and the results. 
 2 Expected Outcome Based on Marginal Probabilities.
 The simplest model for generating simulated data is based on the expected item outcome according to marginal probabilities, as represented by the student general skill level and the item difficulty. This model presumes of no conceptual or skill structure behind the items set. Each item is considered independent of the other and the outcome solely depends on the item difficulty and the ability of the student. Within this framework, the generation of sample test outcome can be conceptualized as a random sampling process using the expected probabilities. Assuming two vectors of probabilities: (1) S, that represents the skills mastery level of students, and (2), Q, that represents the (inverse) difficulty of items, then, the outer product of the two vectors is a matrix X = Q× S where each element, mij , represents the expected probability of student i mastering item j. In the current study, we forced the sampling process to exactly replicate the distribution of scores, S, by sampling a pedefined number of successes for each examinee. Since the probability of an item xij being considered a success is solely dependent on the marginal probabilities, Qi and Sj , we will refer to this model as the Marginal Probabilities sampling. 
 3 Q-Matrix Sampling.
 The second model we explore is based on a Q-matrix [8] which defines the links between items and skills. For example, assuming we have I items and K skills, and a response matrix of N students, then the Q-matrix and the response matrix are defined as: 
 FORMULA_1.
 For example, if an item x1 involves only skills k2 and k3, then q1,2 and q1,3 will be set to 1, and all other entries for that item, q1,• will be set to 0. The skill mastery of a set of students can be computed as the dot product of the two matrices: X ·Q. The generation of sample data from this Q-matrix consists in defining the probability of an item outcome as a function of the level of mastery of the set of skills it involves. By defining skill mastery in the range [0,1] (for which case the Q-matrix corresponds to a capability matrix as defined in [1]), then, the probability of a successful outcome to an item xi is defined as the smallest of the mastery value of each skill involved for xi. This is a heuristic estimate that reflects the requirement that all skills must be involved in order to correctly answer xi. Akin to the process described for marginal probability sampling, we can ensure that the scores distribution perfectly matches the real by fixing the number of item successes per examinee. Sampling thus proceeds in a similar manner to the marginal probabilities sampling model, with the difference that instead of marginal probabilities, the item probabilities are derived from concept mastery. In turn, concept mastery is derived, in our experiment, from the student concept mastery distribution of the sample data and the capability matrix. 
 4 Covariance Matrix.
 Another mean of generating simulated student performance data is based on the idea of preserving the covariance (correlation) among items. This method is commonly used in Monte Carlo simulations. In the context of student test data, the method would stipulate that question items are interrelated and that a representative sample of simulated test data preserves the structure of correlation among items. This assumption is not unreasonable as we would, for example, expect that items of similar difficulty and that draw from the same skill set to show correlated student response patterns. The generation of sample data based on item covariance relies on the Cholesky decomposition of the item covariance matrix. Assuming L is the upper triangular matrix of the Cholesky decomposition of the item covariance matrix, a first step is to generate a sample of correlated variables as: 
 FORMULA_2.
 where N is an N × I matrix (number of students by number of items) of normally distributed independent random values having a mean of 0 and a standard deviation of 1. The sample data S will be an N × I matrix for which the item covariance matrix will approach the real data item covariance. It will have an expected mean of 0. The second step is to fit the distribution of this data’s item success rate to the real data by adding the vector of real data item means to each row of S and, finally, to transform values to binary item outcome, setting values above 0.5 to 1 and 0 otherwise. 
 5 Latent Trait Models (IRT).
 The last method of generating simulated student performance data relies on Item Response Theory, also known as latent trait modeling. As mentioned above, some authors have studied the faithfulness of this approach to replicate real data [10][3][6]. We refer the reader to [3] for a more elaborate description of this approach1 We use a 2 parameter logistic IRT model for generating the simulated data. According to this model, the probability of a successful outcome by an examinee s to an item i is defined as: 
 FORMULA_3.
 where θs is the student’s ability level, and where ai and bi are respectively the discrimination and difficulty levels of item i. The values for these three variables are directly estimated from the real data sample and therefore it is possible to replicate simulated data that reflects the real data. Estimates of the discrimination parameter is obtained with the R ltm package2 and values for item difficulty and examinee ability are directly obtained through the logit transformation of the item average success rate and examinee percentage score. We also limit discrimination to values to the interval [0,4] and difficulty values to [-4,4], as is commonly done for IRT with small samples. 
 6 Experiments.
 We mentioned in the introduction that the issue with simulated student performance data is to determine how far the simulated data is representative of the complexity of the real student performance. To address this question, we train the POKS student model [5] over real and simulated data sets and compare its predictive performance across each condition. The simulated data sets are generated to closely resemble the real data according to the underlying model. The four models described above are used for simulated data: (1) MP sampling, marginal probability sampling (section 2), (2) QM sampling, Q-matrix sampling (section 3), (3) Covariance, sampling based on preserving item covariance using the Cholesky decomposition (section 4), and finally (4) IRT, sampling based on the latent trait modeling (section 5). 
 6.1 Adaptive Testing Simulation.
 The results of the different simulated data models are compared in the context of simulated adaptive testing with the POKS model. The process of adaptive testing consists in choosing the most informative item to present to the student and to infer the outcome of other items based on the pattern of previous item outcomes. The performance is measured as the percent-correct predicted item outcome. Items that have been asked represent observed evidence and are considered correct by definition. Thus, performance after all items have been observed always converges to 100%. At the beginning, when no items are observed, item outcome is based on average item success rate: if an item has a success rate above 50%, it is considered mastered, and not mastered otherwise. As new items are observed, the POKS model computes the probability of mastery of each item based on the pattern of previous item outcome, and the predictions are compared to the actual data to compute the percent correct performance. In this experiment, a cross-validation process is used for the College mathematics data set and a leave-one-out process is used for the Unix data set because of the small number of records. 
 6.2 Data Sets.
 The characteristics of the real data sets from which the simulated data is generated can be very influential in this investigation and therefore we provide some details about them here. The experiment is conducted over two data sets: 
 1. Unix. The Unix data set contains 34 questions items that have all been answered by 48 respondents. The average success rate is 53% and it contains a large array of skills and difficulty, with test scores varying from 1/48 to 45/48, and item success rate varying from 1/34 to 34/34. Skills decomposition of this data is done over 9 topics ( ”sys-admin”, ”awk”, ”basic” ”directories”, ”file permissions”, ”input-output redirection”, ”printing”, ”regular expressions” ”shell language”). These topics contain from 3 to 7 items and only one topic is associated with an item. In other words, the row sums of the Q-matrix is always 1. 
 2. College Mathematics. The Math data set is composed of 59 items, which were administered to 250 freshmen students at Polytechnique Montreal. Each item was analyzed by two domain experts who determined if it involved one of the following topics : (1) Algebra, (2) Geometry, (3) Trigonometry, (4) Matrices and Vectors, (5) Differential equations and (6) Integrals. Mean student score is 57%, ranging from 9/59 to 55/59. 
 Contrary to the Unix data set, most items are linked from two to four topics (only 17 are single topic, 32 are linked to two topics, 9 to three topics, and 1 to four topics). The simulated performance data is generated to reflect as closely as possible the characteristics of the two real data sets. The similarity of the simulated data can be compared to the real one by looking at the correlation between success rates of students and items. Table 1 reports a number of similarity measures that represent the averages for 10 simulated data sets (numbers in parenthesis represent the standard deviations): • Mean and Sim. mean: The percentage of correct responses over the whole data set. This number is to be compared to 53% for Unix and 57% for Math. The data generation process for the QM sampling and MP sampling methods were devised to match exactly this parameter. • Cor. exami.: Pearson correlation between the simulated and real respondent test scores. • Cor. items: Pearson correlation between the simulated and real average item scores. • Cor. concepts: Pearson correlation between the simulated and real average concept mastery scores of students. Concept mastery for the students is computed on the basis of the dot product X ·Q (see section 3), but with a normalization that ensures the scores range is between [0,1]. This normalization corresponds to the notion of a capability matrix (see [1]). • % diff.. Percentage of items with different outcome. 
 Table 1: Similarity of simulated data with real data. 
 The patterns of similarity vary considerably across the different sampling methods, but the most consistent one is the IRT method, in particular for the Unix data set, with correlations of 0.98 for both item success rate and examinee scores. Whilst these correlations are very high, we find that 15% of items differ from the real to the generated samples. We will see from the data in table 2 that this 15% difference can considerably degrade the predictive performance if the items are chosen at random. 
 7 Results.
 The CAT simulations experiment results are reported in Figure 1. The graphs depict the predictive performance of POKS over the two data sets. The percent correct number of item outcome prediction (accuracy) is reported over the different experimental conditions. Both graphs start a 0% observations, where the accuracy corresponds to guesses based on item average success rate. They end at 100% of questions observed for each data set, where the accuracy converges to 1 because observed item outcome are considered correctly “predicted”. For indicative purpose, a straight line is drawn that starts at the initial guess of the real data, (0, y0), and ends at (1,1). It corresponds to the theoretical baseline accuracy of random guesses over non-observed items and provides an idea of the prediction gain obtained with the student model (note that only the real data line is drawn). Standard errors over simulation runs are not shown on the graphs to avoid cluttering, but they are at most around 7% and have no significant affect on the general patterns observed. The different curves correspond to the four methods respectively described in section 2 to section 5 (see section 6 for label correspondance). 
 Table 2 provides a single score for the predictive performance, termed here the accuracy gain. This score represents the gain from guessing the outcome based on the initial probabilities of items and its range is [0,1]. It provides a simple means of comparing the overall predictive performances across the simulations and corresponds to the error reduction averaged over all intervals. It is computed as: accuracy gain = 
 FORMULA_4.
 where N is the total number of intervals (we arbitrarily use 50), yi is the accuracy at interval i (the x value) and yˆi is the baseline accuracy at that same interval as represented by the straight diagonal of the figures (there exists one diagonal per curve but only the one for the real curve is represented in the figures). 
 Figure 1: Results predictive accuracy simulation experiments with real student performance data compared with different models of simulated student data. 
 Table 2: Global accuracy gain over baseline. 
 For indicative purposes and in addition to the four methods, table 2 also reports a score corresponding to randomly changing the values of item outcome for 15% of the items, which is the proportion of items differing from the IRT simulated data to the real data. In the case of the Unix data, the results indicate that the IRT method is able to generate data over which the POKS model is similar the performance, with accuracy gains of 0.77 for real data agains 0.80 for IRT. The Covariance method comes second with a performance of 0.58 instead of 0.77. In the case of the Math data, the general predictive performance of all methods is substantially lower than for the Unix data. The Covariance and IRT methods both yield performance relatively close to the real data, but this time the Covariance method is closer to the real data performance. 
 8 Discussion.
 This investigation is limited to two real world data sets and to predictions based on a single student model, namely POKS. As such, further investigations are necessary to draw stronger conclusions. Nevertheless, we can still hint at some conclusions. First, the simpler methods of generating data, based on marginal probabilities and on concept mastery, yield simulated data that do not appropriately reflect the underlying structure of the real student performance data. However, the IRT method, based on the 2 parameter model (difficulty and discrimination), does appear to reflect the characteristics of real data, but not systematically for all data sets, as a non neglectible difference can be observed in the case of the Math data set. Furthermore, the Covariance method actually generates data for which the predictive accuracy is slightly closer to real data then the IRT method is. It also is close overall to the real data, standing at 0.37 accuracy gain compared to 0.40 for real data. This investigation focused on models for generating data which allow their parameters to replicate real data characteristics, namely items difficulty, student skill levels, concept mastery as defined by the Q-matrix, and item covariance. Not all models allow this replication as readily as for these approaches. The DINA model used in [1] contains parameters that cannot be readily estimated from data, such as performance slips. Validating the faithfulness of such models is a desirable endeavour that would require means to estimate such parameters and constitutes an interesting research avenue. Indirectly, such investigations are in fact a means to validate if a model can actually reflect the characteristics of real data and, thus, they can be considered as an assessment of the external validity of a student model. Turning back to the fundamental question of whether we can rely on simulated data to validate a student model, the simulations in this study suggest that simulated data from the 2 parameter IRT model can appropriately reflect some data set characteristics, but not with equal faithfullness for all data sets. It suggests that the validation of a model based on the indirect and independent measures of skill mastery may be indispensable to ensure a proper validation, as we outlined in the introduction. Alternatively, we could argue that the approach which consists in validating predictive performance over observable items only is just as indispensable. If we assume that the accuracy of a model for predicting item outcome is directly and monolitically linked to the accuracy of non observable parameters estimates of a model, then item outcome represents a good indirect measure of skills and concept mastery.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>On the Faithfulness of Simulated Student Performance Data</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/michel-c-desmarais"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/michel-c-desmarais"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ildiko-pelczer"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ildiko-pelczer"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/163/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/michel-c-desmarais"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/ildiko-pelczer"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/164">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Unsupervised Discovery of Student Learning Tactics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/164/authorlist"/>
		<swrc:abstract>Unsupervised learning algorithms can discover models of student behavior without any initial work by domain experts, but they also tend to produce complicated, uninterpretable models that may not predict student learning. We propose a simple, unsupervised clustering algorithm for hidden Markov models that can discover student learning tactics while incorporating student-level outcome data, constraining the results to in- terpretable models that also predict student learning. This approach is robust, domain-independent, and does not require domain experts. The models have tecst-set correlations with learning gain as high as 0.5 and the findings suggest possible improvements to the scaffolding used by many software tutors.</swrc:abstract>
		<led:body><![CDATA[ 1. We will postpone most discus- sion of the data for later, but the tutor’s representation of geometry problems is especially important. Each problem is shown on a separate page along with a geometry diagram. Stu- dents are expected to enter values, such as angle magnitudes, into answer cells. 
 Figure 1: Geometry Cognitive Tutor, circa 1998. 
 Table 1: Mapping from (Action,Duration) to a single variable.
 Solving for and filling in one of these cells is called a “step”. The first steps on a problem tends to involve givens; the later steps require values from the previous steps. Students can switch between steps at will, but can only switch problems by finishing them. Students can also request hints (fairly common) or read the glossary (very uncommon). Within each step of each problem, a student performs actions (also called transactions in other literature). An action could be entering an answer (right or wrong), requesting a hint, or reading a definition from the glossary. For simplicity, we will group hints and the rare glossary request together, labeling them both as hints. The definition of an action has one additional wrinkle: each action has a corresponding duration. For example, a student might take 10 seconds to type an answer or 4 seconds to read a hint. Thus, actions are divided into two categories, long and short, using a threshold to be discussed later. Table 1 shows this mapping. For example, Aaaaaa denotes one long attempt followed by many short attempts and might be considered a guessing tactic. In sum, the data for this study consists of students working on geometry problems with each problem split into steps. The problems and steps are broken down into actions (a, A, h, or H). These rudimentary features are much simpler than human-constructed features. Beal et. al., for example, work from researcher-constructed features like “independent- inaccurate problem solving” [3]. Their approach, while successful, is dependent on the quality of the constructed features. Similarly, Baker et. al. use a set of human-constructed features as inputs to a feature construction algorithm [2]. Their approach can generate new composite features, but the initial features must be expert-defined. Another distinction is that the actions used in this study are atomic: there is no lower granularity of data available. In contrast, Baker computes aggregate features over a roaming window of actions. 
 Figure 2: Example HMM. 
 3 Hidden Markov Models.
 The goal of this study is to build models for student learning tactics. An example of a learning tactic, as defined in this paper, is, “The student requests hints quickly, over and over, until the tutor provides the solution. The student then enters the solution.” From this example, a learning tactic can be generalized to be an observable, predictable, and repeated pattern of behavior that is sufficiently abstract to include multiple observed instantiations. We implement learning tactics using hidden Markov models. A hidden Markov model (HMM) is a set of unobserved states, each state related to observations through a probability distribution. Here, the observations are student actions. Figure 2 shows an example HMM. Each unobserved state is represented by a circle; each arrow between states or looping back to a state represents a transition; the number above the arrow is a transition probability. The tables below the states show the probabilities of observing an action. When an HMM generates an action symbol, we say it emits the symbol. Let a series of observed actions be a sequence. Sequences can be defined for either all actions in a problem or all actions in a step. Given a set of student sequences associated with an HMM, the Baum-Welch algorithm can relearn the parameters of that HMM to better fit the observed data. This is a standard method for learning a single HMM. Single HMMs, as described above, have been used in many studies to model student be- havioral traces. In a particularly relevant study, Beal et. al. used tutoring system log data to learn HMMs modeling patterns of student behavior [3]. Their study differs from this one in several ways: they define the structure of the HMMs by hand, they use outputs from another algorithm as inputs to their HMMs, they learn one HMM per student, and they per- form clustering of students (not tactics) only after learning the HMMs. However, their key result is very relevant: HMMs work as both descriptive and predictive models for student learning behaviors and can find patterns without using cognitive models or domain content knowledge. 
 3.1 HMM Clustering.
 Let each individual HMM represent a single learning tactic. Discovering learning tactics requires discovering sets of HMMs. Let a set of HMMs be called a collection. In a col- lection, an observed sequence of actions is classified by whichever HMM is most likely to generate it. This results in a partitioning of the set of sequences, with each partition corresponding to one HMM. Each partition thus includes all observed examples of a given tactic. The Baum-Welch algorithm can only learn parameters for a single HMM, but cluster- ing algorithms can learn sets of HMMs, and thus sets of tactics. The usual objective of an HMM clustering algorithm is to maximize the total likelihood of generating the ob- served sequences. This type of problem has historically been tackled with Expectation- Maximization (E-M) algorithms and, for HMM clustering, given an initial set of HMMs, one iteration of the E-M algorithm is: • Assign each sequence to the HMM most likely to generate it. • For each HMM, relearn its parameters with Baum-Welch using the sequences in its partition. This process begins with initial seed HMMs and repeats until a termination criterion is met, such as when an iteration results in fewer than 10 sequences being reclassified. A collection learned by this algorithm fits the data well if the likelihood of generating the observed sequences is high. This algorithm, here forth called HMM-Cluster, is provably guaranteed to converge to a local maximum. Further, HMM-Cluster will never change the number of HMMs in the collection (k) or the number of states per HMM (n); only the parameters and partitions will change. There have been many prior uses of similar E-M HMM clustering algorithms, beginning with Rabiner et. al. for word recognition [6]. While there are newer variants, most HMM clustering is still done with Rabiner’s original algorithm. A particularly illustrative study was done by Schliep et. al. to analyze gene expression data[9]. The paper discusses, amongst other things, the expressiveness of the models, the interpretation of results (for genetics), the inclusion of human labels, and the comparison of HMM clusters to other time series models. 
 3.2 Stepwise-HMM-Cluster.
 Unfortunately, naive HMM-Cluster has issues from both machine learning and educational perspectives: • Like most E-M algorithms, HMM-Cluster gets trapped in local maxima. • The choice of k and n determines the effectiveness of HMM-Cluster. If they are too large, the collection will overfit; if they are too small, no collection will fit the data. • Collections that fit the data may not actually predict learning. 
 Figure 3: Example Stepwise Regression Inputs. 
 In principle, a better algorithm would search over values of k and n with a bias towards fewer, smaller HMMs, leading to better generalization and easier interpretation of the final collection. One such algorithm is Stepwise-HMM-Cluster, which is to HMM-Cluster what stepwise regression is to normal regression. An iteration of Stepwise-HMM-Cluster, for k HMMs and n states per HMM, proceeds: • Begin with a collection of HMMs C. • If |C| < k, generate (k − |C|) new HMMs with n states per HMM. • Run HMM-Cluster on C. • Pick the “good” HMMs from C and use them for the next iteration. A critical step in Stepwise-HMM-Cluster is the selection of “good models” from a collec- tionC. This step allows Stepwise-HMM-Cluster to incorporate external data and iteratively improve its fit across iterations of the algorithm. For this study, HMMs are selected using forward stepwise linear regression: the total number of sequences classified by each HMM for each student is used as the independent variable and the pre-test to post-test learning gain is used as the dependent variable. A toy example is shown in Figure 3. Stepwise-HMM-Cluster serves two goals at once: it tries to build a collection of HMMs to fit the observed sequences of actions, but also requires that the collection predict student learning gain. The incorporation of external data, such as pre-post learning gain, has been traditionally difficult when applying machine learning algorithms to educational data. This selection step addresses that issue, allowing student-level measures to influence the learning of much lower-level HMMs. In this case, learning gain is used to constrain the search for problem- and step-level HMMs. In future work, other data sources could be added, such as grade-point averages, survey information, or expert labels. For this study, the parameters are restricted to 2 ≤ k ≤ 8 and 2 ≤ n ≤ 8. The limits of 8 HMMs and 8 states per HMM were chosen to maximize interpretability, but both limits exceed the complexity of any optimal collections actually found. 
 4 Data.
 This study uses two data sets, 02 and 06. Both data sets in this study originate in previous experiments, so only the control groups for each study are used. 
 Table 2: Correlations with Learning, Best Collections, Test Data. 
 Both data sets involve geometry tutoring systems that use the same general interface. However, the 02 data is from the angles unit, while the 06 data is from the circles unit. The 06 tutor also has some interface differences, including a minimum time per hint request. In the 06 data, students do fewer actions per step, complicating direct comparisons between the two data sets. Also, the 06 post-test used counter-balanced hint conditions between problems, e.g., sometimes students could get a hint at the cost of partial credit. This makes the test scores noisier and harder to predict. • 02 data - First published in 2002, includes 21 students and 57204 actions [1]. • 06 data - First published in 2006, includes 16 students and 7429 actions [7]. 
 5 Results.
 Stepwise-HMM-Cluster has several parameters. First, there is a threshold value between long and short actions. Let that threshold be denoted by τ . Second, Stepwise-HMM-Cluster can learn either problem-level or step-level tactics. For the former, HMMs are trained on sequences that include an entire problem. In the latter case, each sequence only contains actions from one step. The software implementation was built on the GHMM package [8] and, for a given search using a fixed value of τ , approximately 100 candidate collections reach the model selection stage. Collections are learned from the first 80% of sequences per student; the remaining 20% are saved as test data. The main measure of a “good” collection is that it provides an accurate prediction of learning when applied to test data. To apply a collection of HMMs to test data, the HMMs are first used to classify test-data sequences. The total number of sequences per HMM per student is entered into the regression as shown earlier, now using parameters learned from training data. Table 2 shows the best correlations, for both data sets, between predicted learning gain and actual learning gain, as computed on the test data. Each column contains the best results for a specific run of Stepwise-HMM-Cluster with the rows split by value of τ . In practice, Table 2 can be interpreted as showing upper bounds for predictions on with- held test data. However, even in this simple table, it’s already clear that the 06 data is harder. The conclusion from Table 2 is that there are collections with a good fit to test data, if we can find them. The caveat to Table 2 is that it shows best collections picked after applying to test data; our actual goal is to find good collections using only the training data. To do so naively, however, invites overfit. This suggests the use of a selection heuristic: pick the collection with the best adjusted R2 score on training data. 
 Table 3: Correlations with Learning, Selected Collections, 02 Test Data. 
 Figure 4: Dominant HMM for τ = 6, 02 data. 
 R2, unadjusted, is defined as the sum-of-squared-error divided by the total sum of squares. For standard linear regression, R2 is equal to the square of the correlation coefficient. The adjusted R2 includes an additional term that grows in the number of model parameters, penalizing complex collections. Table 3 shows, for 02 data, test-set correlations for collec- tions selected using adjusted R2, the number of HMMs in the best collection (step-only), and the maximum number of states per HMM in the best collection (step-only). The correlations in Table 3 are statistically significant (α < 0.05) and almost as high as those in Table 2. They clearly show that, for the 02 data, it’s possible to pick collections that generalize to with-held, within-student test data. However, the same table for the 06 data (not shown) is much less convincing. In 06 data, naively picking collections using the adjusted R2 produces collections with poor predictions of learning. However, in 06, amongst the collections with the highest adjusted R2, some collections do have a high test- set correlation with learning gain. For example, for τ = 6, the fourth-best collection has a 0.46 test-set correlation with learning gain. The problem is selecting the right collection: while the adjusted R2 is effective for 02 collections, it selects poorly from 06 candidates. In the end, educational data mining requires interpretable results that have educational implications. Fortunately, Stepwise-HMM-Cluster outputs simple, interpretable HMMs. In particular, there is one HMM that occurs, with slightly different parameters, in every 02 collection shown in Table 3. This HMM-archetype always classifies a plurality of se- quences, and so will be called the “dominant” HMM. Figure 4 shows a dominant HMM for τ = 6, trained on 02 data at the step level. These dominant HMMs tend to be small, often only two states and never more than four. 
 Figure 5: Repeated Guessing HMM for τ = 6, 02 data. 
 While interpreting the structure of individual HMMs is not actually meaningful (to be ad- dressed), it is still a useful comprehension exercise. Here, the dominant HMM emits a and A with high probability, and emit both actions equally often (over the course of many sequences). One possible explanation is that the dominant HMMs select short sequences where the student already knows the answers and can solve each step in one attempt. How- ever, the correlation between the frequency of first-try-correct sequences and learning gain is −0.24. Instead, an alternative interpretation for these HMMs is that they represent a persistence-trait. Students that attempt to solve repeatedly are more likely to learn the ma- terial than those that rely on hints. This is borne out by the resilience of the HMM to changes in τ , and by the duration-agnostic nature of the HMM, which emits both a and A. However, this conflicts with common sense. The HMM shown in Figure 4 has a high probability1 of emitting a sequence of type Aaaaaa, i.e., a single long attempt followed by many short ones. This is generally considered poor learning behavior [2]. Intuitively, it represents a failed attempt followed by repeated, unthinking guessing. This paradox can be resolved by noting that no single HMM in any collection can be interpreted alone. Each HMM exists only as part of an entire collection and, thus, other HMMs in the collection can remove specific, degenerate sequences. Take the τ = 6 collection as an example. It con- tains an HMM, shown in Figure 5, that has a high probability of emitting repeated-guessing type sequences. The repeated-guessing HMM, a highly specialized model, removes only the guessing sequences from the dominant HMM’s partition. This relationship between HMMs in a collection, where a specific HMM can be tuned to special cases of a more general HMM, allows collections to be more expressive than the sum of their individual HMMs. However, this feature is also what makes the interpretation of the structure of in- dividual HMMs meaningless, as a high probability sequence for one HMM may actually belong to another HMM’s partition. 
 A more appropriate way of interpreting the HMM clusters is to directly examine the sequences classified by a particular HMM. For example, consider the dominant HMM for τ = 6, 02 data, step-level. The five most commonly observed sequences in the HMM’s par- tition are: A, AA, Aa, AAA, AAa. None of these sequences are of the repeated-guessing type, yet they account for 95.5% of all sequences in the partition. Longer sequences in the partition follow the same pattern: example sequences include AAaaAA and AAaAaA. As noted above, guessing sequences, e.g., Aaaaaa, are about as likely to be generated by the dominant HMM as the above sequences, but are actually captured by the repeated-guessing HMM. Similar results apply to the collections discovered for other values of τ . The general interpretation of these results is that students learn more when using persistence- type tactics, as long as they don’t guess repeatedly. Interestingly, this is largely independent of the choice of threshold τ . The most likely explanation is that very short or very long actions contains the most information about the student, and thus the actions that are re- classified by small changes in τ are relatively unimportant. Finally, across all the best collections, hint-heavy tactics are negatively associated with learning. However, many of the more complex collections (3 or 4 HMMs) contain a “noise” HMM that generates all sequences with nearly uniform probability. Thus, hint-specific HMMs are actually very specialized, usually emitting mostly h actions. This explains the negative association with learning. Some “good” HMMs do involve hints, but those HMMs are not structurally consistent enough to permit conclusions without more data or analysis. 
 6 Conclusions.
 Most educational data mining methodologies either rely on domain experts or discover uninterpretable models. In contrast, Stepwise-HMM-Cluster, an unsupervised algorithm, can generate collections of HMMs that predict learning, but are also interpretable. For at least some data sets, Stepwise-HMM-Cluster produces collections of HMMs that can provide good predictions on with-held test data. This algorithm thus satisfies multiple educational data mining goals: it produces interpretable models, the models generalize (within-student), and the models not only fit data, but also predict learning outcomes. Additionally, Stepwise-HMM-Cluster produced models with potential educational impli- cations. Our results provide an additional argument that the most common type of hint- scaffolding in software tutors may be sub-optimal and that most learning may arise from persistent attempts to solve. This suggests a paradigm for tutoring systems that emphasizes attempts and provides hints or worked examples only when strictly necessary; however, there are other feasible explanations and more extensive exploration of this issue is re- quired. In particular, there may exist learning tactics that are both productive and involve hints, but are difficult to detect due to noise or rarity. There are many opportunities for future work. First, the evidence for statistical generaliza- tion is still weak; collections learned on one data set should be tested on another data set en- tirely. Second, adjustedR2 appears to be a poor model selection criterion in some cases and other criteria may be more successful. Third, τ is a problematic parameter: while Stepwise- HMM-Cluster was robust to changes in τ in this study, it may not be robust in general. For example, some data sets may require three action strata (“Long”,“Medium”,“Short”) or require a different τ for hints versus attempts. Fourth, while there is already a procedure for interpreting clusters, there is significant room for improvement. A promising approach is to construct a visualization of the unfolding of sequences with sets of sequences with the same prefix or suffix grouped together. Finally, the algorithm itself is overly simple. The heart of Stepwise-HMM-Cluster is a strict assignment, E-M clustering algorithm using Baum-Welch; probabilistic mixture models or another method, such as spectral clustering, might improve the results, as might a better HMM learning algorithm. Despite its limitations, Stepwise-HMM-Cluster has many potential applications. First, in its present form, the algorithm can already learn interesting models with little dependence on data. However, it is also flexible and extendible. Glossary requests could be separated from hints; new action types could be added for new data sets; tactics could be learned on the level of class sessions or curriculum units instead of problems and steps; human labels could be incorporated into the model selection step; actions could be redefined to include domain information, such as skill models. Further, there is the potential to develop a fully hierarchical algorithm that could simultaneously learn HMMs for individual step tactics, learn HMMs to classify problem tactics as a series of previously learned step tactics, and so on up, as far as sample size will permit. However, the most important contribution of this study is neither the algorithm nor the edu- cational implications. Rather, our results suggest an opportunity for a new paradigm where algorithms can simultaneously leverage multiple data sources at different granularities. In particular, constraining low-level models using student-level measures could potentially improve many existing algorithms and lead to important educational insights.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Unsupervised Discovery of Student Learning Tactics</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-shih"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-shih"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-scheines"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-scheines"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/164/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-shih"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-scheines"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Hierarchical Structures of Content Items in LMS</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[ INTRODUCTION.
 Of the many applications enabled by new technologies, the most commonly used in higher education are Learning Management Systems (LMSs), e.g, Moodle, BlackBoard, which enable a wide range of Web-supported courses. LMSs enable the instructors to develop websites for their courses to support face-to-face teaching by means of different tools. Although most of the LMSs offer an enriched environment that goes beyond the usual content management tools (including communication tools and course management modules), these systems are mainly used for transferring information and increasing accessibility of learning materials [1-3]. Usually, the content modules in these systems enable the construction of a hierarchical repository of information items; consequently, the instructor is able to create folders and upload files creating variety of repository structures, which are presented to students in the course site. The main purpose of this research is to empirically study the types of online hierarchical structures of content items presented to university students in Web-supported courses. Three research questions are addressed in the study: 1) What is the extent of content items presented to university students in online repositories within Web-supported courses? 2) Which types of hierarchical structures of content items are empirically revealed? 3) What are the associations between the types of structures and Number of Items, Course Size, and Content Consumption? Three groups of variables were defined, describing characteristics of each course, as following: 
 Repository Size Variables A. Number of Items: total number of content items in the repository B. Number of Folders: total numbers of folders in the repository 
 Repository Structure Variables C. Average Folder Size: Number of Items divided by Number of Folders (=A/B) D. Largest Folder: number of items in the largest folder in the repository E. Largest Folder Share: ratio of Largest Folder to Number of Items (=D/A) F. Hierarchical Depth: maximal repository depth (i.e., length of a path from the root) G. Visible Width: number of folders located immediately under the root; this number represents the width of the repository as presented to the students. H. Width-depth Proportion: ratio of Visible Width to Hierarchical Depth (=E/F). 
 Course Characteristics (Independent) I. Course Size: number of registered students J. Content Consumption: average consumption of content item per student 
 The research was carried out on a full sample of Fall term courses in Tel Aviv University (academic year 2008/9) which were accompanied by a Website within the HighLearn LMS (by Britannica Knowledge Inc.), N=1,747. Raw data was extracted using SQL queries on HighLearn databases. In this data file, each row corresponds to a single content item within the system, and documents the unique ID of the course to which the item belongs and its full path within the repository. The data file consisted of 72,753 rows (i.e., content items) of 1,747 courses. Revealing types of repository structures was done using Two-step Cluster Analysis on a reduced population which included only courses the repositories of which consisted of 15 content items or more, N=1,203. Results suggest that Number of Items is largely varied between 1 and 1,029, with an average of 41.64 files (SD=69.10). The mean of Number of Folders was found to range between 1 and 185, with an average of 10.69 (SD=16.78). This average demonstrates a large growth in content items delivery at Tel Aviv University, comparing to earlier studies of the very same LMS [1,4]. Regarding the repository hierarchical structures, five types were found: 1) Main-folder Structure: no depth, almost all files piled, large folders (n=67); 2) Extensive Filing: high depth, small folders (n=120); 3) Flat Small Folders: flat hierarchy, small folders (n=222); 4) Pile in Hierarchy Filing: pile exists, small folders (n=354); 5) Pile in Flat Filing: flat hierarchy, big pile exists (n=440). Association was found between the repository structure and Number of Items, according to which large repositories are associated with extensive filing. It was also found that Course Size is statistically significantly different between courses demonstrating Main- folder and Extensive Filing structures: The average Course Size took the highest value in the Extensive Filing courses (63.49, SD=61.95), and the lowest in the Main-folder courses (34.78, SD=34.43). In addition, association was found between the repository structure and its consumption, as measured by Content Consumption. On average, lowest Content Consumption was demonstrated in the Extensive Filing courses (0.77, SD=0.51), and the highest – in the Pile in Flat Filing courses (1.46, SD=0.72). LMSs are often being studied using usage analysis for various purposes [5]. In this study, we used automatically collected data describing the structures of content items presented in Web-supported courses. However, it is not clear that this research falls into one of the 3 classical categories of Web mining (usage mining, content mining, and structure mining) [6]. As EDM research widens its horizons and examines a wide range of data originated in many different learning contexts, the categorization of Web mining studies might be re-examined.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Hierarchical Structures of Content Items in LMS</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ronit-azran"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ronit-azran"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/ronit-azran"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>A Case Study: Data Mining Applied to Student Enrollment</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166/authorlist"/>
		<swrc:abstract>One of the main problems faced by university students is deciding the right learning path based on available information such as courses, schedules and professors. In this context, this paper presents a recommender system based on data mining. This recommender system intends to create awareness of the difficulty and amount of workload entailed by a chosen set of courses. For the purpose of building the underlying model, this paper describes the generation of domain specific variables that are capable of representing students’ past performance. The objective is to improve students’ performance in general, by reducing the rate of misguided enrollment decisions.</swrc:abstract>
		<led:body><![CDATA[ 1. Introduction.
 University Curricula allow students great flexibility and freedom of choice in terms of which and how many courses they can sign up for each term. The amount of workload and their ability to balance it successfully depends on these choices. Their results are also dependent on their own ability for a particular area of knowledge. The main objective of this paper is to propose an enrollment recommender system to assist students in their decision making. The main contribution is the generation of two domain specific variables, namely the potential of student and the difficulty of courses. A similar study was conducted by Al-Radaideh [1], in which he uses classification algorithms to evaluate the performance of students who studied the C++ course in Yarmouk University in 2005. To build a reliable classification model, it adopts the CRISP-DM methodology. 
 2. Domain Specific Variables.
 Domain specific metrics increase the representativeness of models. In this particular case we used two variables: the course difficulty and ability of a student towards a course; the latter is referred to as potential. The course difficulty is represented by the average of the grades obtained by students. On the other hand, the potential is calculated per student for each course he may take. It is represented by the average of the grades a student has obtained in the prerequisites of a course and in previous attempts to pass the course; each grade is divided by the course difficulty. 
 3. Recommender System.
 The model that supports the recommender system was built using the Knowledge Discovery in Databases Methodology [2] using the C4.5 algorithm as classification engine [3]. It included the domain specific variables presented in Section 2 in order to improve its effectiveness. This system is integrated into the current Enrollment System. During enrollment students choose a set of courses and obtains a forecast for each of them: PASS/FAIL. This enables them to make informed and conscious decisions hence indirectly improving their performance. (Figure 1) shows the sequence of this process in detail. 
 Figure 1. Recommendation Sequence Diagram.
 4. Conclusions and Future Work.
 The main benefit of the proposed system is the awareness created in students about the enrollment process: they will be aware of the fact that they are not currently skilled enough to success in courses in a particular area; it will also help them appraise the difficulty of courses and realize the possibility of unbalanced workloads. In the long run, these positive effects will lower failure rate of students hence improving their learning process and their learning paths. Future works should strive to improve data cleaning to remove noise from the model. The model itself could be enhanced by improving the C4.5 pruning method or by adding significant attributes. 
 Acknowledgement.
 This work has been funded by Spanish Ministry of Science and Education through the HADA projects (TIN2007-64718) and the Universidad de Lima through the IDIC (Research Institute).]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>A Case Study: Data Mining Applied to Student Enrollment</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/cesar-vialardi"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/cesar-vialardi"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jorge-chue"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jorge-chue"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/alfredo-barrientos"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/alfredo-barrientos"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-victoria"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-victoria"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jhonny-estrella"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jhonny-estrella"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/juan-pablo-peche"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/juan-pablo-peche"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/cesar-vialardi"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jorge-chue"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/alfredo-barrientos"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-victoria"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/jhonny-estrella"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/juan-pablo-peche"/>
		<rdf:_7 rdf:resource="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/167">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Predicting Task Completion from Rich but Scarce Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/167/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[ INTRODUCTION.
 We present a data-driven model for predicting task completion in Project LISTEN’s Reading Tutor, which takes turns picking stories and listens to the child read aloud [1]. However, children do not always finish stories, and we would like to understand why, or at least detect when they are about to stop.  So our EDM challenge is to learn a model to predict task completion – a widely used metric of dialogue systems’ performance.  Such a model could help detect imminent disengagement in time to address it, and identify factors that influence task completion, including tutor behaviors, thereby providing useful guidance to make tutors engage students longer and more effectively. The richness of multimodal tutorial interaction over time makes the space of possible features to describe it large relative to the amount of data.  When the number of features is large compared to the amount of data, classifier learners tend to overfit the data, so we need a method that learns robust models from few training examples with many features. Consider the supervised learning problem with training data S = {(x(i) , y(i) )}, i = 1…n, where each data point is a p-dimensional vector x(i), and y(i) is its label. The number of features p may exceed the number of data points (p >> n).  A binary logistic regression model has the following form, where the vector θ contains the p parameters of the model: 
 FORMULA_1.
 ℓ1-regularized logistic regression [2] finds the vector θ* that maximizes this expression: 
 FORMULA_2.
 Here the term in the first box represents how well the model fits the training data according to Equation (1), and the second term penalizes the model by the sum of its parameters’ absolute values (||θ||1).  By discouraging non-zero parameters – which select the features actually used – this penalty can prevent overfitting.  The hyper-parameter λ controls the trade-off between bias and variance, and can be set by internal cross- validation using a held out set of training data.  For λ = 0,  Equation (2) reduces to conventional logistic regression.  As λ increases, the model’s complexity is penalized more strongly, reducing the number of features it uses. Our data points to test this method are 2112 story readings by 161 children, lasting four or more sentences.  We want to distinguish completed readings from unfinished readings. We truncate each positive example to match the number of sentences to the one of a negative example, so as to sample potential stopping points, not just the end of the story. Negative examples are the entire unfinished readings, which can end anywhere. We use both static and dynamic features.  Static features, e.g. student grade (K-6) and story length, remain static over a story reading.  Dynamic features, e.g. number of sentences read, words read per minute, or clicks logged, change throughout a reading, so we compute separate values for 1, 2, 3, and 4 sentences from the beginning and end of the reading.  To avoid cheating, we exclude features of the last sentence read, e.g. whether the child clicked to exit. Altogether we have 17,163 raw, squared, and threshold features. Figure 1 shows classification accuracy for balanced subsets of different sizes, with the same number of positive and negative examples drawn randomly from the 2112 readings. We used 10-fold cross-validation splitting data randomly.  We also tried splitting across students, but this doesn’t affect accuracy on the full set, yet it is noisy for small subsets due to students with sparse data. The error bars represent the 90% confidence interval. As Figure 1 shows, the method achieves 60% accuracy by training on only 500 examples, increasing to 70% with 600 examples, and asymptoting at 78% above 1500 examples. The three most predictive features are derived from the percentage of the story completed so far, consistent with the intuition that children are likelier to finish shorter stories. 
 Figure 1:  Classification Accuracy on Data Sets of Different Sizes. 
 This paper has presented a novel model to predict students’ task completion in a multimodal tutor, using a method that can train models from data with many dimensions but few examples.  The method, used successfully elsewhere, should interest the EDM community because of its potential to cope with the curse of dimensionality inflicted by the richness of tutorial interaction. 
 Acknowledgements.
 This work was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant R305A080628 to Carnegie Mellon University. The opinions expressed are those of the authors and do not necessarily represent the views of the Institute or U.S. Department of Education.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Predicting Task Completion from Rich but Scarce Data</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-p-gonzalez-brenes"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-p-gonzalez-brenes"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/167/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-p-gonzalez-brenes"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Identifying Students' Inquiry Planning Using Machine Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168/authorlist"/>
		<swrc:abstract>This research investigates the detection of student meta-cognitive planning processes in real-time using log tracing techniques. We use fine and coarse-grained data distillation, in combination with coarse-grained text replay coding, in order to develop detectors for students’ planning of experiments in Science Assistments, an assessment and tutoring system for scientific inquiry. The goal is to recognize student inquiry planning behavior in real-time as the student conducts inquiry in a micro-world; the eventual goal is to provide real- time scaffolding of scientific inquiry.</swrc:abstract>
		<led:body><![CDATA[ 1. Hypothesizing widget (left) and data collection panel (right) for the phase change microworld. 
 We scaffold students’ inquiry processes by organizing these tasks into different inquiry stages, namely, “observe”, “hypothesize”, “experiment”, and “analyze data”. Students start in the hypothesizing stage and move between stages in a suggested order but can navigate back and forth between some of the inquiry phases. For example, from the “analysis” stage students can collect more data by returning to the “experiment” stage, they can create new hypotheses by returning to the “hypothesize” stage (starting a new inquiry loop), or can submit their final experimentation procedures and analyses and begin the next problem. While in the hypothesizing stage, they can either explore the microworld or begin collecting data in the experiment phase. Finally, within the experiment phase, students can only move to the analysis phase. This learning environment has a moderate degree of learner control, less than in purely exploratory learning environments [2], but more than in model-tracing tutors [17] or constraint-based tutors [20]. Though our scaffolding restricts when students can switch inquiry phases, there is enough freedom such that students can approach these inquiry tasks in many ways, e.g., while experimenting, students could set up and run as many different experiments as they desired. In the Hypothesis stage, the student is prompted to build a hypothesis using drop down boxes (Hypothesis Builder). The fields are: independent variable, change to the independent variable, dependent variable, and change to dependent variable. So, for example, the student can change the first [Choose One…] box to “amount of ice”, which enables the next box. Proceeding, the student can create the hypothesis “If I change the [amount of ice] so that it [increases], the [melting point][doesn’t change]. When students reach the experimentation stage, they can then change independent variables, such as Level of Heat, and see the results within the microworld by running a trial (by clicking on Run). They can also view representations of their full set of hypotheses (by clicking on Show hypotheses list) and they can view the trial run data (clicking on Show Table). Both the data table and the hypothesis list provide external memory aid, allowing the student use information about previous decisions to reflect and plan new experimental trials. As students solve these inquiry problems, they could engage in a number of behavior patterns. Particular to collecting data, systematic [24] students collect data that test their hypotheses by designing and running controlled experiments. Additionally, such students may use the table tool and hypothesis list to reflect upon their results and plan for additional experiments they may need to run. Students who are unsystematic in their experimental design and collection of data may exhibit haphazard behaviors such as: constructing experiments that do not test their hypotheses, not collecting enough data to support their hypotheses, not using CVS, or running the same experimental setup multiple times [17]. 
 3 Data Set.
 Participants were 148 eighth grade students, ranging in age from 12-14 years, from a public middle school in Central Massachusetts. These students used the phase change microworld. Students engaged in authentic inquiry problems using the phase change and density microworlds within the Science Assistments learning environment. As part of the phase change activities, students attempted to complete four tasks using our interactive tools. Each of these students completed at least one data collection activity in the phase change environment (two other students did not use the microworld, and were excluded from analysis). As students solved these tasks, we recorded fine-grained actions within the inquiry support tools and microworlds. The set of actions logged included creating hypotheses, setting up experiments, showing or hiding support tools, running experiments, creating interpretations of data, and transitioning between inquiry activities (i.e., moving from hypothesizing to data collection). Each action’s type, current and previous values (where applicable – for instance, a variable’s value), and timestamp were recorded. In all, 27,257 student actions for phase change were logged. These served as the basis for generating text replay clips consisting of contiguous sequences of actions specific to experimenting. 
 4 Method.
 The data used for this study was collected by Science Assistments, which logs every widget action performed by the student including button clicks, checkbox choices, etc. Each action has a time stamp, student/problem identifiers, and widget information, and is tagged as to its step (step tag) in the inquiry process. The step tags are a level above a simple action (this is captured by the widget information), representing a step within the inquiry process, across microworlds. This allows us to analyze similar actions across microworlds. These step tags are used for two purposes: as markers to create clips for text replay coding and to categorize data for fine-grain feature extraction. 
 4.1 Text Replay Coding.
 Text replay hand coding presented our team with two significant challenges: specific codes and grain size. In designing our text replays, it was necessary to use a coarser grain-size than in prior versions of this method [4]. In particular, it is necessary to show significant periods of experimentation in order to put usage of the table and hypothesis list into context, while limiting clip size to reduce memory load. We decided to use clips that include both the hypothesis and the experiment stages, which is long enough to see context, but short enough to tractably code. Another important issue in grain-size selection is that trial run data from one hypothesis test can be used in another to make inferences about the hypothesis at hand (for instance, by comparing a current trial to one conducted earlier). To compensate for this, we code using both the actions in testing the current hypothesis, and cumulative measures which include actions performed when testing previous hypotheses. 
 Figure 2 - Clip showing a single Hypothesis-Experiment run (clips may be significantly longer). 
 To support coding in this fashion, a new tool for text replay tagging was developed in Ruby, shown in Figure 2. The start of the clip is triggered by a hypothesis variable change after the beginning of a new problem. The tool displays all student actions (hypothesis and experiment) until the student transitions to the analysis stage. Subsequent clips include previous clips and any single new cycle which includes the Hypothesis and Experiment stage. A clip could be tagged with one of 10 tags: “Never Change Variables”, “Repeat Trials”, “Non-Interpretable Action Sequence”, “Indecisiveness”, “Used CVS”, “Tested Hypothesis”, “Used Table to Plan”, “Used Hypothesis Viewer to Plan”, “No Activity”, and “Bad Data.” Specific to our study, we tagged a clip as “Used Table to Plan” (TablePlan) if the clip contained actions indicative that the student viewed the trial run data table in a way consistent with planning for subsequent trials. “Used Hypothesis Viewer to Plan” (HypPlan) was chosen if the clip had actions indicating that the student viewed the hypotheses list in a way consistent with planning for subsequent trials. 
 4.2 Coding Agreement.
 Two coders (the third and fourth authors) tagged the data collection clips using at least one of the ten tags. To ensure that a representative range of student clips were coded, we stratified our sample of the clips on condition, student, problem, and within-problem clip order (e.g. first clip, second clip, etc.). The corpus of hand-coded clips contained exactly one randomly selected clip from each problem each student encountered, resulting in 581 clips. Each coder tagged the first 50 clips; the remaining clips were split between the coders. Of the 50 clips tagged, 7 were discarded because of a problem with an early version of the text replay tool where the problem number of the code did not match the problem number of the microworld. For the 43 clips tagged by each coder, there was high overall tagging agreement, average  = 0.86. Of particular relevance to this study, there was strikingly high agreement on the TablePlan,  and of HypPlan, also Kappa at this level suggests particularly good agreement between coders, which was achieved in part through extensive discussion and joint labeling prior to the inter-rater reliability session. In particular, the coders found these two categories easy to code, as students either tended to spend significant amounts of time reflecting on these tools, or viewed them extremely briefly (or not at all). These categories were also relatively rare, potentially increasing  by chance; only 8% of clips involved TablePlan and only 4% of clips involved HypPlan. 
 4.3 Feature Distillation.
 Features extracted can be grouped into 10 categories: all actions, total trial runs, incomplete trial runs, complete trial runs, pauses, data table display, hypothesis list display, field changes in Hypothesis Builder, hypothesis made, and microworld variable changes. For each of these categories we traced the number of times the action and the time taken for each action. Two other categories were included indirectly related to actions: the number trials where only one independent variable was different between the two trials and the number of times a trial was repeated. These last two had no times associated with them. The microworld activity was divided into tasks in which the focus was a specific independent variable. Since there were four independent variables, there were four tasks. Within a task, the student is allowed to make and test several hypotheses. For each of the 12 categories above, we extracted data for each hypothesis the student worked on (non- cumulative data), and across all hypotheses in the task (cumulative data). The reason for this is that within each task, the data table accumulates the trial run data across hypotheses. This allows the students to compare trial runs testing previous hypotheses with the runs made in the current hypothesis. Lastly, the time data was distilled to obtain the following values: minimum, maximum, standard deviation, mean and mode. It is these values plus the count which was used in the machine learning model. This data was arranged in a comma-delimited flat file suitable for input into RapidMiner. The data was divided into files, one for each coded feature. The coded feature being the first item on the line, followed by the distilled features described above. 
 4.4 Machine Learning Algorithms.
 Machine-learned detectors of the two behavioral patterns were developed within RapidMiner 4.6 [19] using the default settings. Detectors were built using J48 decision trees, with automated pruning to control for over-fitting, the same technique used in [26] and [6]. Six-fold cross-validation was conducted at the student level (e.g. detectors are trained on five groups of students and tested on a sixth group of students). By cross- validating at this level, we increase confidence that detectors will be accurate for new groups of students. We assessed the classifiers using two metrics. First, we used A’ [16]. A' is the probability that if the detector is comparing two clips, one involving the category of interest (TablePlan or HypPlan) and one not involving that category, it will correctly identify which clip is which. A' is equivalent to both the area under the ROC curve in signal detection theory, and to W, the Wilcoxon statistic [16]. A model with an A' of 0.5 performs at chance, and a model with an A' of 1.0 performs perfectly. In these analyses, A’ was used at the level of clips, rather than students. Statistical tests for A’ are not presented in this paper. The most appropriate statistical test for A’ in data across students is to calculate A’ and standard error for each student for each model, compare using Z tests, and then aggregate across students using Stouffer’s method [5] – however, the standard error formula for A’ [16] requires multiple examples from each category for each student, which is infeasible in the small samples obtained for each student in our text replay tagging. Another possible method, ignoring student-level differences to increase example counts, biases undesirably in favor of statistical significance. Second, we used Kappa (), which assesses whether the detector identifies is better than chance at identifying the correct action sequences as involving the category of interest. A Kappa of 0 indicates that the detector performs at chance, and a Kappa of 1 indicates that the detector performs perfectly. As Kappa looks only at the final label, whereas A’ looks at the classifier’s degree of confidence, A’ can be more sensitive to uncertainty in classification than Kappa. 
 5  Results.
 We constructed and tested detectors using our corpus of hand-coded clips. TablePlan and HypPlan detectors were constructed from a combination of the subset of the first 43 clips that the two coders agreed on, the remaining clips, tagged separately by the two coders. In total, 570 tagged clips were used for each detector. Of these clips, 47 out of 570 were tagged with TablePlan (8%) and 20 out of 570 (4%) were tagged with HypPlan. 
 Table 1. Best results for detectors of each coding category. 
 Detectors were generated for each behavior using J48 decision trees and two sets of attributes, cumulative and non-cumulative attributes. Thus, four different detectors were constructed two for TablePlan and two for HypPlan. The TablePlan detector using cumulative attributes (A’ = .94,  = .46) performed slightly better than the detector built with non-cumulative attributes (A’ = .96,  = .36). Both versions of the detector achieved excellent performance, comparable to detectors of gaming the system refined over several years (e.g., Baker & de Carvalho, 2008), and are very likely to be appropriate for use in interventions. The HypPlan detectors did not perform as well, achieving A’ = 0.93,  = 0.14 for the non-cumulative attributes and A’=.97,  = 0.02 for the cumulative attributes. The substantial difference between A’ and is unusual. It appears that what happened in this case is that the model, on cross-validation, classified many clips incorrectly with low confidence; in other words, A’ by considering pair-wise comparisons catches the overall rank-ordered correctness of the detector across confidence values even though many clips were mis-categorized at the specific threshold chosen by the algorithm. One possibility is that the low number of HypPlan labels in the data set made the detectors more prone to over-fitting. This result suggests that the HypPlan detector is probably acceptable for fail- soft interventions, where students assessed with low confidence (in either direction) can receive interventions that are not costly if mis-applied. 
 6 Discussion and Conclusions.
 In this paper, we have presented models for detecting planning within science inquiry learning. Our efforts to detect planning from data table usage have met with greater initial success than our attempts to detect planning within the hypothesis list, although both detectors are, we feel, good enough to use for some forms of instructional intervention. The detector for showing data table use (TablePlan) in planning can detect a student using the data table effectively from one not using the data table effectively for planning 94% of the time. The  is respectable, so this detector can be used robustly to scaffold table use for planning during inquiry. If we detect that a student is not using the table effectively, we can suggest that the user look at the table and provide hints on how to compare one table row with another, and how to use this to plan the next trial. On the other hand, the detector for using the hypotheses table for planning (HypPlan) did not perform as well. Although it had a very good A’ (.93 and .97), the  was low, meaning that if we used this detector for scaffolding, we will need to do it in a fail-soft manner. There is reason to believe this approach may be successful. For example, an early detector of gaming the system [7] with a similar and lower A’ was found to be effective for improving gaming students’ learning when used in a fail-soft manner. In addition, combining the HypPlan detector with another (for example, one that detects control for variables strategy or CVS) may compensate for its low  So for example, if a detector indicated that CVS was not being used, this detector also can be used to decide if scaffolding should include a hint regarding how the student should use the hypothesis table in order to reflect on their work. In this fashion, interventions based on this detector will only be given when there is additional reason to believe that intervention is needed. Future work will include improving our for HypPlan and finding other meta-cognitive tasks that can be detected effectively. This would require an expansion of the tags we used and perhaps a way to track student progress from one problem to another, since lesson-wide attributes may be useful for measuring students’ progress. By detecting planning in real time, rich adaptive scaffolding becomes feasible [13, 14]. In addition, with helping students learn both content and inquiry skills, scaffolding for planning can help them become better learners, possibly by influencing their meta- cognitive skill development [1, 23]. This study makes an important contribution towards linking these two areas of research, namely, meta-cognitive skills and planning during scientific inquiry. 
 Acknowledgements.
 This research is funded by the National Science Foundation (NSF-DRL#0733286), Janice Gobert, Principal Investigator, Neil Heffernan, Ryan Baker, and Carolina Ruiz, Co- Principal Investigators, and the U.S. Department of Education (R305A090170), Janice Gobert, Principal Investigator, Neil Heffernan, Ken Koedinger, and Joe Beck, Co- Principal Investigators. Any opinions expressed are those of the authors and do not necessarily reflect those of the funding agencies.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Identifying Students' Inquiry Planning Using Machine Learning</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/orlando-montalvo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/orlando-montalvo"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-nakama"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-nakama"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/orlando-montalvo"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-nakama"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Class Association Rule Mining from Students' Test Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169/authorlist"/>
		<swrc:abstract>In this paper we propose the use of a special type of association rules mining for discovering interesting relationships from the students’ test data collected in our case with Moodle learning management system (LMS). Particularly, we apply Class Association Rule (CAR) mining to different data matrices such as the score-matrix, the relationship-matrix and the knowledge- matrix. These matrices are constructed based on the data relate to students’ performance in the test and on the domain knowledge provided by the instructor. We describe how to obtain these matrices and then we have applied a CAR mining algorithm.</swrc:abstract>
		<led:body><![CDATA[ 1.  Matrices created from test’s data. 
 We have applied the Apriori-CAR mining algorithm over the previously described data matrices. In the first experiment, we have used the score-matrix, and we have selected/filtered as input-attributes (antecedent) only the item answers, and as class the final score. In this way, we can see the relationships between items and how they can predict/determine the final score obtained by students. In the second experiment, we have used the knowledge-matrix, and we have selected the knowledge of concepts as input- attributes (antecedent) and the final score as a class attribute. In this way, we can discover the relationships between concepts and between the level of knowledge of these concepts and the final score obtained by students. 
 3 Conclusions and Future Work.
 In this paper, we proposed to use a special type of association rules over the assessment data in a particular scenario. We mined different test data matrices rather than only the typical score-matrix. Particularly we used an item-concept relationship matrix created by the instructor and a student-concept knowledge level matrix automatically created based on the information from the other two matrices. Finally, it is important to notice that concepts themselves may need to be presented as a hierarchy rather than a 'flat' set of independent concepts. Mining interesting patterns in such settings is one of the directions of our further work.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Class Association Rule Mining from Students' Test Data</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Observing Online Curriculum Planning Behavior of Teachers</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170/authorlist"/>
		<swrc:abstract>Curriculum planning is perhaps one of the most important tasks teachers must perform before instruction.  While this task is facilitated by a wealth of existing online tools and resources, teachers are increasingly over- whelmed with finding, adapting and aligning relevant resources that support them in their planning.  Consequently, ripe research opportunities exist to study and understand online planning behavior in order to more generally characterize planning behavior.  In this paper, we introduce a web-based curriculum planning tool and study its use by middle and high school Earth science teachers.  We ex- amine the web analytics component of the tool and apply clustering algorithms to model and discover patterns of the use within the system.  Our initial results provide insights into the use of the tool over time and indicate teachers are en- gaging in behavior that show affinity for the use of interactive digital resources as well as social sharing behaviors.  These results show tremendous promise in developing teacher-centric analysis techniques to improve planning technologies and techniques to study online curriculum planning patterns. The use of the Internet in the classroom, applied either as a direct instructional tool or as a student learning tool for research and self-directed learning, has become essential to teachers and learners alike. Much empirical research indicates that Americans in general and K-12 students in particular are using technology in their day-to-day lives more than ever before; communication technologies that leverage the Internet are particularly popu- lar with young people [2]. A large body of education research indicates that the best learning experiences are those that make direct connections to students’ existing know- ledge and life experiences [1]. Thus, it is vital that K-12 education leverage Internet tech- nology not only because it offers instructional benefits in and of itself but because it can bridge students’ in-class experiences with their out-of-class lives, thus making learning personally relevant. Tools supporting teachers through planning, organizing and integrating instruction around the complexities of individual student skill, curriculum goals, district-wide stan- dards, etc. lack maturity, perhaps because the fluid nature of planning in general or the changing demands of the classroom. Despite the myriad of teacher resources in the form of shared ideas and re-usable lesson plans, activities, etc., successfully integrating these resources yet requires a fair amount of customization. Teachers often become over- whelmed by the customization task that it becomes more time consuming to re-use and re-purpose existing materials than develop their own. This poster describes the application context, research questions, initial experiments and results of an online curriculum planning and development tool called the Curriculum Customization Service (CCS). The tool was deployed for use by 6th and 9th grade middle and high school teachers within the Denver Public School system, and usage observations were made over the course of a semester of tool use. We detail the tool : its motivation, interface and content, as well as the web analytics data generated by the end user interac- tions. We describe the initial exploration and selection of data features and the applica- tion of clustering algorithms to analyze system usage. Our research focuses on develop- ing and applying tools and techniques for observing and classifying teachers’ online be- havior in educational applications, offering a unique view port into educators’ online usage patterns and behaviors.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Observing Online Curriculum Planning Behavior of Teachers</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-e-maull"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-e-maull"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/manuel-gerardo-saldivar"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/manuel-gerardo-saldivar"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tamara-sumner"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tamara-sumner"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-e-maull"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/manuel-gerardo-saldivar"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/tamara-sumner"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/171">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>DISCUSS: Enabling Detailed Characterization of Tutorial Interactions Through Dialogue Annotation</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/171/authorlist"/>
		<swrc:abstract>EMPTY</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>DISCUSS: Enabling Detailed Characterization of Tutorial Interactions Through Dialogue Annotation</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/lee-becker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/lee-becker"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/wayne-h-ward"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/wayne-h-ward"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sarel-vanvuuren"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sarel-vanvuuren"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/171/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/lee-becker"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/wayne-h-ward"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/sarel-vanvuuren"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Conceptualizing Procedural Knowledge Targeted at Students of Different Skill Levels</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172/authorlist"/>
		<swrc:abstract>Conceptualizing procedural knowledge is one of the most challenging tasks of building systems for intelligent tutoring. We present a novel algorithm that enables teachers to accomplish this task (semi)automatically. Furthermore, it is desired to adapt the level of conceptualization to the skill level of particular students. We argue that our algorithm facilitates such adaptation in a straight- forward fashion. We demonstrate this feature of the algorithm with a case study.</swrc:abstract>
		<led:body><![CDATA[ 1.Ne5-d3 achieving the goal.”.
 Figure 1: Interaction between computer and teacher: explanation of a critical example. 
 An example interaction between the method and the teacher is shown in Fig. 1.  The teacher is presented with a critical example, i. e., the example where the current set of rules suggested a bad goal (“push black king to the edge of the board” can be achieved, but is not leading to solution). The teacher was therefore asked to provide a better goal for this position, which was then used in the construction of a new set of goal-based rules. The process was completed when all critical examples were explained by the expert. The final rules1 were presented to three chess teachers (among them a selector of Slovenian women's squad and a selector of Slovenian youth squad) to evaluate their appropriateness for teaching chess-players. They all agreed on the usefulness of the presented concepts and found the derived strategy suitable for educational purposes at the level targeted for. Among the reasons to support this assessment was that the instructions “clearly demonstrate the intermediate subgoals of delivering checkmate.”]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Conceptualizing Procedural Knowledge Targeted at Students of Different Skill Levels</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-mozina"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-mozina"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/matej-guid"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/matej-guid"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksander-sadikov"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksander-sadikov"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/vida-groznik"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/vida-groznik"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jana-krivec"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jana-krivec"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ivan-bratko"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ivan-bratko"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-mozina"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/matej-guid"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksander-sadikov"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/vida-groznik"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/jana-krivec"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/ivan-bratko"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Higher Contributions Correlate with Higher Learning Gains</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173/authorlist"/>
		<swrc:abstract>Students interacted with an Intelligent Tutoring System called Operation ARIES!,which involves two agents interacting with the human in natural language trialogs. We investigated the conditions in which the length of the students’ contributions is correlated with learning. Word count and the proportional learning gains scores were correlated, especially in the later phases of the curriculum. The link between student contribution length and learning supports previous findings in human one-on-one tutoring.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Higher Contributions Correlate with Higher Learning Gains</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/carol-forsyth"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/carol-forsyth"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/heather-butler"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/heather-butler"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/diane-halpern"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/diane-halpern"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-millis"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-millis"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/zhiqiang-cai"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/zhiqiang-cai"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jonathan-wood"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jonathan-wood"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/carol-forsyth"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/heather-butler"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/diane-halpern"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-millis"/>
		<rdf:_6 rdf:resource="http://data.linkededucation.org/resource/lak/person/zhiqiang-cai"/>
		<rdf:_7 rdf:resource="http://data.linkededucation.org/resource/lak/person/jonathan-wood"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/174">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Sentiment Analysis in Student Experiences of Learning</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/174/authorlist"/>
		<swrc:abstract>In this paper we present an evaluation of new techniques for automatically detecting sentiment polarity (Positive or Negative) in the students responses to Unit of Study Evaluations (USE). The study compares categorical model and dimensional model making use of five emotion categories: Anger, Fear, Joy, Sadness, and Surprise. Joy and Surprise are taken as a Positive polarity, whereas Anger, Fear and Sadness belong to Negative polarity in the binary classes, respectively. We evaluate the performances of category-based and dimension-based emotion prediction models on the 2,940 textual responses. In the former model, WordNet-Affect is used as a linguistic lexical resource and two dimensionality reduction techniques are evaluated: Latent Semantic Analysis (LSA) and Non-negative Matrix Factorization (NMF). In the latter model, ANEW (Affective Norm for English Words), a normative database with affective terms, is employed. Despite using generic emotion categories and no syntactical analysis, NMF-based categorical model and dimensional model result in better performances above the baseline.</swrc:abstract>
		<led:body><![CDATA[ 1. The learning outcomes and expected standards of this unit of study were clear to me. 2. The teaching in this unit of study helped me to learn effectively. 3. This unit of study helped me develop valuable graduate attributes. 4. The workload in this unit of study was too high. 5. The assessment in this unit of study allowed me to demonstrate what I had understood. 6. I can see the relevance of this unit of study to my degree. 7. It was clear to me that the staff in this unit of study were responsive to student feedback. 8. My prior learning adequately prepared me to do this unit of study. 9. The learning and teaching interaction helped me to learn in this unit of study. 10. My learning of this unit of study was supported by the faculty infrastructure. 11. I could understand the teaching staff clearly when they explained. 12. Overall I was satisfied with the quality of this unit of study. 
 Eleven items (I1-I11) focus on students’ experience and one item (I12) on student satisfaction. Students indicate the extent of their agreement with each statement based on a 5 - point Likert scale: 1 - strongly disagree, 2 - disagree, 3 - neutral, 4 - agree and 5 - strongly agree. Below each statement there is a space requesting students to explain their response. Question 4 has a different sentiment structure therefore was removed in this study. The USEs of subjects taught by two academics collected over a period of six years were used to create the dataset. After removing responses to question 4, the dataset contains a total of 909 questionnaires (each with 11 ratings), and out of the possible 9,999, students responded with 3,008 textual responses (each expected to be a description of a rating), a textual response rate of 30.1 %. Out of these we removed internal referencing (e.g. ‘see above’) and meaningless text (e.g. ‘?’). The textual data has two characteristics that may significantly affect the classifiers. First the sentences are hand-written in an informal style, containing spelling errors, abbreviated non-dictionary words or hard to read text. The lack of proper grammar would make it extremely challenging to use part-of-speech (POS) tagging or other computational linguistic approaches. Examples include: “Computers in labs too slowk no lecture notes” (spelling mistakes and non-grammar), “tutes were overcrowded, stopping teacher / student interaction” (non-standard words). For these reasons, the techniques used in the experiment are based on the bag-of-words assumption (so word order is not used) and we do not use POS tagging that would require relatively correct grammar. 
 Table 1.  Number of comments and sample comments for each sentiment. 
 4 Experiments and Results.
 The following five different approaches are implemented in Matlab. One categorical model that has two variants, according to three corresponding methods of dimension reduction, one dimensional method, and two similarity comparison methods for each model are implemented. For evaluation purposes, we employ Majority Class Baseline (MCB) as our baseline and Keyword Spotting (KWS). We remove stop words and use stemming. Text to Matrix Generator (TMG), a Matlab toolkit [15], is used to generate term-by-sentence Matrix. • Majority Class Baseline (MCB): classification that always predicts the majority class, which in this dataset is Positive across all sentiment classifications. • Keyword Spotting (KWS):  a naïve approach that counts the presence of obvious affect words like “frustrating” and “satisfaction”, which are extracted from WordNet-Affect for five emotion categories. • CLSA: LSA-based categorical classification • CNMF: NMF-based categorical classification • DIM: Dimension-based estimation Five emotion categories are utilized (Anger, Fear, Joy, Sadness, and Surprise) in which Joy and Surprise emotions are assigned to positive class while Anger, Fear, and Sadness are the members of negative class, respectively. Negative emotion, disgust, is removed because the emotion is similar to anger and leads to making sentiment classes biased. Likewise, strongly agree and agree belong to positive, and strongly disagree and disagree are referred to negative. The number of sentences for each rating and sentiment used in our experiment is shown in Table 1. In addition, sample comments of the annotated corpus appear in Table 1. Table 2 shows the precision, recall, and F-measure values obtained by the five approaches for the automatic classification of three sentiments. The highest results are marked in bold for each individual class. We do not include accuracy values in our results due to the imbalanced categories (see Table 1). The accuracy metric does not provide adequate information, whereas precision, recall, and F-measure can effectively evaluate the classification performance with respect to imbalanced datasets [16]. 
 Table 2.  Sentiment identification results.
 As can be seen from the table, the performances of each approach depend on each sentiment category. In case of the positive class, which has the largest number of sentences, MCB and CNMF get the best sentiment detection performance in terms of recall and F-measure. DIM achieves rather high precision score in comparison with all other classifications. We can see that DIM approach gives the best results for negative class. When it comes to neutral, KWS shows the best performance with respect to recall and F-measure. On the other hand, CNMF particularly outperforms the others for precision. Figure 1 indicates a result of the 3-dimensional and 2-dimensional attribute evaluation for USEs. 
 Figure 1.  Distribution of the USEs dataset in the 3-dimensional (left) and 2-dimensional (right) sentiment space. The ‘x’ denotes the location of one comment corresponding to valence, arousal, and dominance. 
 A notable aspect observed in the USE data is that there are somewhat inconsistencies between students’ ratings and written responses illustrated with examples in Table 3. For instance, the third row is unambiguously negative but the student graded this sentence as neutral. Therefore, all approaches have a weakness in recognizing sentiments due to the peculiarity of this data. Another factor, which makes the automatic classification difficult, is that all classifiers are not specific to education domains. For this reason, we speculate that this mediocre performance of the methods is owing to poor coverage of the features found in education domains. 
 Table 3.  Sample feedbacks from misclassified results. (Positive values are those rates 4 as 5, neutral as 3 and negative 1 or 2). 
 Table 4 shows overall precision, recall, and F-measure comparison with respect to MCB, KWS, CLSA, CNMF, and DIM in two averaging perspectives (micro-averaging and macro-averaging). The notable difference between these to calculate is that micro- averaging gives equal weight to every sentence whereas macro-averaging weights equally all the categories. From this summarized table, we can see that MCB, KWS, and CLSA perform less effectively with a little low number of evaluation scores compared with CNMF and DIM. In case of macro-averaging, CNMF is superior to other classifications in precision, while DIM surpasses the others in recall and F-measure. On the other hand, DIM has the best precision and CNMF performs better for F-measure in micro averaging. Overall, CNMF and DIM vie with each other in precision, recall and F-measure and the best F-measure is obtained with the approach based on CNMF or DIM for each average. Our KWS conducted in all experiments is inferior to CNMF, DIM as well as CLSA. The result implies that keyword spotting techniques cannot handle the sentences which evoke strong emotions through underlying meaning rather than affect keywords. In addition, we can infer that the models (CNMF and DIM) with non-negative factors are appropriate for dealing with text collections. In summary, NMF-based categorical model and dimensional model shows the better sentiment recognition performance as a whole. The most frequent words used by students to describe aspects of their experience, include terms such as labs, lecturer, lectures, students, tutors, subject, and work. When we remove these terms, the words most frequently used to describe positive experiences include: good (n=263), helpful and helped (n=183), online (n=79), understand (n=49). Those used to describe negative experiences include: hard (n=72), understand (n=67), time (n=47). Neutral experiences contain a combination of both. These words lists are obtained from CNMF and DIM because two classifications have better overall performance as aforementioned. Stemming was not used for this analysis since in this particular corpus it might hide important differences as between ‘lecturer’ and ‘lecture’. 
 Table 4.  Overall average resutls.
 5 Discussion.
 This paper described a dataset of ratings and textual responses of student evaluations of teaching. Sentiment analysis techniques for automatically rating textual responses as positive, negative or neutral using the students’ ratings were evaluated. In particular, the performance of categorical model and dimensional model were compared, each of which makes use of different linguistic resources. This paper highlighted that NMF-based categorical and dimensional models have a better performance than the others. Moreover, despite not having an appropriate set of emotional categories to use, the efficacy of two emotion lexicons (WordNet-Affect and ANEW) promises to be useful in these sentiment classification tasks. While two models and two lexicons are promising for identifying sentiments, there are still challenges to overcome. We believe that affective expressivity of text is on the basis of more complex linguistic features such as morphological features. Hence, we are going to delve into Natural Language Processing (NLP) to recognize fine-grained emotion in the future. Future work will include extending the corpora with more student evaluations and this should provide more reliable results. The categorical model should be evaluated with a set of emotion categories better grounded in the educational research literature and we suspect that the literature on motivation would be particularly useful. With regards to the use of normative databases to study the dimensional model, we are aware that the terms in ANEW are not the best suited for the vocabulary that students use to describe their experiences, but we are not aware of other more appropriate databases. 
 Acknowledgement.
 This project was partially funded by a TIES grant from the University of Sydney.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Sentiment Analysis in Student Experiences of Learning</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/sunghwan-mac-kim"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/sunghwan-mac-kim"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/174/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/sunghwan-mac-kim"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/175">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>EDM Visualization Tool: Watching Students Learn</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/175/authorlist"/>
		<swrc:abstract>This poster describes a visualization tool for educators that allows the exploration of educational data. We display an entire classes sequence of actions to the user using a tree-graph. Our preliminary results suggest that EDM visualization tools are a promising area for future research in EDM.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 This poster describes a visualization tool that allows educators to visualize the process in which students solved procedural problems, in logic, using an intelligent tutoring system. The purpose of this tool is to allow educators to be able to navigate, explore and gain insights about student performance. This allows educators to better understand the strengths and deficiencies of students, so that lectures or homework adjustments can be made to better aid student learning. The field of InfoVis has much to offer educators and data repositories of educational data, like Carnegie Mellon's Data Shop. Fekete et. al. shows us that InfoVis is well equipped for exploring data to learn more, make new discoveries, and gain insight[3]. Card et al. defines the purpose of visualization to “amplify cognition” about data [2]. In our case amplifying an educator's cognition about the way their students solve problems, a main advantage Intelligent tutoring systems have over traditional homework methods. 
 Related Work.
 This work is an extension of the work of John Stamper and Tiffany Barnes [6,1]. We extend their work to include an interactive visualization tool which centralizes and streamlines their data processing, and adds exploration and navigation interactions. In our visualization we made use of Shneidermans's seven tasks of visualization[5]: overview, zoom, filter, details-on-demand, relate, history and extract; often considered standard in information visualization. Romero and Ventura surveyed EDM techniques in [4] “...information obtained from usage statistics is not always easy to interpret to the educators and then other techniques have to be used...Infovis techniques.” They also concluded that educational data mining tools require “good visualization facilities to make their results meaningful to educators and e-learning designers”. 
 EDM Visualization Tool.
 The EDM Vis tool is a software tool that presents student work to educators in a simple way. First, students use a logic tutor where each 'state' of the user is recorded, along with each action, common in intelligent tutoring systems. Actions take the user from one state to another. In the case of Tic-Tac-Toe an action would be placing your 'X' or 'O' piece on the board, the state would be the resulting configuration of X's and O's. These states and actions are then displayed as a tree-graph of nodes and edges respectively. We use logic tutor data which stores the set of premises as a single state, in a root node. Consecutive states are generated based upon the actions that were taken by the student(s). The depth of each node represents the number of steps taken. Edge width is based on the frequency of students who performed the same action(s). Filtering and other interactions allow educators to observe trends, common mistakes and gain insights into their students' ways of thinking. 
 Results & Future Work.
 The EDM Vis tool is still in development but unofficial results show we can gain insights about student progress. Our first insight was that only ten percent of students were able to find the shortest solution, or expert path. We also noticed problematic areas that we were not previously aware of. Next we will make a standard file format that can support data from the Data Shop at Carnegie Mellon, allowing acces to more people and data. Also we will allow annotations to be made in the Vis tool which will export separate files, to be read into our logic tutor. This will extend the visualization tool to creation, allowing educators to gain insights then act using their new knowledge. Lastly, the EDM Vis tool is built for visualizing sequential data, so single step problems, like short answer questions, are unsupported. Developing other visualization tools to support other data types seems to be a meaningful avenue for future research.]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>EDM Visualization Tool: Watching Students Learn</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/matthew-w-johnson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/matthew-w-johnson"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/175/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/matthew-w-johnson"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/176">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/proceedings"/>
		<dc:title>Using LiMS (the Learner Interaction Monitoring System) to Track Online Learner Engagement and Evaluate Course Design</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/176/authorlist"/>
		<swrc:abstract>This poster will describe the Learner Interaction Monitoring System (LiMS), designed to capture data demonstrating learner online engagement with course materials. The poster presentation will explain how the LiMS ‘event capture model’ collects detailed real-time data on learner behavior in self- directed online learning environments, and interprets these data by drawing on behavioral research. We believe that LiMS offers education and training managers in corporate contexts a valuable tool for the evaluation of learner performance and course design. By permitting more detailed demonstration of ROI in education and training, LiMS allows managers to make the case for web based courseware that reflects appropriate and evidence-based instructional design, rather than budgetary constraints.</swrc:abstract>
		<led:body><![CDATA[]]></led:body>
		<swrc:month></swrc:month>
		<swrc:year>2010</swrc:year>
		<rdfs:label>Using LiMS (the Learner Interaction Monitoring System) to Track Online Learner Engagement and Evaluate Course Design</rdfs:label>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/peter-sorenson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/peter-sorenson"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/176/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/peter-sorenson"/>
	</rdf:Description>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/applied-research-associates,-inc,-raleigh">
		<rdfs:label>Applied Research Associates, Inc., Raleigh</rdfs:label>
		<foaf:name>Applied Research Associates, Inc., Raleigh</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mladen-a-vouk"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/aroline-ag">
		<rdfs:label>Aroline AG</rdfs:label>
		<foaf:name>Aroline AG</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-kruger"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/australian-catholic-university">
		<rdfs:label>Australian Catholic University</rdfs:label>
		<foaf:name>Australian Catholic University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hema-soundranayagam"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/beuth-hochschule-fur-technik-berlin">
		<rdfs:label>Beuth Hochschule fur Technik Berlin</rdfs:label>
		<foaf:name>Beuth Hochschule fur Technik Berlin</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/agathe-merceron"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sebastian-schwarzrock"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/beuth-university-of-applied-sciences">
		<rdfs:label>Beuth University of Applied Sciences</rdfs:label>
		<foaf:name>Beuth University of Applied Sciences</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-wolf"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/boulder-language-technologies">
		<rdfs:label>Boulder Language Technologies</rdfs:label>
		<foaf:name>Boulder Language Technologies</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/wayne-h-ward"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/cairo-microsoft-innovation-lab">
		<rdfs:label>Cairo Microsoft Innovation Lab</rdfs:label>
		<foaf:name>Cairo Microsoft Innovation Lab</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nayer-wanas"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/cairo-university">
		<rdfs:label>Cairo University</rdfs:label>
		<foaf:name>Cairo University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nevin-darwish"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university">
		<rdfs:label>Carnegie Mellon University</rdfs:label>
		<foaf:name>Carnegie Mellon University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ilya-m-goldin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vincent-aleven"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/bruce-m-mclaren"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-v-yudelson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/emma-brunskill"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/martina-a-rau"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jung-in-lee"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/leigh-ann-sudol-delyser"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kelly-rivers"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/richard-scheines"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/john-c-stamper"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/derek-lomas"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jessica-kalka"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/angela-z-wagner"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/gail-w-kusbit"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-p-gonzalez-brenes"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jack-mostow"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-a-mclaughlin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/stephen-e-fancsali"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yanbo-xu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/geoff-gordon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nan-li"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/noboru-matsuda"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/william-w-cohen"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/weisi-duan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/s-isotani"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/m-munna"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/s-wu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/bao-hong-tan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/albert-t-corbett"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jon-m-fincham"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/john-r-anderson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/shawn-betts"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jennifer-l-ferris"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-nugent"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/turadg-aleahmad"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-kraut"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/benjamin-shih"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hao-cen"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carolyn-p-rose"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-cui"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/adriana-mjb-de-carvalho"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/xiaonan-zhang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-valeri"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lili-wu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kyle-cunningham"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/alida-skogsholm"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/brett-leber"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/center-for-technology-innovation">
		<rdfs:label>Center for Technology Innovation</rdfs:label>
		<foaf:name>Center for Technology Innovation</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sherry-hsi"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/claremont-graduate-university">
		<rdfs:label>Claremont Graduate University</rdfs:label>
		<foaf:name>Claremont Graduate University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/heather-butler"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/diane-halpern"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/ecole-national-superieure-d-informatique">
		<rdfs:label>Ecole national Superieure d'Informatique</rdfs:label>
		<foaf:name>Ecole national Superieure d'Informatique</foaf:name>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/ecole-polytechnique-de-montreal">
		<rdfs:label>Ecole Polytechnique de Montreal</rdfs:label>
		<foaf:name>Ecole Polytechnique de Montreal</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/behzad-beheshti"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michel-c-desmarais"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rhouma-naceur"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/edlab">
		<rdfs:label>EdLab</rdfs:label>
		<foaf:name>EdLab</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ankit-ranka"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/faisal-anwar"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hui-soo-chae"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/educational-initiatives-pvt-ltd">
		<rdfs:label>Educational Initiatives Pvt. Ltd.</rdfs:label>
		<foaf:name>Educational Initiatives Pvt. Ltd.</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/suchismita-srinivas"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/muntaquim-bagadia"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anupriya-gupta"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology">
		<rdfs:label>Eindhoven University of Technology</rdfs:label>
		<foaf:name>Eindhoven University of Technology</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rafal-kocielnik"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/natalia-sidorova"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/n-trcka"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/p-de-bra"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/evgeny-knutov"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sicco-verwer"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/wil-van-der-aalst"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/gerben-w-dekker"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jan-m-vleeshouwers"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/toon-calders"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/electronics-research-institute">
		<rdfs:label>Electronics Research Institute</rdfs:label>
		<foaf:name>Electronics Research Institute</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nabila-khodeir"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nadia-hegazy"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/institute-of-cognitive-science">
		<rdfs:label>Institute of Cognitive Science</rdfs:label>
		<foaf:name>Institute of Cognitive Science</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tamara-sumner"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/laboratoire-de-paris-6">
		<rdfs:label>Laboratoire de Paris 6</rdfs:label>
		<foaf:name>Laboratoire de Paris 6</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nabila-bousbia"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jean-marc-labat"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/metropolitan-state-college-of-denver">
		<rdfs:label>Metropolitan State College of Denver</rdfs:label>
		<foaf:name>Metropolitan State College of Denver</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/haiyun-bian"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/north-carolina-state-university">
		<rdfs:label>North Carolina State University</rdfs:label>
		<foaf:name>North Carolina State University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jennifer-l-sabourin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/bradford-w-mott"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/james-c-lester"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jonathan-p-rowe"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kristy-elizabeth-boyer"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/robert-phillips"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/eun-young-ha"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-d-wallis"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/northern-illinois-university">
		<rdfs:label>Northern Illinois University</rdfs:label>
		<foaf:name>Northern Illinois University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-millis"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/philips-research-laboratories">
		<rdfs:label>Philips Research Laboratories</rdfs:label>
		<foaf:name>Philips Research Laboratories</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksandra-tesanovic"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/polytechnique-montreal">
		<rdfs:label>Polytechnique Montreal</rdfs:label>
		<foaf:name>Polytechnique Montreal</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ildiko-pelczer"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/alejandro-villarreal"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michel-gagnon"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/quizzicle">
		<rdfs:label>Quizzicle</rdfs:label>
		<foaf:name>Quizzicle</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/peter-sorenson"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/school-of-education">
		<rdfs:label>School of Education</rdfs:label>
		<foaf:name>School of Education</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/manuel-gerardo-saldivar"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/sess-robert-morris-university">
		<rdfs:label>SESS Robert Morris University</rdfs:label>
		<foaf:name>SESS Robert Morris University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/james-a-bernauer"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jay-c-powell"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/sharif-university-of-technology">
		<rdfs:label>Sharif University of Technology</rdfs:label>
		<foaf:name>Sharif University of Technology</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mohammad-hassan-falakmasir"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jafar-habibi"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/sri-international">
		<rdfs:label>SRI International</rdfs:label>
		<foaf:name>SRI International</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/m-feng"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/tallinn-university-of-technology">
		<rdfs:label>Tallinn University of Technology</rdfs:label>
		<foaf:name>Tallinn University of Technology</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jaan-ubi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/innar-liiv"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/tel-aviv-university">
		<rdfs:label>Tel Aviv University</rdfs:label>
		<foaf:name>Tel Aviv University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rafi-nachmias"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ronit-azran"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/universidad-autonoma-de-madrid">
		<rdfs:label>Universidad Autonoma de Madrid</rdfs:label>
		<foaf:name>Universidad Autonoma de Madrid</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/javier-bravo"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/universidad-de-lima">
		<rdfs:label>Universidad de Lima</rdfs:label>
		<foaf:name>Universidad de Lima</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/cesar-vialardi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jorge-chue"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/alfredo-barrientos"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/daniel-victoria"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jhonny-estrella"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/juan-pablo-peche"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/universite-de-paris-sud">
		<rdfs:label>Universite de Paris-Sud</rdfs:label>
		<foaf:name>Universite de Paris-Sud</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/issam-rebai"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-british-columbia">
		<rdfs:label>University of British Columbia</rdfs:label>
		<foaf:name>University of British Columbia</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/s-kardan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/c-conati"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-california">
		<rdfs:label>University of California</rdfs:label>
		<foaf:name>University of California</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-n-rafferty"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michelle-m-lamar"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/thomas-l-griffiths"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/stuart-russell"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-canterbury">
		<rdfs:label>University of Canterbury</rdfs:label>
		<foaf:name>University of Canterbury</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/antonija-mitrovic"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/moffat-mathews"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tanja-mitrovic"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-colorado">
		<rdfs:label>University of Colorado</rdfs:label>
		<foaf:name>University of Colorado</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/keith-e-maull"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-colorado-at-boulder">
		<rdfs:label>University of Colorado at Boulder</rdfs:label>
		<foaf:name>University of Colorado at Boulder</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lee-becker"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sarel-vanvuuren"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba">
		<rdfs:label>University of Cordoba</rdfs:label>
		<foaf:name>University of Cordoba</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mi-lopez"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jm-luna"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/c-romero"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/s-ventura"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mm-molina"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/r-pedraza-perez"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-raul-romero"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/enrique-garcia"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carlos-de-castro"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/amelia-zafra"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-electro-communications">
		<rdfs:label>University of Electro-Communications</rdfs:label>
		<foaf:name>University of Electro-Communications</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/pokpong-songmuang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/maomi-ueno"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-glasgow">
		<rdfs:label>University of Glasgow</rdfs:label>
		<foaf:name>University of Glasgow</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nema-dean"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-ljubljana">
		<rdfs:label>University of Ljubljana</rdfs:label>
		<foaf:name>University of Ljubljana</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/martin-mozina"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/matej-guid"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/aleksander-sadikov"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vida-groznik"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jana-krivec"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ivan-bratko"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-maryland">
		<rdfs:label>University of Maryland</rdfs:label>
		<foaf:name>University of Maryland</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/andre-a-rupp"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/shauna-j-sweet"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/younyoung-choi"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-massachusetts">
		<rdfs:label>University of Massachusetts</rdfs:label>
		<foaf:name>University of Massachusetts</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ivon-arroyo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hasmik-mehranian"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/beverly-park-woolf"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/david-h-shanabrook"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/david-g-cooper"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-memphis">
		<rdfs:label>University of Memphis</rdfs:label>
		<foaf:name>University of Memphis</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vasile-rus"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/cristian-moldovan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nobal-niraula"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/fazel-keshtkar"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/brent-morgan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carol-forsyth"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/philip-pavlik"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/zhiqiang-cai"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mae-lynn-germany"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sidney-d-mello"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/blair-lehman"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/whitney-cade"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/andrew-olney"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jonathan-wood"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mihai-lintean"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-north-carolina-at-charlotte">
		<rdfs:label>University of North Carolina at Charlotte</rdfs:label>
		<foaf:name>University of North Carolina at Charlotte</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-john-eagle"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/matthew-w-johnson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/tiffany-barnes"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/leena-joseph"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/b-mostafavi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/m-croy"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lorrie-lehman"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-pittsburgh">
		<rdfs:label>University of Pittsburgh</rdfs:label>
		<foaf:name>University of Pittsburgh</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/pamela-jordan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/peter-brusilovsky"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/wenting-xiong"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/diane-litman"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/christian-schunn"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/collin-lynch"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kevin-d-ashley"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/moses-hall"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-sydney">
		<rdfs:label>University of Sydney</rdfs:label>
		<foaf:name>University of Sydney</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/roberto-martinez-maldonado"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/judy-kay"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/irena-koprinska"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-katrina-dominguez"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/james-r-curran"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rajibussalim"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vilaythong-southavilay"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sunghwan-mac-kim"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-sydney">
		<rdfs:label>University of Sydney</rdfs:label>
		<foaf:name>University of Sydney</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/roberto-martinez-maldonado"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kalina-yacef"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/judy-kay"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/irena-koprinska"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-katrina-dominguez"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/james-r-curran"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rajibussalim"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vilaythong-southavilay"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sunghwan-mac-kim"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-waterloo">
		<rdfs:label>University of Waterloo</rdfs:label>
		<foaf:name>University of Waterloo</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/john-champaign"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/robin-cohen"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/utah-state-university">
		<rdfs:label>Utah State University</rdfs:label>
		<foaf:name>Utah State University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/christine-garrard"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/vanderbilt-university">
		<rdfs:label>Vanderbilt University</rdfs:label>
		<foaf:name>Vanderbilt University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/john-s-kinnebrew"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/gautam-biswas"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/larry-howard"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/julie-johnson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carin-neitzel"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hogyeong-jeong"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute">
		<rdfs:label>Worcester Polytechnic Institute</rdfs:label>
		<foaf:name>Worcester Polytechnic Institute</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/qing-yang-wang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/shubhendu-trivedi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yutao-wang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/joseph-e-beck"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sujith-m-gowda"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-wixon"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/aatish-salvi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jaclyn-ocumpaugh"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lisa-rossi"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/gabor-n-sarkozy"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/c-heffernan"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yue-gong"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dovan-rai"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/j-gobert"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-b-goldstein"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/matt-bachmann"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/orlando-montalvo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/adam-nakama"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mingyu-feng"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/carolina-ruiz"/>
	</foaf:Organization>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mladen-a-vouk">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/applied-research-associates,-inc,-raleigh"/>
		<rdfs:label>Mladen A. Vouk</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Mladen</foaf:firstName>
		<foaf:lastName>A. Vouk</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Mladen A. Vouk</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/andre-kruger">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/aroline-ag"/>
		<rdfs:label>Andre Kruger</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Andre</foaf:firstName>
		<foaf:lastName>Kruger</foaf:lastName>
		<foaf:mbox_sha1sum>a2c91d581588048330e687b7e559c7f726fdcf5e</foaf:mbox_sha1sum>
		<foaf:name>Andre Kruger</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/andre-kruger">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/aroline-ag"/>
		<rdfs:label>Andre Kruger</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Andre</foaf:firstName>
		<foaf:lastName>Kruger</foaf:lastName>
		<foaf:mbox_sha1sum>a2c91d581588048330e687b7e559c7f726fdcf5e</foaf:mbox_sha1sum>
		<foaf:name>Andre Kruger</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hema-soundranayagam">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/australian-catholic-university"/>
		<rdfs:label>Hema Soundranayagam</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Hema</foaf:firstName>
		<foaf:lastName>Soundranayagam</foaf:lastName>
		<foaf:mbox_sha1sum>635eae97cf4660325eea47038c3063cec32ff9e5</foaf:mbox_sha1sum>
		<foaf:name>Hema Soundranayagam</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/130"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/agathe-merceron">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/beuth-hochschule-fur-technik-berlin"/>
		<rdfs:label>Agathe Merceron</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Agathe</foaf:firstName>
		<foaf:lastName>Merceron</foaf:lastName>
		<foaf:mbox_sha1sum>8ee6764a7b05a55e48bddb0480c946579d970f51</foaf:mbox_sha1sum>
		<foaf:name>Agathe Merceron</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/agathe-merceron">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/beuth-hochschule-fur-technik-berlin"/>
		<rdfs:label>Agathe Merceron</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Agathe</foaf:firstName>
		<foaf:lastName>Merceron</foaf:lastName>
		<foaf:mbox_sha1sum>8ee6764a7b05a55e48bddb0480c946579d970f51</foaf:mbox_sha1sum>
		<foaf:name>Agathe Merceron</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/benjamin-wolf">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/beuth-university-of-applied-sciences"/>
		<rdfs:label>Benjamin Wolf</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Benjamin</foaf:firstName>
		<foaf:lastName>Wolf</foaf:lastName>
		<foaf:mbox_sha1sum>92d592e3ac4282627e87e37fd080b91fbf17bdb1</foaf:mbox_sha1sum>
		<foaf:name>Benjamin Wolf</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/benjamin-wolf">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/beuth-university-of-applied-sciences"/>
		<rdfs:label>Benjamin Wolf</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Benjamin</foaf:firstName>
		<foaf:lastName>Wolf</foaf:lastName>
		<foaf:mbox_sha1sum>92d592e3ac4282627e87e37fd080b91fbf17bdb1</foaf:mbox_sha1sum>
		<foaf:name>Benjamin Wolf</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/113"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/156"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/wayne-h-ward">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/boulder-language-technologies"/>
		<rdfs:label>Wayne H. Ward</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Wayne</foaf:firstName>
		<foaf:lastName>H. Ward</foaf:lastName>
		<foaf:mbox_sha1sum>cd002162acb6df14cfb4e89b0e3d1951b4cc4c2f</foaf:mbox_sha1sum>
		<foaf:name>Wayne H. Ward</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/171"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nayer-wanas">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/cairo-microsoft-innovation-lab"/>
		<rdfs:label>Nayer Wanas</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Egypt"/>
		<foaf:firstName>Nayer</foaf:firstName>
		<foaf:lastName>Wanas</foaf:lastName>
		<foaf:mbox_sha1sum>d2c21942df4fea697ce5718d03f1f93ee8298580</foaf:mbox_sha1sum>
		<foaf:name>Nayer Wanas</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/115"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nevin-darwish">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/cairo-university"/>
		<rdfs:label>Nevin Darwish</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Egypt"/>
		<foaf:firstName>Nevin</foaf:firstName>
		<foaf:lastName>Darwish</foaf:lastName>
		<foaf:mbox_sha1sum>78d0c25e161fbec2411997cc3e568d91b977861f</foaf:mbox_sha1sum>
		<foaf:name>Nevin Darwish</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/115"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jennifer-l-ferris">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Jennifer L. Ferris</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jennifer</foaf:firstName>
		<foaf:lastName>L. Ferris</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Jennifer L. Ferris</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/135"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Kenneth R. Koedinger</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Kenneth</foaf:firstName>
		<foaf:lastName>R. Koedinger</foaf:lastName>
		<foaf:mbox_sha1sum>0f1e313636d51d167f86b5b0daddf63f02ca3ca3</foaf:mbox_sha1sum>
		<foaf:name>Kenneth R. Koedinger</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/133"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/164"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jack-mostow">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Jack Mostow</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jack</foaf:firstName>
		<foaf:lastName>Mostow</foaf:lastName>
		<foaf:mbox_sha1sum>cfcda98863a9ad325506c095b9e5e5f3e019de5d</foaf:mbox_sha1sum>
		<foaf:name>Jack Mostow</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/140"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/167"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/john-c-stamper">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>John C. Stamper</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>John</foaf:firstName>
		<foaf:lastName>C. Stamper</foaf:lastName>
		<foaf:mbox_sha1sum>bfaeaa7192e9815d69eb5153d89e3962b4bcd7c5</foaf:mbox_sha1sum>
		<foaf:name>John C. Stamper</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/133"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/162"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/bao-hong-tan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Bao Hong Tan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Bao</foaf:firstName>
		<foaf:lastName>Hong Tan</foaf:lastName>
		<foaf:mbox_sha1sum>7fa88535cb496a3c0461e084ea0772a664c8bf2d</foaf:mbox_sha1sum>
		<foaf:name>Bao Hong Tan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/140"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/john-c-stamper">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>John C. Stamper</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>John</foaf:firstName>
		<foaf:lastName>C. Stamper</foaf:lastName>
		<foaf:mbox_sha1sum>bfaeaa7192e9815d69eb5153d89e3962b4bcd7c5</foaf:mbox_sha1sum>
		<foaf:name>John C. Stamper</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/133"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/162"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jon-m-fincham">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Jon M. Fincham</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jon</foaf:firstName>
		<foaf:lastName>M. Fincham</foaf:lastName>
		<foaf:mbox_sha1sum>72bf480e7c7516df60a160a6ae5305c59183f755</foaf:mbox_sha1sum>
		<foaf:name>Jon M. Fincham</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/135"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/turadg-aleahmad">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Turadg Aleahmad</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Turadg</foaf:firstName>
		<foaf:lastName>Aleahmad</foaf:lastName>
		<foaf:mbox_sha1sum>bba56bebe344fed2a1e7bf12007c9b0e020e2f71</foaf:mbox_sha1sum>
		<foaf:name>Turadg Aleahmad</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/158"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/benjamin-shih">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Benjamin Shih</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Benjamin</foaf:firstName>
		<foaf:lastName>Shih</foaf:lastName>
		<foaf:mbox_sha1sum>51a67fffd13772fbc5ff6f407b8e5ae51d90700a</foaf:mbox_sha1sum>
		<foaf:name>Benjamin Shih</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/164"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/michael-v-yudelson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Michael V. Yudelson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Michael</foaf:firstName>
		<foaf:lastName>V. Yudelson</foaf:lastName>
		<foaf:mbox_sha1sum>724e2d5688c3582fc40b081160422935169cdb3d</foaf:mbox_sha1sum>
		<foaf:name>Michael V. Yudelson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/118"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jose-p-gonzalez-brenes">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Jose P. Gonzalez-brenes</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jose</foaf:firstName>
		<foaf:lastName>P. Gonzalez-brenes</foaf:lastName>
		<foaf:mbox_sha1sum>a1f8215a0ce2f7728feacc856afd06a07170f719</foaf:mbox_sha1sum>
		<foaf:name>Jose P. Gonzalez-brenes</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/167"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/p-pavlik-jr">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>P. Pavlik Jr</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>P.</foaf:firstName>
		<foaf:lastName>Pavlik Jr</foaf:lastName>
		<foaf:mbox_sha1sum>85528a9cf2d288d3dae150bfbc031ae708989272</foaf:mbox_sha1sum>
		<foaf:name>P. Pavlik Jr</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/153"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/john-r-anderson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>John R. Anderson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>John</foaf:firstName>
		<foaf:lastName>R. Anderson</foaf:lastName>
		<foaf:mbox_sha1sum>c62c7ca2cb35e29eebed1daf5e576f381309521f</foaf:mbox_sha1sum>
		<foaf:name>John R. Anderson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/135"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/vincent-aleven">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Vincent Aleven</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Vincent</foaf:firstName>
		<foaf:lastName>Aleven</foaf:lastName>
		<foaf:mbox_sha1sum>694107a0b6904a685e35b22902a10a7376409e41</foaf:mbox_sha1sum>
		<foaf:name>Vincent Aleven</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/158"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kenneth-r-koedinger">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Kenneth R. Koedinger</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Kenneth</foaf:firstName>
		<foaf:lastName>R. Koedinger</foaf:lastName>
		<foaf:mbox_sha1sum>0f1e313636d51d167f86b5b0daddf63f02ca3ca3</foaf:mbox_sha1sum>
		<foaf:name>Kenneth R. Koedinger</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/133"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/164"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jack-mostow">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Jack Mostow</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jack</foaf:firstName>
		<foaf:lastName>Mostow</foaf:lastName>
		<foaf:mbox_sha1sum>cfcda98863a9ad325506c095b9e5e5f3e019de5d</foaf:mbox_sha1sum>
		<foaf:name>Jack Mostow</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/140"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/167"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/shawn-betts">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Shawn Betts</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Shawn</foaf:firstName>
		<foaf:lastName>Betts</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Shawn Betts</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/135"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/robert-kraut">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Robert Kraut</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Robert</foaf:firstName>
		<foaf:lastName>Kraut</foaf:lastName>
		<foaf:mbox_sha1sum>f95866af103eee3a125bf828f876da35dbce861f</foaf:mbox_sha1sum>
		<foaf:name>Robert Kraut</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/158"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rebecca-nugent">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Rebecca Nugent</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Rebecca</foaf:firstName>
		<foaf:lastName>Nugent</foaf:lastName>
		<foaf:mbox_sha1sum>8e3f2493492992c603e53cd69fa4b57e54c2c860</foaf:mbox_sha1sum>
		<foaf:name>Rebecca Nugent</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/152"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/richard-scheines">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/carnegie-mellon-university"/>
		<rdfs:label>Richard Scheines</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Richard</foaf:firstName>
		<foaf:lastName>Scheines</foaf:lastName>
		<foaf:mbox_sha1sum>b5357d50a33f46391728d9d3a3c5f3a3e0a90760</foaf:mbox_sha1sum>
		<foaf:name>Richard Scheines</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/164"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sherry-hsi">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/center-for-technology-innovation"/>
		<rdfs:label>Sherry Hsi</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Sherry</foaf:firstName>
		<foaf:lastName>Hsi</foaf:lastName>
		<foaf:mbox_sha1sum>82e4d424f134cfdc704e9955069d1e5bddbb5980</foaf:mbox_sha1sum>
		<foaf:name>Sherry Hsi</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/diane-halpern">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/claremont-graduate-university"/>
		<rdfs:label>Diane Halpern</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Diane</foaf:firstName>
		<foaf:lastName>Halpern</foaf:lastName>
		<foaf:mbox_sha1sum>1294232ac57f92ba70bd7c23455bc8aa74626161</foaf:mbox_sha1sum>
		<foaf:name>Diane Halpern</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/heather-butler">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/claremont-graduate-university"/>
		<rdfs:label>Heather Butler</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Heather</foaf:firstName>
		<foaf:lastName>Butler</foaf:lastName>
		<foaf:mbox_sha1sum>e127637578bf9815b43aa06009f24de9e93cd268</foaf:mbox_sha1sum>
		<foaf:name>Heather Butler</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/amar-balla">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/ecole-national-superieure-d-informatique"/>
		<rdfs:label>Amar Balla</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Algeria"/>
		<foaf:firstName>Amar</foaf:firstName>
		<foaf:lastName>Balla</foaf:lastName>
		<foaf:mbox_sha1sum>f6524961fcb43d8ab5ac17c5ced3d7be6e1dc19b</foaf:mbox_sha1sum>
		<foaf:name>Amar Balla</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/114"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/michel-c-desmarais">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/ecole-polytechnique-de-montreal"/>
		<rdfs:label>Michel C. Desmarais</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Michel</foaf:firstName>
		<foaf:lastName>C. Desmarais</foaf:lastName>
		<foaf:mbox_sha1sum>a706c2f86e96db418a4437eea2da031a806698fc</foaf:mbox_sha1sum>
		<foaf:name>Michel C. Desmarais</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/163"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ankit-ranka">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/edlab"/>
		<rdfs:label>Ankit Ranka</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ankit</foaf:firstName>
		<foaf:lastName>Ranka</foaf:lastName>
		<foaf:mbox_sha1sum>3b9e3ea59033bbcbe66487da56ec14b256f87112</foaf:mbox_sha1sum>
		<foaf:name>Ankit Ranka</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/119"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/faisal-anwar">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/edlab"/>
		<rdfs:label>Faisal Anwar</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Faisal</foaf:firstName>
		<foaf:lastName>Anwar</foaf:lastName>
		<foaf:mbox_sha1sum>af81a7c357b3e49ba482e3f7abb8ae0c5882d576</foaf:mbox_sha1sum>
		<foaf:name>Faisal Anwar</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/119"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hui-soo-chae">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/edlab"/>
		<rdfs:label>Hui Soo Chae</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Hui</foaf:firstName>
		<foaf:lastName>Soo Chae</foaf:lastName>
		<foaf:mbox_sha1sum>38e1384ec3215e900831141e9f7d3742f8f2fb2e</foaf:mbox_sha1sum>
		<foaf:name>Hui Soo Chae</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/119"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/anupriya-gupta">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/educational-initiatives-pvt-ltd"/>
		<rdfs:label>Anupriya Gupta</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/India"/>
		<foaf:firstName>Anupriya</foaf:firstName>
		<foaf:lastName>Gupta</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Anupriya Gupta</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/155"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/suchismita-srinivas">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/educational-initiatives-pvt-ltd"/>
		<rdfs:label>Suchismita Srinivas</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/India"/>
		<foaf:firstName>Suchismita</foaf:firstName>
		<foaf:lastName>Srinivas</foaf:lastName>
		<foaf:mbox_sha1sum>0a3e4efb5bd7c420a90f6c25f5a5bbe9f6d8f95b</foaf:mbox_sha1sum>
		<foaf:name>Suchismita Srinivas</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/155"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/muntaquim-bagadia">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/educational-initiatives-pvt-ltd"/>
		<rdfs:label>Muntaquim Bagadia</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/India"/>
		<foaf:firstName>Muntaquim</foaf:firstName>
		<foaf:lastName>Bagadia</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Muntaquim Bagadia</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/155"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/evgeny-knutov">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Evgeny Knutov</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Evgeny</foaf:firstName>
		<foaf:lastName>Knutov</foaf:lastName>
		<foaf:mbox_sha1sum>609536f66d859c69b636745568afcc271df0255b</foaf:mbox_sha1sum>
		<foaf:name>Evgeny Knutov</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sicco-verwer">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Sicco Verwer</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Sicco</foaf:firstName>
		<foaf:lastName>Verwer</foaf:lastName>
		<foaf:mbox_sha1sum>a00fc9ff9519901ee6f5e246ac22efbe29ba891e</foaf:mbox_sha1sum>
		<foaf:name>Sicco Verwer</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Ekaterina Vasilyeva</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Ekaterina</foaf:firstName>
		<foaf:lastName>Vasilyeva</foaf:lastName>
		<foaf:mbox_sha1sum>836e3a34e8066b0b9c2708abc07e75eab24445a8</foaf:mbox_sha1sum>
		<foaf:name>Ekaterina Vasilyeva</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/p-de-bra">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>P. De Bra</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>P.</foaf:firstName>
		<foaf:lastName>De Bra</foaf:lastName>
		<foaf:mbox_sha1sum>5e6e0140af8fc08b5b9df45f327ebf20cf224f18</foaf:mbox_sha1sum>
		<foaf:name>P. De Bra</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ekaterina-vasilyeva">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Ekaterina Vasilyeva</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Ekaterina</foaf:firstName>
		<foaf:lastName>Vasilyeva</foaf:lastName>
		<foaf:mbox_sha1sum>836e3a34e8066b0b9c2708abc07e75eab24445a8</foaf:mbox_sha1sum>
		<foaf:name>Ekaterina Vasilyeva</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Mykola Pechenizkiy</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Mykola</foaf:firstName>
		<foaf:lastName>Pechenizkiy</foaf:lastName>
		<foaf:mbox_sha1sum>92ec847241e20acbe9bd8be3fda6d568619228d8</foaf:mbox_sha1sum>
		<foaf:name>Mykola Pechenizkiy</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mykola-pechenizkiy">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/eindhoven-university-of-technology"/>
		<rdfs:label>Mykola Pechenizkiy</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Mykola</foaf:firstName>
		<foaf:lastName>Pechenizkiy</foaf:lastName>
		<foaf:mbox_sha1sum>92ec847241e20acbe9bd8be3fda6d568619228d8</foaf:mbox_sha1sum>
		<foaf:name>Mykola Pechenizkiy</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nadia-hegazy">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/electronics-research-institute"/>
		<rdfs:label>Nadia Hegazy</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Egypt"/>
		<foaf:firstName>Nadia</foaf:firstName>
		<foaf:lastName>Hegazy</foaf:lastName>
		<foaf:mbox_sha1sum>67ae72f04b090327b48937192d9c590e57399bfc</foaf:mbox_sha1sum>
		<foaf:name>Nadia Hegazy</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/115"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nabila-khodeir">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/electronics-research-institute"/>
		<rdfs:label>Nabila Khodeir</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Egypt"/>
		<foaf:firstName>Nabila</foaf:firstName>
		<foaf:lastName>Khodeir</foaf:lastName>
		<foaf:mbox_sha1sum>e7d02429ee49e0fa21a37d62db0079395c38e96a</foaf:mbox_sha1sum>
		<foaf:name>Nabila Khodeir</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/115"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tamara-sumner">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/institute-of-cognitive-science"/>
		<rdfs:label>Tamara Sumner</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Tamara</foaf:firstName>
		<foaf:lastName>Sumner</foaf:lastName>
		<foaf:mbox_sha1sum>fdccbaaeb06da284065f35bf5eecabcb860d5ef4</foaf:mbox_sha1sum>
		<foaf:name>Tamara Sumner</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tamara-sumner">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/institute-of-cognitive-science"/>
		<rdfs:label>Tamara Sumner</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Tamara</foaf:firstName>
		<foaf:lastName>Sumner</foaf:lastName>
		<foaf:mbox_sha1sum>fdccbaaeb06da284065f35bf5eecabcb860d5ef4</foaf:mbox_sha1sum>
		<foaf:name>Tamara Sumner</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nabila-bousbia">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/laboratoire-de-paris-6"/>
		<rdfs:label>Nabila Bousbia</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/France"/>
		<foaf:firstName>Nabila</foaf:firstName>
		<foaf:lastName>Bousbia</foaf:lastName>
		<foaf:mbox_sha1sum>f98421604c509f0b95ff24a29ad18a6a82ca4c2a</foaf:mbox_sha1sum>
		<foaf:name>Nabila Bousbia</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/114"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jean-marc-labat">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/laboratoire-de-paris-6"/>
		<rdfs:label>Jean-marc Labat</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/France"/>
		<foaf:firstName>Jean-marc</foaf:firstName>
		<foaf:lastName>Labat</foaf:lastName>
		<foaf:mbox_sha1sum>05e6819ec68ecaa7737ef732be51203ba9374cb1</foaf:mbox_sha1sum>
		<foaf:name>Jean-marc Labat</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/114"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/haiyun-bian">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/metropolitan-state-college-of-denver"/>
		<rdfs:label>Haiyun Bian</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Haiyun</foaf:firstName>
		<foaf:lastName>Bian</foaf:lastName>
		<foaf:mbox_sha1sum>27b31f59d031890e52122a09265f92c39c82895f</foaf:mbox_sha1sum>
		<foaf:name>Haiyun Bian</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/159"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kristy-elizabeth-boyer">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/north-carolina-state-university"/>
		<rdfs:label>Kristy Elizabeth Boyer</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Kristy</foaf:firstName>
		<foaf:lastName>Elizabeth Boyer</foaf:lastName>
		<foaf:mbox_sha1sum>a243459903fdc6c0775ca784ba11451bb1d7d328</foaf:mbox_sha1sum>
		<foaf:name>Kristy Elizabeth Boyer</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/james-c-lester">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/north-carolina-state-university"/>
		<rdfs:label>James C. Lester</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>James</foaf:firstName>
		<foaf:lastName>C. Lester</foaf:lastName>
		<foaf:mbox_sha1sum>4f9ea8082ae93f21ce163b5c872adf7da67bcac4</foaf:mbox_sha1sum>
		<foaf:name>James C. Lester</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/robert-phillips">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/north-carolina-state-university"/>
		<rdfs:label>Robert Phillips</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Robert</foaf:firstName>
		<foaf:lastName>Phillips</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Robert Phillips</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/eun-young-ha">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/north-carolina-state-university"/>
		<rdfs:label>Eun Young Ha</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Eun</foaf:firstName>
		<foaf:lastName>Young Ha</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Eun Young Ha</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/michael-d-wallis">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/north-carolina-state-university"/>
		<rdfs:label>Michael D. Wallis</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Michael</foaf:firstName>
		<foaf:lastName>D. Wallis</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Michael D. Wallis</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/120"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/keith-millis">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/northern-illinois-university"/>
		<rdfs:label>Keith Millis</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Keith</foaf:firstName>
		<foaf:lastName>Millis</foaf:lastName>
		<foaf:mbox_sha1sum>2fe741898c3d64d651a8459dca19be2994ccafa6</foaf:mbox_sha1sum>
		<foaf:name>Keith Millis</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/aleksandra-tesanovic">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/philips-research-laboratories"/>
		<rdfs:label>Aleksandra Tesanovic</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Aleksandra</foaf:firstName>
		<foaf:lastName>Tesanovic</foaf:lastName>
		<foaf:mbox_sha1sum>79f30ec07341ef5eb76251d26727eb4f78b83385</foaf:mbox_sha1sum>
		<foaf:name>Aleksandra Tesanovic</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/121"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ildiko-pelczer">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/polytechnique-montreal"/>
		<rdfs:label>Ildiko Pelczer</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Ildiko</foaf:firstName>
		<foaf:lastName>Pelczer</foaf:lastName>
		<foaf:mbox_sha1sum>3c4ea9b01c40949386d3c0a2ba9d3e7885b13e00</foaf:mbox_sha1sum>
		<foaf:name>Ildiko Pelczer</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/163"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/peter-sorenson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/quizzicle"/>
		<rdfs:label>Peter Sorenson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Peter</foaf:firstName>
		<foaf:lastName>Sorenson</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Peter Sorenson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/176"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/manuel-gerardo-saldivar">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/school-of-education"/>
		<rdfs:label>Manuel Gerardo Saldivar</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Manuel</foaf:firstName>
		<foaf:lastName>Gerardo Saldivar</foaf:lastName>
		<foaf:mbox_sha1sum>6d50e04064e13a105c869e36234394318605c1bf</foaf:mbox_sha1sum>
		<foaf:name>Manuel Gerardo Saldivar</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/manuel-gerardo-saldivar">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/school-of-education"/>
		<rdfs:label>Manuel Gerardo Saldivar</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Manuel</foaf:firstName>
		<foaf:lastName>Gerardo Saldivar</foaf:lastName>
		<foaf:mbox_sha1sum>6d50e04064e13a105c869e36234394318605c1bf</foaf:mbox_sha1sum>
		<foaf:name>Manuel Gerardo Saldivar</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/james-a-bernauer">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/sess-robert-morris-university"/>
		<rdfs:label>James A. Bernauer</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>James</foaf:firstName>
		<foaf:lastName>A. Bernauer</foaf:lastName>
		<foaf:mbox_sha1sum>5c8de8d4fe78bbf2804c8de0c4eb2758c43d0f5b</foaf:mbox_sha1sum>
		<foaf:name>James A. Bernauer</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/142"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jay-c-powell">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/sess-robert-morris-university"/>
		<rdfs:label>Jay C. Powell</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Jay</foaf:firstName>
		<foaf:lastName>C. Powell</foaf:lastName>
		<foaf:mbox_sha1sum>107156bbb5c9813fd47706237da1712b499e3561</foaf:mbox_sha1sum>
		<foaf:name>Jay C. Powell</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/142"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mohammad-hassan-falakmasir">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/sharif-university-of-technology"/>
		<rdfs:label>Mohammad Hassan Falakmasir</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Mohammad</foaf:firstName>
		<foaf:lastName>Hassan Falakmasir</foaf:lastName>
		<foaf:mbox_sha1sum>8a7bf035f74285f904fca13e360b4bd4a44a0269</foaf:mbox_sha1sum>
		<foaf:name>Mohammad Hassan Falakmasir</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/129"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jafar-habibi">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/sharif-university-of-technology"/>
		<rdfs:label>Jafar Habibi</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Jafar</foaf:firstName>
		<foaf:lastName>Habibi</foaf:lastName>
		<foaf:mbox_sha1sum>d757289a319cf934703ce8a251083120d82d5b0b</foaf:mbox_sha1sum>
		<foaf:name>Jafar Habibi</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/129"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/m-feng">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/sri-international"/>
		<rdfs:label>M. Feng</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>M.</foaf:firstName>
		<foaf:lastName>Feng</foaf:lastName>
		<foaf:mbox_sha1sum>32a8a25e23d8861213f01ff0c74b19baf5cd802a</foaf:mbox_sha1sum>
		<foaf:name>M. Feng</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jaan-ubi">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tallinn-university-of-technology"/>
		<rdfs:label>Jaan Ubi</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Estonia"/>
		<foaf:firstName>Jaan</foaf:firstName>
		<foaf:lastName>Ubi</foaf:lastName>
		<foaf:mbox_sha1sum>aebd7a9c110cb5ea16aac19edf4b66b01e355524</foaf:mbox_sha1sum>
		<foaf:name>Jaan Ubi</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/137"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/innar-liiv">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tallinn-university-of-technology"/>
		<rdfs:label>Innar Liiv</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Estonia"/>
		<foaf:firstName>Innar</foaf:firstName>
		<foaf:lastName>Liiv</foaf:lastName>
		<foaf:mbox_sha1sum>b11b2ac7e7aeb0db204167fc222958d2cf3c8628</foaf:mbox_sha1sum>
		<foaf:name>Innar Liiv</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/137"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sharon-hardof-jaffe">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tel-aviv-university"/>
		<rdfs:label>Sharon Hardof-jaffe</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Israel"/>
		<foaf:firstName>Sharon</foaf:firstName>
		<foaf:lastName>Hardof-jaffe</foaf:lastName>
		<foaf:mbox_sha1sum>28023b0d370b7100ddc364f74ec8c1dc03d748f1</foaf:mbox_sha1sum>
		<foaf:name>Sharon Hardof-jaffe</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ronit-azran">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tel-aviv-university"/>
		<rdfs:label>Ronit Azran</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Israel"/>
		<foaf:firstName>Ronit</foaf:firstName>
		<foaf:lastName>Azran</foaf:lastName>
		<foaf:mbox_sha1sum>a2b0ff325676cbc8d8d77d1fe78df57b6204b751</foaf:mbox_sha1sum>
		<foaf:name>Ronit Azran</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rafi-nachmias">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tel-aviv-university"/>
		<rdfs:label>Rafi Nachmias</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Israel"/>
		<foaf:firstName>Rafi</foaf:firstName>
		<foaf:lastName>Nachmias</foaf:lastName>
		<foaf:mbox_sha1sum>cea4310497b4026a3128713418a30f3d559cc461</foaf:mbox_sha1sum>
		<foaf:name>Rafi Nachmias</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/125"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rafi-nachmias">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/tel-aviv-university"/>
		<rdfs:label>Rafi Nachmias</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Israel"/>
		<foaf:firstName>Rafi</foaf:firstName>
		<foaf:lastName>Nachmias</foaf:lastName>
		<foaf:mbox_sha1sum>cea4310497b4026a3128713418a30f3d559cc461</foaf:mbox_sha1sum>
		<foaf:name>Rafi Nachmias</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/125"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/alvaro-ortigosa">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidad-autonoma-de-madrid"/>
		<rdfs:label>Alvaro Ortigosa</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Alvaro</foaf:firstName>
		<foaf:lastName>Ortigosa</foaf:lastName>
		<foaf:mbox_sha1sum>0c34f239fd95d91588d1ee7abae8ec4b50a9a802</foaf:mbox_sha1sum>
		<foaf:name>Alvaro Ortigosa</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/cesar-vialardi">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidad-de-lima"/>
		<rdfs:label>Cesar Vialardi</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Cesar</foaf:firstName>
		<foaf:lastName>Vialardi</foaf:lastName>
		<foaf:mbox_sha1sum>b2f31cab75c97ca4e6675279e9a13f4b1c68e88f</foaf:mbox_sha1sum>
		<foaf:name>Cesar Vialardi</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/juan-pablo-peche">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidad-de-lima"/>
		<rdfs:label>Juan Pablo Peche</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Juan</foaf:firstName>
		<foaf:lastName>Pablo Peche</foaf:lastName>
		<foaf:mbox_sha1sum>6d5fe10e8e7e35ae7d7dc2714b49eeecd42b0e27</foaf:mbox_sha1sum>
		<foaf:name>Juan Pablo Peche</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jorge-chue">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidad-de-lima"/>
		<rdfs:label>Jorge Chue</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Jorge</foaf:firstName>
		<foaf:lastName>Chue</foaf:lastName>
		<foaf:mbox_sha1sum>44bc12867bdff029d509531c2e5f59a7809b9ef7</foaf:mbox_sha1sum>
		<foaf:name>Jorge Chue</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/alfredo-barrientos">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidad-de-lima"/>
		<rdfs:label>Alfredo Barrientos</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Alfredo</foaf:firstName>
		<foaf:lastName>Barrientos</foaf:lastName>
		<foaf:mbox_sha1sum>133f8f3055f9e15dae061b5aab549dbe00c63694</foaf:mbox_sha1sum>
		<foaf:name>Alfredo Barrientos</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/daniel-victoria">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidad-de-lima"/>
		<rdfs:label>Daniel Victoria</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Daniel</foaf:firstName>
		<foaf:lastName>Victoria</foaf:lastName>
		<foaf:mbox_sha1sum>dd73bd154c943a7176bdb229c40f8da842e23a3d</foaf:mbox_sha1sum>
		<foaf:name>Daniel Victoria</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jhonny-estrella">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universidad-de-lima"/>
		<rdfs:label>Jhonny Estrella</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Jhonny</foaf:firstName>
		<foaf:lastName>Estrella</foaf:lastName>
		<foaf:mbox_sha1sum>42470c6f4cf65bb08632d8f9fc23011b386cd992</foaf:mbox_sha1sum>
		<foaf:name>Jhonny Estrella</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/166"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/issam-rebai">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/universite-de-paris-sud"/>
		<rdfs:label>Issam Rebai</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/France"/>
		<foaf:firstName>Issam</foaf:firstName>
		<foaf:lastName>Rebai</foaf:lastName>
		<foaf:mbox_sha1sum>781059e4aa833a42931041ebd514708c46ed8bee</foaf:mbox_sha1sum>
		<foaf:name>Issam Rebai</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/114"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-british-columbia"/>
		<rdfs:label>Leah P. Macfadyen</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Leah</foaf:firstName>
		<foaf:lastName>P. Macfadyen</foaf:lastName>
		<foaf:mbox_sha1sum>63662c7bcb45a03680406a2b156674c63dc52d20</foaf:mbox_sha1sum>
		<foaf:name>Leah P. Macfadyen</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/176"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/elizabeth-ayers">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-california"/>
		<rdfs:label>Elizabeth Ayers</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Elizabeth</foaf:firstName>
		<foaf:lastName>Ayers</foaf:lastName>
		<foaf:mbox_sha1sum>aae9cb2462a54bbcff7dc90d448896423efc64ab</foaf:mbox_sha1sum>
		<foaf:name>Elizabeth Ayers</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/152"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/moffat-mathews">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-canterbury"/>
		<rdfs:label>Moffat Mathews</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Moffat</foaf:firstName>
		<foaf:lastName>Mathews</foaf:lastName>
		<foaf:mbox_sha1sum>c24fded4b25b25920d7d180c0e4ff513a80d7c3a</foaf:mbox_sha1sum>
		<foaf:name>Moffat Mathews</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/118"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/antonija-mitrovic">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-canterbury"/>
		<rdfs:label>Antonija Mitrovic</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Antonija</foaf:firstName>
		<foaf:lastName>Mitrovic</foaf:lastName>
		<foaf:mbox_sha1sum>1a2bb700c501d6389fd15af263593c11c9144f1f</foaf:mbox_sha1sum>
		<foaf:name>Antonija Mitrovic</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/118"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/keith-e-maull">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-colorado"/>
		<rdfs:label>Keith E. Maull</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Keith</foaf:firstName>
		<foaf:lastName>E. Maull</foaf:lastName>
		<foaf:mbox_sha1sum>aabbc41bbb3fa4ecf63def883506fcc83e9aba69</foaf:mbox_sha1sum>
		<foaf:name>Keith E. Maull</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/keith-e-maull">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-colorado"/>
		<rdfs:label>Keith E. Maull</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Keith</foaf:firstName>
		<foaf:lastName>E. Maull</foaf:lastName>
		<foaf:mbox_sha1sum>aabbc41bbb3fa4ecf63def883506fcc83e9aba69</foaf:mbox_sha1sum>
		<foaf:name>Keith E. Maull</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/132"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/170"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sarel-vanvuuren">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-colorado-at-boulder"/>
		<rdfs:label>Sarel Vanvuuren</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Sarel</foaf:firstName>
		<foaf:lastName>Vanvuuren</foaf:lastName>
		<foaf:mbox_sha1sum>18aa3540b15ef6193fd242b6c0768edbfff558e2</foaf:mbox_sha1sum>
		<foaf:name>Sarel Vanvuuren</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/171"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/lee-becker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-colorado-at-boulder"/>
		<rdfs:label>Lee Becker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Lee</foaf:firstName>
		<foaf:lastName>Becker</foaf:lastName>
		<foaf:mbox_sha1sum>c4614881896d3006155bba51e755a5420edd8b33</foaf:mbox_sha1sum>
		<foaf:name>Lee Becker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/171"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/c-romero">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>C. Romero</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>C.</foaf:firstName>
		<foaf:lastName>Romero</foaf:lastName>
		<foaf:mbox_sha1sum>4fa368945fa5e56c76943d4dc36bc95f3b25899f</foaf:mbox_sha1sum>
		<foaf:name>C. Romero</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jm-luna">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>J.m Luna</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>J.m</foaf:firstName>
		<foaf:lastName>Luna</foaf:lastName>
		<foaf:mbox_sha1sum>cca8bd2c797fd5edf641d90dceacfe904c02ba1f</foaf:mbox_sha1sum>
		<foaf:name>J.m Luna</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/s-ventura">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>S. Ventura</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>S.</foaf:firstName>
		<foaf:lastName>Ventura</foaf:lastName>
		<foaf:mbox_sha1sum>7cc996020e6d4f219b9c38c3863805c7682d4571</foaf:mbox_sha1sum>
		<foaf:name>S. Ventura</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/s-ventura">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>S. Ventura</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>S.</foaf:firstName>
		<foaf:lastName>Ventura</foaf:lastName>
		<foaf:mbox_sha1sum>7cc996020e6d4f219b9c38c3863805c7682d4571</foaf:mbox_sha1sum>
		<foaf:name>S. Ventura</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/c-romero">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>C. Romero</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>C.</foaf:firstName>
		<foaf:lastName>Romero</foaf:lastName>
		<foaf:mbox_sha1sum>4fa368945fa5e56c76943d4dc36bc95f3b25899f</foaf:mbox_sha1sum>
		<foaf:name>C. Romero</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/169"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jose-raul-romero">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-cordoba"/>
		<rdfs:label>Jose Raul Romero</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Spain"/>
		<foaf:firstName>Jose</foaf:firstName>
		<foaf:lastName>Raul Romero</foaf:lastName>
		<foaf:mbox_sha1sum>a2189f76943c4024f51fae52c70f5fe1c9d9e782</foaf:mbox_sha1sum>
		<foaf:name>Jose Raul Romero</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/122"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/maomi-ueno">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-electro-communications"/>
		<rdfs:label>Maomi Ueno</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Maomi</foaf:firstName>
		<foaf:lastName>Ueno</foaf:lastName>
		<foaf:mbox_sha1sum>d1bb9903fd2fd65f724d3e3b304c6cb88309d636</foaf:mbox_sha1sum>
		<foaf:name>Maomi Ueno</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/147"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/pokpong-songmuang">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-electro-communications"/>
		<rdfs:label>Pokpong Songmuang</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Pokpong</foaf:firstName>
		<foaf:lastName>Songmuang</foaf:lastName>
		<foaf:mbox_sha1sum>878fed45087a77cc26f6d1b4561c083acaaf4fdb</foaf:mbox_sha1sum>
		<foaf:name>Pokpong Songmuang</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/147"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nema-dean">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-glasgow"/>
		<rdfs:label>Nema Dean</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Nema</foaf:firstName>
		<foaf:lastName>Dean</foaf:lastName>
		<foaf:mbox_sha1sum>5847deff91aa9447d6418367c31eb9fdc80cc552</foaf:mbox_sha1sum>
		<foaf:name>Nema Dean</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/152"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jana-krivec">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-ljubljana"/>
		<rdfs:label>Jana Krivec</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Slovenia"/>
		<foaf:firstName>Jana</foaf:firstName>
		<foaf:lastName>Krivec</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Jana Krivec</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/martin-mozina">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-ljubljana"/>
		<rdfs:label>Martin Mozina</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Slovenia"/>
		<foaf:firstName>Martin</foaf:firstName>
		<foaf:lastName>Mozina</foaf:lastName>
		<foaf:mbox_sha1sum>13cfa75f60e513d770be8493bdf599c7bd06e6eb</foaf:mbox_sha1sum>
		<foaf:name>Martin Mozina</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ivan-bratko">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-ljubljana"/>
		<rdfs:label>Ivan Bratko</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Slovenia"/>
		<foaf:firstName>Ivan</foaf:firstName>
		<foaf:lastName>Bratko</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Ivan Bratko</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/matej-guid">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-ljubljana"/>
		<rdfs:label>Matej Guid</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Slovenia"/>
		<foaf:firstName>Matej</foaf:firstName>
		<foaf:lastName>Guid</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Matej Guid</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/aleksander-sadikov">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-ljubljana"/>
		<rdfs:label>Aleksander Sadikov</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Slovenia"/>
		<foaf:firstName>Aleksander</foaf:firstName>
		<foaf:lastName>Sadikov</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Aleksander Sadikov</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/vida-groznik">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-ljubljana"/>
		<rdfs:label>Vida Groznik</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Slovenia"/>
		<foaf:firstName>Vida</foaf:firstName>
		<foaf:lastName>Groznik</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Vida Groznik</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/172"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/andre-a-rupp">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-maryland"/>
		<rdfs:label>Andre A. Rupp</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Andre</foaf:firstName>
		<foaf:lastName>A. Rupp</foaf:lastName>
		<foaf:mbox_sha1sum>3fef4c4478a50acfdbdb7796f52064b140ba215d</foaf:mbox_sha1sum>
		<foaf:name>Andre A. Rupp</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/160"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/shauna-j-sweet">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-maryland"/>
		<rdfs:label>Shauna J. Sweet</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Shauna</foaf:firstName>
		<foaf:lastName>J. Sweet</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Shauna J. Sweet</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/160"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/younyoung-choi">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-maryland"/>
		<rdfs:label>Younyoung Choi</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Younyoung</foaf:firstName>
		<foaf:lastName>Choi</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Younyoung Choi</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/160"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/david-g-cooper">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-massachusetts"/>
		<rdfs:label>David G. Cooper</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>David</foaf:firstName>
		<foaf:lastName>G. Cooper</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>David G. Cooper</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ivon-arroyo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-massachusetts"/>
		<rdfs:label>Ivon Arroyo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ivon</foaf:firstName>
		<foaf:lastName>Arroyo</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Ivon Arroyo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/146"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/beverly-park-woolf">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-massachusetts"/>
		<rdfs:label>Beverly Park Woolf</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Beverly</foaf:firstName>
		<foaf:lastName>Park Woolf</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Beverly Park Woolf</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/146"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hasmik-mehranian">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-massachusetts"/>
		<rdfs:label>Hasmik Mehranian</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Hasmik</foaf:firstName>
		<foaf:lastName>Mehranian</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Hasmik Mehranian</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/146"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ivon-arroyo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-massachusetts"/>
		<rdfs:label>Ivon Arroyo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ivon</foaf:firstName>
		<foaf:lastName>Arroyo</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Ivon Arroyo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/146"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/beverly-park-woolf">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-massachusetts"/>
		<rdfs:label>Beverly Park Woolf</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Beverly</foaf:firstName>
		<foaf:lastName>Park Woolf</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Beverly Park Woolf</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/146"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/david-h-shanabrook">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-massachusetts"/>
		<rdfs:label>David H. Shanabrook</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>David</foaf:firstName>
		<foaf:lastName>H. Shanabrook</foaf:lastName>
		<foaf:mbox_sha1sum>1c5239d8fc13549d3e025f0fcddd8b7817f7e91e</foaf:mbox_sha1sum>
		<foaf:name>David H. Shanabrook</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/154"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/whitney-cade">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Whitney Cade</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Whitney</foaf:firstName>
		<foaf:lastName>Cade</foaf:lastName>
		<foaf:mbox_sha1sum>ec9662ee2d2e6766f52ef3ccb35bab7ffcab65f0</foaf:mbox_sha1sum>
		<foaf:name>Whitney Cade</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/151"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/157"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Arthur C. Graesser</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Arthur</foaf:firstName>
		<foaf:lastName>C. Graesser</foaf:lastName>
		<foaf:mbox_sha1sum>875f275886f339a746b596e3c7fc9d5b56380ee6</foaf:mbox_sha1sum>
		<foaf:name>Arthur C. Graesser</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/123"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/andrew-olney">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Andrew Olney</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Andrew</foaf:firstName>
		<foaf:lastName>Olney</foaf:lastName>
		<foaf:mbox_sha1sum>96ccac875eabfb93ae5e226c0274b4b97d8fdc3d</foaf:mbox_sha1sum>
		<foaf:name>Andrew Olney</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/151"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/157"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/blair-lehman">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Blair Lehman</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Blair</foaf:firstName>
		<foaf:lastName>Lehman</foaf:lastName>
		<foaf:mbox_sha1sum>db849dfbb437a40ffba306908b0915c643f90e7d</foaf:mbox_sha1sum>
		<foaf:name>Blair Lehman</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/151"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/whitney-cade">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Whitney Cade</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Whitney</foaf:firstName>
		<foaf:lastName>Cade</foaf:lastName>
		<foaf:mbox_sha1sum>ec9662ee2d2e6766f52ef3ccb35bab7ffcab65f0</foaf:mbox_sha1sum>
		<foaf:name>Whitney Cade</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/151"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/157"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sidney-d-mello">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Sidney D'mello</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Sidney</foaf:firstName>
		<foaf:lastName>D'mello</foaf:lastName>
		<foaf:mbox_sha1sum>7ffc8c7ad38a7a4266d77822640091a1b466d2ca</foaf:mbox_sha1sum>
		<foaf:name>Sidney D'mello</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/123"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carol-forsyth">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Carol Forsyth</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Carol</foaf:firstName>
		<foaf:lastName>Forsyth</foaf:lastName>
		<foaf:mbox_sha1sum>f5fb31f87b3641c24d565678d7fac0fc4f7dfdff</foaf:mbox_sha1sum>
		<foaf:name>Carol Forsyth</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/zhiqiang-cai">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Zhiqiang Cai</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Zhiqiang</foaf:firstName>
		<foaf:lastName>Cai</foaf:lastName>
		<foaf:mbox_sha1sum>13ca59177c72a2fb862739cba2893481ecea32d8</foaf:mbox_sha1sum>
		<foaf:name>Zhiqiang Cai</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/andrew-olney">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Andrew Olney</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Andrew</foaf:firstName>
		<foaf:lastName>Olney</foaf:lastName>
		<foaf:mbox_sha1sum>96ccac875eabfb93ae5e226c0274b4b97d8fdc3d</foaf:mbox_sha1sum>
		<foaf:name>Andrew Olney</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/151"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/157"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/arthur-c-graesser">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Arthur C. Graesser</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Arthur</foaf:firstName>
		<foaf:lastName>C. Graesser</foaf:lastName>
		<foaf:mbox_sha1sum>875f275886f339a746b596e3c7fc9d5b56380ee6</foaf:mbox_sha1sum>
		<foaf:name>Arthur C. Graesser</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/123"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jonathan-wood">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-memphis"/>
		<rdfs:label>Jonathan Wood</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jonathan</foaf:firstName>
		<foaf:lastName>Wood</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Jonathan Wood</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/173"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/matthew-w-johnson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-north-carolina-at-charlotte"/>
		<rdfs:label>Matthew W. Johnson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Matthew</foaf:firstName>
		<foaf:lastName>W. Johnson</foaf:lastName>
		<foaf:mbox_sha1sum>2722345a0e1bf3d4d680c6f4e23cd91e4794d003</foaf:mbox_sha1sum>
		<foaf:name>Matthew W. Johnson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/175"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tiffany-barnes">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-north-carolina-at-charlotte"/>
		<rdfs:label>Tiffany Barnes</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Tiffany</foaf:firstName>
		<foaf:lastName>Barnes</foaf:lastName>
		<foaf:mbox_sha1sum>daf05503be29d62c7a709507c606228daa499645</foaf:mbox_sha1sum>
		<foaf:name>Tiffany Barnes</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/162"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/175"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/tiffany-barnes">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-north-carolina-at-charlotte"/>
		<rdfs:label>Tiffany Barnes</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Tiffany</foaf:firstName>
		<foaf:lastName>Barnes</foaf:lastName>
		<foaf:mbox_sha1sum>daf05503be29d62c7a709507c606228daa499645</foaf:mbox_sha1sum>
		<foaf:name>Tiffany Barnes</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/162"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/175"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/m-croy">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-north-carolina-at-charlotte"/>
		<rdfs:label>M. Croy</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>M.</foaf:firstName>
		<foaf:lastName>Croy</foaf:lastName>
		<foaf:mbox_sha1sum>b12851595d518e2a95690fbb8d187fc0657109d4</foaf:mbox_sha1sum>
		<foaf:name>M. Croy</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/162"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/christian-schunn">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-pittsburgh"/>
		<rdfs:label>Christian Schunn</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Christian</foaf:firstName>
		<foaf:lastName>Schunn</foaf:lastName>
		<foaf:mbox_sha1sum>537570ced370410e3f094b9dc6f4b0d970e6e3b9</foaf:mbox_sha1sum>
		<foaf:name>Christian Schunn</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/141"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/wenting-xiong">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-pittsburgh"/>
		<rdfs:label>Wenting Xiong</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Wenting</foaf:firstName>
		<foaf:lastName>Xiong</foaf:lastName>
		<foaf:mbox_sha1sum>c94554cac9750c73379bc437523ae611f29e7bc0</foaf:mbox_sha1sum>
		<foaf:name>Wenting Xiong</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/141"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/peter-brusilovsky">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-pittsburgh"/>
		<rdfs:label>Peter Brusilovsky</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Peter</foaf:firstName>
		<foaf:lastName>Brusilovsky</foaf:lastName>
		<foaf:mbox_sha1sum>1fa1a097b3fae4594fa0ec18e2e3da11ac7e0349</foaf:mbox_sha1sum>
		<foaf:name>Peter Brusilovsky</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/118"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/diane-litman">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-pittsburgh"/>
		<rdfs:label>Diane Litman</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Diane</foaf:firstName>
		<foaf:lastName>Litman</foaf:lastName>
		<foaf:mbox_sha1sum>943139c4d5ec1801c7472e123fb1870a469ca2b7</foaf:mbox_sha1sum>
		<foaf:name>Diane Litman</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/141"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kalina-yacef">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>Kalina Yacef</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Kalina</foaf:firstName>
		<foaf:lastName>Yacef</foaf:lastName>
		<foaf:mbox_sha1sum>a996139b89c064243c13504322ae7fc688ace4b5</foaf:mbox_sha1sum>
		<foaf:name>Kalina Yacef</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/126"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/130"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sunghwan-mac-kim">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>Sunghwan Mac Kim</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Sunghwan</foaf:firstName>
		<foaf:lastName>Mac Kim</foaf:lastName>
		<foaf:mbox_sha1sum>e9dcf9f8a22550efd1db95081e9dee8e0a786eea</foaf:mbox_sha1sum>
		<foaf:name>Sunghwan Mac Kim</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/174"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/vilaythong-southavilay">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>Vilaythong Southavilay</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Vilaythong</foaf:firstName>
		<foaf:lastName>Southavilay</foaf:lastName>
		<foaf:mbox_sha1sum>d96e689b4da9c69983dd28b65c9219434339bb27</foaf:mbox_sha1sum>
		<foaf:name>Vilaythong Southavilay</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/james-r-curran">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>James R. Curran</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>James</foaf:firstName>
		<foaf:lastName>R. Curran</foaf:lastName>
		<foaf:mbox_sha1sum>216ce6017e7a1af315c421281907dc1739cd59f8</foaf:mbox_sha1sum>
		<foaf:name>James R. Curran</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/126"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>Rafael A. Calvo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Rafael</foaf:firstName>
		<foaf:lastName>A. Calvo</foaf:lastName>
		<foaf:mbox_sha1sum>7c1aaaad0c34747b5b9e7b8624583937f8eac0fb</foaf:mbox_sha1sum>
		<foaf:name>Rafael A. Calvo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/174"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kalina-yacef">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>Kalina Yacef</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Kalina</foaf:firstName>
		<foaf:lastName>Yacef</foaf:lastName>
		<foaf:mbox_sha1sum>a996139b89c064243c13504322ae7fc688ace4b5</foaf:mbox_sha1sum>
		<foaf:name>Kalina Yacef</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/126"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/130"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rajibussalim">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>Rajibussalim</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName></foaf:firstName>
		<foaf:lastName>ajibussalim</foaf:lastName>
		<foaf:mbox_sha1sum>e05a95fab379063bc9a6ac2d89e84653d7994c74</foaf:mbox_sha1sum>
		<foaf:name>Rajibussalim</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/127"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rafael-a-calvo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>Rafael A. Calvo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Rafael</foaf:firstName>
		<foaf:lastName>A. Calvo</foaf:lastName>
		<foaf:mbox_sha1sum>7c1aaaad0c34747b5b9e7b8624583937f8eac0fb</foaf:mbox_sha1sum>
		<foaf:name>Rafael A. Calvo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/174"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kalina-yacef">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>Kalina Yacef</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Kalina</foaf:firstName>
		<foaf:lastName>Yacef</foaf:lastName>
		<foaf:mbox_sha1sum>a996139b89c064243c13504322ae7fc688ace4b5</foaf:mbox_sha1sum>
		<foaf:name>Kalina Yacef</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/126"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/130"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/149"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/anna-katrina-dominguez">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-sydney"/>
		<rdfs:label>Anna Katrina Dominguez</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Australia"/>
		<foaf:firstName>Anna</foaf:firstName>
		<foaf:lastName>Katrina Dominguez</foaf:lastName>
		<foaf:mbox_sha1sum>ce068018614c84c0e79242adf668a04ca66bdeb0</foaf:mbox_sha1sum>
		<foaf:name>Anna Katrina Dominguez</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/126"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/john-champaign">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-waterloo"/>
		<rdfs:label>John Champaign</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>John</foaf:firstName>
		<foaf:lastName>Champaign</foaf:lastName>
		<foaf:mbox_sha1sum>684d351029d0c51623ed5482edd108e8f3db145b</foaf:mbox_sha1sum>
		<foaf:name>John Champaign</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/124"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/145"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/robin-cohen">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-waterloo"/>
		<rdfs:label>Robin Cohen</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Robin</foaf:firstName>
		<foaf:lastName>Cohen</foaf:lastName>
		<foaf:mbox_sha1sum>2c8ee4228e789fe742a1bee369dfaee2e268add1</foaf:mbox_sha1sum>
		<foaf:name>Robin Cohen</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/124"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/145"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/robin-cohen">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-waterloo"/>
		<rdfs:label>Robin Cohen</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>Robin</foaf:firstName>
		<foaf:lastName>Cohen</foaf:lastName>
		<foaf:mbox_sha1sum>2c8ee4228e789fe742a1bee369dfaee2e268add1</foaf:mbox_sha1sum>
		<foaf:name>Robin Cohen</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/124"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/145"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/john-champaign">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-waterloo"/>
		<rdfs:label>John Champaign</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/"/>
		<foaf:firstName>John</foaf:firstName>
		<foaf:lastName>Champaign</foaf:lastName>
		<foaf:mbox_sha1sum>684d351029d0c51623ed5482edd108e8f3db145b</foaf:mbox_sha1sum>
		<foaf:name>John Champaign</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/124"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/145"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/beijie-xu">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utah-state-university"/>
		<rdfs:label>Beijie Xu</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Beijie</foaf:firstName>
		<foaf:lastName>Xu</foaf:lastName>
		<foaf:mbox_sha1sum>95764d5b6e21f4f1c2e2aac06b865a0a56a56984</foaf:mbox_sha1sum>
		<foaf:name>Beijie Xu</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/131"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/christine-garrard">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utah-state-university"/>
		<rdfs:label>Christine Garrard</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Christine</foaf:firstName>
		<foaf:lastName>Garrard</foaf:lastName>
		<foaf:mbox_sha1sum>62fd65f479296ddfe98ab78df4a31beb3aa52ef5</foaf:mbox_sha1sum>
		<foaf:name>Christine Garrard</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mimi-m-recker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utah-state-university"/>
		<rdfs:label>Mimi M. Recker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Mimi</foaf:firstName>
		<foaf:lastName>M. Recker</foaf:lastName>
		<foaf:mbox_sha1sum>11bf17dad9deec0b8157d8bbbcd850140ba8a9e4</foaf:mbox_sha1sum>
		<foaf:name>Mimi M. Recker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/131"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mimi-m-recker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utah-state-university"/>
		<rdfs:label>Mimi M. Recker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Mimi</foaf:firstName>
		<foaf:lastName>M. Recker</foaf:lastName>
		<foaf:mbox_sha1sum>11bf17dad9deec0b8157d8bbbcd850140ba8a9e4</foaf:mbox_sha1sum>
		<foaf:name>Mimi M. Recker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/131"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/beijie-xu">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utah-state-university"/>
		<rdfs:label>Beijie Xu</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Beijie</foaf:firstName>
		<foaf:lastName>Xu</foaf:lastName>
		<foaf:mbox_sha1sum>95764d5b6e21f4f1c2e2aac06b865a0a56a56984</foaf:mbox_sha1sum>
		<foaf:name>Beijie Xu</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/131"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/134"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/larry-howard">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/vanderbilt-university"/>
		<rdfs:label>Larry Howard</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Larry</foaf:firstName>
		<foaf:lastName>Howard</foaf:lastName>
		<foaf:mbox_sha1sum>cf90d0816cfebdfc6f1c7da000cc92596f38cceb</foaf:mbox_sha1sum>
		<foaf:name>Larry Howard</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/117"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/julie-johnson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/vanderbilt-university"/>
		<rdfs:label>Julie Johnson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Julie</foaf:firstName>
		<foaf:lastName>Johnson</foaf:lastName>
		<foaf:mbox_sha1sum>5acb834cf98ab4dd7f7c2740757602ea6138e0c2</foaf:mbox_sha1sum>
		<foaf:name>Julie Johnson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/117"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/carin-neitzel">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/vanderbilt-university"/>
		<rdfs:label>Carin Neitzel</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Carin</foaf:firstName>
		<foaf:lastName>Neitzel</foaf:lastName>
		<foaf:mbox_sha1sum>e0bf8a18694d677a8def61fa5520b429fb5c5365</foaf:mbox_sha1sum>
		<foaf:name>Carin Neitzel</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/117"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hogyeong-jeong">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/vanderbilt-university"/>
		<rdfs:label>Hogyeong Jeong</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Hogyeong</foaf:firstName>
		<foaf:lastName>Jeong</foaf:lastName>
		<foaf:mbox_sha1sum>2a0a33dd8e91855f6a4c1c2e46416924c2793fc6</foaf:mbox_sha1sum>
		<foaf:name>Hogyeong Jeong</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/gautam-biswas">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/vanderbilt-university"/>
		<rdfs:label>Gautam Biswas</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Gautam</foaf:firstName>
		<foaf:lastName>Biswas</foaf:lastName>
		<foaf:mbox_sha1sum>da39a3ee5e6b4b0d3255bfef95601890afd80709</foaf:mbox_sha1sum>
		<foaf:name>Gautam Biswas</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/julie-johnson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/vanderbilt-university"/>
		<rdfs:label>Julie Johnson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Julie</foaf:firstName>
		<foaf:lastName>Johnson</foaf:lastName>
		<foaf:mbox_sha1sum>5acb834cf98ab4dd7f7c2740757602ea6138e0c2</foaf:mbox_sha1sum>
		<foaf:name>Julie Johnson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/117"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/larry-howard">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/vanderbilt-university"/>
		<rdfs:label>Larry Howard</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Larry</foaf:firstName>
		<foaf:lastName>Howard</foaf:lastName>
		<foaf:mbox_sha1sum>cf90d0816cfebdfc6f1c7da000cc92596f38cceb</foaf:mbox_sha1sum>
		<foaf:name>Larry Howard</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/117"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/148"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dovan-rai">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Dovan Rai</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Dovan</foaf:firstName>
		<foaf:lastName>Rai</foaf:lastName>
		<foaf:mbox_sha1sum>c4c50aefe737ad599086b7de8aa484a6efb0122b</foaf:mbox_sha1sum>
		<foaf:name>Dovan Rai</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/161"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/orlando-montalvo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Orlando Montalvo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Orlando</foaf:firstName>
		<foaf:lastName>Montalvo</foaf:lastName>
		<foaf:mbox_sha1sum>802ac5dbb2d4e2691ca7cb0d98ee68b1b548b9a3</foaf:mbox_sha1sum>
		<foaf:name>Orlando Montalvo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Michael A. Sao Pedro</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Michael</foaf:firstName>
		<foaf:lastName>A. Sao Pedro</foaf:lastName>
		<foaf:mbox_sha1sum>4d53761cc8231dbcc882cb59462ddbc7cc52e7c3</foaf:mbox_sha1sum>
		<foaf:name>Michael A. Sao Pedro</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Neil T. Heffernan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Neil</foaf:firstName>
		<foaf:lastName>T. Heffernan</foaf:lastName>
		<foaf:mbox_sha1sum>8c86318b5e87ca4e61bed8db77402ba0b24d7701</foaf:mbox_sha1sum>
		<foaf:name>Neil T. Heffernan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/yutao-wang">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Yutao Wang</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Yutao</foaf:firstName>
		<foaf:lastName>Wang</foaf:lastName>
		<foaf:mbox_sha1sum>71952f2176a0df3a7f49846371c333dc76c412bf</foaf:mbox_sha1sum>
		<foaf:name>Yutao Wang</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-e-beck">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Joseph E. Beck</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>E. Beck</foaf:lastName>
		<foaf:mbox_sha1sum>98757703f9ddfd60ced2fbeb80219aee5e11c1b6</foaf:mbox_sha1sum>
		<foaf:name>Joseph E. Beck</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/161"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/adam-b-goldstein">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Adam B. Goldstein</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Adam</foaf:firstName>
		<foaf:lastName>B. Goldstein</foaf:lastName>
		<foaf:mbox_sha1sum>0dbc10bc372e41851379960a010b5d66817f0f88</foaf:mbox_sha1sum>
		<foaf:name>Adam B. Goldstein</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Ryan S.j.d. Baker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ryan</foaf:firstName>
		<foaf:lastName>S.j.d. Baker</foaf:lastName>
		<foaf:mbox_sha1sum>188538b9d7ab9a3c2883dc5640511f67db9e3aab</foaf:mbox_sha1sum>
		<foaf:name>Ryan S.j.d. Baker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/128"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Ryan S.j.d. Baker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ryan</foaf:firstName>
		<foaf:lastName>S.j.d. Baker</foaf:lastName>
		<foaf:mbox_sha1sum>188538b9d7ab9a3c2883dc5640511f67db9e3aab</foaf:mbox_sha1sum>
		<foaf:name>Ryan S.j.d. Baker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/128"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/yue-gong">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Yue Gong</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Yue</foaf:firstName>
		<foaf:lastName>Gong</foaf:lastName>
		<foaf:mbox_sha1sum>080a8dbc1e3d9dbd79c27a1eeeb52fc943f6d801</foaf:mbox_sha1sum>
		<foaf:name>Yue Gong</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Neil T. Heffernan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Neil</foaf:firstName>
		<foaf:lastName>T. Heffernan</foaf:lastName>
		<foaf:mbox_sha1sum>8c86318b5e87ca4e61bed8db77402ba0b24d7701</foaf:mbox_sha1sum>
		<foaf:name>Neil T. Heffernan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Arnon Hershkovitz</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Arnon</foaf:firstName>
		<foaf:lastName>Hershkovitz</foaf:lastName>
		<foaf:mbox_sha1sum>a63c9b6def54cbcc5d6415c35de988d400ab954e</foaf:mbox_sha1sum>
		<foaf:name>Arnon Hershkovitz</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/125"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/matt-bachmann">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Matt Bachmann</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Matt</foaf:firstName>
		<foaf:lastName>Bachmann</foaf:lastName>
		<foaf:mbox_sha1sum>add4e1fa43006603462ac10ec7dc58c434524ee8</foaf:mbox_sha1sum>
		<foaf:name>Matt Bachmann</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Ryan S.j.d. Baker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ryan</foaf:firstName>
		<foaf:lastName>S.j.d. Baker</foaf:lastName>
		<foaf:mbox_sha1sum>188538b9d7ab9a3c2883dc5640511f67db9e3aab</foaf:mbox_sha1sum>
		<foaf:name>Ryan S.j.d. Baker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/128"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/arnon-hershkovitz">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Arnon Hershkovitz</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Arnon</foaf:firstName>
		<foaf:lastName>Hershkovitz</foaf:lastName>
		<foaf:mbox_sha1sum>a63c9b6def54cbcc5d6415c35de988d400ab954e</foaf:mbox_sha1sum>
		<foaf:name>Arnon Hershkovitz</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/125"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/165"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/michael-a-sao-pedro">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Michael A. Sao Pedro</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Michael</foaf:firstName>
		<foaf:lastName>A. Sao Pedro</foaf:lastName>
		<foaf:mbox_sha1sum>4d53761cc8231dbcc882cb59462ddbc7cc52e7c3</foaf:mbox_sha1sum>
		<foaf:name>Michael A. Sao Pedro</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/orlando-montalvo">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Orlando Montalvo</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Orlando</foaf:firstName>
		<foaf:lastName>Montalvo</foaf:lastName>
		<foaf:mbox_sha1sum>802ac5dbb2d4e2691ca7cb0d98ee68b1b548b9a3</foaf:mbox_sha1sum>
		<foaf:name>Orlando Montalvo</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-e-beck">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Joseph E. Beck</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>E. Beck</foaf:lastName>
		<foaf:mbox_sha1sum>98757703f9ddfd60ced2fbeb80219aee5e11c1b6</foaf:mbox_sha1sum>
		<foaf:name>Joseph E. Beck</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/161"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-e-beck">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Joseph E. Beck</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>E. Beck</foaf:lastName>
		<foaf:mbox_sha1sum>98757703f9ddfd60ced2fbeb80219aee5e11c1b6</foaf:mbox_sha1sum>
		<foaf:name>Joseph E. Beck</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/161"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/j-gobert">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>J. Gobert</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>J.</foaf:firstName>
		<foaf:lastName>Gobert</foaf:lastName>
		<foaf:mbox_sha1sum>7421f8860251e1bfb07ed89c5d56c6d4a324189a</foaf:mbox_sha1sum>
		<foaf:name>J. Gobert</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Neil T. Heffernan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Neil</foaf:firstName>
		<foaf:lastName>T. Heffernan</foaf:lastName>
		<foaf:mbox_sha1sum>8c86318b5e87ca4e61bed8db77402ba0b24d7701</foaf:mbox_sha1sum>
		<foaf:name>Neil T. Heffernan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/adam-nakama">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Adam Nakama</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Adam</foaf:firstName>
		<foaf:lastName>Nakama</foaf:lastName>
		<foaf:mbox_sha1sum>f5d62bc3d2dd679dd8a60bf4104dc9a4f19f8491</foaf:mbox_sha1sum>
		<foaf:name>Adam Nakama</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/adam-nakama">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Adam Nakama</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Adam</foaf:firstName>
		<foaf:lastName>Nakama</foaf:lastName>
		<foaf:mbox_sha1sum>f5d62bc3d2dd679dd8a60bf4104dc9a4f19f8491</foaf:mbox_sha1sum>
		<foaf:name>Adam Nakama</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ryan-sjd-baker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Ryan S.j.d. Baker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Ryan</foaf:firstName>
		<foaf:lastName>S.j.d. Baker</foaf:lastName>
		<foaf:mbox_sha1sum>188538b9d7ab9a3c2883dc5640511f67db9e3aab</foaf:mbox_sha1sum>
		<foaf:name>Ryan S.j.d. Baker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/128"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Neil T. Heffernan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Neil</foaf:firstName>
		<foaf:lastName>T. Heffernan</foaf:lastName>
		<foaf:mbox_sha1sum>8c86318b5e87ca4e61bed8db77402ba0b24d7701</foaf:mbox_sha1sum>
		<foaf:name>Neil T. Heffernan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/joseph-e-beck">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Joseph E. Beck</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Joseph</foaf:firstName>
		<foaf:lastName>E. Beck</foaf:lastName>
		<foaf:mbox_sha1sum>98757703f9ddfd60ced2fbeb80219aee5e11c1b6</foaf:mbox_sha1sum>
		<foaf:name>Joseph E. Beck</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/161"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/zachary-a-pardos">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Zachary A. Pardos</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Zachary</foaf:firstName>
		<foaf:lastName>A. Pardos</foaf:lastName>
		<foaf:mbox_sha1sum>08ca8bfb6323c3c3b0d306d519cc1ff39b2cfb2e</foaf:mbox_sha1sum>
		<foaf:name>Zachary A. Pardos</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/j-gobert">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>J. Gobert</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>J.</foaf:firstName>
		<foaf:lastName>Gobert</foaf:lastName>
		<foaf:mbox_sha1sum>7421f8860251e1bfb07ed89c5d56c6d4a324189a</foaf:mbox_sha1sum>
		<foaf:name>J. Gobert</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/neil-t-heffernan">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Neil T. Heffernan</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Neil</foaf:firstName>
		<foaf:lastName>T. Heffernan</foaf:lastName>
		<foaf:mbox_sha1sum>8c86318b5e87ca4e61bed8db77402ba0b24d7701</foaf:mbox_sha1sum>
		<foaf:name>Neil T. Heffernan</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/116"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/136"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/143"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/144"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/150"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/sujith-m-gowda">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>Sujith M. Gowda</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Sujith</foaf:firstName>
		<foaf:lastName>M. Gowda</foaf:lastName>
		<foaf:mbox_sha1sum>dea68080c0936df588ea81c99113afbf1708b10d</foaf:mbox_sha1sum>
		<foaf:name>Sujith M. Gowda</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/128"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/j-gobert">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/worcester-polytechnic-institute"/>
		<rdfs:label>J. Gobert</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>J.</foaf:firstName>
		<foaf:lastName>Gobert</foaf:lastName>
		<foaf:mbox_sha1sum>7421f8860251e1bfb07ed89c5d56c6d4a324189a</foaf:mbox_sha1sum>
		<foaf:name>J. Gobert</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/138"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/139"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/conference/edm2010/paper/168"/>
	</foaf:Person>
</rdf:RDF>