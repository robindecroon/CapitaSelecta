<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:dcterms="http://purl.org/dc/terms/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:ical="http://www.w3.org/2002/12/cal/ical#"
    xmlns:swrc="http://swrc.ontoware.org/ontology#"
    xmlns:bibo="http://purl.org/ontology/bibo/"
    xmlns:swc="http://data.semanticweb.org/ns/swc/ontology#"
    xmlns:led="http://data.linkededucation.org/ns/linked-education.rdf#"
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" >
	<swc:ConferenceEvent rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12">
		<swc:completeGraph rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/complete"/>
		<swc:hasAcronym>JETS2012</swc:hasAcronym>
		<swc:hasRelatedDocument rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<rdfs:label>Journal of Educational Technology and Society Special Issue on Learning and Knowledge Analytics</rdfs:label><foaf:homepage rdf:resource="http://www.ifets.info/issues.php?id=56"/>
	</swc:ConferenceEvent>
	<swrc:Proceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings">
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/67"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/68"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/69"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/70"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/71"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/72"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/73"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/74"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/75"/>
		<swc:hasPart rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/76"/>
		<swc:relatedToEvent rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12"/>
		<swrc:booktitle>Journal of Educational Technology and Society Special Issue on Learning and Knowledge Analytics</swrc:booktitle>
		<swrc:month>July</swrc:month>
		<swrc:series></swrc:series>
		<swrc:year>2012</swrc:year>
	</swrc:Proceedings>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/67">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>Translating Learning into Numbers: A Generic Framework for Learning Analytics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/67/authorlist"/>
		<swrc:abstract>With the increase in available educational data, it is expected that Learning Analytics will become a powerful means to inform and support learners, teachers and their institutions in better understanding and predicting personal learning needs and performance. However, the processes and requirements behind the beneficial application of Learning and Knowledge Analytics as well as the consequences for learning and teaching are still far from being understood. In this paper, we explore the key dimensions of Learning Analytics (LA), the critical problem zones, and some potential dangers to the beneficial exploitation of educational data. We propose and discuss a generic design framework that can act as a useful guide for setting up Learning Analytics services in support of educational practice and learner guidance, in quality assurance, curriculum development, and in improving teacher effectiveness and efficiency. Furthermore, the presented article intends to inform about soft barriers and limitations of Learning Analytics. We identify the required skills and competences that make meaningful use of Learning Analytics data possible to overcome gaps in interpretation literacy among educational stakeholders. We also discuss privacy and ethical issues and suggest ways in which these issues can be addressed through policy guidelines and best practice examples.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 In the last few years, the amount of data that is published and made publicly available on the web has exploded. This includes governmental data, Web2.0 data from a plethora of social platforms (Twitter, Flickr, YouTube, etc.), and data produced by various sensors such as GPS location data from mobile devices. In the wake of this, data-driven companies like Google, Yahoo, Facebook, Amazon, etc. are growing exponentially by commercially exploiting such data for marketing or in the creation of new personalised services. The new “data economy” empowers companies to offer an increasing amount of data products at little or no cost to their users (e.g., Google Flu Trends, bit.ly customised URLs, Yahoo Pipes, Gapminder.com). This growth in data also renewed the interest in information retrieval technologies. Such technologies are used to analyse data and offer personalised data products customised to the needs and the context of individual users. It is already evident that data in combination with information retrieval technologies are not only the basis for the emergent data economy, but also hold substantial promises for use in education (Retalis et al., 2006; Johnson et al., 2011). One example of this is the research on personalisation with information retrieval technologies which has been a focus in the educational field for some time now (Manouselis et al., 2010). The main driver is the vision of improved quality, effectiveness, and efficiency of the learning processes. It is expected that personalised learning has the potential to reduce delivery costs while at the same time creating more effective learning experiences, accelerating competence development, and increasing collaboration between learners. Not so long ago, for universities and companies alike, gathering data on their users met with substantial limitations in terms of cost, time requirements, scope, and authenticity of the data, as this was typically done using questionnaires or interviews with a selected representative number of stakeholders. The new data economy has made data collection very much an affordable activity. It is based on the highly economic electronic data mining of people’s digital footprints and the automated analysis of behaviours of the entire constituency rather than sampling. Because data mining is not a separate act to normal user behaviour, the information retrieved is also highly authentic in terms of reflecting real and uninterrupted user behaviour. As such, data mining is more comparable to observational data gathering than to intrusive collection via direct methods. This will not make questionnaires and structured interviews obsolete, but it will greatly enhance our understanding and highlight possible inconsistencies between user behaviour and user perception (Savage and Burrows, 2007). The proliferation of interactive learning environments, learning management systems (LMS), intelligent tutoring systems, e-portfolio systems, and personal learning environments (PLE) in all sectors of education produces vast amounts of tracking data. But, although these e-learning environments store user data automatically, exploitation of the data for learning and teaching is still very limited. These educational datasets offer unused opportunities for the evaluation of learning theories, learner feedback and support, early warning systems, learning technology, and the development of future learning applications. This leads to the importance of Learning Analytics (LA) being increasingly recognised by governments, educators, funding agencies, research institutes, and software providers. The renewed interest in data science and information retrieval technologies such as educational data mining, machine learning, collaborative filtering, or latent semantic analysis in Technology-Enhanced Learning (TEL) reveals itself through an increasing number of scientific conferences, workshops and projects combined under the new research term Learning Analytics. Examples are the 1st Learning Analytics conference in Banff, Canada, 2011; the 4th International Conference on Educational Data Mining 2011 in Eindhoven, Netherlands; the 1st dataTEL workshop on Educational Datasets for Technology-Enhanced Learning at the Alpine-Rendez-Vous conference La Clusaz, France 2011; the 2nd International Conference on Learning Analytics and Knowledge (LAK12), Vancouver 2012; the 1st Workshop on Learning Analytics and Linked Data (LALD 2012); and more. Thus, the increasing amount of dedicated research events and publications make a meta-analysis of the domain timely and needed in order to establish a solid scientific basis which facilitates the development of new learner-oriented services. 
 Critical dimensions of learning analytics.
 Despite the great enthusiasm that is currently surrounding LA, it also raises substantial questions for research. In addition to technically-focused research questions such as the compatibility of educational datasets, or the comparability and adequacy of algorithmic and technological approaches, there remain several “softer” issues and problem areas that influence the acceptance and the impact of Learning Analytics. Among these are questions of data ownership and openness, ethical use and dangers of abuse, and the demand for new key competences to interpret and act on LA results. We shall point at these issues in more detail below. This means that the implementation of LA in learning processes requires to be carefully crafted in order to be successful and beneficial. This necessity motivated us to identify six critical dimensions (soft and hard) of LA, which need to be covered by the design to ensure an appropriate exploitation of LA in an educationally beneficial way. By soft issues we mean challenges that depend on assumptions being made about humans or the society in general, e.g., competences or ethics. They are opposed by the hard challenges of the fact-based world of data and algorithms (cf. also the similar soft-hard distinction in Dron, 2011). In its coverage of soft issues, our framework differs from other, more workflow oriented models for LA, like that by Siemens (2011), although in his presentation he does acknowledge these as of concern. Rather than being a process model such as those collected in Elias (2011), we aim at a description framework that can later be developed into a domain model or ontology. The critical dimensions highlighted here have been deduced from discussions in the emerging research community using a general morphological analysis (GMA) approach (cf. Ritchey, 2011). In this early formation stage of the LA community, scientific exchanges such as open online courses (MOOC) in Learning and Knowledge Analytics (LAK11, LAK12), or the above-mentioned events and congregations soon began to revolve around a number of key questions, like: Who is the target group for LA? What are we trying to achieve? How do we deal with privacy and data protection? These questions are naturally extended by other on-going debates such as the openness of data, which has been a topic for some time in the EDM and Open Linked Data domain, as well as technical and theoretical questions on achieving meaningful extraction of information from data. Our chosen approach leading to the proposed framework consisted of a number of gathering and analysis processes. First, as a matter of opinion mining, we scanned the scientific interactions from proceedings and presentations of the conferences and working groups mentioned above. We conducted a brief literature review of abstracts in the field of Learning Analytics and Educational Data Mining. Additionally, we scanned the live discussions on the LA Google Groups (http://groups.google.com/group/learninganalytics and http://groups.google.com/group/LAK11), as well as the LAK11 MOOC (presentation chats and social networking exchanges). Furthermore, we looked back at recent RTD projects that contained elements of analytics and the questions and lessons they produced, e.g., the Language Technologies for Lifelong Learning project (http://www.ltfll-project.org) contained an analytics approach related to learner positioning and conceptualisation. Following these reviews, we applied cognitive mapping (Ackermann, Eden, and Cropper, 2004) for synthesising and sense making. We analysed these discussions and clustered them into the proposed six fields of attention, which we then presented as the first draft of the framework to a community of commercial and academic experts for evaluation and feedback at the SURF seminar on Learning Analytics (Eindhoven, 30-31 August 2011). The number six in the framework is not chosen for any particular reason, and other divisions are of course possible. However, we find the dissection into these six dimensions a useful and easy to follow domain orientation. With the framework, we take the presumption that responsible designers of analytic processes will not only implement what is technically possible to do and legally allowed (or at least not prohibited), but to consider holistically the outcomes for stakeholders and, even more importantly, the consequences for the data subjects, i.e., the people supplying the data (cf. the section on stakeholders below). The framework intends to be a guide as much as a descriptor of the problem zones. Hence we refer to it as a “design framework” that can and should be used to design LA services from an inclusive perspective. We will argue below that this will help the transferability of LA approaches between different contexts of application and research. 
 Proposed design framework for learning analytics.
 Our proposed model for the domain and application of LA in figure 1 below considers six critical dimensions. Each of the dimensions can be subdivided into several instantiations falling into that dimension. For example, the generic “stakeholder” dimension can have instantiations (values) like “learners” and “teachers.” The list of instantiations in the diagram is not exhaustive and can be extended on a case-by-case basis. To stay with the above example, commercial service providers and even automated agents could also function as stakeholders in a LA process. It is useful to note that through connecting various (and also multiple) different instantiations of each dimension, concrete use cases can be constructed. We call the dimensions “critical” in the sense that each of the six fields of attention is required to have at least one instantiation present in a fully formulated LA design. We realise, though, that some dimensions are vaguer than others in this respect. 
 Figure.

 1. Critical dimensions of learning analytics. 
 The six dimensions of the proposed LA framework are (cf. Figure 1): stakeholders, objectives, data, instruments, external constraints, and internal limitations. We will discuss each of these dimensions individually in the following and exemplify their instantiations and impact on the LA process and the benefits and opportunities they may determine. We will also elaborate apparent problem zones and limitations that may hinder any envisaged benefits. Before embarking on the abstract dimensions in detail, we would like to illustrate the purpose and possible usage of the framework on the following sample use case, which is created out of a number of instantiations of the six dimensions. This specific example relates to conducting a social network analysis of students discussing in a forum using the SNAPP tool, based on the work by Dawson et al. (Dawson, 2008; Macfadyen & Dawson, 2010). 
 Table 1. Sample use case and values for dimensions. 
 The above use case can be used (1) as a checklist when designing a purposeful LA process; (2) as a sharable description framework to compare context parameters with other similar approaches in other contexts, or for replication of the scientific environment. The framework allows an indefinite number of use cases with the respective value arguments. 
 Stakeholders.
 The stakeholder dimension includes data clients as well as data subjects. Data clients are the beneficiaries of the LA process who are entitled and meant to act upon the outcome (e.g., teachers). Conversely, the data subjects are the suppliers of data, normally through their browsing and interaction behaviour (e.g., learners). It is important to make this distinction in order to understand the impact of the process on individuals. In certain cases, the two types of stakeholder groups can be the same, as is the case if a LA application feeds back information to learners about their own learning rather than to inform the teacher, as would be a common case in informal learning scenarios. In the traditional learner-teacher scenario, the teacher would act as the data client, who receives information gathered from the data subjects, i.e., the learners. 
 As shown in the framework model (Figure 1), the main stakeholder groups of LA in formal learning situations are learners, teachers, and educational institutions. These may be expanded or substituted by other stakeholder groups, such as researchers, service providers, or governmental agencies. Each of the groups has different information needs and can be provided with tailored views on information using LA. Information flow between stakeholders can best be exemplified with the common hierarchical model taken from formal education (Figure 2). What the diagram illustrates as an example is by which ways benefits might be obtained from LA. The pyramid encapsulates the academic layers of education and training institutions. In the most direct way, data analysis from the student level, e.g., via a LMS, can inform the above layer, in this case the teachers. Teachers can then use the analytics information to plan targeted interventions or adjust their pedagogic strategies. Institutions can, similarly, retrieve benefits from student and teacher data in order to provide staff development opportunities or to plan policies like quality assurance and efficiency measures. We also want to stress the major benefits LA offers for self-reflection on every level (cf. left side of the diagram). We would like to see institutions enabling and actively encouraging students to reflect on their learning data. But also teachers and institutions can gain new insights by reflecting on their performance. Not immediately involved in the learning processes, researchers (right of the diagram) could harvest data for the purpose of evaluating or innovating teaching processes or learning services. Finally (on top of the diagram), Government agencies may collect cross-institutional data to assess the requirements of Higher Education Institutes (HEI) and their constituencies. 
 Figure 2. Information flow between LA stakeholders. 
 Although they are the most widespread form in formal education, hierarchies are not the only flow models to describe where benefits can be retrieved. For example, peer evaluation using Personal Learning Environments (PLE) may be another information environment for LA. Peer environments also prevail in academic transactions like conferences or publications that are based on peer review systems. Practical examples for a horizontal peer-related information flow are the various scientific impact measures that exist, e.g., citation indexes. Equally, serious games can provide a non-hierarchical approach and/or team perspective to collaborative learning, e.g., how fast a team completed a level. In each of these, however, lie some issues of dependency and possible legal constraints (cf. further below). Example opportunities for LA with respect to different stakeholder groups are: Students can be supported with specific learning process and reflection visualisations that compare their performance to the overall performance of a course. Furthermore, they can be provided with personalised recommendations for suitable learning resources, learning paths, or peer students (Gaviria et al., 2011). 
 Teachers can be provided with course monitoring systems that inform them about knowledge gaps of particular pupils and thus enable them to focus their attention on those pupils. They can also harvest emergent group models that can lead to shared understanding of domain topics or processes for better curriculum design and on-the-fly adaptations. Institutions can monitor the performance of students regarding drop-out and graduation rate on a much finer granular level. In this way, they can evaluate their courses and improve outcomes of their courses. Other stakeholders: We would like to emphasise that stakeholders need not be confined to formal education settings, but include all formal, non-formal, or informal environments, such as professional development (CPD). In these cases, the stakeholders are to be substituted by the relevant entities. For non-formal learning, for example, stakeholders would include a “learner” instantiation with (only) a self-reflection dimension in which feedback is mirrored back to the same person. In work-based learning, employees and line-managers may be the most common stakeholder groups involved. More notably, computer agents can also serve as stakeholders, for example as data clients that take further decisions on the learner’s behalf or trigger an event (e.g., notification e-mail, recommendation of content or peer, etc.). 
 Objectives.
 The main opportunities for LA as a domain are to unveil and contextualise so far hidden information out of the educational data and prepare it for the different stakeholders (see above). Monitoring and comparing information flows and social interactions can offer new insights for learners as well as improve organisational effectiveness and efficiency. This new kind of information can support individual learning processes but also organisational knowledge management processes (Butler & Winne, 1995). We can distinguish two fundamentally different objectives: reflection and prediction (cf. Figure 1 above). Reflection: Reflection is seen here as the critical self-evaluation of a data client as indicated by their own datasets in order to obtain self-knowledge. Wolf (2009) calls this process the “quantified self”, i.e., self-observation and reacting to one’s own performance log data. There already is a growing number of Personal Informatics Systems, i.e., humancomputer interaction systems that support this process (Li & Forlizzi, 2010). However, reflection may also be seen as critical self-evaluation based on other stakeholders’ datasets. This would especially be true if, for example, a teacher was led to reflect upon their teaching style as indicated by the datasets of their students. In the above hierarchical flow model (Figure 2), the higher order stakeholder would have the ability to utilise all the datasets from lower constituencies for their own reflection. On an individual level, LA can support reflection of learning processes and offer personalised information on the progress of the learner (Govaerts et al., 2010). On the institutional level, LA can enhance monitoring processes and suggest interventions or activities for particular students. Greatest care should however be taken not to confuse objectives and stakeholders in the design of a LA process and not to let, e.g., economic and efficiency considerations on the institutional level dictate pedagogic strategies, as this would possibly lead to industrialisation rather than personalisation. LA is a support technology for decision making processes. Therein also lies one of the greatest potential dangers. Using statistical analytic findings is a quantitative not a qualitative support agent to such decision making. We are aware that aligning and regulating performance and behaviour of individual teachers or learners against a statistical norm without investigating the reasons for their divergence may strongly stifle innovation, individuality, creativity and experimentation that are so important in driving learning and teaching developments and institutional growth. Prediction: Apart from support for reflective practice, LA can also be used for predicting and modelling learner activities (Siemens, 2011; Verbert et al., 2011). This can lead to earlier intervention (e.g., to prevent drop-out), or to adaptive services and curricula. Using Machine Learning techniques, for example, learner profiles can be built dynamically and automatically, saving the learner filling in and maintaining profile data. In predictive outcomes lies currently much hope for efficiency gains in terms of establishing acts of automatic decision making for learning paths, thus saving teacher time for other more personal interventions. But prediction suffers potentially from big ethical problems (to which more further below), in that judgements about a person, whether originating from another human or a machine agent, if based on a limited set of parameters could potentially limit a learner’s potential. For example, not every learner who has difficulties mastering subject level two, will automatically not master level three. We have to prevent re-confirming old-established prejudices of race, social class, gender, or other with statistical data, leading to restrictions being placed upon individual learners. Furthermore, there are limitations in the use of LA data as a means for supporting the learning process. Learning processes assume the leading role of the learner, rather than that of the teacher. However, the reliability of a LA-supported learner profile and its usefulness to the learners will remain questionable. For example, what LA data can be used in order to define whether a learning activity had a “high” or “low” impact on the learning process of learners, and at which points in the process itself? The diversity of learning makes it also problematic to judge which learning activity was of high value for learner A but of low value for learner B. With respect to pedagogic theories, we would like to argue that LA does neither support nor ignore specific pedagogic theories, and as an abstract concept is pedagogically neutral. Indeed, we are of the opinion that LA can be used to evaluate different pedagogic strategies and their effects on learning and teaching through the analysis of learner data. This can be defined as a specific pedagogically oriented objective under the current dimension, but, as we will discuss further below, certain technologies are not pedagogically neutral and this will influence the analytics process in one way or another. 
 Educational data.
 LA takes advantage of available educational datasets from different Learning Management (LMS) and other systems. Institutions already possess a large amount of student data, and use these for different purposes, among which administering student progress and reporting to receive funding from the public authorities are the most commonly known. Linking such available datasets would facilitate the development of mash-up applications that can lead to more learner-oriented services and therefore improved personalisation. LA strongly relies on data about learners and one of the major challenges LA researchers are facing is the availability of publicly available datasets to evaluate their LA methods. Most of the data produced in institutions is protected, and the protection of student data and created learning artefacts is a high priority for IT services departments. Nevertheless, similar to Open Access publishing and related movements, calls for more openness of educational datasets have already been brought forward (Drachsler et al., 2010). Anonymisation is one means of creating access to so-called Open Data. Recently, Verbert et al., (in press) presented a state of the art review of existing educational datasets. How open educational data should be, requires a wider debate (cf. section on legal constraints below), but, already in 2010, several data initiatives where started to make more educational data publicly available: dataTEL challenge—The first dataTEL challenge was launched as part of the first workshop on Recommender Systems for TEL (Manouselis et al., 2010), jointly organized by the 4th ACM Conference on Recommender Systems and the 5th European Conference on Technology Enhanced Learning (EC-TEL 2010) in September 2010. In this call, research groups were invited to submit existing datasets from TEL applications that can be used for LA research purposes and recommender systems for TEL. dataTEL workshop—The “Datasets for Technology Enhanced Learning” workshop was organised at the third STELLAR Alpine Rendez-Vous in March 2011. During this workshop, related initiatives that are collecting educational datasets, and apply these in data-driven learning applications were presented, and challenges related to privacy and data protection were discussed. PSLC dataShop (Stamper, 2011) offers an open data repository that provides access to a large number of educational datasets. dataShop has data from students derived from interactions with intelligent tutoring systems. LinkedEducation.org (Dietze et al., 2012) is another initiative that provides an open platform to promote the use of data for educational purposes. At the time of writing, five organizations have contributed datasets. 
 Despite these pioneering activities, it does, by comparison, still seem somewhat bizarre that in the commercial world with clicking the “register” button, the default access to all user data becomes owned by some company, whereas educational institutions operate on the default that everything is protected from virtually everyone. Distinguishing educational data by access rights in open and protected datasets (Figure 1) is not as simple as it sounds. Because the technical systems producing and collecting data are typically owned by the institution, the easiest assumption would be that this data belongs to them. However, which employees of the institution exactly are included in the data contract between a learner (or their parents) and the educational establishment, is as yet unresolved. This poses severe constraints on inner-institutional research or wider institutional use. We will bring up some more legal consideration under the point on external constraints below. Like in related research domains, LA datasets create a new set of challenges for research and practice. These include: - A lack of common dataset formats like the suggested one from the CEN/ISSS PT social data group (cf. CAM Schema at: https://sites.google.com/site/camschema/home; and Wolpers et al., 2007). - The need for version control and a common reference system to distinguish and point to different datasets. - Methods to anonymise and pre-process data according to privacy and legal protection rights (Drachsler et al., 2010). - A standardised documentation of datasets so that others can make proper use of it like that promoted by the dataseal-of-approval initiative (cf. http://www.datasealofapproval.org). - Data policies (licences) that regulate how users can use and share certain datasets. For instance, the Creative Commons licensing rights could be considered as a standard way to grant permissions to datasets. DataCite (Brase, 2009) is an organization that enables to register research datasets and to assign licensing rights to them, so that the datasets can be referenced similar to academic articles. From a technical point of view, idealised datasets probably remain the biggest challenge for analytics. This is to say that the assumption that datasets consist of context-free, meaningful and only meaningful data, is highly optimistic. In most natural settings, users “pollute” databases by producing erroneous or incomplete datasets. For example, teachers who want to see their students’ view on LMS courses often set themselves up as “test students” or create “test courses”. These are not always obvious, but need to be removed from the data to be analysed. Therefore empirical findings coming from a specific dataset are almost certainly affected by the context of data collection and processing. Similarly, data collection often leads to “enmeshed identities” being used for analytics and prediction. A dataset cannot typically distinguish between a single individual and a shared presence in the learning space (group work on a single device). Students who often work together with others on shared devices (laptops, smartphone, lab space, etc.) produce enmeshed fingerprints in their educational data. This may lead to behaviours being attributed to a logged-in identity that may actually have originated from an “invisible” partner. Standardised documentation of datasets can be seen as paramount to raise awareness of this danger. Additionally, from a pedagogic perspective, it remains an on-going challenge to formulate indicators from the available datasets that bear relevance for the evaluation of the learning process. The selection of specific data and their weighting (under the methods applied in the “instruments” dimension) against the real behaviour of students is of greatest importance, as is the process of relating behaviour pattern data to cognitive developments. 
 Instruments.
 Different technologies can be applied in the development of educational services and applications that support the objectives of educational stakeholders. LA takes advantage of so-called information retrieval technologies like educational data mining (EDM; cf. Romero et al., 2008), machine learning, or classical statistical analysis techniques (cf. Figure 1), but other techniques may also be considered relevant, e.g., social network analysis (cf. Buckingham & Ferguson, 2011) or Natural Language Processing (NLP). Through these technologies, LA can contribute tailored information support systems to the stakeholders and report on demand. For instance, LA could be applied to develop a drop-out alert system. High drop-out rates are a challenging problem in education, especially distance education. Further research on LA can contribute to decrease the drop-out rate by developing e.g., a Drop-out Analyser that notifies the teacher of a course in time which students are in danger of falling behind or dropping out. This could be done by using LMS datasets and train a certain information retrieval technology (e.g., a Bayesian classifier) on the datasets to learn behavioural patterns of students that dropped out. Afterwards, the Drop-out Analyser could be applied on a follow-up online course and flag up students that show similar patterns. The teacher of the course could then intervene in an appropriate manner. Preliminary prototypes of such systems are already available, like the Blackboard Early Warning System. Under the dimension “instruments” in our model (Figure 1), we also subsume conceptual instruments such as theoretical constructs, algorithms, or weightings, by which we mean different ways of approaching data. These ways in the broadest sense “translate” raw data into information. The quality of the output information and its usefulness to the stakeholders depend heavily on the methods chosen. Hildebrandt (2010), quite rightly, warns that “invisible biases, based on … assumptions … are inevitably embodied in the algorithms that generate the patterns”. Competing methods, technologies and algorithms applied to the same set of data, will result in different outcomes, and thus may lead to different consequences in terms of decision making based on these outcomes. LA designers and developers need to be aware that any algorithm or method they apply is reductive by nature in that it simplifies reality to a manageable set of variables (cf. Verbert et al., 2011). 
 External constraints.
 Many different kinds of constraints can limit the beneficial application of LA processes, some being “softer” than others. It has been suggested to us to identify them as ethical, legal, and social constraints, but also to feature organisational, managerial, and process constraints. This we find a useful subdivision of external limitations, but other divisions look equally logical. In the abstraction of the diagram above (cf. Figure 1), we propose the preliminary distinction of conventions, under which we count ethics, personal privacy, and similar socially motivated limitations, and, norms that are restricted by laws or specific mandated policies or standards. For reasons of space, we want to elaborate especially on the ethical aspects as this has grown into a field of much recent attention and debate (Bollier, 2010) and even spawned a collaborative effort in the Learning Analytics research community (Siemens, 2012). New ethical and privacy issues arise when applying LA in education. These are challenging and highly sensitive topics when talking about educational datasets, as described in Drachsler et al. (2010). The feeling of endangered privacy may lead to resistance from data subjects toward new developments in LA. In order to use data in the context of LA in an acceptable and compliant way, policies and guidelines need to be developed that protect the data from abuse. Legal data protection may require that data is anonymised before it can be used. At the same time, as much openness of the datasets as possible is desirable (see paragraph on data above). Personal data enjoys strong legal protection, differing by national laws and sometimes competing with other legal frameworks such as the Freedom of Information Act in the United Kingdom. We will not go into the legal details here, but did already above hint at the predicament that faces any data-related venture when using people’s digital footprints. It concerns the lack of legal clarity with respect to data ownership. In current circumstances, data gathered about a person (before it is anonymised) belongs to the owner of the data collection tool, typically also the data client and beneficiary. Up till now, through direct intervention like questionnaires and sign-up processes, this was not a big problem to the data subjects. However, with the dramatic increase of ambient sensors and new technologies, such as location tracking or biometric face-recognition cameras, etc., more and more parts of individual behaviour are logged without the data subject’s approval or even awareness. That being so, the ethic principle of “informed consent” (cf. AoIR, 2002) is very much under threat. The fundamental question legislators need to ask is: who does a person’s life data belong to? (cf. also Hildebrandt, 2006). We believe that this question may in the near future become more and more elevated in importance and prominence. On institutional level, educational and student data was traditionally handled separately, and is legally something of a blind spot. Registration data was kept and maintained by registry staff, IT data by IT staff, and learning data by academic staff. To use LA to its full potential, integration of available institutional datasets needs to happen. Universities, for instance, already collect and report socio-economic data such as students’ post codes or ethnic and linguistic background. Institutions are even legally obliged by funding bodies to do so, but integrating this dataset with educational performance data, would be widely considered unethical or even illegal. As has been already mentioned above, the extent of a student’s data contract with an institution and its individual staff representatives in different roles (teacher, administrator, secretary, researcher, IT support staff, Deans and management, etc.) needs to be urgently clarified. At the same time, privacy directives such as the Data Seal of Approval supported by the Dutch DANS institute (http://www.datasealofapproval.org) and related European data directives like the European Directive on data protection 95/46/EC (Directive, EU, 1995) need to be implemented. Even where in compliance with the law, educational data can easily be abused for purposes inappropriate for educational institutions or for the data subjects (especially where minors are concerned). By principle, the more access to information about a data subject a data client has, the higher the responsibility is to use this information in a sensitive and ethical way. In an inspired article, Hildebrandt (2010) elaborates the ethically limited applicability of automated pattern recognition to the Law domain, but these limitations can be transferred just as easily to the domain of learning. Among the more obvious ethical risks are the exploitation of such data for commercial and similar purposes, or data surveillance issues (social sorting, cumulative disadvantages, digital stalking) and their ethical implications. Ethics don’t stop at the data gathering and integration. The realisation that we may encounter conflicts in values and interests in and through the analysis of people’s behaviour needs to guide the post-analytic decision making process and the conclusions drawn from the approach. It is important to remind stakeholders of LA processes that data can be interpreted in many ways and lead to very different consequent actions. To give a drastic example, imagine being confronted with the insight that children from an immigrant background show reading difficulties, backed by supportive data analysis. This may lead to a wide ranging variety of responses, from developing extracurricular support mechanisms, to segregated classes, up to bluntly racist abuse of various kinds. Ethics in LA may affect students and teachers alike, especially, where institutions aim to use LA to quality assure the performance of their teaching staff. Data can easily be abused as supporting evidence for exercising inappropriate pressures on data subjects to change otherwise perfectly acceptable or explainable performance behaviour. Institutions are therefore challenged with establishing a set of ethical policies and principles, together with, e.g., complaints procedures and safety nets that secure proper use of educational data in teaching and research. We find the ethical guidelines of the Association of Internet Researchers, AoIR (2002), a useful starting point in this respect, in that it has a purpose-oriented approach that supports ethical pluralism and respects the individual. Another ethical consideration is the acceptance of divergence in the data constituency (AoIR, 2002). We already touched upon the danger that the result of algorithmic analysis, consequent policies and exercised pressures may aim at uniformity and at mainstreaming learning and teaching processes, thereby greatly harming creative processes and innovation that diverge from the statistical mean. It is one of the principal shortfalls of statistical prediction that it can only predict average behaviour not outliers. As such, LA provides no means of predicting exceptions to a rule, or exceptions to the exception rule. 
 Internal limitations.
 In complement to the environmental problem areas contained in the above “external constraints” section, we subsume a number of human factors that enable or may pose obstacles and barriers under the dimension heading “internal limitations.” Prominent among these are competences and acceptance. It is already becoming clear that the application of learning analytics requires new higher-order competences to enable fruitful exploitation in learning and teaching. In order to make LA an effective tool for educational practice, it is important to recognise that LA ends with the presentation of algorithmically attained results that require interpretation (Reffay & Chanier, 2003; Mazza & Milani, 2005). There are innumerable ways to present and to interpret data and base consecutive decisions and actions on it, but only some of them will lead to benefits and to improved learning. Basic numeric and other literacies, as well as ethical understanding are not enough to realise the benefits that LA has to offer. In a recent survey we conducted among LA experts, only 21% of the 111 respondents felt that learners would possess the required competences to interpret LA results themselves and determine appropriate actions/interventions from it (Drachsler & Greller, 2012). Therefore, the optimal exploitation of LA data requires some high level competences in this direction, but interpretative and critical evaluation skills (cf. Figure 1) are to-date not a standard competence for the stakeholders, whence it may remain unclear to them what to do as a consequence of a LA outcome or visualisation. 
 Figure 3. Social network diagrams may look attractive, but may not always be the best way to present information. 
 Interpretation of LA results is often facilitated by enticing visualisations that are aimed to serve as a functional aid (Figure 3). One inherent danger that we perceive is that the simplicity and attractive display of data information may delude the data clients, e.g., teachers, away from the full pedagogic reality. This may negatively affect the pedagogic assessment and grading of a student’s performance which should not be based alone on the visualisation of log files from a Learning Management System. To illustrate this danger take the example of marking student essays. An automated spell-check on orthographic mistakes presents itself as a quick and simple to interpret translation of the learner artefact into numbers. This makes it ideal for an efficient, cognitively effortless, and egalitarian grading mechanism. In an education environment that increasingly suffers from time constraints and calls for more efficiency in teacher activities, it is easily imaginable that the traditional qualitative assessment of essays gives way to such quick number crunching being over-proportionally reflected in student marks. In our model (cf. Figure 1), we include among the key competences for LA, critical evaluation skills, because superficial digestion of data presentations can lead to wrong conclusions. It has to be strongly emphasised that data not included in the respective LA approach, is equally if not more important than the dataset that is included. To judge a learner’s performance merely on, e.g., LMS quantitative data is like looking at a single puzzle piece. As learning is more and more happening in a lifelong and diverse ecosystem, an exclusive data view on single elements may provide a stimulus for reflection but not a sound basis for assessment. The necessary competences notwithstanding, acceptance factors can further influence the application or decision making that follows an analytics process. This can, as is regularly seen in political debates, lead to blunt rejection of the results or applied methods from the constituency or parts thereof. In a learning context, ways to increase acceptance is vitally important also in order to produce usable outcomes. To get a better grasp on this issue, current scientific debate, therefore, should focus on empirical evaluation methods of learning analytics tools (Ali et al., 2012) and on advanced technology acceptance models (cf. Venkatesh & Bala 2008), inspired by the early work in this area (Davis, 1989, 1993). For LA, a revised technology acceptance model (TAM) could be an interesting approach to evaluate the emergent analytic technologies for all stakeholders described in our framework and also the needed implementation requirements to guarantee successful exploitation. 
 The place of pedagogy in the learning analytics framework.
 LA holds promises in the context of TEL by offering new methods and tools to diagnose learner needs and provide personalised instructions to better address these needs. It is not yet clear to what extent LA will lead to more personalised learning experiences rather than merely clustering people into behaviouristic “learner models” (e.g., as “outliers” of mainstream models). Consequently, more empirical evidence is needed to identify which pedagogic theory LA serves best. LA has been effectively used for behaviourist-instructivist style approaches (but see the critical reflection by Pardo & Kloos, 2011), but there is as yet little evidence for the support of constructivist approaches to learning (Duffy & Cunningham, 2001), where learning is seen as an active cognitive process in which learners construct their own concepts of the world around them. In LA, the latter is mostly inferred indirectly, by relating grades of learning outcomes with activities during the learning process (Dawson, 2012). In these correlations, it emerges that active students get better results. However, the role LA plays in this has not yet been conclusively demonstrated. Despite these questions, we would like to maintain that as knowledge and experiences vary considerably among learners, the diversity of learning can more effectively be addressed by LA methods than with current learning environments. In our model, LA can work in support of a multitude of pedagogic strategies and learning activities as manifested and represented by the available data. This means we can only see pedagogies through the data. Because of this, we do not include them as part of the analytics process (Figure 1) but as implicitly contained in the input datasets that encapsulate the pedagogic behaviour of users. As we know, this behaviour depends a great deal on the platform and the pedagogic vision the developers built in (Dron & Anderson, 2011). For example, data from a content sharing platform will have a behaviourist/cognitivist pedagogy attached to the learner behaviour, since this is the pedagogic model underlying the technology. In any case, only the pedagogic patterns exhibited in the dataset can be analysed; and this will vary. Additionally, pedagogy can be explicitly addressed in the goals and objectives that the LA designer sets (“objectives” dimension). The LA method (“instruments” dimension) will determine the outcome of the analysis and together with the interpretation applied may lead to a large variety of options for consequences and interventions. If such pedagogic interventions were applied, they would lead to new behaviours which, once again, could be analysed through the available data (Figure 4). In the graph below, we refer to pedagogic behaviour as learner/teacher behaviour that is motivated by didactic designs (learning designs). Pedagogic consequences, similarly, are adjustments to the didactic strategy or learning design based on the outcomes of the LA process. 
 Figure 4. Learning analytics and pedagogy. 
 A simple analogy would be boiling water in a pan. At any time (or continuously) you can immerse a thermometer and measure its temperature. The goal would be to determine whether you need to turn up the heat or not. The result of the analysis can then lead to the actions you want to take. The thermometer is only one method for such an analysis. An alternative would be to observe and wait until the water bubbles. Setting a benchmark (in the objectives design) can inform you when it is time for the teabag to go in. When applied to a learning process, immersing the thermometer into the water equates to the LA data gathering and analysis of learning in progress. It is here where learning is translated into numbers. It should be noted that the pedagogic input factors are not confined to behaviour alone, but also include beliefs, (societal) values, and implicit theories of knowledge and learning. However, the LA application can only see these in the way they manifest themselves in the data. We also want to point at the possibility to apply computer agents to determine specific interventions. These could be as simple as sending a notification or recommendation to one of the stakeholders. An example for pedagogic consequences is the following (Dawson, 2012): Using SNAPP as a tool to do a social network analysis (SNA) on discussion forums in a learning environment, the moderator or tutor might discover that certain changes in the moderation, the organisation, or the task, may lead to more or less engaged discussion among participants. In this way, the information gained through the LA process can support the fine-tuning of pedagogic effectiveness in a particular activity, depending on the desired learning outcome. The model takes note that pedagogic success and performance is not the only thing that LA can measure. LA collects snapshots taken from educational datasets. These snapshots can be used to reflect or predict, in order to make adjustments and inform interventions, either by a human or by a system. Apart from offering efficiency benchmarking and business information for education providers, new support services for learning and more qualitative personal experiences can be achieved. 
 Conclusion and outlook.
 In summary, the proposed framework model in figure 1 above stresses the inherent connections between the six different dimensions and the impact of the analytics process on the end user and the data suppliers. If one of the parameters changes, the outcome and anticipated benefits will change. It is therefore our conviction that only the consideration of all six dimensions in the design process can lead to optimal exploitation of LA. Additionally, substantial work on new ethical guidance, data curation, and ownership needs to happen at universities and in legislation to reduce the risks connected to the application of LA and to protect the data subject, usually the learner. Because of the inherent dependencies, we argue that all six dimensions are mandatory to be argumentatively present in a fully flexed LA design. We would, therefore, strongly welcome if application developers and researchers would not only make their technical environment known and open, but also describe the contextual environment and expectations from the users (e.g., required competences) along the lines of the framework. This would allow scientific comparison, replication, external analysis, and alternative contextualisation. To validate the framework as both a descriptive approach as well as a guide to the design process of LA applications, we suggest evaluating the growing number of LA application showcases and testing for consistency in the descriptive values of the model. Additionally, we want to create a selected number of use cases that encompass the six dimensions and their instantiations. LA is very much at the dawn of its existence and considered by many as one of the technological advances that will bring learning onto the next higher level. While we join in with this chorus of positive expectations, we are also aware that LA shows facets of a double nature: In its most optimistic outlook, learners will be provided with personal information about their current needs, while, at the same time, the educational system will be evolved from a “onesize-fits-all” approach into a highly personal competence-driven educational experience. But this view is not without flaws, because of the real dangers that the extended and organised collection of learner data may not so much bring added benefits to the individual, but instead provides a tool for HEIs, companies, or governments to increase manipulative control over students, employees, and citizens, thereby abusing LA as a means to reinforce segregation, peer pressure, and conformism rather than to help construct a needs-driven learning society. We therefore believe that it will be of critical importance for its acceptance that the development of LA takes a bottom-up approach focused on the interests of the learners as the main driving force. LA has the potential for new insights into learning processes by making hitherto invisible patterns in the educational data visible to researchers and end users, and to enable development of new instruments for everyday educational practice. However, there are substantial uncertainties about the extent of impact LA will have on education and learning in general. The proposed framework model is motivated by the potential and opportunities that LA offers in its relevance for educational development and opportunities to personalise learning. While we agree with the Horizon report’s forecast and its claim for a prosperous future of LA (Johnson et al., 2011), we also strongly feel that this development should not happen without a guiding framework that combines use of educational data with the protection of individuals and their learning. Decisions based on LA are of concern, because they determine the usefulness and consequences for the stakeholders as well as the extent of its impact. Data analysis could have dramatic (and unwanted) consequences if not used with the necessary care. It is here where ethics play an enormously important role. Building of trust and confidence throughout the data constituencies has to be a priority from the start, and, here again, this proposed framework hopes to act as a useful guide. One of the major questions in LA is the relation with theories of learning, teaching, cognition and knowledge. We hinted above at the opportunity that LA may support the evaluation of concrete didactic approaches which in turn may provide supportive evidence for particular pedagogic theories of learning and knowledge. At the same time, technologies are not pedagogically neutral; hence the evaluation will be influenced by the approach chosen. We consider this debate as an on-going one which will require further research and demonstration of applications and the impact they make on the process of learning. It is still too early to base education fully on LA approaches alone, and we expect it never will be possible to do so. However, at the very least, opportunities this new discipline has to offer are to provide new support for learning activities and stimuli for reflection. In our opinion, it is these opportunities that LA should pursue.]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Translating Learning into Numbers: A Generic Framework for Learning Analytics</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>framework</dc:subject>
		<dc:subject>educational data mining</dc:subject>
		<dc:subject>ethics</dc:subject>
		<dc:subject>domain design</dc:subject>
		<dc:subject>data for learning</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/wolfgang-greller"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/wolfgang-greller"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/67/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/wolfgang-greller"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/68">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>Analyzing Interactions by an IIS-Map-Based Method in Face-to-Face Collaborative Learning: An Empirical Study</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/68/authorlist"/>
		<swrc:abstract>This study proposes a new method named the IIS-map-based method for analyzing interactions in face-to-face collaborative learning settings. This analysis method is conducted in three steps: firstly, drawing an initial IISmap according to collaborative tasks; secondly, coding and segmenting information flows into information items of IIS; thirdly, computing attributes of information flows and analyzing relationships between attributes and group performance. An example illustrates how the methodology uncovers the interaction process based on information flows. The empirical study aims to validate the effectiveness of this method through thirty groups’ interactions. The result indicates that quantity of activation of the targeting knowledge network can predict group performance and the IIS-map-based analysis method can analyze interactions effectively. The primary contribution of this paper is the methodology for analysis of interactions based on information flows.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 In the past decade, more and more attention has been paid to collaborative learning. A major theme in the collaborative learning field is why some groups are more successful than others (Barron, 2003; Suthers, 2006). Lately, researchers have sought to address this issue by analyzing interaction processes in collaborative learning reasoning that human cognition is based on interactions between individuals and social context or community (Engeström, 1987). Various methods have been developed in previous research to analyze interactions. The following analytic methods have been widely used: (a) Conversation analysis (Sacks, 1962,1995), identifying closings and openings of action sequences (Zemel, Xhafa, & Stahl, 2005); (b) Social network analysis (Wasserman & Faust, 1994) , investigating patterns of interaction (de Laat, Lally, Lipponen, & Simons, 2007) and examining the response relations among participants during online discussions (Aviv, Erlich, Ravid, & Geva, 2003; De Liddo et al., 2011); (c) Content analysis (Chi, 1997), using coding schemes to categorize and count user actions to analyze argumentative knowledge construction (Weinberger & Fischer, 2006), evidence use for the knowledge building principles (van Aalst & Chan, 2007), depth of understanding (Zhang, Scardamalia, Reeve, & Richard, 2009); (d) Sequential analysis, using transitional state diagrams to compute transitional probabilities between coded discourse moves in argumentation (Jeong et al., 2011). Each method has limitations. Table 1 summarizes the analysis approaches, focus and limitations of the different methods. 
 Table.

 1. Comparison of different analysis methods. 
 At present, the most often used method is content analysis (Strijbos & Stahl, 2007). Content analysis technique is defined as “a research methodology that builds on procedures to make valid inferences from text” (Rourke, Anderson, Garrison, & Archer, 2001). The essential step of content analysis is to code discussions according to the selected coding scheme. However, different researchers put forward different coding schemes. Well-known examples include content coding schemes for the analysis of the learning process in computer conferencing (Henri, 1992), co-construction of understanding and knowledge (Zhu, 1996), the social construction of knowledge in computer conferencing (Gunawardena, 1997), the social presence in the community of inquiry (Rourke, 1999), the collaborative construction of knowledge (Veerman & Veldhuis-Diermanse, 2001; Pena-Shaff & Nicholls, 2004), the cognitive presence in community of inquiry (Garrison et al., 2001), the teaching of the community of inquiry (Anderson et al., 2001), and argumentative knowledge construction (Weinberger & Fischer, 2006). De Wever et al. (2006) compared 15 content analysis instruments from the perspective of the theoretical base, unit of analysis and inter-rater reliability and pointed out that existing analysis instruments need to be improved. Every content analysis scheme uses its own specific unit of analysis and data type. The analysis units are not identical in a variety of coding schemes, such as messages (Gunawardena et al., 1997), sentences (Fahy et al., 2001), paragraphs (Hara et al., 2000) and thematic units (Henri, 1992). The selection of the unit of analysis is very challenging for researchers. Although many researchers use “thematic unit” as the unit, the categorization standard of the “thematic unit” is very ambiguous. The complexity of interaction makes researchers use different vocabularies to code transcripts into different speech acts. For example, Fahy et al. (2001) coded transcripts into five kinds of speech acts (question, state, reflection, comment, and quote). The coding scheme developed by Pena-Shaff and Nicholls (2004) consisted of eleven kinds of speech acts (question, reply, clarification, interpretation, conflict, assertion, consensus building, judgment, reflection, support and other). Pilkington (2001) believes that coding schemes may categorize at too coarse a level to distinguish real communicative differences, or they may be too fine-grained to represent similarities. Porayska-Pomsta (2000) argues that categorizing speech acts is not useful in modeling teacher’s language and cannot account for the phenomena encounter in the dialogues. Furthermore, coding assigns each speech act an isolated meaning and does not record the indexicality of the meaning or contextual evidence (Suthers et al., 2010). In addition, the difficulty with content analyses of communications stems from a lack of guidelines for performing them validly and reliably (Rourke et al., 2001; Strijbos et al., 2006). Rourke et al. (2001) also discussed the importance of inter-rater reliability in the method of content analysis and pointed out that many researchers did not report coder reliability. Strijbos et al. (2006) believed that researchers should be cautious of the statistical test results when they did not report reliability parameters. The works of Dillenbourg (1999) and Stahl, Koschmann, and Suthers (2006) call for the need to develop process-oriented methodologies to analyze interactions. We believe that coding interaction transcripts into speech acts is very difficult because purposes of human’s speech acts are implicit; thus the identification of speech acts is very subjective. Simply focusing on the explicit speech acts will lead to ignorance of an individual’s knowledge construction. This study proposes an innovative method to analyze interactions in face-to-face collaborative learning. This method is IIS-map-based analysis method because it uses the IIS map. The whole study aims to validate the IIS-map-based analysis method that is used to analyze interactions and predict group performance. The empirical study is conducted to explore the effectiveness of the IISmap-based analysis method and to verify hypotheses. 
 Methodology: IIS-map-based analysis method. 
 Modeling and representing the collaborative learning system by IIS-map-based analysis method. 
 You (1993) believes that the instructional system is a complex non-linear system that assumes cause and effect are associated disproportionately and the whole is not simply the sum of the properties of its parts. In addition, complex systems have an “emergence” property. Emergent properties arise at a particular level of system description by virtue of the interaction of relatively simple lower-level components - but cannot be explained at this lower level (Damper, 2000). Kapur et al. (2011) believes that the group is a complex system and convergence in group discussions is an emergent behavior arising from interactions between group members. Therefore the instructional system and collaborative learning system are both complex systems with characteristics of non-linearity and emergence. The complex systems cannot be understood by only analyzing visible factors such as teaching methods, various kinds of media, etc. To deeply understand various complex pedagogy phenomena and their effects, researchers should focus on the information flow within the system and its characteristics, as well as relationships between information flows and functions of the system (Yang & Zhang, 2009). We argue that the instructional system is an abstract information system. The collaborative learning system is a subsystem of instructional system, so it is also an information system. The function of a collaborative learning system is collaborative construction of knowledge by group members. Information processing and knowledge construction are closely intertwined in the learning process (Wang et al., 2011). The cognitive processes involved in knowledge construction are selecting relevant information from what is presented, organizing selected information into a coherent representation, and integrating presented information with existing knowledge (Mayer, 1996). The interconnection of the prior knowledge with the new information can result in reorganization of the cognitive structure, which creates meaning and constructs knowledge. Learning is a generative process of constructing meaning by linking existing knowledge and incoming information (Osborne & Wittrock, 1983). Based on the theoretical foundations, we argue that the nature of knowledge construction is to encode and decode information implicitly. Therefore information makes significant contributions to knowledge construction. Accordingly, the analytic focus is identified as information flows of the collaborative learning system. The information flow is defined as the output information of group members in the interaction process. The information flows between private information owned by each individual and the information shared by group members. In order to represent and analyze the collaborative learning system, a concept model is designed (see Figure 1). In this concept model IPL denotes the information processing of learners. IPL1, IPL2, IPL3, IPL4 denote information processing of multiple learners in one group. The internal information processes of IPL are not directly observable. However, the input and output information of IPL are visible. So {X} denotes the input information of IPL and {Y} denotes the output information of IPL. Because the output information {Y} is used for the purpose of sharing information, {Y} is abstractly generalized into an information set. This abstract information set is defined as Interactional Information Set (IIS). IIS is for sharing information in the interaction process. Thus {Y} is regarded as the input information of IIS and {X} is regarded as the output information of IIS. Vygotsky (1978) argue that learning takes place inter-subjectively through social interaction before it takes place intra-subjectively. IIS is generated and formed when information are externalized and shared in the social interaction process. Therefore IIS can account for social aspects of learning. We argue that IIS can represent the outcome of internal information processing of IPL. Because knowledge is constructed through processing information implicitly, some characteristics of IIS are closely related to the quality of co-construction of knowledge. The whole collaborative learning system is a functional coupling system which consists of IPL, {X}, {Y} and IIS. 
 Figure 1. The concept model of the collaborative learning system. 
 Coding and representing the input information of IIS. 
 According to the concept model, three kinds of objects need to be represented. The first kind of object is the input information of IIS, namely {Y}. Because {X} is the input information of IPL and {Y} is the output information of IPL, {X} can be finally embodied and represented by {Y}, the analysis of {X} is unnecessary and the analysis focus is {Y}. In order to analyze the collaborative learning system, the attributes of input information of IIS need to be defined. These attributes include time, information processing of learners (IPLi), cognitive levels, information types, representation formats, knowledge network sub-map, annotation and the quality of information. Table 2 below shows the definition of each attribute. The coding format of input information items of IIS is defined as: <time><IPLi><cognitive level><information type><representation format><knowledge network sub-map> [annotation] [the quality of information]. The symbol “< >” denotes the required item and “[ ]” denotes the optional item. Information flows in the interaction process will be represented and transformed into this kind of format. 
 Table 2. The definition of each attribute. 
 Visualizing information flows into sequences of IIS. 
 In order to clearly represent information flows in the interaction process, the input information items of IIS are visualized into sequences according to the coding format. The sequences of input information items are the second kind of object that needs to be represented. Collaborators’ information items are chronologically on the timeline or under the timeline. The sequences of information items in face-to-face collaboration are linear sequences. Figure 2 shows the portion of sequential representation of IIS. The four long strands specify the portion of information sequences being shared at different time. 
 Figure 2. The portion of sequential representation of IIS. 
 The representation of IIS—— Knowledge Network Maps.
 The third kind of object need to be represented is IIS. We believe that IIS is closely related to the outcome of collaborative construction of knowledge. However, IIS is an abstract information set and it is not the real storage medium. Thus in order to represent IIS, a knowledge network map is used for visualizing knowledge and their relationships. From the beginning the knowledge network map is empty. As the interaction proceeds, the knowledge network map is gradually enlarged. Finally, it becomes the stable knowledge network map. The IIS-map is the knowledge network map with marks. The marks refer to attributes of information flows. The analysis of interactions can be reflected by the IIS-map. So this method is known as the IIS-map-based method. IIS-maps serve as abstract transcripts that can represent and visualize the interaction process. 
 The Steps of IIS-map-based analysis method: An example.
 This section illustrates the steps of the IIS-map-based analysis method by using the example of co-constructing knowledge of graphs in the data structures curriculum. The IIS-map-based analysis method is conducted in three steps: Firstly, draw an initial IIS-map according to collaborative learning tasks. In this example, collaborative learning tasks are to design the algorithm of touring the campus so as to provide services for visitors. The learning objective of this task is to acquire storage structures of a graph, the algorithm of the minimum spanning tree and the shortest path of a graph. The detailed descriptions of tasks are as follows: 1. Design a campus plan, including the south gate, Jingshi Square, gymnasium, playground, library, technology building, and art building. How many kinds of storage structures can represent this problem? Explain the relative merits respectively. 2. Provide some information such as the name, code number and introduction of each scenery spots. 3. If sewer pipes are laid between each scenic spot, please design how to lay them down at the minimum cost. 4. If a visitor wants to tour from the Jingshi Square to the art building, please design the shortest path between them; In addition, please design the second shortest path between these two scenic spots. 5. If a visitor wants to tour the library and the technology building, please design the shortest path between the Jingshi Square and the art building, passing by the library and the technology building. 6. If a visitor sets out from the south gate and tours each scenic spot only once, and then leaves via the south gate, what will be the shortest tour path? The initial IIS-map can visually represent the domain knowledge structure. Generally speaking, the initial IIS-map represents teachers’ understanding of domain knowledge. The initial IIS-map is drawn according to the knowledge modeling norm (Yang, 2010a). This norm specifies seven categories of knowledge and eighteen kinds of relationships of knowledge. For more information refers to (Yang, 2010a). Figure 3 shows the portion of the initial IIS-map, in which the node represents the knowledge and the edge represents mutual relationship of the knowledge. Of course any kind of accepted norm can be adopted to draw the initial map. 
 Figure 3. Portion of the initial IIS-map. 
 Secondly, code and segment information flows into information items sequences of IIS according to the representation format. Specifically, the segmentation is in chronological order and it is based on the change of attributes of information items. The rules for segmentation and coding information flows are as follows: 1. When contributors of information change, the information flow will be segmented. 2. When cognitive levels change, the information flow will be segmented. The definition of cognitive level originates from the theory of instructional objective classification (Yang, 2010a). Discriminating, recalling and understanding are the same level. When the level changes into “applying” the information flow will be segmented because “applying” means a higher cognitive level. 3. When types of information change, the information flow will be segmented. 4. When the knowledge network sub-map changes, the information flow will be segmented. 5. The values of representation format do not influence the segmentation of information flows because there are many representation formats in each information flow such as text, sound, graphics and body language. All information flows in the interaction process need to be coded and segmented independently by two raters according to the rules. Table 3 shows fragments of information flows that originate from face-to-face interactions of a group. We code and segment information flows of Table 3 into sequences of information items of IIS, as shown in Figure 4. 
 Table 3. Fragments of information flows. 
 Figure 4. Portion of the sequences of input information items of IIS. 
 Thirdly, compute attributes of information flows and generate the knowledge network map with marks. The marks refer to the quantity of activation. The quantity of activation is the information entropy that is generated by activating. We believe that some of the knowledge network maps are activated when information flows map the IIS. As long as the information flow has the knowledge network sub-map attribute, the knowledge is activated. The quantity of activation is an abstract attribute of information flows. It is designed to represent the quality of knowledge construction. Yang (2010b) proved that the quantity of activation of knowledge was positively correlated with learning outcomes in the context of classroom teaching. The higher the quantity of activation of knowledge is, the better learning outcomes will be. The algorithm of the quantity of activation of knowledge was designed by Yang (2010b) as formula (1): 
 (FORMULA_1).
 Where: F is an adjustable parameter. F = 1 when there is <= 7 information items between the current information item and the preceding information item that activated the current knowledge. F = (k + 1) / (2k + 1) when there are more than seven information items. The value of k is equal to the number of information items between the current information item and the preceding information item that activated the current knowledge subtracting seven. Log (d+2) denotes the activation entropy and “d” is the number of the activated edges. Log (n*(D–d+2)) denotes the complexity. “n” denotes the categories of edges that are not activated. D denotes the total number of edges that connected with the vertex. r is an adjustable parameter. r = 1 when the vertex is directly activated. r =1/d when the vertex is activated only once by its adjacent vertex. r = 1/d2 when the vertex is activated twice by its adjacent vertex. 
 The IIS-map-based analysis method aims to model and represent the collaborative learning system by information flows. It can visualize the interaction process and represent information flows onto the IIS map. The IIS-map-based analysis method regards the knowledge network map as the objective reliance body. The main content loaded by information flows is reflected and represented by the knowledge network map. This reduces the arbitrariness of coding discourse to a large extent, which makes the segmentation of information flows more objective and scientific. 
 An Empirical study.
 This empirical study aims to verify if IIS-map-based analysis method can analyze interactions and some attributes of information flows can predict the group performance. The group performance is measured as the quality of knowledge construction, which can be calculated according to formula (2): 
 (FORMULA_2).
 Where P denotes the difficulty coefficient of test items. The difficulty coefficient equals the ratio of the average score to the full mark of test items. CV denotes the coefficient of variation. It equals the percentage of the ratio of standard deviation to mean of score. Xiposttest and Xipretest respectively represent the score of pretest and posttest. N denotes the number of group members. This algorithm of group performance will be more objective in contrast to only computing the mean deviation between the pretest and posttest. 
 The definitions of attributes of information flows.
 Interactions are very complex and some groups may deviate from learning objectives in the interaction process. Therefore on the IIS-map there is the targeting knowledge network map which is composed of the targeting knowledge. The targeting knowledge network can be selected and identified by teachers according to collaborative learning objectives. In this study we focus on one of the important attributes of information flows, namely characteristics of the targeting knowledge network map. Several indicators are designed to compute the characteristics of targeting knowledge network map, including the quantity of activation, average degree, average path length, density and degree entropy of the targeting knowledge network map. 
 Quantity of activation of the targeting knowledge network map.
 Collaborative learning is different from the instruction which emphasizes the delivering of new knowledge to students. The precondition of collaborative learning is that the participants understand each other enough to accomplish their work (Stahl, 2011b). Furthermore, collaborative learning emphasizes collaborative construction of meaning among group members. Therefore the quantity of activation of the targeting knowledge network map is used for indicating the quality of knowledge co-construction. Many algorithms are tested. Finally, quantity of activation of the targeting knowledge network map is defined as the sum of quantity of activation of the targeting knowledge on the IIS map, as is shown in formula (3). 
 (FORMULA_3).
 Where Ai denotes the quantity of activation of the targeting knowledge which can be calculated according to formula (1). N denotes the number of vertices of the targeting knowledge network map. 
 Average degree.
 The average degree indicates the mean number of neighbors per vertex of the targeting knowledge network. The average degree can be calculated using formula (4): 
 (FORMULA_4).
 Where E denotes the total number of edges. N denotes the total number of vertices. 
 Average path length.
 The average path length indicates the scale of the targeting knowledge network. The average path length can be calculated using formula (5): 
 (FORMULA_5).
 Where N denotes the total number of vertices. d ij denotes the quantity of edges of the shortest path between any two vertices. 
 Density.
 Density can indicate the interconnectivity and closeness of associations of the targeting knowledge. The density of the vertex is calculated as the proportion of the number of actual edges to the number of possible edges. The density can be calculated using formula (6): 
 (FORMULA_6).
 Where Ei denotes the number of actual edges. ki denotes the degree of the vertex. N denotes the total number of vertices. 
 Degree entropy.
 The degree entropy shows the heterogeneity of targeting knowledge network. We adopt the algorithm of degree entropy in the complex network (Ferrer & Sole, 2003). It is defined as formula (7): 
 (FORMULA_7).
 Where pk is the frequency of vertices having degree “k”. We can apply these measures to provide insight into the relationship between attributes of information flows and group performance. 
 Hypotheses.
 We suppose that the following attributes of information flows can predict group performance. So five hypotheses of this study are proposed: H1: The quantity of activation of the targeting knowledge network map can predict group performance. H2: The average degree of the targeting knowledge network can predict group performance. H3: The average path length of the targeting knowledge network can predict group performance. H4: The density of the targeting knowledge network can predict group performance. H5: The degree entropy of the targeting knowledge network can predict group performance. As long as one hypothesis is verified, the effectiveness of the IIS-map-based analysis method is validated. 
 Experiment Procedure.
 The experiment used pretest posttest research design. The section presents the step-by-step process for this study: 1. Setting collaborative learning objectives and selecting some knowledge as learning objects. In this study, tasks were originated from data structures. The detailed descriptions were shown in the section of the first step of the IIS-map-based analysis method. Teachers designed the collaborative task according to the learning objectives. This task was based on a real-life authentic situation, which was not explicitly taught in the course of data structures. Meanwhile, subjective testing items of the pretest and posttest were designed according to learning objective, which could be used to evaluate group performance. The detailed descriptions of pretest and posttest are in Appendix. The targeting domain knowledge included storage structures of a graph, the algorithm of the minimum spanning tree and the shortest path of a graph. 2. Drawing an initial IIS-map of the targeting domain knowledge. The initial IIS-map was composed of targeting knowledge and their relationships. The initial IIS-map was viewed as the standard map, and it represented what teachers expected to discuss in the interaction process (Figure 3). 3. Recruiting subjects and dividing them into different groups. The experiment selected thirty groups of subjects. There were three or four students in each group. The group members were divided into each group randomly. Then the subjects were placed in different laboratory rooms and collaborated via face to face. Group members collaborated for about two hours to solve the problem and designed at least one algorithm. Each group wrote down procedures of the algorithm in the last collaborative phase as the artifact. The pretest preceded the collaboration, followed immediately by the posttest. In order to record the authentic interaction process, we videoed the entire interaction process of thirty groups, and each group formed the total of approximately 120 minutes of video file in this experiment, according to which the initial knowledge network map was modified. In the experiment, the tasks were the same for the thirty groups, as were the questions for pretest and posttest. 4. Coding, segmenting and transforming information flows into sequences of information items according to the segmentation rules, as shown in Figure 4. 5. Computing the attributes of information flows, such as the quantity of activation, average degree, average path length, density and degree entropy of the targeting knowledge network, then generating the knowledge network map with marks, as shown in Figure 5. The numbers beside the knowledge in Figure 5 are the quantity of activation and they are calculated with formula (1). Finally analyzing and interpreting the relationships between group performance and attributes of information flows.   Figure 5. Portion of IIS-map with the quantity of activation. 
 Figure 6. Screenshots of the analytic tool. 
 Analytic Tool.
 We have developed the analytic tool that can draw the initial map, segment and transform information flows into sequences and compute attributes of information flows. Figure 6 shows the screen shots of the analytic tool. 
 Inter-rater reliability.
 In this study, two raters coded and segmented information flows of thirty groups and assessed all test papers independently. One was the first author and the other was the research assistant, a graduate student majoring in computer science. The coders were trained on how to segment information flows and assess test items. After the training sessions, each rater coded information flows of thirty groups and assessed ninety-two test papers independently. In order to confirm the reliability of the segmentation and assessing the test items, the percent agreement statistic proposed by Holsti’s (1969) was used to evaluate for inter-rater reliability. The percent agreement index was most common used method in content analysis and it reflected the number of agreements per total number of coding decisions (Rourke & Anderson, 2001). The total checklist included all items of the pretest and posttest and segmented information items of thirty groups. The two coders discussed and resolved all discrepancies. Reliability coefficient was calculated and results showed all values were above 0.9, regarded as an indication of excellent agreement. 
 Result.
 In order to test the hypotheses, the correlation analysis and linear regression analysis were conducted for group performance and attributes of information flows. Table 4 shows the mean and standard deviation of group performance, quantity of activation, average degree, average path length, density and degree entropy of the targeting knowledge network. Furthermore, the relationships between group performance and attributes of information flows were examined. 
 Table4: Descriptive statistics for group performance and predictors. 
 H1 assumed that the quantity of activation of the targeting knowledge network map could predict group performance. The result indicated that quantity of activation of the targeting knowledge network was significantly positively correlated with group performance (r = .559, p = .001). To examine the predictive validity of the quantity of activation on group performance, a linear regression analysis was conducted. The normal Q-Q plot was used to test normality of data. This test confirmed that the group performance variable had normal data. Consistent with hypothesis 1, the quantity of activation of the targeting knowledge network could predict group performance (t = 3.565, β = .559, p = .001). The quantity of activation could explain 28.8% of the total variance (Adjusted R2 =.288, F = 12.712). This indicates that the quantity of activation of the targeting knowledge network map is the significant predictor. The main reason is that the quantity of activation of the targeting knowledge network map can measure the semantic relationships and deep structure of knowledge network map. H2 assumed that the average degree of the targeting knowledge network could predict group performance. However, the data did not support this hypothesis: average degree was not correlated with group performance (r = .063, p = .739). So the average degree cannot predict group performance. The reason for this result is that average degree cannot represent the complexity of domain knowledge. 
 H3 assumed that the average path length of the targeting knowledge network could predict group performance. However, the data did not support this hypothesis: the average path length was not correlated with group performance (r = .324, p = .081). So the average path length cannot predict group performance. The major reason is that the average degree is a simple indicator for the size of knowledge network map. H4 assumed that the density of the targeting knowledge network could predict group performance. However, the data did not support this hypothesis: the density was not correlated with group performance (r = .233, p =.215). So the density cannot predict group performance. The reason for this finding is that density cannot represent the deeper understanding of subject matter and semantic level. H5 assumed that the degree entropy of the targeting knowledge network could predict group performance. However, the data did not support this hypothesis: the degree entropy was not correlated with group performance (r = -.252, p = .180). So degree entropy cannot predict group performance. The reason for this is that degree entropy only indicates the static typology structure of knowledge network map. 
 Discussion.
 The results of this study show that quantity of activation of the targeting knowledge network map can effectively predict group performance. This study also verifies that the IIS-map-based analysis method can analyze interactions in face-to-face collaborative learning. However, the results indicate that other four kinds of attributes including average degree, average path length, density and degree entropy of the targeting knowledge network cannot predict group performance. In fact these attributes can only reflect topological structures of the targeting knowledge network. They are static characteristics of the targeting knowledge network and cannot represent adequately complex knowledge structures. By contrast, quantity of activation can reflect the dynamic features of interactions. In formula (1) “d” denotes the number of the activated edge and it dynamically changes over time. So quantity of activation of the targeting knowledge network map can effectively predict group performance. The implication of this result for collaborative learning is that teachers can predict which group is more successful by computing the quantity of activation of different groups. The higher the quantity of activation, the better group performance will be. The quantity of activation is one of the attributes of information flows, which can assess group performance. This attribute can be obtained without testing and it can be calculated by the analytic tool. The IIS-map-based analysis method with the objective of seeking objectivity and reliability has a systematic approach to analyzing interactions. The collaborative learning system is considered as the complex information system. The information flow of this system is the main concern. IIS is an abstract generalization of an output sharing information set. Learners need to access information to acquire content knowledge and formulate hypotheses (Jonassen, 1999). So information is very important for knowledge gains. The IIS-map-based method aims to analyze the level of social construction of knowledge reflected by information flows. The approach of the IIS-map-based analysis method is different from previous studies. It sets aside some factors, such as attitudes, motivation, entry skills of learners, types of tasks, a variety of interactive strategies and various media or tools, which are classified as environmental factors of collaborative learning systems. On the contrary, information flows are identified as the analysis focus in this approach. Stahl (2011a) believe that sequentiality and timing play an important role in how the postings are understood. The IIS-map-based analysis method highlights the temporal sequentiality by segmenting information flows in chronological order. It can fully convey the dynamic nature of interactions. So this method is of great significance for understanding the complicated interactions and knowledge construction. The basic step of the IIS-map-based analysis method is to construct the IIS-map. Drawing the initial IIS-map needs to follow the accepted norm. Identifying the knowledge and constructing the correct relationships are the vital procedure. Of course, part of the initial IIS-map is likely revised when information flows of interaction process are inconsistent with the initial IIS-map. In addition, the test items that are used to test learning outcomes should be subjective items so as to reflect the cognitive process. The targeting knowledge need to be identified according to learning objectives that have been set up when designing collaborative tasks. The sample of the IIS-map-based analysis method is not participants, but the targeting knowledge network map. One group’s interactive information flows generate one targeting knowledge network map. This study aims to analyze relationships between characteristics of targeting knowledge network map and learning outcome of the targeting knowledge network. This means that the IIS-map-based analysis method focuses on analyzing different attributes of the same object. We believe that this is more scientific than analyzing some attributes of different objects such as the learning outcomes of students and use of media. Barron (2003) analyzed interactions of the twelve sixth grade triads by coding discourse into three kinds of speech acts (acceptance, discussion and rejection or ignorance). He drew a conclusion that less successful groups ignored or rejected correct proposals, whereas more successful groups discussed or accepted them. We examined the method of coding transcripts into speech acts using the data we collected. We selected fifteen more successful groups and fifteen less successful groups. The discourse transcripts were also coded into the same kinds of speech acts (acceptance, discussion and rejection or ignorance) by two coders. The reliability for acceptance, discussion and rejection or ignorance responses was 0.7, 0.75 and 0.68 respectively by computing Cohen’s (1960) Kappa. The result indicated that the more successful groups and less successful groups had no significant difference in accepting (t = .258, p = 0.798), discussing (t = -1.371, p = 0.181) and rejecting or ignoring (t = .744, p = 0.463). This result also proved that the method of coding transcripts into speech acts cannot distinguish between less successful groups and more successful groups. So Barron’s conclusion was not supported by our examination. When coding discourse we experienced the strong subjectivity and arbitrariness because coding was often based on subjective and borderline judgments and there was no objective reference. So it was easy to lead to the arbitrary interpretation and the low reliability conclusion. In contrast to the approach of coding discourse into speech acts, the IIS-map-based analysis method is more objective and has a stronger predictive power. The essence of the IIS-map-based analysis method is to map information flows that are represented by natural language onto IIS-maps. The IIS-map serves as the frame of reference when information flows are segmented. This will help to diminish the subjectivity of coding and segmentation to a large extent so as to draw a reliable conclusion. The IIS-map with marks can distinctly reveal the characteristics of the collaborative learning system. The IIS-map-based analysis method can highlight the contribution of communicating analysis to learning analytics. This study has several limitations. First, we only verify that the quantity of activation of the targeting knowledge network map can predict group performance. Other attributes of information flows will be explored in the future. Second, the sample is small and it originates from thirty group’s interactions. There is mainly one category of knowledge in this study. Following studies will explore more categories of knowledge and enlarge samples. Further, the follow-up study will explore how to represent and calculate emotions, attitudes and values on the IIS-map. 
 Conclusion.
 To sum up, the IIS-map-based method can support the analysis of interactions, and the quantity of activation of the targeting knowledge network map can predict group performance. Results of this study further indicate that the IISmap-based method is an effective method for interaction analysis. The IIS-map-based method is a methodology for explaining a process-oriented account of group interactions. This method can provide insight into how collaborative learning takes place empirically. It is relatively objective because the IIS-map is the reference object of segmentation. Thus, the arbitrariness of coding discourse can be diminished in order to ensure the objectivity of segmentation of information flows. In addition, the quantity of activation can be calculated objectively. There are multiple benefits to the IIS-map-based method as an analysis instrument. First, the IIS-map-based analysis method is useful for visualizing the interaction process and aiding the understanding and analysis of interactions. It is a useable and replicable instrument in collaborative learning. Second, an IIS map can unify data and has been used to support multiple analytic practices. The data represented by IIS-map are brief in revealing the characteristics of the collaborative learning system. Third, the IIS-map-based analysis method can directly examine the interaction process to predict group performance without traditional tests. The IIS-map-based analysis method removes the subjectivity and arbitrary nature of value judgments when coding discussions and gives fully detailed analysis of interactions. This method will become a new analysis approach with high objectivity and strong feasibility. It will be widely applicable in the future. 
 Acknowledgement.
 This study was supported by the Twelfth Five-Year Plan of the National Educational Science in China (ECA110330). Many thanks for anonymous reviewers for their helpful comments and suggestions.]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Analyzing Interactions by an IIS-Map-Based Method in Face-to-Face Collaborative Learning: An Empirical Study</rdfs:label>
		<dc:subject>collaborative learning</dc:subject>
		<dc:subject>iis-map-based analysis method</dc:subject>
		<dc:subject>interaction analysis</dc:subject>
		<dc:subject>information flows</dc:subject>
		<dc:subject>knowledge construction</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/lanqin-zheng"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/lanqin-zheng"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kaicheng-yang"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kaicheng-yang"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ronghuai-huang"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ronghuai-huang"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/68/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/lanqin-zheng"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/kaicheng-yang"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/ronghuai-huang"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/69">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>Numbers Are Not Enough. Why e-Learning Analytics Failed to Inform an Institutional Strategic Plan</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/69/authorlist"/>
		<swrc:abstract>Learning analytics offers higher education valuable insights that can inform strategic decision-making regarding resource allocation for educational excellence. Research demonstrates that learning management systems (LMSs) can increase student sense of community, support learning communities and enhance student engagement and success, and LMSs have therefore become core enterprise component in many universities. We were invited to undertake a current state analysis of enterprise LMS use in a large research-intensive university, to provide data to inform and guide an LMS review and strategic planning process. Using a new e-learning analytics platform, combined with data visualization and participant observation, we prepared a detailed snapshot of current LMS use patterns and trends and their relationship to student learning outcomes. This paper presents selected data from this “current state analysis” and comments on what it reveals about the comparative effectiveness of this institution’s LMS integration in the service of learning and teaching. More critically, it discusses the reality that the institutional planning process was nonetheless dominated by technical concerns, and made little use of the intelligence revealed by the analytics process. To explain this phenomenon we consider theories of change management and resistance to innovation, and argue that to have meaningful impact, learning analytics proponents must also delve into the socio-technical sphere to ensure that learning analytics data are presented to those involved in strategic institutional planning in ways that have the power to motivate organizational adoption and cultural change.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 The promise of learning analytics.
 Learning analytics employs sophisticated analytic tools and processes in investigation and visualization of large institutional data sets, in the service of improving learning and education (Brown, 2011; Buckingham Shum & Ferguson, 2011). Building on the demonstrated strategic advantages of “business analytics” in the corporate world, learning analytics also draws on the related fields of web analytics, academic analytics (Goldstein & Katz, 2005), educational data mining (see Romero & Ventura (2010) for review) and action analytics (Norris, Baer, Leonard, Pugliese, & Lefrere, 2008) to support decision-making and strategic planning in academic settings. “Academic analytics” approaches are typically applied in educational settings to address administrative and operational concerns such as “advancement/fundraising, business and finance, budget and planning, institutional research, human resources, research administration, and academic affairs” (Fritz, 2011). Projects undertaken under the auspices of “learning analytics” extend the potential of analytics to the level of individual learning, by selecting, capturing and interpreting data on teaching and learning activities, with the goal of improving teaching and learning outcomes. Institutions and senior administrators are key users and stakeholders, and enhancement of institutional decisionmaking processes and resource allocation are core objectives (Romero & Ventura, 2010). In the postmodern context of constant and dynamic change in higher education and technological innovation, then, learning analytics offers higher education institutions a valuable tool in their ongoing efforts to select actions that are “achievable within the capacity of the organization to absorb change and resource constraints” (Kavanagh & Ashkanasy, 2006). 
 The importance of strategic investment in learning technologies and e-learning 
 In this paper, we present a learning analytics case study of LMS implementation and use in a large research-intensive university that routinely ranks within the top five in national magazine league tables (Macleans, 2010). The institution achieves high annual scores in the presage variables compiled in such rankings: institutional resources, research funding and reputation. We know from meta-analytic studies of decades of available data, however, that the quality of education offered by an institution is not predicted by the size of institutional budgets, numbers of or dollar values of research awards, or even by measures such as student: faculty ratios or “hours spent in class.” Instead, the best institutional predictors of educational gain are “measures of educational process: what institutions do with their resources to make the most of whatever students they have” (Gibbs, 2010, p. 2). Citing a major 2004 study (Gansemer-Topf, Saunders, Schuh, & Shelley, 2004), Gibbs (2010) argues that the feature that distinguishes effective institutions from less effective schools is their strategic use of available funding to support “a campus ethos devoted to student success” (p. 14). In other words, decision-making processes relating to organization of institutional resources – human and material – and planning for more effective use of existing resources are a critical feature of excellent institutions. Within the teaching context, Gibbs argues that the most significant predictors of educational gain “concern a small range of fairly well understood pedagogical practices that engender student engagement” (2010, p. 5). At least a decade of research and writing has demonstrated that learning technologies, when used appropriately, can help educators adopt the “seven principles of good practice in undergraduate education” (Chickering & Gamson, 1987) and improve the overall quality of an institution’s educational delivery (Chickering & Ehrmann, 2002). Moreover, recent work in the field of learning analytics has demonstrated that the communicative affordances of ICTs and learning management systems (LMSs) can increase student sense of community (Dawson, 2006) support learning communities and enhance student engagement (Dawson, Burnett, & O'Donohue, 2006; Dawson, Heathcote, & Poole, 2010; Macfadyen & Dawson, 2010). The teaching climate within higher education is becoming increasingly complex. Student enrollment numbers continue to rise (Patrick & Gaële, 2007) and universities are catering to an increasingly diverse student body (living far from campus, studying part-time, returning to education after a long break or juggling the demands of study with career or family life (OECD, 2008; Twigg, 1994)). In this context, learning tools that support and enhance student engagement with peers, instructors and learning materials have become essential enterprise resources. It should be no surprise then, that, like 93% of US-based higher education institutions (Campus Computing, 2010), the institution in this case study has invested heavily in the campus-wide implementation of a web-based LMS since the late 1990s. The institution also hosts and supports a number of additional learning technology platforms (e.g., WordPress, MediaWiki), and subscribes to others (e.g., Turnitin). The LMS is therefore embedded within a wider network of platforms and systems involved in supporting the teaching and learning mission, and is viewed as a core component of the university’s teaching infrastructure. 
 The catalyst for change.
 Given this university’s substantial investment in an institutional LMS, and the ever-evolving market in LMSs and learning technologies, as well as shifting economic conditions (Campus Computing, 2010), strategic decisionmaking and forward planning regarding technology choices and related resource allocation are clearly of the essence. Reviews of available learning technologies have routinely been undertaken as part of the institution’s standard quality assurance practices. These reviews have aimed to evaluate the current state of use, to ensure that the suite of adopted technologies reflect the broader learning and teaching mandate and to ensure that the university is deploying its limited resources most effectively to support learning and teaching. In addition, in 2010, a further catalyst for a new LMS review was the LMS vendor’s announcement that the current LMS product would not be supported after 2013. Together, these conditions generated the necessary impetus for the next round of institutional review, with the goal of selecting as the next enterprise LMS the product that would best support the university’s teaching and learning goals. In his well-established eight-step change model, Kotter (1996) notes that the critical first step in effectively managing change is one that creates a sense of urgency and the necessary levels of motivation required for sustaining the change process. This step involves a careful examination of the current context, to allow identification of potential “threats” and opportunities, and envisioning of future scenarios. In this light, the application of learning analytics focused on the current state of LMS integration presented an opportunity to understand the specific teaching and learning context, and develop a strategic vision and operational pathway for continual improvement. 
 Employing e-learning analytics to undertake a current state analysis 
 Gathering as much data as possible regarding current LMS usage across a university is no small feat, and is particularly challenging in such a highly decentralized institution as the one under study here, which comprises numerous Faculties/Divisions. Some LMSs do capture and store large amounts of course and user activity and interaction data. Until recently, however, investigators have only been able to access, aggregate, analyze, visualize and interpret this data via slow and cumbersome manual processes. While the majority of commercial and open source LMS are rapidly recognizing the importance for integrating sophisticated learning analytics, the associated reporting functionality is still largely under development (Dawson, McWilliam, & Tan, 2008; Mazza & Dimitrova, 2007). To overcome the challenge of poor analytics functionality in the current LMS, the university considered here has partnered with an analytics software company to customize and implement an analytics reporting tool that allows extraction, analysis and dis/aggregation of detailed information about uptake and use of the enterprise LMS. We made use of this analytics platform to carry out the requested “current state analysis” of LMS usage, and to seek answers to questions about the extent and complexity of LMS adoption. This analysis was undertaken with the goal of informing and guiding the institution’s campus-wide strategic planning process for learning technology and LMS integration. The availability of the new e-learning analytics platform allowed us to undertake, on the university’s behalf, the most comprehensive examination of its LMS use to date. The process revealed, and will continue to reveal, an array of LMS use patterns and practices, finally allowing the university to “know itself” in terms of learning technology uptake and integration in the service of teaching and learning. We hoped that this e-learning analytics exercise would provide compelling data that would generate the sense of urgency necessary for motivating broad scale institutional change associated with learning, teaching and technology. Through participant observation in the review and planning process we were able to investigate the degree to which the e-learning intelligence revealed influenced institutional decision-making. In this paper we present selected examples of data from the analysis. More critically, we discuss the reality that the data developed in this e-learning analytics process did not significantly inform subsequent strategic decision-making and visioning processes, and consider some of the factors that may have limited its impact. 
 Approach and tools.
 Ethics and privacy.
 There are very real concerns about ethics and information privacy issues relating to the collection, analysis and dissemination of data on student, faculty and staff online activity and on student achievement and demographics. For this reason, our approach is informed by the institution’s policies on research involving human subjects, and the TriCouncil Policy Statement: Ethical Conduct for Research Involving Humans (“TCPS”) (Government of Canada, 2010). These policies mandate an ethical review process for research projects that involve human subjects. Furthermore, data that is gathered through institutional research is subject to the provisions of the Freedom of Information and Protection of Privacy Act (Government of British Columbia, 2012) (whereas data generated in the course of “traditional academic research” is under the control of the individual faculty members involved and exempted under section 3(1)(e) of the Act.). This research complies with all stated policies and in accordance with FIPPA, our data is protected. Data access is limited to a small number of research investigators; raw data is maintained on secure data servers only; and all individual identifiers are removed from any disseminated data or analysis. 
 Selection of data.
 LMS usage data was mined for a single academic year (2009-2010). Only data for credit-bearing courses was examined (LMS adoption by the institution’s various continuing education units for non-credit programming, and usage by non-teaching units, was excluded, as well as numerous non-credit LMS-based online “training and orientation” modules created by Faculties and support units). Data on the total number of courses offered across the institution in the 2009-2010 academic year was prepared by manual analysis of course section listings exported from an institutional database listing all sections available for student registration in 2009-2010. Sections listed as summer courses, registration placeholders for overseas exchange courses, or registration placeholder course numbers for students completing Masters or Doctoral theses were removed, before determining section counts and statistics. The following section outlines some of the analyses that were undertaken. 
 Analytics and data visualization tools.
 E-learning data presented in this paper was extracted, collated and analyzed using an e-learning analytics platform based on MicroStrategy Business Intelligence software that allows development of customized reports investigating user, course, Department, Faculty and institutional metrics during time periods of interest. User interaction data within the institutional network can be readily captured, categorized, and analyzed. These large data sets can be further interrogated to identify patterns of user-behaviour that can inform teaching and learning practice. The software makes use of a pre-built enterprise data warehouse, optimized for use by educational organizations, with common, conformed dimensions, enabling cross-platform intelligence. At the institution under study here, the analytics platform has been configured to pull course identifier data and student grades from the institution’s student information system, LMS data from LMS tracking and meta-data (captured on production servers). Data is presented to end users via a web interface with extensive report authoring, ad hoc querying, data analysis and report distribution capabilities. Selected data was also visualized using Tableau Desktop 6.1 data visualization software. (For more information http://www.tableausoftware.com/) 
 Participant observation.
 To investigate the subsequent impact of our e-learning analytics reporting on institutional decision-making processes in relation to LMS selection and learning technology planning, we undertook a longitudinal participant observation process (Douglas, 1976). In participant observation, investigators are also subjects. This qualitative research methodology typically involves direct observation, participation in the life of the group, collective discussions, and analysis of documents produced within the group. It is usually undertaken over an extended period of time, ranging from several months to many years. The institution in question hosts a standing advisory committee on learning technologies that comprises at least 35 representatives from across its academic, information technology and learning technology units, and is jointly chaired by senior figures in academic affairs and information technology. We participated in and observed the activities and collective discussions of the committee over a period of approximately 18 months, during which time it was tasked with evaluating current usage of the institutions LMS and other tools, and development of a vision, roadmap and plan for the institution’s next generation learning technology environment. We also undertook review and analysis of public and private process documents developed by the committee. The lead author of this work participated with ‘observer status’ only, and played no role in decision-making processes. 
 Selected findings.
 Institutional data indicated that in 2009-2010 academic year, a total of 18,909 course sections were offered (of which 14,201 were undergraduate course sections). This total includes 388 distance learning sections, of which 304 (1.6% of total sections) were offered in fully online format, and 84 in print-based format. 
 Numbers of courses and students using the enterprise LMS.
 Assessment of the proportion and characteristics of course sections implementing the LMS provides a sound indication of broad-scale institutional adoption rates and overall diversity of adoption across year levels and class sizes. In this instance, 21% (3,905) of all course sections had an associated LMS course site. Based on institutional staffing and student enrollment figures for 2009-2010: - 80.3% of all students were enrolled in at least one LMS-supported course during the 2009-2010 academic year (total student enrollment: 52,917) - Most LMS-supported sections (61%) were employed for medium-sized course sections of 15-79 students. A further 22% of sections were employed for large classes of 80+ students. - 1,118 instructors or roughly 30% of all teaching staff used the LMS for instructional purposes (total teaching staff of 3,061, including part-time and full-time Professors; Associate, and Assistant Professors; lecturers; instructors; and clinical, visiting, adjunct and emeritus Faculty). The institution’s LMS is currently used by courses across all year levels (1st-4th year undergraduate courses, as well as graduate level courses), with roughly 14% of lower level course sections and graduate course sections, and 25% of upper level course sections making use of the LMS. Across the undergraduate years, numbers of unique student users are similar (ranging from 12,000-19,000 unique student users), demonstrating that in upper level (3rd and 4th year courses) the LMS is, on average, being used to support smaller course sections than at the lower level. While fully online courses represent only

 1.6% of course section offerings in 2009-2010, 4,661 students or 11% of the total enrollment completed at least one fully online course during this period. 
 LMS user time online.
 User time online within LMS-supported course sites varied immensely by user role (designer, instructor, teaching assistant, student), by Faculty, by Department, and by course mode (fully online versus LMS-supported). Table 1 shows comparative average user time online per term for LMS-supported and fully online courses. 
 Table 1. Comparative user time online by role for LMS-supported and fully online course sections, 2009-2010. 
 Average user time figures mask real variation between Faculties, Departments and even individual course sections. Students in LMS-supported courses in the Faculty of Arts, for example, spent an average of 7 ± 6 hours per course section using LMS-based course resources, while students in the Faculty of Agriculture spent an average of 16 ± 15 hours online per course section. Similarly, instructor time online varied tremendously, even when courses were taught in a fully online modality. Examination of instructor data for fully online courses shows a range of instructor time online from 61 ± 261 hours per section at the high end, to 5 ± 4 hours per section at the low end in the 2009-2010 academic year. 
 What learners are doing online.
 Measures of “average time online” using an LMS is a crude indicator of student (or instructor) time investment in teaching and learning. In order to further unpack what students are doing while logged in to LMS-based course sites, we investigated data on LMS tool use. The current institutional LMS offers instructors and students a range of tools for presenting learning materials, communication, collaborative work, assessment and administrative tasks. In addition, a number of web-based products and services (Turnitin, MediaWiki, the Wimba suite of tools) offer “plugins” – known as Powerlinks – that allow their integration into an LMS-based course. Assessment of LMS tool “presence” in LMS-supported course sections during the period of interest shows that the standard suite of LMS tools are typically implemented (i.e., available for use) as well as a number of Powerlinks (data not shown). However, a more nuanced representation of LMS tool use is provided in Figure 1, which illustrates average actual tool use time per student (measured in minutes) for all available tools and Powerlinks in the 2009-2010 session. 
 Figure 1. Student usage of LMS tools shown as minutes of use time per student enrolled in LMS-supported course(s), 2009-2010. 
 Figure 2. Overall composition of the institution’s LMS-based course content, represented as relative numbers of each file type. 
 To explore the nature of actual learning materials (i.e., course content) we investigated which file types are contained in the entire LMS course content database. The diversity and proportion of file types is represented in Figure 2. This data can provide insight into the types of learning and teaching approaches adopted for a particular course, or can allow a more generalized assessment across a Department or Faculty. 
 Figure 3. Relative distribution of average student time per “learning activity category”, by Faculty, 2009-2010. 
 The aggregation of tool use data based on tool “purpose” provides an effective method for interpreting the broad pedagogical intention of online learning materials and activities. Dawson (Dawson et al., 2008) has previously proposed that LMS tools can be broadly organized into four categories representing the core activities within LMSsupported and online courses: - Engagement with learning community - Working with content - Assessment - Administrative tasks 
 Table 2. LMS tools assigned to “learning activity categories”. 
 Figure 4. Correlation between student achievement and selected LMS tool use frequency in LMS-supported course sections. 
 This categorization offers a useful approach for interpreting LMS tool use data, especially in light of increasing evidence that student engagement with peers in a learning community has the strongest positive effect on learning success (Astin, 1993; Light, 2001; Macfadyen & Dawson, 2010; Tinto, 1998). Table 2 outlines our assignment of currently available LMS tools into these learning activity categories. Figure 3 represents average learner time using each tool category in LMS-enabled course sections proportionately for each Faculty, regardless of absolute time use figures. This allows easy comparison of relative tool category use time per student between Faculties. LMS tracking data and student grade data for 95,132 undergraduate student enrollments in LMS-supported courses was merged, visualized and analyzed using Tableau 6.1. Student grades were binned into deciles and best fit lines determined. Correlation coefficients for the selected LMS activities shown here with binned student final grade are as follows: number of discussion messages posted, r = .83, p<.01; number of discussion messages read, r =.95, p<.0001, number of discussion replies posted, r =.94, p<.0001); number of content pages viewed (0.89, p<.001); number of visits to the “My Grades” tool (0.93, p<.0001). Subsequent data analysis confirmed and extended our earlier reporting of significant correlation between student learning outcomes (as represented by student final grade in the relevant course) and their use of engagement tools (discussions, mail) in fully online courses (Macfadyen & Dawson, 2010). Visualization of LMS use data for LMSsupported classroom-based courses again shows significant positive correlation between student participation in course-based discussions and their final grade (for number of discussion messages posted, r = .83, p<.01; for number of discussion messages read, r =.95, p<.0001, and for number of discussion replies posted, r =.94, p<.0001). A significant positive correlation with final grade is also observed with student use of LMS-based course content materials (0.89, p<.001), and also, most interestingly, with student visits to the “My Grades” tool (0.93, p<.0001) that allows students to monitor their own progress (Figure 4). 
 Outcomes of participant observation.
 The institution’s standing committee on learning technologies convened monthly during 2010 and 2011, with the goal of developing a vision, roadmap and plan for selection of the new institutional LMS. A detailed analytics report on the current state of implementation and use of the institution’s existing LMS was presented at an early stage in this process. Subsequently, meeting minutes and reports on later stages of decision-making were made available to the university community (data not shown). From review of these documents, and from participation in continuing committee discussions, we observed that although completion of the current state analysis was noted, no further references to or interpretations of the findings were made in later meetings or documentation. 
 Discussion and implications.
 Benchmarking the institution’s LMS usage.
 This e-learning analytics case study revealed multiple layers of data that can be re-purposed, aggregated and analyzed in new ways. By revealing details of actual LMS use patterns and their relationship to student learning outcomes, these data not only re-emphasize the value of the LMS in supporting student learning at the institution, but offer benchmarks by which the institution can measure its LMS integration both over time, and against comparable organizations. The 2010 Campus Computing Survey (Campus Computing, 2010) reports that US public universities now make use of an institutional LMS in delivery of an average of 60% of their course sections, suggesting that LMS uptake in the university under study here, at only 21% of course sections, is comparatively low. Similarly, a 2007 study (Allen, Seaman, & Garrett, 2007) reported that US higher education institutions offered an average of 10.6% (median 5%) of their course sections in fully online mode, and that this number appears to be increasing rapidly as institutions further embrace distance and flexible models of education. In this case study, the institution’s small offering of online courses (1.6% of total course sections) again indicates a low level of penetration. Furthermore, at least 70% of teaching staff did not make any use of the institutional LMS during the 2009-2010 academic year. While instructor use of the LMS may therefore be considered to be in the early adoption phase (Rogers, 1995), students are heavily exposed to the LMS, confirming the impression that the LMS is a core component of the institution’s overall learning experience. The vast majority (>80%) of students were enrolled in at least one course that made use of the LMS in 2009-2010, and 11% of students completed at least one fully online course. This latter figure, in particular, suggests that the student cohort is beginning to recognize the strategic advantages of online courses as they plan their timetables, meet program requirements and attempt to manage the time demands of work, study and commuting to campus. Workload and “time online” are core issues for teaching staff, and can also be significant areas of concern for students. With particular relevance to online courses, in which almost all courserelated activity is mediated by the LMS, data show a very wide range of total student engagement time with peers and course materials across different disciplines. These data begin to provide lead indicators of the appropriateness of the course load as a result of the implemented learning activities. A more detailed understanding of what, exactly, is occupying student time in LMS-supported course sites provides a more meaningful representation of how an LMS is being used, and therefore the degree to which LMS use complements effective pedagogical strategies. Contemporary educational theorists emphasize the importance of peer to peer interaction for facilitating the learning process (Astin, 1993; Chickering & Gamson, 1987; Light, 2001; Tinto, 1998). Nevertheless, when we examine the categories of activity that are occupying student time in LMS-supported course sites, it is clear that across the institution (and regardless of course mode), the dominant use of the LMS is for content delivery. This observation is further supported by the number of static text files contained within the LMS (Figure 2, text and pdf files). Adoption of technological innovations in a manner that simply replicates existing hegemonic practice (Reiser, 2007) is not limited to LMSs. Such “first stage” adoption appears to allow a familiarization phase, before broader innovations can be undertaken. It is only at this later innovation stage that learning technologies will be fully utilized to support a pedagogical practice of engagement that will significantly enhance the overall student learning experience. However, this will also necessitate the kind of cultural changes described by McWilliam (2005). A wealth of literature describes the enriched learning possibilities permitted by such a shift. In relation to LMS functionality, while more than half of all LMS-supported courses at the case study university implemented a common suite of tools, mining the data on actual student use time for tools assists with overall interpretation for informed action. The current LMS tools that support online discussions, presentation and organization of course content, and assessment activities (usually quizzes) are the only tools which are heavily used. It can be argued that use of some LMS tools simply require little time investment – for example, tools that allows students to read a quick announcement, check their grades, or upload assignments. It is clear, however, that a range of available tools that could be used to increase student engagement and collaboration (MediaWiki, the Wimba suite of voice and video tools) remain poorly utilized. Further investigations are required to better understand why and how the adoption of these resources can be improved. Together, these findings indicate that the institution has some distance to go in maximizing effective and strategic use of its enterprise LMS. 
 Informing strategic planning?.
 Although the data gathered in this case study analysis suggest that the potential use of the enterprise LMS is yet to be realized at this institution, it nevertheless confirms that the LMS is central to the student learning experience – a reality that should highlight the importance of careful planning for future learning technology uptake. The mandate of the standing committee on learning technology at this university is that it will support the institution's teaching and learning mission by assisting in the development of a campus-wide vision for technology use, and will lead the planning process for technology implementation. It is the only group explicitly tasked with integrating technology with the institution’s learning and teaching mission. With this in mind, it might seem surprising that subsequent steps in the institutional LMS review process did not appear to incorporate or build upon the intelligence revealed by this learning analytics exercise. While this committee might be considered to be the “powerful coalition” that Kotter (1996) identifies as a key actor in motivating and managing successful change, their subsequent discussions and deliberations did not include any critical consideration of current LMS use patterns in the development of a vision and strategic plan. Presentation of data indicating apparent correlations between student online engagement and student achievement did not catalyze debate about the pedagogical benefits of technology, or about whether the institution as a whole appears to be making best use of available learning technologies. In essence, the findings derived from the learning analytics process failed to generate the sense of urgency or motivation for change as it related to technology adoption within the institution. Diverse approaches to change management and leadership (see, for example, literature cited in Kavanagh & Ashkanasy (2006)) agree that development of an organizational vision, and a strategy by which to reach it, is a critical step. In this case study, learning analytics offered the institution a means of measuring its current state and future progress towards an institutional vision for teaching and learning with technology. However, through participant observation of committees responsible for moving the institutional LMS review and selection process forward, we noted that subsequent deliberations and decision-making focused almost exclusively on technical questions relating to “ease of migration.” Critical interpretation of the implications of data describing the institution’s current LMS use was almost entirely absent. These observations are reflected in public and private reports documenting the committee’s activities (not shown). While there is an obvious imperative to ensure that any new enterprise technology is functional, scalable and reliable, an exclusive focus on technology integration issues, in the absence of development of a pedagogical vision, quickly neutralizes the likelihood that learning analytics data may catalyze organizational change with a focus on the student experience and learning outcomes. A focus on technological issues merely generates “urgency” around technical systems and integration concerns, and fails to address the complexities and challenges of institutional culture and change. The e-learning analytics data generated in this case study clearly demonstrate that some substantial changes are needed in order to better facilitate adoption and integration of learning technologies into daily curricular activities and support the ethos of student success to which the institution aspires. Recalling Gibbs’ (Gibbs, 2010) assertion, this institution already possesses the potential human, financial and technological resources (whichever new LMS it selects) to improve the quality of the education it offers. What will determine whether it succeeds or fails in this effort will be its ability to develop a clear vision for learning technologies and lead the cultural change that reaching it requires. Simple availability of new knowledge made available through e-learning analytics has, however, failed to influence institutional planning in this regard, and has failed to inform development of a strategic vision for learning technology at this institution. Interestingly, this mismatch between opportunity and implementation may be more widespread than enthusiastic analytics literature suggests. In their 2005 review of 380 institutions that had successfully implemented analytics, Goldstein & Katz (2005) note that analytics approaches have overwhelmingly been employed thus far “to identify students who are the strongest prospects for admission…[and]…to identify students who may be at risk academically” – that is, to improve enrollment and retention, rather than for institutional strategic planning. Similarly a recent survey of literature on implementation of educational data mining found that only a small minority of these report on the application of EDM to institutional planning processes (Romero & Ventura, 2010). 
 Why numbers are not enough.
 Why is it that the output of powerful learning analytics reporting processes, acknowledged by institutional leaders as giving new insight into organizational patterns and practices, fail to influence institutional planning and strategic decision-making processes? We suggest here that this may be the result of lack of attention to institutional culture within higher education, lack of understanding of the degree to which individuals and cultures resist innovation and change, and lack of understanding of approaches to motivating social and cultural change. Although social systems such as educational institutions do evolve and change over time, they are inherently resistant to change and designed to neutralize the impact of attempts to bring about change (Kavanagh & Ashkanasy, 2006). This reality is reflected in Rogers’ theory of diffusion of innovation (1995), which attempts to model the factors that determine the adoption rate of (or, conversely, resistance to) new innovations. This model integrates variables at the level of the individual with variables introduced by the nature of the social system in question. 
 Perceived attributes of an innovation.
 Even if senior management and scattered individuals recognize the need for institutional change in order to better integrate technological innovations into teaching and learning, no vision or plan will emerge or be embraced without the support of faculty and staff (Bates, 2000). Indeed, numerous writers have noted that a firm resistance to the changes that may be created by integration of e-learning must be expected (see Levy, 2003, and references therein). Rogers’ theory emphasizes the ways in which individuals will assess and resist proposed innovations according to the perceived attributes. Overwhelmingly, an individual’s reaction to change reflects their cognitive evaluation of the way in which a new event or context will affect their personal wellbeing (Lazarus & Folkman, 1984). When change is proposed, individuals will assess it situationally for its “relative advantage”: the degree to which change may offer something “better” than the current state. They will assess it for “compatibility”: the degree to which it is consistent with existing practice and values, and with needs of potential adopters. And they will assess it for “complexity”: the degree to which it is perceived to be difficult to understand or to use (Rogers, 1995). Concerns surrounding academic workload have been commonly cited as reasons for a lack of adoption (Bates, 200; Levy, 2003; Macfadyen, 2004). For instance, faculty may view the introduction of technologies into teaching as a time-consuming imposition, as something that diverts them from current research and teaching activities, or as antithetical to the current institutional culture. Faculty and staff may see technology as bringing an extra (and unpaid) workload. Moreover, the potential for learning technologies to enhance teaching and learning may be poorly understood and incongruent with individual perceptions and beliefs surrounding good teaching practice. In particular, faculty may worry that spending time on technology will actually hamper their career due to poor evaluations of teaching. Such concerns are not without foundation: academic culture still rewards faculty for verifiable teaching expertise, publication output as a measure of research success, and independent achievement. The (often) contextspecific nature of online teaching, the current lack of standardized methods of assessment of online teaching expertise, the time-commitment needed for quality instructional design, and the cooperative nature of effective teambased course development mean that incentives are often very low for faculty to invest time in working with technology (for overviews of these issues see Levy, 2003; Macfadyen, 2004; Oslington, 2005). In institutions of higher education, senior representatives of university units—such as the Deans, Heads of Departments and other members of the senior administration participating in committees charged with LMS review and selection—are typically senior faculty members rather than professional managers. Rogers’ model illuminates for us that this cohort is most likely to evaluate proposed changes to the LMS infrastructure not by coherence with vision or strategy, but by assessing the degree to which any change will burden themselves and their colleagues with the need to learn how to use complex new tools, and/or the need to redesign change their teaching habits and practices, without offering any appreciable advantage or reward. Information technology managers and staff similarly are most likely to assess proposals for new technology innovations from the perspective of workload and technical compatibility with existing systems, and have an even smaller investment in student learning outcomes. In this context, and in the absence of a strategic goal or vision (and of any clear incentives to strive towards such a strategic vision), analytic data reporting on current LMS data have little motivating power. 
 The realities of university culture.
 While faculty may be resistant to certain learning technologies, a more serious form of “institutional resistance” is found in the very culture of academic institutions—no less than a cultural clash. Bates (2000) characterizes the dominant Western university and college culture as a mixture of “industrial” and “agrarian.” In particular, the agrarian foundations of university culture is manifest today in a university structure in which learning is tightly regulated in a cohort/semester system, in which the faculty member is responsible for all aspects of teaching from selection of content to delivery to student assessment, and in which the accepted route for handing down knowledge is one of “apprenticeship” via supervised graduate study within a discipline (Macfadyen, 2004). In spite of the hierarchical management structures introduced by industrial models, the agrarian model gives insight into the persisting culture of cull faculty control of teaching. At the institutional level, this “quality-and-effectiveness”-focussed culture offers a number of major obstacles to change: consensus governance (rather than industrial-style hierarchical management); faculty control over the major goal activities (teaching and research); an organizational culture that supports change by adding resources rather than by strategically reallocating resources, and a curriculum structure that makes false (though some would argue, necessary) assumptions about learner homogeneity (Volkwein, 1999). Change management theorists lay heavy emphasis on the role of leaders in motivating and managing successful change and innovation (Kavanagh & Ashkanasy, 2006), but while university presidents are expected to be inspiring leaders, any direct interference in faculty democracy is not welcome. Similarly, introduction of policy that is seen to impinge on faculty autonomy in teaching is usually strenuously resisted, especially if it is perceived to derive from the “cost-consciousness-andefficiency” culture of a management bureaucracy or corporate/industrial model for education (Macfadyen, 2004). 
 Where to from here?.
 Social marketing theorists (Kotler & Zaltman, 1971) and change management experts (Kavanagh & Ashkanasy, 2006; Kotter, 1996) agree that social and cultural change (that is, change in habits, practices and behaviours) is not brought about by simply giving people large volumes of logical data (Kotter & Cohen, 2002). These authors insist that in order to overcome individual and group resistance to innovation and change, planning processes must create conditions that allow participants to both think and feel positively about change—conditions that appeal to both the heart and the head. Learning analytics has the capacity to do both, but only if certain conditions are met. Certainly, logical presentation of real institutional data can contribute to creating changes in thinking and behaviour, especially if it is used to highlight progress and room for growth against a backdrop of institutional targets and vision—and if participants are committed to the vision and motivated to achieve it. Interpretation remains critical. Data capture, collation and analysis mechanisms are becoming increasingly sophisticated, drawing on a diversity of student and faculty systems. Interpretation and meaning-making, however, are contingent upon a sound understanding of the specific institutional context. As the field of learning analytics continues to evolve we must be cognizant of the necessity for ensuring that any data analysis is overlaid with informed and contextualized interpretations. In addition, we propose that greater attention is needed to the accessibility and presentation of analytics processes and findings so that learning analytics discoveries also have the capacity to surprise and compel, and thus motivate behavioural change. Rogers (1995) describes a further factor that influences resistance to innovation: “observability,” or the degree to which the results of change and innovation are visible to self and others. As Romero & Ventura (2010) note, to date, efforts to mine educational data have been hampered by the lack of data mining tools that are easy for non-experts to use; by poor integration of data mining tools with e-learning systems; and by a lack of standardization of data and models so that tools remain useful only for specific courses/frameworks. Collectively, these difficulties make analytics data difficult for non-specialists to generate (and generate in meaningful context), to visualize in compelling ways, or to understand, limiting their observability and decreasing their impact. As governments and institutions further seek to establish quality measurements and demonstrate learning and teaching impact, learning analytics will be increasingly in demand. However, while learning analytics tools and processes will doubtless continue to rapidly evolve, research must also delve into the socio-technical sphere to ensure that learning analytics data are presented to those involved in strategic institutional planning in ways that have the power to motivate organizational adoption and cultural change.]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Numbers Are Not Enough. Why e-Learning Analytics Failed to Inform an Institutional Strategic Plan</rdfs:label>
		<dc:subject>learning management system (lms)</dc:subject>
		<dc:subject>virtual learning environment (vle)</dc:subject>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>strategic planning</dc:subject>
		<dc:subject>student engagement</dc:subject>
		<dc:subject>change management</dc:subject>
		<dc:subject>institutional culture</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/69/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/70">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>Integrating Data Mining in Program Evaluation of K-12 Online Education</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/70/authorlist"/>
		<swrc:abstract>This study investigated an innovative approach of program evaluation through analyses of student learning logs, demographic data, and end-of-course evaluation surveys in an online K–12 supplemental program. The results support the development of a program evaluation model for decision making on teaching and learning at the K– 12 level. A case study was conducted with a total of 7,539 students (whose activities resulted in 23,854,527 learning logs in 883 courses). Clustering analysis was applied to reveal students’ shared characteristics, and decision tree analysis was applied to predict student performance and satisfaction levels toward course and instructor. This study demonstrated how data mining can be incorporated into program evaluation in order to generate in-depth information for decision making. In addition, it explored potential EDM applications at the K12 level that have already been broadly adopted in higher education institutions.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 Traditionally, the majority of online instructors and institutional administrators rely on web-based course evaluation surveys to evaluate online courses (Hoffman, 2003). The data and information are then used to help inform online program effectiveness and generate information for program-level decision-making. While it enjoys wide use, the survey method only provides learners’ self-report data, not their actual learning behaviors. Several studies have found self-reported data were not consistent with actual learning behaviors (Hung & Crooks, 2009; Picciano, 2002). This inconsistency can potentially compound the already problematic lack of direct observation opportunities. Online program administrators need more effective tools to provide customized learning experiences, to track students’ online learning activities for overseeing courses (Delavari, Phon-amnuaisuk, & Beikzadeh, 2008), to depict students’ general learning characteristics (Wu & Leung, 2002), to identify struggling students (Ueno, 2006), to study trends across courses and/or years (Hung & Crooks, 2009), and to implement institutional strategies (Becker, Ghedini, & Terra, 2000). Each of these needs can be addressed by mining educational data. Nowadays, various educational data are stored in database systems. This is especially true for online programs, wherein student learning behaviors are recorded and stored in Leaning Management Systems (LMS). Program administrators can take advantage of emerging knowledge and skills by extracting and interpreting those data. The purpose of this study is to propose a program evaluation framework using Educational data mining. 
 Program evaluation.
 Program evaluation is the means by which a program assures itself, its administration, accrediting organizations, and students that it is achieving the goals delineated in its mission statement (Nichols & Nichols, 2000). Evaluation can be done by a variety of means. The most common form of evaluation is through surveying students regarding courses/faculty/programs (e.g., Cheng, 2001; Hoffman, 2003; Spirduso & Reeve, 2011). However, making causal inferences based on a one-time assessment is risky (Astin & Lee, 2003). Nevertheless, perceptional survey data cannot accurately reflect real learning behaviors (Hung & Crooks, 2009; Picciano, 2002). Although various scholars (e.g., Grammatikopoulous, 2012; Vogt & Slish, 2011) have proposed systematic frameworks (e.g., interviews and observation) in order to obtain objective knowledge via multiple means, these methods are difficult to implement in a fully online program. 
 Educational data mining.
 Data mining (DM) is a series of data analysis techniques applied to extract hidden knowledge from server log data (Roiger & Geatz, 2003) by performing two major tasks: Pattern discovery and predictive modeling (Panov, Soldatova, & Dzeroski, 2009). Educational data mining (EDM) is a field which adopts data mining algorithms to solve educational issues (Romero & Ventura, 2010). Romero & Ventura (2010) reviewed 306 EDM articles from 1993 to 2009 and proposed desired EDM objectives based on the roles of users. For the purpose of this study, which is designed to inform administrators, the list is limited to objectives for administrators: - Enhance the decision processes in higher learning institutions  - Streamline efficiency in the decision making process  - Achieve specific objectives  - Suggest certain courses that might be valuable for each class of learners  - Find the most course effective way of improving retention and grades  - Select the most qualified applicants for graduation  - Help to admit students who will do well in higher education settings    Based on the theory of bounded rationality, decision-making is a fully rational process of finding an optimal choice given the information available (Elster, 1983). An ideal program evaluation framework should provide multiple facets of information to decision makers. Therefore, integrating more than one data source and analytic method is essential for an effective program evaluation. 
 Figure.

 1. Program evaluation framework. 
 Program evaluation framework.
 Figure 1 shows the framework of the proposed program evaluation method. The core strategy of this framework is data triangulation (Jick, 1979) which combines multiple data sources (learning logs, course evaluation survey, and demographic data) and multiple methods (pattern discovery and predictive modeling) to generate accurate, in-depth results. Using this framework, the authors conducted a program evaluation case study to evaluate how the proposed program evaluation framework can support administrators’ decision making. 
 Method.
 Data source.
 In this case study, data were collected from a statewide K–12 online institution that serves over 16,000 students in a northwestern state in the U.S. The institution provides fully online courses to K–12 students. Courses were designed by subject-matter curriculum designers and subject-matter teachers to standardize course materials. Teachers were required to complete an online orientation prior to teaching courses for the institution. Teachers received the same or similar training for online teaching provided by the institution. Site coordinators are located at each district in the state and regional principals oversee teacher evaluation. The following data were collected for the academic year of 2009-2010 (3,604 students enrolled in Fall 2009 and 3,935 students in Spring 2010): 1) LMS activity logs; 2) student demographic data; and 3) course evaluation survey data. All data tables were stored in the database and interconnected with unique identifiers (e.g., course ID). 
 LMS activity logs.
 The LMS activity logs were collected from the Blackboard activity accumulator (Blackboard Inc., 2010) for the Fall 2009 and Spring 2010 academic terms. The following records were removed in data preprocessing: irrelevant fields (e.g., group ID), irrelevant records (e.g., login failure), and data stored in wrong or mismatched fields (about 11.8% of overall activity logs). After data preprocessing, a total of 23,854,527 activity logs were collected from 7,539 students in 883 courses. These students took 1 to 18 courses in the 2009–2010 academic year. Student demographic data The following demographic data were collected for data analysis, including age, gender, graduation year, city, school district, number of online course(s) taken, number of online course(s) passed, number of online course(s) failed, and final grade average. 
 Course evaluation survey. 
 A course evaluation survey investigated students’ satisfaction toward their course and instructor. Course satisfaction contained eight questions related to course content, five related to course structure, and eleven related to instructor satisfaction. Records containing any missing values were removed from the analysis. In addition, because student identifiers were not collected during the Fall 2009 survey implementation, which prevents the researchers from associate survey responses with demographic data and LMS activity logs, only Spring 2010 survey data (2618 respondents) were analyzed in this study. 
 Engagement level.
 Engagement is considered to be a key variable for enabling and encouraging learners to interact with the material, with the instructor, and with one another, as well as for learning in general. In this study, engagement level was measured by the frequency of various learning interactions that happened within the LMS. Variables under the category “Student Engagement Variable” in Table 1 were applied to measure each student’s engagement level, which included: - Average frequency of logins per course. - Average frequency of tab accessed per course (if the course was organized using “tabbed” navigation). - Average frequency of module accessed per course (if the course was organized using “modules”). - Average frequency of clicks per course. - Average frequency of course accessed per course (from Blackboard portal to course site). - Average frequency of page accessed per course (content created using the page tool). The Page tool allows instructors to include files, images, and text as links on the course menu - Average frequency of course content accessed per course (content created using the content tool). The Content tool allows instructors to create course content within the content area. - Average number of discussion board entries per course. 
 Variables.
 Table 1 lists variables collected from Blackboard, the student demographic database, and the course evaluation survey. Some variables were transformed with calculations in order to generate more meaningful variables for analysis. For example, student’s birth year was transformed to age. The summary of all learning activities was aggregated to a new variable called “frequency of clicks” that represents each student’s total frequency of clicks in the Blackboard LMS. If students took more than one course during the analysis period, variables of learning activities (e.g., frequency of total clicks and frequency of course access), performance (e.g., final grade), and survey (e.g., course satisfaction and instructor satisfaction) were averaged. 
 Table 1. Variables for data mining. 
 Analytic tools.
 SAS Enterprise Miner 6.1 (SAS Institute Inc., USA) was employed to perform the following data mining tasks in this study: 1) student clustering which describes shared characteristics of students who passed or failed their course; 2) perception and performance predictions which identify key predictors of course satisfaction, instruction satisfaction, and final grade. Because one of the major target audiences of this article is K–12 administrators, the authors utilized methods like decision tree and K-means clustering, which can produce results more intuitive for non-data miners. 
 Results.
 Student clustering.
 Clustering analysis.
 K-means algorithm (Hartigan & Wongm, 1979; Budayan, Dikmen, & Birgonul, 2009) was applied to group students based on their shared characteristics (Internal Standardization = Range; Maximum Number of Clusters = 6). Total clusters were limited to avoid trivially small or exclusive groups, the identification of which was outside the purposes of this case study. A pass rate equal to “1” means a student passed all courses during the period of analysis. A pass rate equal to “0” means a student failed all courses during the period of analysis. A pass rate between “0” and “1” means a student passed some, but not all, courses during the period of analysis. In clustering analysis, pass rate was set up as the standard for classification and six clusters were generated. Table 2 includes the results of clustering analysis in academic year 2009-2010. The following are shared characteristics of each cluster. - Cluster 1 (316 students, pass rate = 55.07%, all males): Cluster 1 consists of students who are older than Cluster 3 to 6. They were lower-engaged than Cluster 5 and 6 but higher than Cluster 3 and 4. On average, each student took 2.76 courses and failed about half of them. - Cluster 2 (320 students, pass rate = 56.11%, all females): Similar to Cluster 1, Cluster 2 consists of students who are older than Clusters 3 to 6. They are lower-engaged than Cluster 5 and 6 but higher than Cluster 3 and 4. On average, each student took 3.03 courses and failed about half of the courses. - Cluster 3 (594 students, pass rate = 0%, all males): Cluster 3 and 4 includes the lowest-engaged students. Cluster 3 students are all male. On average, each student took 1.43 courses and failed all of them. - Cluster 4 (601 students, pass rate = 0%, all females): Cluster 4 includes the lowest-engaged female students. On average, each student took 1.39 courses and failed all of them. - Cluster 5 (2,311 students, pass rate = 100%, all males): Cluster 5 and 6 represent the highest-engaged students. Cluster 5 students are all male. On average, each student took 1.59 courses and passed all of them. - Cluster 6 (3,397 students, pass rate = 100%, all females): Cluster 6 represents the highest-engaged female students. On average, each student took 1.64 courses and passed all of them. 
 Table 2. Results of clustering analysis. 
 The clusters generated from cluster analysis were associated with two geographical variables: city and school district, in order to examine whether certain types of students were from specific areas. Differences in engagement were found depending on location. Clusters 1 to 6 had similar geographical distributions except for three larger cities (populations larger than 100,000). Cluster 5 (all male, pass rate = 100%) included a larger group of students from one large city. Cluster 6 (all female, pass rate = 100%) included a larger group of students from the other two large cities. There is no notable difference of school district distributions across clusters. 
 Findings. 
 Findings below were summarized from the clustering analysis. 1) Students with higher engagement levels usually had higher performance. 2) Younger students (CLs 5 & 6) who lived in larger cities were more successful than those in smaller cities (CLs 3 & 4) and older students (CLs 1 & 2). 3) All-failed students who were also low-engaged consisted of approximately 15.9% on average per course. 4) All-passed students who were also high-engaged consisted of approximately 75.7% students on average per course. 5) Based on Cluster 1 and 2, on average, older students (age > 16.91) tended to take more than two courses with pass rates ranging from 54.09-56.11%. 6) On average, high-engaged students demonstrated engagement levels twice that of low-engaged students. 7) Frequencies of reading behaviors (such as content access and page access) were much higher than discussion behaviors (p<0.001). 8) Female students were more active than male students in online discussions (with higher DB_Entry avg frequency). 9) Female students had higher pass rates than male students. 
 Average clicks per course in different subject areas.
 Table 3 shows students’ average frequencies of total clicks and performances per course in different subject areas. Total clicks were equal to the summarized frequency of overall learning activities. The results show that Math and Science had the highest number of total clicks per course and of total clicks per student per course. However, for those who took Math and/or Science courses, their average final grades (56.70 and 64.41 accordingly) were lower than the overall final grade average (71.11). This indicates students participated actively in courses of these two subject areas, but they failed to achieve expected outcomes (70 or higher). Possible reasons for this outcome could be related to course design and/or best practice in teaching strategies for Math and Science courses. On the other hand, English courses received a lower number of clicks combined with less than expected outcomes. Encouraging motivation and engagement in these courses could have a profound effect on future outcomes. Students in Foreign Language and Health not only participated in learning activities actively, but also obtained the highest grades, on average, in each of these two subject areas. 
 Table 3. Average frequencies of total clicks and performance in different subject areas. 
 Findings. 
 10) Subjects where the level of activity was effective and consistent with student outcomes included Driver Education, Electives, Foreign Language, Health, and Social Studies. 11) Subjects where the level of activity was inconsistent with student outcomes included Math, Science and English. Math and Science courses had high activity levels with less than expected outcomes. 12) Subjects where the level of activity was low and consistent with low student outcomes included English. Subject preferences Figure 2 shows percentages of female and male students in different subject areas. Because the original female versus male ratio is 1.34, subject preferences for female and male students were revealed by comparing female/male ratio in each subject with the original ratio. Subjects above the dashed line are those with higher female ratios. Findings 13) Female students preferred taking Electives, Foreign Language, and Social Studies. 14) Male students preferred taking Drivers Education, Math, and Science. 
 Figure 2. Gender preferences (female students/male students) by subject areas. 
 Pass rate in different subject areas.
 Table 4 consists of two parts. The first part examines whether pass rates of female and male students in different subjects have significant differences. “F vs. M” compares gender pass rate difference using t-tests. The second part examines pass rate difference between Fall 2009 and Spring 2010 within the same gender. For example, “F vs. F” compares pass rate difference between Fall 2009 and Spring 2010 female students in difference subjects by using ttests. Numbers marked with asterisks represent differences that have statistical significance. 
 Table 4. Pass rate comparisons and statistical tests by gender and subject areas. 
 Note: Statistical significance refers to the possible accurate rate of a statement by testing it with statistical methods. “p < .05” means the statement is at least 95% accurate (error rate is less than 5%). *p < .05, **p < .001 
 Findings.
 15) Overall, females significantly performed better than male students, especially in the following subject areas: Electives, English, and Social Studies. 16) The fail rates during the Fall 2009 term was significantly higher than those during the Spring 2010 term, especially in those subjects with higher fail rates such as English, Math, Science, and Social Studies. After these results were revealed the researchers subsequently learned from the administrators of the program reported in this case study that they had adopted an early alarm system in Spring 2010 to track all communications between instructors and students. The results show those strategies could have improved students’ pass rates in most subject areas. 
 Student performance and engagement by course number.
 Due to the previous results indicating that students in Math, Science, and English had lower performance than those in other subject areas, researcher were interested in identifying potential anomalies within this group which might help to explain the reasons for the results. Further analysis was applied to identify which Math, Science, and English courses resulted in the highest performance and which Math, Science, and English courses resulted in the lowest performance. Researchers divided courses into three conditions: (a) high-engaged, high-performance, (b) high-engaged, low performance, and (c) low-engaged, low-performance based on student behaviors within the course. Courses categorized as high-engaged and high-performance might represent courses with both effective design and effective implementation because students were highly engaged and achieved expected outcomes. Those categorized as highengaged and low-performance might represent courses with less effective course design because students were unable to achieve expected outcomes despite what appears to be effective implementation. Finally, courses categorized as low-engaged and low performance might represent courses with less effective course design and less effective course implementation. Our analysis revealed that regardless of the content area, most high-engaged, low performance, or low-engaged, low performance courses were entry-level courses. Most high-engaged, high performance courses were advanced level courses. Students’ responses to the survey question asking students to indicate their reasons for taking an online course were then incorporated to help further interpret the results. The majority of responses from young students enrolled in courses categorized as high-engaged and high-performing were “the course was not available in my school.” The majority of older student responses in courses that were categorized as low-engaged and low performance were “I was making up a class I had failed.” 
 Findings.
 17) Regardless of Math, Science, or English subject-matter, entry level courses tended to have lower performance whether students were categorized as low-engaged or high-engaged. This may speak more to course structure, design, and support than to the effectiveness of instruction. 18) The reasons students enrolled in a course may influence their engagement level and performance. Student survey responses indicated that students who retook courses they have previously failed, tended to demonstrate lower engagement and lower performance. If students took courses which were not available in their schools, these students were usually high-engaged and high performing. 
 Predictive analysis.
 CRT Decision Tree analysis (Breiman, Friedman, Olshen, & Stone, 1984) was applied to construct predictive models combining course related data and survey results (Splitting Criterion: Gini; Leaf Size: 60; Maximum Depth: 10; Assessment Measure: Average Squared Error). These settings allow for a larger sequence of sub-trees in order to enrich the study’s findings. Decision Trees classifies instances by sorting them down the tree from the root to the leaf nodes. In the tree structures, leaf nodes represent classifications, and branches represent conjunctions of features that lead to different target values. The following three variables were adopted as dependent variables in the Decision Tree analysis: 1) Average course grade; 2) average course satisfaction; and 3) average instructor satisfaction. Survey questions on course satisfaction and instructor satisfaction can be retrieved from http://goo.gl/x8j18 - Average course grade is each student’s final course grade (range: 0-100). If a student took more than one course, average course grade is the average of multiple courses. - Average course satisfaction was generated by averaging the scores from eight survey questions related to course content and the five survey questions related to course structure (range: 1–7). If a student took more than one course, average course satisfaction is the average of satisfaction scores from multiple courses. - Average instructor satisfaction was generated by averaging the scores from 11 survey questions related to the instructor satisfaction (range: 1–7). If a student took more than one course, average instructor satisfaction is the average of satisfaction scores from multiple courses.   Final grade prediction All variables in Table 1 were imported for final grade prediction. Average course grade was used as the dependent variable and the remainders were treated as independent variables. Because the tree results contained too much information, blank nodes were used to represent the results excluded from the data interpretation. Figure 3 shows the decision tree for final grade prediction. In academic year 2009-2010, 75.7% of students passed all courses, 15.9% of students failed all courses, and 8.4% passed some but not all of their courses. The left branch of the decision tree represents students who passed all courses. The results indicate a positive correlation between engagement level and performance (higher engaged => higher performance). The right branch of the decision tree represents students who had failed in one or more courses. The results imply a negative correlation between engagement level and performance (lower engaged => lower performance). 
 Figure 3. Final grade prediction (complete chart: http://goo.gl/NIfWu). 
 Findings.
 19) Engagement level and gender have stronger effects on student final grades than age, school district, school, and city. For most students, high engaged => high performance. 20) Compared with other Blackboard components such as discussion board entries and content access, tab access has negative effects on student performance (higher tab accessed => lower performance). 21) Female students performed better than male students. 
 Final grade prediction (external variables).
 Additional decision tree analysis was conducted to investigate how external variables (i.e., non-learning activity variables) influenced student performance. Figure 4 is a portion of the decision tree for academic year 2009-2010. 
 Figure 4. Final grade prediction with external variables only (complete chart: http://goo.gl/B8AvB). 
 Findings.
 22) Based on the predictive model, female students performed better than male students. 23) Students who were around 16 years old or younger performed better than those who were 18 years or older. 
 Figure 5. Course satisfaction prediction (complete chart: http://goo.gl/5NLWl). 
 Satisfaction prediction.
 Decision tree analysis was also conducted to predict students’ satisfaction levels toward their course and instructor. Fall 2009 survey data could not be associated with variables in Blackboard, so the following results are limited to Spring 2010 only.   
 Course satisfaction.
 All the scores calculated from the responses to survey questions on course satisfaction were averaged into the scores of one course satisfaction variable. The value of “7” for this variable represents highest satisfaction with a course and “1” represents lowest satisfaction with a course. Figure 5 is a portion of the decision tree regarding course satisfaction. 
 Findings.
 24) Students with higher average final grades (> 73.25, with a maximum score of 100) had higher course satisfaction. 25) Students who passed all courses or passed some of their courses had higher course satisfaction than all-failed students. 26) Students who took two or more courses in Spring 2010, whether they passed those courses or not, had higher course satisfaction. 27) Female students had higher course satisfaction than male students. 28) Online behaviors (i.e., frequency of page accessed and number of discussion board entries) had minor effects on course satisfaction (higher frequency/number => higher course satisfaction). 29) Students in different cities showed different course satisfaction levels. 
 Instructor satisfaction.
 All the scores calculated from the responses to survey questions on instructor satisfaction were averaged into the scores of one instructor satisfaction variable. The value of “7” for this variable represents highest satisfaction with an instructor and “1” represents lowest satisfaction with an instructor. Figure 6 is a portion of the decision tree regarding instructor satisfaction. 
 Figure 6. Instructor satisfaction prediction (complete chart: http://goo.gl/QCdpw). 
 Findings.
 30) Students with higher average final grades (> 73.25, with a maximum score of 100) indicated higher instructor satisfaction. 31) Students who took two or more courses in Spring 2010, whether they passed those courses or not, showed higher instructor satisfaction. 32) Female students indicated higher instructor satisfaction than male students. 33) Online behaviors (frequency of module accessed) had minor effects on instructor satisfaction (higher frequency => higher course satisfaction). However, there were six students indicated low instructor satisfaction, despite extremely high frequency of course access and high final grades. 34) Older students taking one course (> 17.5 years old) had higher instructor satisfaction 35) Students from different schools showed different satisfaction levels for their online instructors.. 36) Younger female students (<15.5 years old) with lower average final grade (<76.5) indicated lower instructor satisfaction. 
 Discussion.
 This study is a first attempt at program evaluation combining multiple data sources. The goal of this project was to propose a new program evaluation framework in order to generate sufficient information for program-level decisionmaking. The advantages of this framework are data triangulation and data interpretation. Below are triangulation and interpretation results: - Female students generally performed better than male students (findings (9), (15), (21), and (22)); however, the findings were limited to the following subject areas: Electives, English, and Social Science (finding (15)). - High-engaged students generally performed better than low-engaged students (findings (1), (10), (12) and (19)); however, the findings were limited to non-STEM courses (findings (10) and (11)). One possible factor influencing high-engaged students’ inability to consistently achieve expected outcomes may have been poor course design. - Younger students generally performed better than older students (findings (2) and (23)); however, the findings were limited to students in larger cities (findings (2)). - One possible explanation for older students’ generally lower performances may be that older students took more than two courses per semester for credit recovery (findings (5) and (18)). Younger students took fewer courses, but the fact the courses were generally not available in school district may have increased motivation (findings (2) and (18)). Overall, using multiple forms of data allows for a more meaningful analysis of actual student behaviors, and the identification of potential relationships with demographic data, satisfaction data, and student outcomes. The result is a much richer and deeper analysis of student performance and teaching, as well as of effective course design, than could ever be accomplished with survey data or behavior mining alone. 
 Demographic and performance.
 Based on results revealed by the program evaluation framework, some indicators can be applied to identify students more likely to be successful and those more likely to be at-risk. In this study, a student who possessed more of the characteristics listed below was more likely to be successful: - Female - Younger than 16.5 years - Took one or two courses per semester - Took a Foreign Language or Health course  - Lived in a larger city  A student who possessed more of the characteristics below was more likely to be at risk of failure. - Male - Older than 18 years - Took more than two courses per semester - Took entry-level courses in Math, Science, or English - Lived in a smaller city These indicators can be applied to develop an early warning system (Macfadyen & Dawson, 2010), so administrators and teachers can have a list of successful and at-risk students before each semester starts. 
 Engagement and performance.
 Based on data mining analysis, higher-engaged students usually had higher performance. This finding is also supported by previous studies (e.g., Hung & Crooks, 2008; Hung & Zhang, 2009). However, the conclusion may be limited to courses which were well designed and implemented. In this study, entry-level courses tended to have lower performance, regardless as to whether students were categorized as low-engaged or high-engaged. This means high-engaged students might still have lower final grades if they were in a course with course structure, design, and/or support issues. Lim & Morris (2009), when studying post-secondary students, found junior and senior students had significantly higher survey mean scores in perceived learning, learning application, and learning involvement than freshman and sophomore students. Assuming higher perceived learning, learning application, and learning involvement equates to high motivation, the authors could not explain why older students had significantly higher engagement than young students. Our study, by combining analysis of engagement and performance through data mining with survey responses, revealed why students had different levels of engagement and performance. For example, the majority of responses from the students enrolled in courses categorized as high engagement and high performance were, “The course was not available in my school.” Meanwhile, the majority of responses from students in courses that were categorized as low engagement and low performance were, “I was making up a class I had failed.” The level of engagement in our study may have been influenced by motivation. In addition, Lim & Morris (2009) found older students had a better chance to be successful in online learning at the higher education level. However, our study found older students were more likely to be at-risk students in K-12 online education. Students older than 18 tended to be low engaged and lower performing in their courses. 
 Satisfaction and performance.
 There is no confirmed relationship between student performance and satisfaction (positive correlation—Eiszler 2002, Nasser & Hagtvet 2006; No relation—Ladebo, 2003, Walker & Palmer, 2011). In the case study, students with higher final grades usually had higher course and instructor satisfaction. In addition, survey results showed female students reported significantly higher satisfaction with courses and instructors. Similar results were also found in Hermans, Haytko, & Mott-Stenerson’s study (Hermans, Haytko, & Mott-Stenerson, 2009). Survey results showed that Health courses required the least effort and were considered the least challenging subject. Students also had high satisfaction with Health courses. However, although students had high satisfaction and engagement levels in Science courses, their average final grades were the third lowest. These findings showed high satisfaction and engagement levels could not guarantee high performance. While students obtained the lowest average grades in Math and English courses, they did not show significantly lower satisfaction levels in these two subject areas. Assuming perception data only reflected positive experiences, the picture of students’ experiences in courses could be misrepresented if students’ learning behaviors were not analyzed. These findings illustrate the flaws in solely relying on self-reporting and perception data in program evaluation to inform strategic decisions. 
 Limitations.
 It should be noted that this project was not without challenges. We are in the early stages of data mining, and learning management system providers are often not cognizant of the importance of their data collection strategies. For example, the Blackboard activity accumulator grouped wide-ranging learning behaviors into only five categories, which significantly simplified findings that could potentially be revealed by data mining. In addition, issues with missing data, data stored in the wrong fields, or mismatched data fields in the database limited some of the analysis techniques attempted in this evaluation. Blackboard failed to track ever forum reply behavior, and in order to avoid using inaccurate data to generate false results, we discarded students-student and student-instructor interaction analysis. Although multiple data sources provide rich information for data interpretation, some unexpected results were revealed by survey design and investigation. For example, why did students from different schools show different satisfaction levels for their instructors? Further data interpretation could be undertaken with additional open-ended questions. 
 Proof of the framework.
 This study demonstrates the benefits of incorporating data mining into the program evaluation of K-12 online education. It is important to note that demographic data and course evaluation survey data are also indispensable in supporting data-mining-results evaluation and interpretation. In exploring EDM applications at the K-12 level that have already been broadly adopted in higher education institutions we have demonstrated: 1) how data mining can be incorporated into program evaluation in order to support decision-making at the institutional level, 2) how a framework of data triangulation generates high-quality and non-partial results that can be combined with student learning logs, demographic data and course evaluation surveys and that 3) characteristics of successful and at-risk students can be generated by identifying important predictors of student performance, course satisfaction, and instructor satisfaction for K–12 online education. 
 Future research.
 Future study should continue in the following directions: 1) Evaluate the usefulness of the information by surveying administrators; and 2) Generalize findings by incorporating more data from various program evaluation projects. For example, many average numbers in this study could be important indicators for retention and performance improvement. The numbers would be more stable for generalization with additional study data; 3) Develop an early warning system using tables and figures in this study to simplify administrators’ decision-making processes; 4) Add further possible open-ended questions for data interpretation; 5) Conduct a follow up study. For example, entry-level Science courses were identified as classes that might have course design issues. Follow up research might help identify which parts are difficult for students; and 6) Utilize the framework to validate or generate educational models.]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Integrating Data Mining in Program Evaluation of K-12 Online Education</rdfs:label>
		<dc:subject>educational data mining</dc:subject>
		<dc:subject>program evaluation</dc:subject>
		<dc:subject>k-12 virtual school</dc:subject>
		<dc:subject>pattern discovery</dc:subject>
		<dc:subject>predictive modeling</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/jui-long-hung"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/jui-long-hung"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/yu-chang-hsu"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/yu-chang-hsu"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/kerry-rice"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/kerry-rice"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/70/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/jui-long-hung"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/yu-chang-hsu"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/kerry-rice"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/71">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>Using Data Mining for Predicting Relationships between Online Question Theme and Final Grade</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/71/authorlist"/>
		<swrc:abstract>As higher education diversifies its delivery modes, our ability to use the predictive and analytical power of educational data mining (EDM) to understand students’ learning experiences is a critical step forward. The adoption of EDM by higher education as an analytical and decision making tool is offering new opportunities to exploit the untapped data generated by various student information systems (SIS) and learning management systems (LMS). This paper describes a hybrid approach which uses EDM and regression analysis to analyse live video streaming (LVS) students’ online learning behaviours and their performance in their courses. Students’ participation and login frequency, as well as the number of chat messages and questions that they submit to their instructors, were analysed, along with students’ final grades. Results of the study show a considerable variability in students’ questions and chat messages. Unlike previous studies, this study suggests no correlation between students’ number of questions / chat messages / login times and students’ success. However, our case study reveals that combining EDM with traditional statistical analysis provides a strong and coherent analytical framework capable of enabling a deeper and richer understanding of students’ learning behaviours and experiences.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 According to a recent survey conducted by Campus Computing (campuscomputing.net) and WCET (wcet.info), almost 88% of the surveyed institutions reported having used an LMS (Learning Management System) as a medium for course delivery for both on-campus and online offerings. In addition to various student information management systems (SISs), LMSs are providing the educational community with a goldmine of unexploited data about students’ learning characteristics, behaviours, and patterns. The turning of such raw data into useful information and knowledge will enable institutes of higher education (HEIs) to rethink and improve students’ learning experiences by using the data to streamline their teaching and learning processes, to extract and analyse students’ learning and navigation patterns and behaviours, to analyse threaded discussion and interaction logs, and to provide feedback to students and to faculty about the unfolding of their students’ learning experiences (Hung & Crooks, 2009; Garcia, Romero, Ventura, & de Castro, 2011). To this end, data mining has emerged as a powerful analytical and exploratory tool supported by faster multi-core 64 CPUs with larger memories, and by powerful database reporting tools. Originating in corporate business practices, data mining is multidisciplinary by nature and springs from several different disciplines including computer science, artificial intelligence, statistics, and biometrics. Using various approaches (such as classification, clustering, association rules, and visualization), data mining has been gaining momentum in higher education, which is now using a variety of applications, most notably in enrolment, learning patterns, personalization, and threaded discussion analysis. By discovering hidden relationships, patterns, and interdependencies, and by correlating raw/unstructured institutional data, data mining is beginning to facilitate the decision-making process in higher educational institutions. This interest in data mining is timely and critical, particularly as universities are diversifying their delivery modes to include more online and mobile learning environments. EDM has the potential to help HEIs understand the dynamics and patterns of a variety of learning environments and to provide insightful data for rethinking and improving students’ learning experiences. This paper is focused on understanding live video streaming (LVS) students’ learning behaviours, their interactions, and their learning outcomes. More specifically, this study explores how the interaction of students with each other and with their instructors predicts their learning outcomes (as measured by their final grades). By investigating these interrelated dimensions, this study aims to enrich the existing body of literature, while augmenting the understanding of effective learning strategies across a variety of new delivery modes. This paper is divided into four sections. It begins by reviewing the literature dealing with the use of data mining in administrative and academic environments, followed by a short discussion of the way in which data mining is used to understand various dimensions of learning. The second section explains the purpose and the research questions explored in this paper. The third section describes the background of the study and details its methodological approach (sampling, data collection, and analysis). The paper concludes by highlighting key findings, by discussing the study's limitations, and by proposing several recommendations for distance education administrators and practitioners. 
 Data mining applications in administrative and academic environments 
 At the intersection of several disciplines including computer science, statistics, psychometrics (Garcia et al., 2011), data mining has thrived in business practices as a knowledge discovery tool intended to transform raw data into highlevel knowledge for decision support (Hen & Lee, 2008). To this end, a wide range of tools that can be used for collecting, storing, analysing, and visualizing data, such as the SPSS Modeler (formerly Clementine) and the SAS Enterprise Miner, have been developed in the business world. These tools use sophisticated computing paradigms including decision tree construction, rule induction, clustering, logic programming, and statistical algorithms. Although data mining has been widely used in business environments to predict future trends and consumer behaviours (Harding, Shahbaz, Srinivas, & Kusiak, 2006; Ngai, Xiu, & Chau, 2009), the data mining method has been dramatically under-used in education research in general (Faulkner, Davidson, & McPherson, 2010). Only recently have higher education institutions started to exploit the potential of this powerful analytical tool (Black, Dawson, & Priem, 2008). However, according to Romero and Ventura (2010), educational data mining (EDM) has emerged as a new field of research capable of exploiting the abundant data generated by various systems for use in decision making. The enthusiastic adoption of data mining tools by higher education has the potential to improve some aspects of the quality of education, while it lays the foundation for a more effective understanding of the learning process (Baker & Yacef, 2009). EDM, when integrated into an iterative cycle (Romero, Ventura, & Garcia, 2008) in which mined knowledge is integrated into the loop of the system not only to facilitate and enhance learning as a whole, but also to filter mined knowledge for decision making (Romero et al., 2008) or even to create intelligence upon which students, instructors, or administrators can build, can notably change academic behaviour (Baepler & Murdoch, 2010). From an administrative perspective, Chang (2006) argues that the predictive capacity of data mining can further enhance enrolment management strategies by increasing the HEIs’ understanding about their admitted applicants. Similarly, Delavaria, Phon-Amnuaisuka, and Reza Beikzadehb (2008) contend that data mining knowledge techniques are capable of enabling higher learning institutions to make better decisions, to put more advanced planning into place to direct students, and to predict individual behaviours with higher accuracy, and, in so doing, to enable the institutions to allocate resources and staff more effectively. Without inflating the merits of data mining in rethinking administrative and academic processes, it is clear that data-mining is gaining ground and is providing powerful analytical tools capable of converting untapped LMS and EPR data into critical decision-making tools with the potential of enhancing students’ learning experiences (Garcia et al., 2011). From a learning perspective, according to Castro, Vellido, Nebot, and Mugica (2007), data mining is being used in higher education - to assess students’ learning performance - to provide feedback and adapt learning recommendations based on students’ learning behaviours - to evaluate learning materials and web-based courses, and - to detect atypical students’ learning behaviours. Following this line of thinking, Perera, Kay, Koprinska, Yacef, and Zaiane (2009) used clustered data mining techniques to support the learning of group skills by building automated mirroring tools capable of facilitating group work. In a similar study, Sun, Cheng, Lin, and Wang (2008) used rules based on data mining results to form high interaction-learning groups. For their part, Hung and Zhang (2008) applied data mining techniques to server logs, both to reveal online learning behaviour patterns and to support online learning management, facilitation, and design. Their study’s results revealed students’ behavioural patterns and preferences, which helped them to identify active and passive learners and which extracted important parameters for the prediction of the students’ performance (Hung & Zhang). Using a similar approach, Ba-Omar, Petrounias, and Anwar (2007) analysed web access logs to identify learning patterns and offline learning styles. In a recent study, Abdous and He (2011) used text mining as a detection tool for the common technical problems faced by students taking video streaming courses. Elsewhere, Zaiane and Luo (2001) analysed server logs to understand online learners’ behaviours in an effort to improve their web-based learning environments. Later, Zaiane (2002) used association rule mining to construct a recommender-system based on data from online learners’ profiles, access histories, and collective navigation patterns. This system can “intelligently” recommend learning activities or shortcuts to learners, based on the actions of previous learners. Similarly, Burr and Spennemann (2004) have pointed out that analysis of the patterns of user behaviour is important from both the technical and the pedagogical perspectives in order to predict network and traffic load, to align pedagogy with users’ behaviours, and to plan and deliver services in a timely manner. For their part, Dringus and Ellis (2005) proposed a data mining approach for “discovering and building alternative representations for the data underlying asynchronous discussion forums.” This approach is intended to improve the instructor’s ability to evaluate the progress of a threaded discussion. More recently, Lin, Hsieh, and Chuang (2009) conducted a study to investigate the potential of an automatic genre classification system (GCS) that can be used to facilitate the coding process of the content analysis of a threaded discussion forum, Of particular relevance to our study, we discovered several studies which have used various EDM techniques to predict students’ performance as measured by final grades. Minaei-Bidgoli and Punch (2003) used web-use features such correct answers, number of attempts for doing homework, total time spent on problems, participation in communication, and reading of material as predictors of students’ final grades. Their prediction accuracy varied between 51% and 86.8%, depending on the type of classifier used. Similarly, Falakmasir & Jafar (2010) used data mining to rank students’ activities which affected their performance, as measured by their final grade. Their findings suggest that students’ participation in virtual classrooms had the greatest impact on their final grades. For their part, Zafra and Ventura (2009) used a grammar-guided genetic programming algorithm to predict students’ success or failure. These predictions were used to provide alternative learning activities that would enhance the students’ chances of success. Using Learning Management Systems-generated student tracking data (Macfadyen & Dawson, 2010), we propose the development of a customizable dashboard-like reporting tool. This tool is intended to provide instructors with real-time data on both students’ engagement and the likelihood of their success. Unsurprisingly, their findings confirm that students’ contribution to the course discussion board is the strongest predictor of their success. In reviewing the literature, Romero, Espejo, Zafra, Romero, and Ventura (2010) identified several avenues for using classification in educational settings: discovering student groups with similar characteristics, identifying learners with low motivations, proposing remedial actions, and predicting and classifying students using intelligent tutoring systems. For their parts, Anand Kumar & Uma (2009) used the classification process to examine various attributes affecting student performance. Castellano and Martínez (2008) used collaborative filtering techniques to exploit students’ grades in order to generate group profiles which could facilitate academic orientation. Along the same lines, Vialardi et al. (2011) used data mining techniques which employed the students’ academic performance records to design a recommender system in support of the enrolment process. In sum, this quick overview of the literature suggests that using various data mining techniques to predict students’ performance as measured by final grades has been examined by several different studies of traditional learning management systems. However, none of the studies has explored the dynamics of online interaction in a live video streaming environment. With these considerations in mind, we aim to apply both regression analysis and clustering analysis in order to explore students’ learning behaviours (students’ participation, login frequency, number of chat messages, and the type of questions submitted to instructor) along with their final grades. More specifically, we attempt to answer the following two questions: - What are the major themes emerging from LVS students’ online questions? - How do these emerging themes predict the students’ course grades? 
 Figure.

 1. LVS interface. 
 Methodology.
 Context of the study.
 This study was conducted in a public research university in the mid-Atlantic region which serves 17,000 undergraduate and 6,000 graduate students and offers more than 70 bachelor’s degree programs, 60 master’s degree programs, and 35 doctoral degree programs in a variety of fields. Located in a major maritime, military, and commerce hub, this institution offers strong emphases in science, engineering, and technology, especially in the maritime and aerospace sciences. The university is also known as a national leader in technology-mediated distance learning, having served students at over 50 sites in Virginia, Arizona, and Washington state for more than twentyfive years. This extended distance learning capability provides the university with a variety of delivery mode options (i.e., ways in which a course can be delivered). Courses can be offered simultaneously via three different delivery formats: face-to-face, via satellite broadcasting, and via live video-streaming. Using the live video-streaming (LVS) delivery mode, students participate in the class, in real time, via personal computer, over which they view a live feed of the class lecture and during which they can interact with their instructor by sending text messages through the LVS course interface. Using the same interface, LVS students are able to chat with their LVS classmates during class. At the receiving end (i.e., in the physical classroom), questions submitted by LVS students are displayed instantaneously on a monitor next to the instructor. Instructors have the option to read/answer the messages, or to save, archive, and email them for later review. This tool is intended to enable instructors to seamlessly integrate LVS students into their classroom dynamic, without distraction and without overburdening instructors during their class time (Figure 1). 
 Figure 2. Interaction in VS courses. 
 Participants.
 In total, 1,144 students completed 138 courses in a variety of subjects (e.g., accounting, computer engineering, information technology, human services, etc.) via the video streaming (VS) delivery mode during the Fall semester of 2009. All of the student-to-instructor questions, the two-way student-to-student chat messages, and the total login times were collected. Those VS students who never asked questions or chatted with their peers online were excluded from the actual analysis. The reasons why those students failed to get involved in the VS course discussion are suggested to be included in future investigation. One possible explanation is that some instructors never took the effort to invite their VS students to ask questions or to engage in online discussion. As a result of the pre-processing of the data, 298 students (those with complete information about their number of questions, number of chat messages, total login times, and final grade) were included in the data analysis. Due to factors such as privacy and university policy, the university’s registrar’s office could not provide us with the age or gender of these students, nor could we obtain the grading scales of each course. (The grading scale for each course at our university is determined by that course’s instructor.) 
 Table 1. Distribution of students by college. 
 Table 2. Distribution of students by academic level. 
 The questions and chat messages posted by those 298 students, along with their course ID, Student ID, the date, and a time stamp were saved in the database. 
 Clustering analysis: Identification of emerging themes.
 Our analytical approach included three phases: pre-processing, in which raw data is transformed into a usable format, mainly by cleaning, assigning attributes, and integrating data; mining the data by applying various mining strategies and tools such as classification, clustering, and visualization; and post-processing, which allows for interpretation and use of the gained knowledge in rethinking processes or in making decisions (Garcia et al., 2011). All of the questions posted by the students were recorded in the Microsoft SQL server Database. To prepare the data processing for clustering analysis, we wrote a program using the PHP programming language to aggregate questions from the same students within the same course in order to form a case which included the sequence of questions posted by the students. Subsequently, we used NVivo 9 software to apply an automatic coding technique to each of the student question cases. Nvivo is a leading qualitative analysis tool on the market and has been used and tested by many researchers for content analysis (Zha, Kelly, Park & Fitzgerald, 2006). Automated coding one of NVivo 9’s features; it allows for automatic coding of a text document by text strings. After nodes were generated from each student question case, a clustering analysis was conducted in order to classify these nodes into different clusters with NVivo 9. According to Nvivo, nodes are containers for specific themes, people, places, organizations, or other areas of interest. Researchers can organize nodes into hierarchies – moving from more general topics (the parent node) to more specific topics (child nodes) – in order to support their particular research needs. Clustering analysis is a well-studied technique in data mining (Lin, et al., 2009) that uses an exploratory technique to visualize patterns by grouping sources which share similar words or attribute values, or which are coded similarly. From a data mining perspective, clustering is the unsupervised discovery of a hidden data concept. This approach is used in those situations in which a training set of pre-classified records is unavailable. In other words, this technique has the advantage of uncovering unanticipated trends, correlations, or patterns; no assumptions are made about the structure of the data (Chen & Liu, 2004) The purpose of clustering analysis in this study is to classify students based on the student-shared characteristics in their questions. The cluster analysis tool in the NVivo 9 software confers upon researchers a different perspective on the unstructured textual data. Using the calculated similarity in each word that appears in the text of the nodes, NVivo 9 groups the nodes into a number of clusters. In our study, a statistical method named the Pearson correlation coefficient (-1 = least similar, 1 = most similar) was used as the similarity metric for the clustering analysis. The Pearson correlation coefficient is the preferred similarity metric used with Nvivo. More information about the clustering analysis of Nvivo can be found in Nvivo’s online documentation website, http://www.qsrinternational.com/support.aspx. To gain further insight from the textual questions or chat messages, we also applied the SPSS Clementine tool, which allowed us to analyse the unstructured textual data. The SPSS Clementine tool provides linguistic methods (extracting, grouping, indexing, etc.) for researchers to use in order to explore and extract key concepts from the text. As the result of the text mining, key concepts in our study were extracted and identified for analysis. 
 Measurement of final grade.
 The students’ final grades, submitted to the University Registrar by each course instructor, were supplied to us by the University Registrar. In the actual data analysis, the final grades were categorized into three groups: A- to A, B- to B+, and Others. 
 Quantitative data analysis: Predictive relationship between online question theme and final grade 
 In the current study, all of the quantitative data analysis was implemented using SPSS 17.0. Furthermore, the alpha levels were set at the .05 level for all significance tests. 
 Due to the ordinal nature of the final grade, ordinal logistic regression analysis (Norusis, 2008; O’Connell, 2006) was implemented in order to examine the predictive relationship between the online question theme as the predictor and the final grade as the criterion variable. Specifically, a cumulative odds model was fitted to the data. The use of ordinal logistic regression, which was closely related to logistic regression, helped to avoid the statistical consequences that could occur from the violation of assumptions in linear regression, such as normality of errors and linearity in the parameters (King, 2008). The log transformation in logistic regression also ensured that the predicted probabilities for the event of interest would range from 0 to 1 without imposing the numerical constraint on the predicted log odds from the logistic equation (Cohen, Cohen, West, & Aiken, 2003). Given the ordinal nature of the final grade, ordinal logistic regression was used to take into account the information regarding the rank ordering of the outcomes (Hosmer & Lemeshow, 2000). The overall predictive utility of the ordinal logistic model with the online question theme as the predictor was assessed by testing the improvement of the model fit relative to the null model with no predictor, with the χ2 likelihood ratio test of the differences in deviances (O’Connell, 2006). The individual parameter estimate (i.e., the location coefficient) for the predictor variable was tested with the Wald test (Norusis, 2008). In ordinal logistic regression, two cutoffs (A- and B-) were used sequentially to form the cumulative odds equal to or higher than those two cutoffs, respectively. As a result, the probabilities of falling into three possible categories of final grade (A- to A; B- to B+; and Others) could be derived. Two different pseudo R2 (Cox and Snell R2 and Nagelkerke R2) were also computed in order to quantify the overall model fit (O’Connell). The larger the pseudo R2, the better the model fit. The parallel lines assumption in ordinal logistic regression was checked with the χ2 likelihood ratio test (Norusis, 2008) to see if the relationship between those two research variables remained the same across two cutoffs (A- and B-). 
 Results.
 Identification of online question themes.
 In the current study, questions from each student during a semester were combined into one student entry so that students could be classified into different clusters based on the characteristics of their questions. The cluster analysis tool calculated each different word that appeared in the text of the entries by using the similarity metric. Then the entries were grouped into a number of clusters by NVivo 9, based on the calculated similarity index between each pair of entries. As a result, four major clusters of students were formed, based on the similarity of their questions. A multi-level, multiple cluster hierarchical structure was generated by clustering analysis (see Figure 3). These clusters were reviewed and interpreted collectively by two researchers and a graduate assistant. The two researchers had recently received specialized training about Nvivo 9 from the software producer. Differences in the review were compared, discussed, and resolved to reach an agreement. The coding results were further reviewed and discussed with an educational researcher to validate their accuracy. 
 Figure 3. A cluster hierarchical structure. 
 After a close review of the student questions in each cluster, four major themes were found (see Table 3). 
 Table 3. Four major themes in students’ online questions. 
 Descriptive statistics of final grade.
 The descriptive statistics of students’ final grades by their online questions are listed in Table 4. Overall, about half (144, 48.32%) of the participants obtained a grade of A- or higher. Among the 298 participants, 90 posted mainly check-in questions and 87 posted questions related to deadline and schedule. The number of participants who posted questions mostly related to learning and comprehension was the lowest, relative to the number of their counterparts posting questions on other themes. 
 Table 4. Descriptive statistics of final grade by online question theme (N = 298). 
 Predictive relationship between online question theme and final grade 
 In the ordinal logistic regression model (see Table 5), the results of the chi-square likelihood ratio test supported a nonzero predictive relationship between the online question theme and the final grade, χ2 (3, N = 298) = 10.017, p < .05. Furthermore, the results did not indicate the violation of the parallel lines assumption, χ2 (3, N = 298) = 2.051, p > .05. Therefore, the predictive relationship between the online question theme and the final grade remained constant across two cutoffs of final grade (Norusis, 2008). The Cox and Snell R2 and the Nagelkerke R2 were .033 and .038 respectively, and indicated a modest predictive relationship. Overall, the online question theme would prove to be a useful predictor for the final grade. The logistic regression coefficients (i.e., the location coefficients) for question themes 1, 2, and 3 were all positive and were statistically significant at the .05 level. Due to the way in which the ordinal logistic regression model was set up in SPSS (Norusis, 2008), the above statistically nonzero, positive regression coefficients suggested that the odds of getting a higher final grade, relative to all lower final grades at various cutoff values, were higher for the participants whose questions concerned learning/comprehension (Theme 4) in comparison with participants with the other three question themes (i.e., 1: Check-in; 2: Deadline/Schedule; 3: Evaluation/Technical). Specifically, for participants with the question theme of Learning/Comprehension, the odds of obtaining a grade equal to or higher than those two cutoffs (A- and B-), relative to all other lower grades, were 2.214 times higher than for the students whose questions had the theme of check-in, 3.020 times higher than those whose questions had the theme of deadline/schedule, and 2.361 times higher than the students whose questions had the theme of evaluation/technical. While using Question Theme 1, or Question Theme 2, or Question Theme 3 as the reference category respectively, no differences in the odds of obtaining better grades were found among the three theme groups. The computed predicted probabilities of obtaining a final grade of A- to A+, B- to B+, and Others, respectively, for participants in those four question theme groups (1: Check-in; 2: Deadline/Schedule; 3: Evaluation/Technical; 4: Learning/Comprehension), were 47.07%, 32.89%, 20.04% in the Theme 1 group, 40.75%, 34.77%, 24.48% in the Theme 2 group, 45.48%, 33.43%, 21.09% in the Theme 3 group, and 66.31%, 23.51%, 10.17% in the Theme 4 group. 
 Table 5. Ordinal logistic model with online question theme as the predictor for final grade (N =298). 
 Two cutoffs were set for the ordinal criterion variable, final grade, to examine how the increase in the faculty engagement score was related to the change in the odds, and in turn, to the probability of obtaining a higher final grade (O’Connell, 2006). The odds of obtaining a higher final grade at two cutoffs were the ratios of the probabilities of: A to all lower grades, and A through B- to all lower grades. The faculty engagement scores as the sample mean (i.e., 20.697 in raw score) and the one standard deviation (i.e., 5.560 in raw score) above the sample mean were examined to demonstrate the way in which the probability of obtaining a higher course final grade changed with the increase in faculty engagement (Norusis, 2008). Given an increase of one standard deviation in the faculty engagement score from the sample mean (i.e., from 20.697 to 26.257), the predicted probability of obtaining a final grade of A increased from 46.71% to 59.78% at the first cut-off. At the second cut-off, the predicted probability of obtaining a final grade of B- or higher increased from 78.79% to 86.30%. Moreover, with the faculty engagement score as the sample mean (i.e., 20.697 in raw score), the predicted probabilities of obtaining one of those three categories of course final grade (A, A- to B-, or Other) were 46.71%, 32.08%, and 21.21% respectively. While the raw faculty engagement score increased by one standard deviation to 26.257, the predicted probabilities of obtaining one of those final grades became 59.78%, 26.52%, and 13.70% respectively. Therefore, the increase in the faculty engagement score was accompanied by the increased probability of obtaining a better course final grade. 
 Discussion.
 A student’s final grade depends on many factors, including the student’s motivation, learning style, and previous background, the instructor’s teaching and grading scales, the exam’s and assignment’s difficulty levels, etc. A holistic view of student demographic and institutional variables, as opposed to the single variable, must be examined in determining the overall online learning experience (Herbert, 2008). In this study, our data shows that online VS student participation cannot be safely used to predict final grades. Perhaps the uniqueness of our VS interface (text-based chat in a live-video-streaming environment) explains our findings. Otherwise, previous studies including Macfadyen and Dawson’s study (2010) found that students’ participation and contribution to discussion boards in traditional learning management systems remain some of the strongest predictors of students’ success. However, our analysis found that there is a correlation between questions posed to instructors and chat messages posted among students. Those who chat often also interact more often with their instructor. We also analysed the chat messages (student-to-student communications) using the SPSS Clementine text mining tool. We noticed two outstanding concepts in the students’ chat messages (among themselves) and their frequency: they discussed technical problems (videos, sound, etc.) at 5% and test/exam issues at 2%. However, they addressed the same concepts in their messages to the instructor with this frequency: technical problems at 2% and test/exam issues at 2%. Thus, it seems that students are more likely to discuss technology problems with their peers and try to help each other than to discuss those issues with their instructors. The messages also revealed interaction patterns including topics related to project and assignment collaboration, discussion of grades, socialization, and greetings. In addition, the data reveals that students with a higher number of logins asked more questions and exchanged more chat messages with their classmates. In contrast, students with fewer logins rarely participated in the class; in fact, some of them rarely even logged into the system. 
 Conclusions and future research.
 This study was conducted in order to exploit the untapped data generated by LVS students. Our results revealed several student learning behaviours, ranging from active participation and interaction with the instructor to a lack of participation or even of attendance. Overall, our findings corroborate those of a previous study (Abdous & He, 2011). In spite of the limitations related to self-selection bias and to the use of final grades as a measurement of student learning outcomes (Abdous & Yen, 2010), we believe that we can provide some ways in which the learning experiences of LVS students can be improved and made more successful, based on our years of experience of working with faculty who teach VS courses. To this end, the following recommendations are made: - Ensure faculty readiness and training prior to teaching LVS courses. - Develop facilitation techniques to assist faculty in integrating LVS students into the dynamics of the classroom. - Implement a tracking system for LVS students’ attendance. - Encourage active participation and interaction during LVS sessions. - Provide students with tips on effective participation and interaction during LVS sessions (writing messages, timing of questions, etc.) As we make these recommendations, we reiterate that educational data-mining is clearly providing powerful analytical tools capable of converting untapped LMS and EPR data into critical decision-making information which has the capability of enhancing students’ learning experiences (Garcia et al., 2011). While adding to the body of literature, our hybrid approach provides a solid framework that can be used to exploit educational data to rethink and improve the learning experiences of students using some of the various new delivery modes that are currently reshaping higher education. Further understanding of students’ engagement and the dynamics of their interaction in and with these new delivery modes will contribute to the promulgation of an effective and engaging learning experience for all.]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Using Data Mining for Predicting Relationships between Online Question Theme and Final Grade</rdfs:label>
		<dc:subject>educational data mining</dc:subject>
		<dc:subject>data mining</dc:subject>
		<dc:subject>live video streaming</dc:subject>
		<dc:subject>clustering analysis</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/m-hammed-abdous"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/m-hammed-abdous"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/wu-he"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/wu-he"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/cherng-jyh-yen"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/cherng-jyh-yen"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/71/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/m-hammed-abdous"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/wu-he"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/cherng-jyh-yen"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/72">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>Dataset-Driven Research to Support Learning and Knowledge Analytics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/72/authorlist"/>
		<swrc:abstract>In various research areas, the availability of open datasets is considered as key for research and application purposes. These datasets are used as benchmarks to develop new algorithms and to compare them to other algorithms in given settings. Finding such available datasets for experimentation can be a challenging task in technology enhanced learning, as there are various sources of data that have not been identified and documented exhaustively. In this paper, we provide such an analysis of datasets that can be used for research on learning and knowledge analytics. First, we present a framework for the analysis of educational datasets. Then, we analyze existing datasets along the dimensions of this framework and outline future challenges for the collection and sharing of educational datasets.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 The need for better measurement, collection, analysis and reporting of data about learners has been identified by several researchers in the Technology Enhanced Learning (TEL) field (Siemens 2010; Romero et al. 2007; Duval 2011). This need has been translated into an emerging strand of research on learning and knowledge analytics (LAK), as reflected by a number of conferences and special issues in recent years (Siemens & Gasevic, 2011). Among others, the analysis of learner data and identification of patterns within these data are researched to predict learning outcomes, to suggest relevant resources and to detect error patterns or affects of learners. These objectives are researched to act upon needs of a variety of stakeholders, including learners, teachers and organizations. This is what drives major initiatives such as the US-based Learning Registry (http://www.learningregistry.org) to collect data and make them publicly available for research and application purposes. Siemens (2010) defines learning analytics as “the use of intelligent data, learner-produced data, and analysis models to discover information and social connections, and to predict and advise on learning.” Contributions to the first conference on learning analytics and knowledge in 2011 indicate that information visualization, social network analysis and educational data mining techniques offer interesting perspectives for this emerging field. Whereas the specific techniques differ depending on context and the intended goals, the main objective of the approaches is to identify needs of target users and to support these needs using intelligent and adaptive systems. Despite the recognition of the importance of LAK, the literature related to this topic is rather limited. Research on web analytics, search engines and recommender systems are excellent examples of how data gathering during an analytics cycle can be used to refine offerings to users (Elias, 2011). Whereas several recommender systems for learning (Manouselis et al., 2011), intelligent tutoring systems (Romero et al. 2007) and visual analytics systems (Govaerts et al. 2010) have been implemented for use in learning scenarios in recent years, many of these intelligent tools often stay in researcher hands and rarely go beyond the prototype stage (Reffay & Betbeder, 2009). Among others, researchers have argued that the time needed by social science to validate prototypes is too long compared to the rate of technology innovation. An important component to facilitate research in this area is the existence of extensive overviews of the available datasets that will provide researchers with a wide array of potential data sources to experiment with, as well as with an analysis of their properties that will help researchers decide about their appropriateness for their experiments. Such an overview is missing from LAK today, since only initial attempts have been made to document and study existing datasets (Drachsler et al., 2010). In this article, we extend our initial analysis (Verbert et al., 2011) of the datasets collected by the dataTEL Theme Team of the European Network of Excellence STELLAR (http://www.teleurope.eu/pg/groups/9405/datatel/), in order to provide a more comprehensive overview of datasets for LAK research. The article makes four primary research contributions: - First, we present related initiatives that are collecting datasets and the needs and opportunities to make educational datasets available for LAK research. - Second, we present a framework for the analysis of educational datasets. In particular, we present properties of educational datasets and LAK objectives that can benefit from the availability of such data. - Third, we analyze existing datasets along the dimensions of our educational dataset framework. We also discuss existing research that has used these datasets for LAK related research. - Finally, we present future challenges to enable the sharing and reuse of datasets among researchers in this field. 
 Background.
 In an increasing number of scientific disciplines, large data collections are emerging as important community resources (Chervenak et al., 2000). These datasets are used as benchmarks to develop new algorithms and compare them to other algorithms in given settings (Manouselis et al., 2010b). In datasets that are used for recommendations algorithms, such data can for instance be explicit (ratings) or implicit (downloads and tags) relevance indicators. These indicators are then for instance used to find users with similar interests as a basis to suggest items to a user. To collect TEL datasets, the first dataTEL Challenge was launched as part of a workshop on Recommender Systems for TEL (RecSysTEL, Manouselis et al., 2010a) that was jointly organized by the 4th ACM Conference on Recommender Systems and the 5th European Conference on Technology Enhanced Learning in September 2010. In this call, research groups were invited to submit existing datasets from TEL applications. A special dataTEL Cafe event took place during the RecSysTEL workshop in Barcelona to discuss the submitted datasets and to facilitate dataset sharing in the TEL community. Related work is carried out at the Pittsburgh Science of Learning Center (PSLC). The PSLC DataShop (Stamper et al., 2010) is a data repository that provides access to a large number of educational datasets derived from intelligent tutoring systems. Currently, more than 270 datasets are stored that record 58 million learner actions. Several researchers of the educational data mining community have used these datasets to predict learner performance. The Mulce project (Reffay & Betbeder, 2009) is also collecting and sharing contextualized interaction data of learners. A platform is available to share, browse and analyze shared datasets. At the time of writing, 34 datasets are available on the portal, including a dataset of the Virtual Math Teams (VMT) project. This project investigated the use of online collaborative environments to support K-12 mathematics learning. These datasets have been used extensively by the Computer Supported Collaborative Learning (CSCL) community (Stahl, 2009). Other efforts have been driven by fields studying child language acquisition. The CHILDES system (MacWhinney, 1996, 2007) helped realize much advancement in this field through sharing language-learning data. TalkBank (http://talkbank.org) is a follow-up project that is researching guidelines for ethical sharing of data, metadata and infrastructure for identifying available data, and education of researchers to the existence of shared data, tools, standards and best practices. LinkedEducation.org is another initiative that provides an open platform to promote the use of data for educational purposes. At the time of writing, five organizations have contributed datasets. Available datasets describe the structure of organizations and institutions, the structure of courses, learning resources and interrelationships between people. In addition, various schemas and vocabularies are provided to describe the internal structure of an academic institution, discourse relationships, activity streams in social networks and educational resources. Such schemas and vocabularies offer interesting perspectives for the sharing and reuse of educational interaction data that is relevant for LAK research. Several other initiatives are available that focus on providing the means to share datasets among researchers in a more generic way. DataCite.org is an organization that enables users to register research datasets and to assign persistent identifiers to them, so that datasets can be handled as citable scientific objects. The Dataverse Network (King, 2007) is an open-source application for publishing, citing and discovering research data. The network was established at Harvard University and is aimed to increase scholarly recognition for data contributions. Fact sheets of datasets are gathered from organizations and researchers are encouraged to make the data publicly available, if possible. The Australian National Data Service (Treloar & Wilkinson, 2008) is a similar initiative in Australia that works on services to help researchers persistently identify and describe data. In this paper, we analyze educational datasets that have been collected by dataTEL and related initiatives. We focus specifically on datasets that contain interaction/usage data of learners and that can be used for analytics’ research. In the next section, we present a framework for educational datasets that can be used to describe and analyze educational datasets. In addition, we discuss how work of related initiatives fits within this framework. Then, we analyze available datasets along the dimensions of this framework. 
 A Framework for Educational Datasets.
 In this section, we present a framework for the analysis of educational datasets. The framework is intended to address questions researchers might have about the potential usefulness of a dataset for their research purposes. As illustrated in Figure 1, the framework constitutes three parts. Dataset properties describe the overall dataset, such as the application and the educational setting from which the data was collected. Data properties define at a finer grained level where data elements available, including action types such as downloads or selects and information about the learner and other entities involved. The third part of the framework defines a list of objectives of LAK research. These objectives are mapped to dataset and data properties in the next section to determine the potential usefulness of a dataset for LAK research purposes. 
 Figure.

 1. Educational dataset framework. 
 Dataset properties.
 We recently (Drachsler et al., 2010) presented a specification of datasets that was used for the first dataTEL challenge. Among others, the specification includes information about the application in which the dataset has been collected, the educational setting, contact person, availability (open access or legal protection rules that describe how and when the dataset can be used), dataset collection method, dataset statistics, and pre-processing steps that have been applied to the data. Related initiatives have also defined formats to package and describe datasets. As illustrated in Figure 2, a Mulce dataset is comprised of the following components: - The instantiation component includes all interaction data, as well as user information. - The learning design component describes the educational scenario. - The research protocol describes the methodology of research with the dataset. - The license component specifies dataset provider and user rights. - The analyses component contains research outputs. 
 Figure 2. Mulce format (Reffay & Betbeder, 2009). 
 The PSLC DataShop project defines a specification for describing datasets that are derived from intelligent tutoring systems. The specification includes the project name, principal investigator, curriculum, collection dates, domain, application, description, hypothesis (e.g., “people who are required to use the tutor show less error on quizzes”), school, statistics and knowledge models of interactions. We discuss such interaction models in the next section. In addition, research papers with the datasets are referenced. As illustrated in Table 1, there are many similarities between the specifications. Explicit information is indicated by “+” signs. This information constitutes explicitly articulated elements of the specifications. Implicit information is indicated by “(+)” signs and represents information that is implied or expressed as part of other elements. For instance, in the dataTEL specification, information about the domain or users can be described as part of the description of the application or environment, but no specific fields are provided for these elements. To date, the Mulce format provides the most comprehensive format for describing datasets. In addition to interaction data, the datasets incorporate a detailed description of the educational scenario in a learning design component and results of various analyses. Therefore, this specification provides the most interesting perspectives for describing educational datasets in a generic way. 
 Data properties.
 In addition to a format for describing datasets, there is a need to identify at a more fine-grained level of granularity which data elements are stored. Such information is essential to identify for which research purposes a dataset is useful. As outlined by Romero et al. (2007), the TEL field differs from the e-commerce analytics field in several ways. In e-commerce, the used data are often simple web server access logs or ratings of users on items. In TEL, many researchers use more information about a learner interaction (Pahl & Donnellan, 2002). The user model and the objectives of the systems are also different in both application domains (Drachsler et al., 2009). 
 Table 1. Overview dataset properties. 
 Figure 3. Learner Action Model. 
 A survey of existing TEL interaction data models has been presented in (Butoianu et al., 2010). Such models capture actions of users on resources, such as open/close, select/unselect or write actions, on resources. In addition, the context in which an action occurred can be captured, such as the current application the author is working with or her current task. The Atom activity stream RDF mapping of the LinkedEducation.org initiative provides such a model for actions of users in social networks. Vocabularies for actions, actors and objects involved and related contextual information are defined. In addition to interaction models, learner models have been elaborated that describe several characteristics of learners. Brusilovsky and Millan (2007) identified the following categories based on an extensive analysis of the literature: knowledge levels, goals and tasks, interests, background and learning and cognitive styles. In addition, several models, standards and specifications have been elaborated to describe learning resources. The IEEE LOM and Dublin Core metadata standards are prominently used to describe learning resources, including general characteristics, such as title, author and keywords, technical and educational characteristics and relations between learning resources. We integrated the various data categories and elements in Figure 3. We use this model in the remainder of this article to identify data elements in existing datasets. The model has been developed by synthesizing existing works on interaction data and context variables in the TEL field that were outlined above. It could be further refined by studying relevant theoretical frameworks, like the Activity Theory (Kaptelinin et al., 1995), which could help reorganize the various categories and elements. Future research work in this area is discussed in the last section. 
 Learning and knowledge analytics objectives.
 In order to provide guidance on the relevancy of datasets for LAK research, we identify a set of objectives that are relevant for LAK applications. We also outline existing research work in related research communities that, when interconnected, can provide substantial synergies to advance the emerging LAK field. - Predicting learner performance and modeling learners. The prediction of learner performance and modeling of learners have been researched extensively by the educational data mining, educational user modeling and educational adaptive hypermedia communities. The objective is to estimate the unknown value of a variable that describes the learner, such as performance, knowledge, scores or learner grades (Romero & Ventura, 2007). Such predictions are for instance used by intelligent tutoring systems to provide advice or hints when a learner is solving a problem. Dynamic learner models are also researched to support adaptation in educational hypermedia systems (Brusilovsky & Millan, 2007). - Suggesting relevant learning resources. Recommender systems for learning have gained increased interest in recent years. A recent survey of TEL recommender systems has been elaborated by Manouselis et al. (2011). These systems typically analyze learner data to suggest relevant learning resources, peer learners or learning paths. - Increasing reflection and awareness. Several researchers are focusing on the analysis and visualization of different learning indicators to foster awareness and reflection about learning processes. These indicators include resource accesses, time spending and knowledge level indicators (Mazza & Milani, 2005). - Enhancing social learning environments. Analysis and visualization of social interactions is researched to make people aware of their social context and to enable them to explore this context (Heer & boyd, 2005). In TEL, this is particularly, but not only, relevant for Computer Supported Collaborative Learning (CSCL) (Stahl, 2009), where the interactions with peer learners are a core aspect of how learning is organized. In CSCL, much research has focused on the analysis of networks of learners, typically with a social network analysis approach (Reffay & Chanier, 2003). - Detecting undesirable learner behaviors. The objective of detecting undesirable learner behavior is to discover those learners who have some type of problem or unusual behavior, such as erroneous actions, misuse, cheating, dropping out or academic failure (Romero & Ventura, 2007). - Detecting affects of learners. Researchers in TEL often refer to the affective states defined by D’Mello et al. (2007). These states are classified as boredom, confusion, frustration, eureka, flow/engagement, versus neutral. Among others, the detection of affects is researched to adjust pedagogical strategies during learning of complex material. The objectives are highly interrelated. For instance, whereas research on affects and awareness and reflection has traditionally focused on an individual perspective, these objectives are also researched increasingly to enhance social learning environments. 
 Datasets for Learning and Knowledge Analytics.
 In this section, we present an analysis of datasets that can be used for a wide variety of LAK research purposes. We analyze the datasets along the dimensions of the dataset framework that we presented in the previous section. 
 Table 2. Overview dataset properties. 
 Dataset properties.
 Table 2 provides an overview of characteristics of available educational datasets, including the application from which data were collected, collection period, statistics and educational context or domain. The full description of the datasets is available on the portals that provide access to these datasets, including the dataTEL (http://www.teleurope.eu/pg/pages/view/50630/), DataShop (https://pslcdatashop.web.cmu.edu/) and Mulce (http://mulce.univ-bpclermont.fr:8080/PlateFormeMulce/) portals. 
 Several dataTEL datasets have been collected from learning management systems (LMS). The UC3M dataset also collects data from a virtual machine that was used in a C programming course. The particularity of this dataset is that it records actions from several tools learners are using. The approach enables to collect a more comprehensive overview of learner activities, such as a learner searching for additional resources on the web. Such an approach is also researched under the prism of personal learning environments (PLEs), where data is tracked from learning environments that assemble relevant tools for course activities. Many other dataTEL datasets were collected from web portals that provide access to large collections of learning resources. Several other datasets are collected from intelligent tutoring systems (ITS) – including a large number of datasets from the PSLC DataShop initiative. We include the “Algebra 2008-2009” and “Bridge to Algebra” datasets that were used for the KDD 2010 Cup on educational data mining (https://pslcdatashop.web.cmu.edu/KDDCup/) in this analysis. In addition, the recommended datasets of the DataShop are analyzed. At the time of writing, 64 datasets are publicly available. Finally, many of the Mulce datasets contain data that were captured from forums, chat and email conversations between learners in collaborative learning settings. The collection period varies from 10 days to 6 years. Several of the Mulce datasets capture data of group work during a specific learning activity. Datasets derived from learning management systems and web portals often capture data during a longer period of time, ranging from a couple of months to several years. Although a few datasets that are available capture data of a large number of users, many other datasets are more limited in size. Some datasets collect data of 1000 to 7000 users. Several other datasets capture data of a few learners only. These datasets are in some cases only a sample that the organization made available or in other cases datasets of a small number of collaborating users, such as the VMT and mce-copeas datasets. Several datasets are openly accessible. For other datasets, legal protection rules apply. We obtained these datasets by sending a statement of our intended research purposes to the organization and then signed an agreement on the use of these data. All datasets contain data that is anonymized, so that it can no longer be linked to an individual. 
 Data properties.
 Table 3 presents a more detailed overview of the data elements that are included in the datasets. The datasets contain a diverse set of actions of users. These actions include attempts of learners on quizzes, search actions, selection, annotation, rating, creation or editing of resources. PSLC datasets derived from intelligent tutoring systems all include attempt actions on activities provided by the tutor. In some datasets, help requests are stored. The input provided by learners is sometimes further specified into select, write or create actions. The Mulce datasets capture social interactions – in most cases these constitute send and receive actions. Explicit information about learners (or teachers) is stored in only a few datasets. The data is in most cases anonymized and little additional information about learners or teachers is stored. Some dataTEL datasets contain information about the language, interests, knowledge level or country of the user. Some DataShop datasets describe the gender and knowledge level of the learner, including her past grades. The mce-copeas dataset divides learners in three groups according to their knowledge level (beginner, medium, expert). Information about country, age, language and gender is often provided in Mulce datasets. Information about resources is available in more datasets. The information ranges from an identifier of the resource to detailed descriptions that include educational characteristics such as duration, minimum age, maximum age and resource type, technical characteristics and annotations such as tags and comments. Such metadata are often provided in dataTEL datasets that were captured from learning repository portals. In the DataShop datasets, educational information such as average duration and required skills are sometimes provided. In addition, compositional relations are provided that define a hierarchy of units and sections. Social relations between learners collaborating are stored in the Mulce and some dataTEL datasets. Additional context information is also stored. Several datasets provide timestamp information. The duration of an action is stored explicitly as a time interval in the DataShop and some LMS datasets. Such information is valuable to calculate the difference of estimated durations, described in resource metadata, and the time the learner needed in practice to complete an activity. Other contextual information is not often available. In datasets that contain data of multiple tools and services, information about the application from which an action was triggered is included. 
 Table 3. Overview data properties. 
 Finally, the result of actions, such as correct or incorrect attempts, rating values or error messages, is stored. In addition, some datasets contain the grade a learner obtained for an activity or course. We elaborate in the next section how such data can be used for LAK research. 
 Usefulness of available datasets for LAK related research objectives. 
 Prediction of learner performance and discovering learner models. 
 Several datasets are available that can support research on prediction of learner performance and discovery of learner models. Among others, such predictions are researched to provide advice when a learner is solving a problem (Romero et al., 2007). Datasets from intelligent tutoring systems that capture attempts of learners provide a rich source of data to estimate the knowledge level of a learner. Some datasets derived from LMSs contain data on the number of attempts and total time spent on assignments, forums and quizzes. Romero et al. (2008) compared different data mining techniques to classify learners based on such LMS data and the final grade obtained for courses. Datasets that have been captured from PLEs offer interesting perspectives to elaborate such research in open learning environments. In addition, many datasets are suitable to identify interests of users based on resource accesses. Several researchers have already experimented with the datasets outlined above to predict learner attributes. The “Algebra 2008-2009” and “Bridge to Algebra” datasets were used in the KDD Challenge 2010. Participants were asked to learn a model from past learner behavior and to predict their future performance. The winners of this competition combined several educational data mining techniques. Cen et al. (2007) performed a learning curve analysis with the “Geometry Area” dataset. They noticed that while learners were required to over-practice some easy target skills, they under-practiced harder skills. Based on this observation, they created a new version of the tutor by resetting parameters that determine how often skills are practiced. References to other studies with these datasets are available on the DataShop portal. 
 Suggesting learning resources. 
 Several dataTEL datasets contain explicit relevance indicators in the form of ratings that are relevant for research on recommendation algorithms for learning. In addition, implicit relevance indicators, such as downloads, search terms and annotations, are available that can be used for such research. If time interval data is available, the data might be suitable to extract reading times in order to determine the relevancy of a resource. In addition, such datasets are useful to analyze information about sequences of resources as a basis to suggest learning paths. Manouselis et al. (2010b) used the Travel well dataset to evaluate recommendation algorithms for learning. Similar experiments have been reported in (Verbert et al., 2011). In this study, the Mendeley and MACE datasets were also used. Although still preliminary, some conclusions were drawn about successful parameterization of collaborative filtering algorithms for learning. Outcomes suggest that the use of implicit relevance indicators, such as downloads, tags and read actions, are useful to suggest learning resources. 
 Increasing reflection and awareness. 
 Several datasets are useful for analysis and visualization of different learning indicators to foster awareness and reflection about learning processes. In addition to indicators about the knowledge level of learners, several datasets contain indicators of the time learners spend on learning activities – such as the PSLC DataShop datasets. Other datasets contain timestamp information that can be used to derive indicators of the time users were active. dataTEL datasets were for instance used to obtain such indicators as a basis to support awareness for teachers (Govaerts et al. 2010). A visualization of these indicators applied to the ROLE dataset is illustrated in Figure 4. Evaluation results indicate that the perceived usefulness for teachers is high. The MACE dataset has been used for research on reflection and awareness of resource accesses. The Zeitgeist application (Shmidz et al., 2009) gives users insight into which learning resources they accessed, how they found them and which topics have been of interest to them (see Figure 5). 
 Figure 4. Visualization of time indicators (Govaerts et al., 2010). 
 Figure 5. MACE Zeitgeist (Shmidz et al., 2009). 
 Enhancing social learning environments.
 Several Mulce datasets are useful for research on collaborative learning. The datasets have been captured from chat tools, forums or email clients. Such data can be analyzed to predict and advice on learning in group work. Datasets that have been captured from LMSs often capture messages within course forums. Some of the PSLC DataShop datasets capture collaborative activities with intelligent tutoring systems, including the “Electric Fields – Pitt” dataset. Several datasets have already been used to support research on enhancing social learning environments. Research with the “Electric Fields – Pitt” dataset suggests that asking learners to solve problems collaboratively with an intelligent tutoring system is a productive way to enhance learning from an ITS (Hausmann et al., 2008). Several Mulce datasets have been used for research on collaborative learning (Stahl, 2009). Among others, the datasets have been used to understand mathematical ideas and reasoning in chat by learners, interaction mechanisms used by online groups to sustain knowledge building over time and the measurement of cohesion in collaborative distance learning. Evaluation studies showed that such analysis, when embodied in visualization tools (see Figure 6), can efficiently assist the teacher in following the group collaboration (Reffay & Chanier, 2003). These analyses were used to highlight isolated people, active sub-groups and various roles of the members in group communication. The mce-copeas dataset has been used to research the influence of synchronous communication during online collaborative writing activities (Ciekanski & Chanier, 2008). Several other studies are documented on the Mulce portal. 
 Figure 6. Matrix and graphical representation of e-mail exchange (Reffay & Chanier, 2003). 
 Detecting undesirable learner behaviors.
 Datasets derived from PLEs provide a rich source of data to detect unusual behavior, as these datasets record actions of learners with several tools they were using during the classes. Data from LMSs can be used to detect potential dropouts when learners are no longer active. ITS datasets are also suitable for research on unusual behavior. Baker et al. (2006) found that learners who were “gaming the system” (i.e., fast and repeated requests for help to avoid thinking) had the largest correlation with poor learning outcomes. 
 Figure 6. Pattern visualization of UC3M dataset (Scheffel et al., 2011). 
 Scheffel et al. (2011) used the UC3M dataset to identify key actions from observed learning behavior. The authors employed data mining techniques to extract frequent patterns of actions. These patterns were visualized to support teaching activities. For instance, the pattern illustrated in Figure 7 points to development flows in which for each compilation students opened a file and closed it again before compiling. According to the teaching staff, such actions translate into a significant increase in development time and should be corrected. 
 Detecting affects of learners.
 Some datasets are suitable for research on the detection of affects and motivational aspects. For instance, PSLC DataShop datasets can be used to extract motivational aspects by comparing the time a learner spends on a learning activity in an ITS with the expected or average time of other learners. The use of emoticons and affective words is researched in social interaction datasets (Reffay et al., 2011). Prominent research in building a user model of affect from real data has been conducted by Conati and Maclaren (2005). Ongoing research with the UC3M dataset is focused on the detection of affects of learners, such as frustration and (dis-)engagement. Based on an analysis of sequences of actions, such as a sequence of error messages of a debugger and successful compilations, information is deduced about potential engagement or frustration. 
 Conclusion and future challenges.
 In this article, we have presented an overview of datasets that can be used for exploratory research on LAK. Several datasets have been identified and analyzed along the dimensions of our educational dataset framework. Our analysis indicates that an initial collection of interesting datasets is already available. These datasets have been used in a successful way by several researchers, for diverse research purposes that are relevant for LAK analytics. The analysis provides researchers with an overview of available datasets, as well as their properties to help decide about their appropriateness for their experiments. In addition, the analysis sheds some light on what data to track for LAK research. The datasets were collected by our dataTEL and related initiatives, with the common objective to make educational datasets available to researchers. Nevertheless, our endeavors to collect and share datasets for research remain quite challenging. A first challenge is related to privacy rights and licensing of educational data. Although an enormous amount of data has been captured from learning environments, it is a difficult process to make these data available for research purposes. The issue of usage rights/licensing needs to be solved from two perspectives. From a user perspective, learners need to be informed and grant permission to collect their data and make it available for research purposes. Also the organization or provider of these data needs to agree with collecting and sharing these data. For instance, researchers have in some cases collected datasets by crawling data from websites and then found out that they were not allowed to do so. The collection of the UC3M dataset is a good example of how data can be made available for research purposes. In a first stage, learners were informed about which data were collected during their course activities and signed an agreement that the data could be used for research purposes, as is required by the Spanish law on data protection and privacy. In a second stage, researchers signed an agreement on the use of the dataset for research purposes. In order to facilitate the collection of educational datasets, it is important to share such practices and to contribute to the documentation of legal data protection and privacy laws. In addition, providing guidelines for anonymization of data and creating incentives for researchers and organizations to share their datasets is important. The first dataTEL challenge requested submissions of dataset fact sheets in a pre-defined format so that required properties could be well documented. The approach is similar to work of the DataVerse Network (King, 2007). A second challenge is related to the heterogeneity of educational datasets. The lack of a standard representation for interaction data within datasets prevents the sharing and reuse of data across systems. In addition, when a custom data format is not well documented, it may be difficult to assess the meaning and usefulness of data elements that are stored. To address this issue, the development of a standard representation for learner interaction data will be taken up by a working group of the CEN Workshop on Learning Technologies (WS/LT). 
 This challenge sets the context of a third challenge, the identification of relevant data about learners and other entities involved for LAK research. Whereas we were able to identify some data elements that can be used as input for the CEN WS/LT working group, additional research is required to identify a broader set of elements that are useful for LAK research. In addition, the model could be further refined by studying relevant theoretical frameworks, like the Activity Theory (Kaptelinin et al., 1995), which could help us reorganize the various categories and elements according to well-established frameworks. A fourth challenge is the development of data sensors to collect data. Data that is tracked within learning management systems provides a good basis for exploratory research on learning and knowledge analytics. However, learners often use a wide variety of tools and services in addition to a traditional LMS. Ongoing work within the ROLE project (http://www.role-project.eu) is focused on collecting data from PLEs that aggregate several tools and services. The approach is inspired by wakoopa (http://social.wakoopa.com) and rescuetime (http://www.rescuetime.com) that install tracker tools on the machine of a user and automatically record all activities. In addition, data sharing and reuse in the educational field needs further research to explore whether the context and scope of the dataset collection can significantly affect its potential reuse. This is a requirement to keep in mind, as relevant discussions on productive multivocality in CSCL have indicated that there is a possibility that data collected using one theoretical framing may be unsuitable for analysis under another framing. The issue also indicates that clear descriptions of datasets along various dimensions describing such properties, as provided by the Mulce and DataShop specifications, are important for exchanging and possibly reusing datasets that have been collected in various settings. Finally, the interconnection of several efforts in the area of educational dataset collection is key to advance this work. dataTEL has recently been accepted as an EATEL Special Interest Group (SIG). The dataTEL SIG aims to bring together existing efforts in the area of dataset collection and sharing. The SIG will organize yearly workshops and three monthly virtual meetings. With the organization of these events, we hope to enable collection and sharing of datasets on a much larger scale than available today. 
 Acknowledgements.
 Part of this work was supported by the dataTEL Theme Team of the FP7 STELLAR Network of Excellence (no. 231913). We thank all members involved, and in particular Martin Wolpers, Miguel-Angel Sicilia, Stefanie Lindstaedt and Riina Vuorikari, for their contributions to the organization of dataTEL events. Katrien Verbert is a Postdoctoral Fellow of the Research Foundation-Flanders (FWO). The work of Nikos Manouselis has been funded with support by the EU project VOA3R-250525 of the CIP PSP Programme (http://voa3r.eu).]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Dataset-Driven Research to Support Learning and Knowledge Analytics</rdfs:label>
		<dc:subject>learning and knowledge analytics</dc:subject>
		<dc:subject>datasets</dc:subject>
		<dc:subject>open science</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/72/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/73">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>Design and Implementation of a Learning Analytics Toolkit for Teachers</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/73/authorlist"/>
		<swrc:abstract>Learning Analytics can provide powerful tools for teachers in order to support them in the iterative process of improving the effectiveness of their courses and to collaterally enhance their students’ performance. In this paper, we present the theoretical background, design, implementation, and evaluation details of eLAT, a Learning Analytics Toolkit, which enables teachers to explore and correlate learning object usage, user properties, user behavior, as well as assessment results based on graphical indicators. The primary aim of the development of eLAT is to process large data sets in microseconds with regard to individual data analysis interests of teachers and data privacy issues, in order to help them to self-reflect on their technology-enhanced teaching and learning scenarios and to identify opportunities for interventions and improvements.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 Learning Management Systems (LMS) or Virtual Learning Environments (VLE) are widely used and have become part of the common toolkits of educators (Schroeder, 2009). One of the main goals of the integration of traditional teaching methods with technology enhancements is the improvement of teaching and learning quality in large university courses with many students. But does utilizing a VLE automatically improve teaching and learning? In our experience, many teachers just upload existing files, like lecture slides, handouts and exercises, when starting to use a VLE. Thereby availability of learning resources is improved. For improving teaching and learning it could be helpful to create more motivating, challenging, and engaging learning materials and e.g., collaborative scenarios to improve learning among large groups of students. Teachers could e.g., use audio and video recordings of their lectures or provide interactive, demonstrative multimedia examples and quizzes. If they put effort in the design of such online learning activities, they need tools that help them observe the consequences of their actions and evaluate their teaching interventions. They need to have appropriate access to data to assess changing behaviors and performances of their students to estimate the level of improvement that has been achieved in the learning environment. With the establishment of TEL, a new research field, called Learning Analytics, is emerging (Elias, 2011). This research field borrows and synthesizes techniques from different related fields, such as Educational Data Mining (EDM), Academic Analytics, Social Network Analysis or Business Intelligence (BI), to harness them for converting educational data into useful information and thereon to motivate actions, like self-reflecting ones previous teaching or learning activities, to foster improved teaching and learning. The main goal of BI is to turn enterprise data into useful information for management decision support. However, Learning Analytics, Academic Analytics, as well as EDM more specifically focus on tools and methods for exploring data coming from educational contexts. While Academic Analytics take a university-wide perspective, including also e.g., organizational and financial issues (Campbell & Oblinger, 2007), Learning Analytics as well as EDM focus specifically on data about teaching and learning. Siemens (2010) defines Learning Analytics as “the use of intelligent data, learner-produced data, and analysis models to discover information and social connections, and to predict and advise on learning.” It can support teachers and students to take action based on the evaluation of educational data. However, the technology to deliver this potential is still very young and research on understanding the pedagogical usefulness of Learning Analytics is still in its infancy (Johnson et al., 2011b; Johnson et al., 2012). 
 It is a current goal at RWTH Aachen University to enhance its VLE—the learning and teaching portal L²P (Gebhardt et al., 2007)—with user-friendly tools for Learning Analytics, in order to equip their teachers and tutors with means to evaluate the effectiveness of TEL within their instructional design and courses offered. These teachers still face difficulties, deterring them from integrating cyclical reflective research activities, comparable to Action Research, into everyday practice. Action Research is characterized by a continuing effort to closely interlink, relate and confront action and reflection, to reflect upon one’s conscious and unconscious doings in order to develop one’s actions, and to act reflectively in order to develop one’s knowledge.“ (Altrichter et al., 2005, p. 6). A pre-eminent barrier is the additional workload, originating from tasks of collecting, integrating, and analyzing raw data from log files of their VLE (Altenbernd-Giani et al., 2009). To tackle these issues, we have developed the “exploratory Learning Analytics Toolkit” (eLAT). The main aim of eLAT is to support reflection on and improvement of online teaching methods based on personal interests and observations. To help teachers reflect on their teaching according to their own interests, the desired Learning Analytics tool is required to provide a clear, simple, easily interpretable and usable interface while, at the same time, being powerful and flexible enough for data and information exploration purposes. Therefore, eLAT was designed to enable teachers to explore and correlate content usage, user properties, user behavior, as well as assessment results based on individually selected graphical indicators. Dyckhoff et al. (2011) gave a short overview of the toolkit. In the remainder of this paper, we present the theoretical background, design, implementation, and evaluation results of eLAT in more detail. In section 2 (theoretical background), we provide information on the theoretical background and briefly describe the results of a requirements analysis and its implications for Learning Analytics tools. In section 3 (eLAT: exploratory Learning Analytics Toolkit), we discuss the design, implementation, and evaluation of eLAT. In section 4 (Related Work), we compare our approach to state-of-the-art solutions. Even though there are some approaches to support teachers in their ongoing evaluation and improvement activities (e.g., Johnson et al., 2011a; García-Saiz & Zorilla PantaLeón, 2011; Mazza & Dimitrova, 2007; Pedraza-Perez et al., 2011), many challenges remain (Chatti et al., 2012). Examples include integration with other VLEs and integration of diverse data sources, minimizing the time delay between the capture and use of data, consideration of data privacy issues, protection of students’ identities and prevention of data misuse, enabling data exploration and visualization manipulation based on individual data analysis interests, providing the right information to the right people right away, and investigating which captured variables may be pedagogically meaningful (Elias, 2011; Johnson et al, 2011b). By developing eLAT, we have tried to tackle these challenges. The final section 5 (Conclusion and Outlook) gives a summary of the main findings of this study according to the challenges mentioned above and outlines perspectives for future work. 
 Theoretical background.
 In TEL, masses of data can be collected from different kinds of student actions, such as solving assignments, taking exams, online social interaction, participating in discussion forums, and extracurricular activities. This data can be used for Learning Analytics to extract valuable information, which might be helpful for teachers to reflect on their instructional design and management of their courses. Usable EDM and Learning Analytics tools for teachers that support cyclical research activities are still missing in most current VLE or are far from satisfactory (Hijon & Carlos, 2006). Romero et al. state “[…] data mining tools are normally designed more for power and flexibility than for simplicity. Most of the current data mining tools are too complex for educators to use and their features go well beyond the scope of what an educator might require” (Romero et al., 2007, p. 369). If tracking data is provided in a VLE, it is often incomprehensible, poorly organized, and difficult to follow, because of its tabular format. As a result, only skilled and technically savvy users can utilize it (Mazza & Dimitrova, 2007). But even for them it might be too time consuming. Moreover, unnecessary personal information of students can be observed by teachers or even fellow students, i.e., data privacy issues are ignored in the design of most VLE (Loser & Herrmann, 2009). Legal issues have to be taken into account to prevent malpractice. In Germany, for example, teachers are not allowed to have access to everything a student does in their online courses. They are only supposed to have access to data that is relevant for teaching in a form that is transparent to students (Directive 95/46/EC, 1995; Federal Data Protection Act, 1990). However, many research questions of teachers are concerned with general learning processes of a whole group of students in contrast to gaining more knowledge about a single student. Therefore, to ensure data privacy, student data could be pseudonymized in a preprocessing step by using, e.g., a hash instead of a student-ID, and by presenting summarized results in form of visualizations that rather show group processes and do not allow to focus on one student. Further deficiencies of reporting tools are related to usability and clarity as well as completeness of the delivered results, such as the lack of possibilities to integrate results of online questionnaires with data from logs. Many teachers are motivated to evaluate their courses and they already have research questions related to their teaching in mind. For example, a teacher who offers weekly online exercises has the intention to help her students to prepare for an exam. But she is not sure if the currently available exercises are helpful enough for this purpose. Therefore, the teachers would like to know if those students who practice with her online exercises on a continually basis are better in the final exam than students who do not use them. A Learning Analytics toolkit could help her to do research on this hypothesis by automatically collecting, analyzing, and visualizing the right data in an appropriate way. Yet, most monitoring and reporting tools found in current VLEs are designed to collect, analyze, and visualize data in a static tabular form that was predefined by system developers. Teachers face the difficulty that appropriate and usable Learning Analytics tools that help them answer their individual questions continuously and efficiently are missing in prevalent VLEs, since most of the work in the area of Learning Analytics is conceptual (Johnson et al., 2011b). Teachers should have access to Learning Analytics tools (e.g., provided via dashboards) that can be integrated into a VLE or other learning environments. These tools should allow for interactive configuration in such a way that its users could easily analyze and interpret available data based on individual interests. Results of Learning Analytics and EDM should be presented in a clear and understandable format, e.g., information visualizations that are understandable without data mining expert knowledge. Card et al. (1999) define the term visualization as “the use of computer-supported, interactive, visual representations of data to amplify cognition.” It “promises to help us speed our understanding and action in a world of increasing information volumes” (Card, 2003, p. 542). It has been widely argued in the EDM literature that teachers can grasp the information more easily and quickly when presented through comprehensible information visualizations. Mazza and Dimitrova (2007, p. 138), for instance, write: “…the effectiveness of [Course Management Systems] can be improved by integrating [Information Visualization] techniques to generate appropriate graphical representations ….” Also, results of a recent study by Ali et al. (2012) showed that “visualization can be an effective mean to deal with larger amounts of data in order to sustain the cognitive load of educators at an acceptable level” (p. 486) and “multiple ways of visualizing data increase the perceived value of different feedback types” (p. 488). Still, it should be noted that in some analytical cases visualizations could be ineffective with respect to a textual or a tabular interface, e.g., if details about many items are very important. In our approach, we only indicate certain facts about the usage and properties of the learning environment and try to visualize them appropriately. Therefore, we revert to the concept of indicators, which can be described as specific calculators with corresponding visualizations, tied to a specific question. For example, if the teacher’s question is “Are those students who practice with online exercise on a continually basis are better in the final exam than students who do not use them,” the corresponding indicator could show a chart that quickly facilitates a visual data comparison. Indicator concepts have been used before. Glahn (2009) for example, introduced the concept of smart indicators, which he defined as “a context aware indicator system, which dynamically aligns data sources, data aggregation, and data presentation to the current context of a learner” (Glahn, 2009, p. 19). However, in our case the target group differs. The eLAT indicators are collecting and visualizing data of students to present them to teachers. A typical Learning Analytics process is depicted in figure

 1. The process starts with the data-gathering step. In this step, data is collected from different learners’ activities when they interact with learning elements within a VLE, LMS or a personal learning environment (PLE). Examples of these activities include participation in collaborative exercises, writing a forum post or reading a document. In the data collection step, it is crucial to address data privacy issues. Often the output of the data extraction and preprocessing step is transferred into a separate database. The second step of the Learning Analytics process is the mining of the preprocessed data, based on different mining techniques, such as clustering, classification, association rule mining, and social network analysis. Thereafter, the results of the mining process can be presented as a widget, which might be integrated into a VLE, a dashboard, or a PLE. Based on appropriate graphical visualizations of the analyzed data, teachers are supposed to be able to more quickly interpret the visualized information, reflect on the impact of their teaching method on the learning behavior and performance of their students, and draw first conclusions about the effectiveness of their teaching, i.e., consider if their goals have been reached. Furthermore, unexpected findings should motivate them to iteratively improve their teaching interventions. However, having a graphical visualization does not guarantee that teachers will be able to interpret the information represented correctly. Indicators must be designed and evaluated carefully. Also, the system should provide instructions for interpretation. 
 Figure 1. The Learning Analytics Process. 
 Requirements for developing such dedicated systems have been collected in a former study by analyzing interests and needs of the target group in more detail (Dyckhoff, 2010). Results of this study showed that teachers already have various questions about their instructional design and the utilization of learning materials, the students’ learning behaviors and correlations between objects of teaching and learning as well as outcomes. Their intentions can be e.g., to find out how well the overall instructional design is appreciated, to learn more about the needs of all or a specific group of students, or to better understand learning processes in general. The conclusion from the study mentioned above was that Learning Analytics tools should support teachers by collecting, integrating, and analyzing data of different sources as well as by providing a step-by-step guidance including semi-automated processes, instead of just presenting large tables of data. “It is undisputable that statistics in isolation represent only one aspect of any real-world situation. To make more meaningful interpretations, [educators] often need to look at the two or more statistics together” (Ali et al., 2012, p. 484). Hence, teachers should be able to choose from a flexible and extendable set of indicators. The system should guide the user throughout the research process, help him or her form research questions, recommend and provide appropriate methods for data collection, integrate data from different sources, and support its collaboratively organized analysis. Such a Learning Analytics tool could e.g., provide extendable lists of supported research questions (indicators) and suitable qualitative as well as quantitative methods for data collection, visualization and analysis. Furthermore, it should be possible to use and integrate the tool with any kind of VLE and learning software. 
 eLAT: exploratory Learning Analytics Toolkit.
 In the following sections, we introduce eLAT by giving an overview about results of the requirement analysis, development stages, design decisions, and evaluation phases, concluding with a discussion of our basic findings. 
 Requirements.
 Requirements for eLAT have been collected through literature analysis (Dyckhoff, 2010), as well as by informally talking to teachers, eLearning experts and system administrators at RWTH Aachen. The requirements analysis concluded the following main design goals: 
 - Usability: prepare an understandable user interface (UI), appropriate methods for data visualization, and guide the user through the analytics process. - Usefulness: provide relevant, meaningful indicators that help teachers to gain insight in the learning behavior of their students and support them in reflecting on their teaching. - Interoperability: ensure compatibility for any kind of VLE by allowing for integration of different data sources. - Extensibility: allow for incremental extension of analytics functionality after the system has been deployed without rewriting code. - Reusability: target for a building-block approach to make sure that re-using simpler ones can implement more complex functions. - Real-time operation: make sure that the toolkit can return answers within microseconds to allow for an exploratory user experience. - Data Privacy: preserve confidential user information and protect the identities of the users at all times. 
 Usability and usefulness: Every course is different, depending on the teachers and students who are involved in it. There are different teaching strategies, different learning goals, etc. Among the teachers there are some who have not used a Learning Analytics tool before, as well as advanced users. A Learning Analytics tool should be easy to use and understandable for all users. It must be usable for both: the beginner, who just looks at it for the first time, as well as for the expert, who already has a specific question and wants to perform deeper analysis. For beginners, a Learning Analytics tool should enable a direct entry and it should motivate to occupy themselves more with the underlying data, e.g., through a dashboards solution. Experts should find ways to explore and do further analysis to keep them well on the ball. Varying learning scenarios will also demand differing sets of indicators. An important future research task is to find out which indicators are useful for whom in what situation. Interoperability, extensibility, and reusability: Most existing Learning Analytics tools cannot be easily adapted for a different VLE. In addition, new e-learning systems are being developed that may contain useful data for Learning Analytics. Also, learning may take place on informal learning platforms. Therefore, an interoperable Learning Analytics tool that integrates with other systems, and can collect and analyze data from different platforms is required. Real-time operation: New issues on a course may arise at any time during a semester and should then usually be answered directly, so that timely improvements can be made. Also, new questions that are worth to be examined more closely, may arise during the answering process of ongoing questions. Therefore, a Learning Analytics tool should provide current data and comprehensive data analysis capabilities and be available at all times, not only at the end of the semester. Also, interactive analysis and visualization features, like filtering options for exploring the data in more detail, should deliver results and changing visualizations within microseconds. Data privacy: Personal data should be protected at all times to prevent abuse. Data privacy acts ensure such protection (e.g., Directive 95/46/EC, 1995; Federal Data Protection Act, 1990). However, an exception is made for teaching and research projects, under the condition that the data will be handled transparently and purposefully (Federal Data Protection Act, 1990). In addition, students or a data protection officer could be asked to consent to the collection and analysis of student data. Many questions regarding teaching, however, do not aim to examine records of individual students. Rather, data of the totality of students or subgroups with specific characteristics are interesting for drawing conclusions on learning processes. Data could be stored and processed pseudonomized to protect the users. As further protection, the tool could ensure that certain kinds of analyses cannot be executed in certain situations, where they would lead to the identification of individual students. 
 Development stages and evaluation methods.
 eLAT was iteratively and incremental developed within two main stages that partially overlapped to meet the requirements described above: (stage 1) the implementation and testing of a backend framework as well as (stage 2) the design and evaluation of a UI (frontend). In the first stage eLAT was designed as a prototype to evaluate different software architectural approaches for Learning Analytics using different VLE platforms. During winter term 2010/2011, we selected four courses that were using the learning and teaching portal L²P of RWTH Aachen University. The courses differed in course sizes (1370, 338, 220, and 38 registered students), learning technologies and teaching styles to ensure realistic usage scenarios. We logged the students’ activity, interaction and (in one case the) assessment data over the duration of three months. By using the data of real courses, it was possible to learn more about meaningfulness of already implemented indicators and to let the teachers of the courses participate in the development process of eLAT. In this way, we could get immediate feedback and comments on prototype stages that already processed analytics based on the real data. The design and evaluation of a UI (stage 2) started parallel to the first development stage described above. It was iteratively conducted as well, whereat each of overall three iterations had a specific objective. The first iteration dealt with the collection and definition of the content. Since eLAT was designed to enable teachers to explore educational data of their students and courses based on graphical indicators, it involved the collection of indicators as well as assigning priorities to them. Thus, semi-structured interviews were conducted to evaluate a set of graphical indicators and to get to know further user requirements. Semi-structured interviews are used to collect facts, opinion and attitudes of the interview partners (Naderer, 2007). The interviews are guided through prepared questions, but it is also possible to ask questions spontaneously to investigate interesting details (Lindlof and Tylor, 2002). The second iteration focused on the layout and data presentation of the UI. The evaluations of the first and second iteration were performed with the help of paper prototypes, while in the third iteration a functional UI, which was implemented based on previous evaluation results, was used to investigate interactivity and usability aspects. Layout and data presentation were designed and evaluated in terms of heuristic evaluation, cognitive walkthrough and pluralistic walkthrough. A heuristic evaluation uses approved usability principles or guidelines to investigate the usability of a UI. Thus, problems can be discovered with less effort in an early development step (Nielsen, 1992). A cognitive walkthrough is more formal than heuristic evaluation. It needs a specification of the UI and tasks to evaluate the usability. With the help of the tasks the UI can be processed step by step to discover usability problems (Polson et al., 1992; Dix et al., 2004). Both methods were chosen to evaluate the prototypes of the UI in an early stage of development. The pluralistic walkthrough is similar to the cognitive walkthrough. It is a meeting of experts of different domains, such as users, designers, and usability specialist. They discuss elements of the interface prototype according to the view of the users (Bias, 1994). We used a variant of the pluralistic walkthrough where a domain expert, a usability experts and the designer discussed the interface from the users’ perspective. Main results of these studies are presented in the section “User interface”. The third iteration, which was mainly concerned with interaction, included a qualitative think-aloud study. Here users were asked to perform tasks with a software prototype, whereat they were talking about what they were doing and thinking. During the tasks the evaluator observed them. This method was chosen to identify areas of interactions where users can make mistakes (Dix et al., 2004). In the following sections, we present the resulting UI, use cases, design and implementation details of eLAT as well as overall evaluations results. 
 User interface.
 The structure and layout of the eLAT user interface (UI) are the result of an iterative approach and were derived from our user studies, which have been elaborately discussed in Bültmann (2011). The UI is designed as a launch pad, which is similar to a dashboard but provides more comprehensive analysis options, additional to an initial overview (Few, 2006). A monitoring view helps to observe several indicators at once (figure 2). Furthermore, analysis views provide a deeper insight into the data of chosen indicators by making it possible to drill down into details by changing parameters of an indicator. Additionally, a mouse over effect shows details about the currently regarded information (figure 3). In the monitoring view, the content of the launch pad is grouped into four widgets. The widgets are containers for indicators related to the categories “document usage,” “assessment/ performance,” “user activity” and “communication.” Each indicator has its own tab in the widget. This hierarchical layout is supposed to help users to get a better overview about the current learning situation. By using widgets and tabs it is possible to put all indicators on one single screen. This concept also helps in terms of personalization, because widgets can be arranged flexible by the users. 
 Figure 2. Monitoring view of the eLAT user interface. 
 Figure 3. Analysis view of the indicator “Activity behavior”. 
 The analysis view of each indicator is consistently accessible in the corresponding tab by clicking “Configure indicator details” (figure 2). This detailed view of the indicator is shown as an overlay on top of the monitoring view (figure 3). Layout and functionality have been designed in a consistent way to gain a better usability (Few, 2006). On the right side of the analysis view is a filtering menu. The filtering of the presented data is context-dependent according to the currently selected indicator. For each tab in the filtering menu of any indicator, the user can determine which information the indicator should present. Hence, there are many options for data exploration, such as, comparing the activity of male and female users or the activity of students of different study programs. But not all filters can be used for each context. Because of data privacy regulations, we cannot allow the use of any kind of user properties like gender or study course, when there are less than a certain number of students, e.g. five users with that certain property in a course. The following paragraphs give an overview about six implemented indicators, which were rated to be interesting as result of the evaluations. Figure 3 shows the analysis view of the indicator “Activity behavior”. Student data is divided into the three groups “very active students” (blue bars), “active students” (red bars), and “inactive students” (yellow bars), and shows their weekly distribution over a time span. An “active student” is determined by calculating the average number of days per week, at which a student was active, i.e., logged into the system. In the current configuration of the indicator “activity behavior”, shown in figure 3, a student is defined to be “active” if he or she logs in at least once a week. A student is defined to be “very active” if he or she logs in on more than five days a week. The user can change the time span and the definition of an “active student.” 
 Figure 4. Indicator “Access and accessing students”. 
 The data in figures 3–7 is based on a programming course, which was finished with a final exam at February 8th 2010. As expected, the indicator above shows an increase of “inactive students” after the exam date. The indicator “Activity behavior” (figure 3) indicates whether continues learning is taking place. A participant of our semistructured interviews considered continues learning as a main factor for good exam results. As a sign of continues learning, the teacher might e.g., expect his students to log in at least twice a week to download new materials and stay up-to-date related to course information. The indicator “Activity behavior” can show tendencies of increasing or decreasing numbers of such active (groups of) students. High numbers of inactive students during the semester could bring the teacher to motivate his students to learn more regularly, e.g. through creating weekly exercises, or initialize further investigations on the reasons of low activity. The indicator “access and accessing students” (figure 4) supports the teachers in monitoring the overall online activity of their course. It shows the number of accesses/clicks (blue line) over the number of unique students (red line) who accessed the virtual learning environment during a time span defined by the user. The blue line represents the sum of every single click on any resources in the learning environment per day or week. It is important for a teacher to observe if e.g., a small group of students clicks many times or many students click once on a resource. The data in figure 4 shows that almost every day about a third of 278 registered students accessed several resources. The peak at the end of the timeline demonstrates a strong increase in accesses before the final exam, but only a small increase in accessing students. Lines converge after the date of the exam. Probably, the students only come back to the virtual course room to check the exam results (one click per student). The indicator “Access and accessing students (figure 4) can show outliers of usual access behavior/frequency. Teachers can relate high or low usage e.g., to teaching events or holidays. They can quickly observe if changes of learning materials or didactics lead to changes in overall usage behaviors. This might motivate them to experiment with didactics to improve the overall access to the learning environment. 
 Figure 5. Indicator “Activity areas”. 
 With help of the “Activity areas” indicator, shown in figure 5, teachers are supposed to identify whether and when students are accessing which parts/areas of a virtual course room per week in a defined time span. Hence, access rates between functions, like wiki pages or discussion forum, can be compared and related to teaching events as well. The x-axis of the indicator shows the days or weeks. If metadata on dates of course events, like the occurrence of specific lectures or the exam, are provided, these events can also be written on the x-axis. The y-axis records the number of students, who were active, i.e., clicked on resources, during that day or week in a specific part of the virtual course room. The red line in figure 5 e.g., shows the number of students accessing a document library with learning materials, such as lecture scripts and exercises. The red line and the yellow line, which represents the number of students, who accessed the discussion forum, peak out 1–3 day before the exam. Students seem to become more active in reading and discussing during that time, so that the case could be made that they are learning more intensively. 
 Figure 6. Indicator “Top 10 resources”. 
 The “Top 10 resources” indicator (figure 6) gives an overview about the most accessed materials. It can help to identify active documents/items that have been accessed more than others. Such a popularity indicator could have differing reasons. A document that shows up in the “Top 10 resources” indicator e.g., could be useful for solving an exercise or might be difficult to understand. The learning materials, presented in figure 6, are exercises (“Uebung 10–13”), code examples (“10-class”), lecture scripts (“12-GUI” and “11-Exc”), a lecture summary (“14-wdhlg”) and an example solution (“Loesung 10”). This could indicate that students mainly have been learning by solving exercises. Based on this indicator, a teacher could start to explore the meaning of the high access of specific learning materials. In case of a difficulty of understanding, learning materials could be improved. 
 Figure 7. Indicator “Forum usage”. 
 The “Forum usage” indicator (figure 7) represents the number of new threads with corresponding answers to these threads (x-axis) per day (y-axis). Teacher can more easily identify increasing discussions and, thus, determine, if collaboration among students is taking place, and whether there might be problems or not. Furthermore, by looking at a thread title of the observed communication activity, problems in understanding or in preparation of learning materials could be identified. Although this indicator does not show the answers per thread it can be an activity measure for forum usage. 
 Figure 8. Indicator “Adoption rate”. 
 The “Adoption rate” indicator (figure 8) deals with the time span from uploading a selected learning material to the time of access by students. With the help of the “adoption rate”, it can be identified how fast how many students access new materials. It also shows the number of students who have utilized a certain material and helps teachers to find out after which time the item has achieved a sufficient distribution amongst his or her students. In some courses this is helpful e.g., because the students might have a reading assignment. Thus, teachers are enabled to estimate, how many students have at least accessed a document that they were supposed to read. If the adoption rate is lower than expected, this could be an explanation for low homework discussion participation during class. Comprehensive and well-fitting indicators are crucial for an effective and successful application of a Learning Analytics tool. Therefore, we conducted semi-structured interviews with eight teachers (6 male, 2 female). During the interviews, example indicators were presented, shortly explained and discussed with the teachers to collect facts, opinion and new ideas. The evaluation goals named by all participants were quality and activity oriented. It is important for seven interview participants to monitor activity, like the frequency of logins or the date and occurrences of access. The principal point is to monitor, if continuous learning is taking place and if this leads to better learning results (mentioned by all participants). Therefore, it is important to relate the usage and access of learning material with performance. These evaluation goals are similar to the results found in our former study (Dyckhoff, 2010). Indicators concerning activity or achievements are valued most. Seven participants rated the indicator “Access and accessing students” (figure 4) as helpful for a general overview, especially in the beginning of using a VLE. But the posed question, if it can always deliver interesting information, remained unanswered. Therefore, the indicator should be used in combination with other data. Six participants stated that “Adoption rate” (figure 8) is important. It can indicate student behavior. Teachers can relate it to teaching events, such as making an announcement, and observe changes in student behaviors. The “Top 10 resources” indicator (figure 6) was identified as a valuable measure (7 called it important and one somehow important). It helps to identify those resources that are somehow relevant for students. A “Top 20” might be better suited for courses with a large amount of resources. By monitoring, e.g., “Activity areas” (figure 5), it is possible to identify patterns, such as at what times students do their exercises, or whether they access learning materials before or after lectures. Activity measures in general were pointed out to be well suited for first impressions during analysis, i.e., for getting a quick overview about what is going on. Regarding the performance, besides correlations with the activity of students, teachers also wished to take a deeper look at correlations with properties, like the program of study, the duration of study, and the mother languages of students. Five of eight participants stated that such indicators are important and one thought it is not important. This indicator could be important for adjusting teaching methods for specific groups of students. Teachers had divergent opinions on examining active participation, e.g. the usage of forums or wiki pages (figure 7). Four participants rated it as an important measure, because communication, discussion and participation are represented in collaborative features. The other half rated it less important, because of a low participation rate in their courses or students using other collaboration tools in the cloud. The assessed usefulness of this indicator heavily depends on the participation of the students in an online course and the relevance a teacher ascribes to it. The reason can be found in the underlying structure of the hybrid courses. Communication often takes place outside the VLE because of blended learning settings at RWTH Aachen University. Students still talk and learn together outside the online learning environment, and thus, communication and its relation to learning cannot be measured adequately. Additional to the indicators described above, we evaluated several other indicators. The final prototype does not implement all indicators evaluated in the previously conducted iterations due to the fact that not all the data needed was available at the time of implementation; e.g., it lacks of data corresponding to session information and durations or more detailed metadata about students. Those indicators, not described in this paper, have let to differing opinions. Their usefulness rating was low or very much dependent on the underlying didactical scenarios. Hence, we draw the overall conclusion that teachers should be enabled to explore data individually, e.g. by arranging their own sets of indicators per course, to facilitate improvement of teaching. 
 Framework design and implementation.
 The architecture of eLAT is presented in an abstract manner in figure 9. The functional requirements for the toolkit demanded a very flexible coupling of VLE infrastructure, the implementation of an indicator evaluation process and a visualization system. eLAT encompasses three main components, namely an indicator framework, a mining database, and a visualizer application. At the heart of eLAT lies the indicator framework, which negotiates and provides report evaluation services, i.e., executions of indicator calculations, between the website and the mining database, while the visualizer component provides an abstraction layer for visualizing different reports in an appropriate manner. 
 Figure 9. Indicator execution process. 
 The implemented course of action for an indicator evaluation is illustrated in the green boxes in figure 9. Triggered by the instructor who is visiting the website and selecting an appropriate indicator (step 1), a Controller Factory in the indicator framework will then dynamically instantiate a controller for this indicator. This will create a view, containing user interfaces for all the available configuration properties (step 2). After the user has finished adjusting all the properties and the configuration has been validated, the framework will generate a report evaluation request, store it in the report database, and send an evaluation request to the evaluation service instances. In the meantime the user will be redirected to a waiting page that displays the current evaluation status and updates automatically (step 3+4). Once the evaluation has completed the report, consisting of the initial configuration and a dataset of raw data tables, it will be stored permanently in the report database (step 5). While querying the report status the client side scripts will eventually learn of the successful evaluation and load the dedicated JavaScripts that will then generate and show the appropriate visualization based on the raw data set obtained by the report service (step 6+7). A very important part of the system architecture of eLAT deals with the extensibility and reusability of the existing code base, so that the scope of operations can grow. Therefore, a single indicator implementation makes use of smaller parts in the form of expressions that are performance-optimized database queries to retrieve specific result sets that can be useful for other indicators as well. The same practice is applied to the dynamic user interface generation for indicator parameters and the visualizers, which operate on standardized datasets and are therefore generic to the data inside the report. This leads to a relatively small effort for implementing new indicators. Since we want to keep the eLAT implementation independent from a particular VLE, we have developed a neutral data model that supports all the major data types as well as an extension model to fit in special types. As illustrated in figure 9, we have modeled a learning environment as a general virtual learning space dedicated to a specific course, which could be implemented in any VLE. From the VLE instance we extract the information needed for further investigation and load them into the mining database: - User data: This primarily stores the pseudonymous user identifier and the role of the user, e.g. “instructor”, “tutor” or “student”. It also holds custom properties like gender and study course, which might not be available for all users. - Content data: In most VLE, there are multiple sources of content data, structured by using content areas and content lists or libraries, which can have predefined recurring types and certain properties for specific objectives. Additionally, there are content type extensions dedicated to extend content items with properties that are not always available, like for example a forum post, which has a property for the depth inside the discussion thread – in any other case that post can be regarded like any other content item, with properties like title, creation date and the user responsible. - Assessment data: We wanted to store assessment instances separately from the user submission to allow for different handling and support of various assessment types. This also supports an extension model to fit in properties like completion time or group work submissions, which are not common, but sometimes available. - Activity data: This contains specific information about any content interaction along with timestamp and user information occurring in the learning environment. - Event data: As typically every course has a calendar and special events like course start, assessment and exam dates, it is useful to use this information in the parameter selection and visualization process. We are interested in narrowing down the activity to certain time spans such as the time between two exams or visualizing the usage statistics of the forum activities only in the time between two evaluations of weekly assignments. 
 Most of the server-side code is written in .NET and uses Windows Communication Foundation (WCF) services for providing data and communication interfaces between the website, the evaluation service and the client site visualizations. The indicator website itself does not generate much load on the CPU or the database (answers below half a second), relative to the load the evaluation processes. This is why we decided to execute evaluation processes by a dedicated service that is running in multiple instances, ideally on a different physical machine, to approach our real-time operation requirements. As a result of the semi-structured interviews, the possibility to filter data was identified as a substantial requirement. However, when implementing the filtering options, we did not yet entirely meet our goal of providing real-time operations, which are necessary for a smooth user experience. One reason for this lies in the structure of the database, which was created in the first development stage and, in its current form, has been designed according to a pattern used for operational databases rather than data warehouses. Hence, we implemented on-the-fly generation of lookup tables, containing pre-calculated intermediate results, to increase the efficiency of filtering. However, a thorough redesign of the database scheme with data warehouse principles in mind is required to address this issue in greater detail. To allow for upgrading and enhancing of analysis methodology while the system is deployed, the implementation makes heavy use of the Managed Extensibility Framework, which allows for dynamic composition and object instantiation of software capabilities and is part of the latest version of the .NET Framework. For our software prototype evaluations we extracted data for our four courses from L²P, which is currently run on SharePoint 2007, and one a Moodle instance, which was used in one course to host assessments. We employed custom code for each VLE to extract and transform data to our data model, as shown in the blue boxes in figure 8. From there the simplest way to load data into the mining database is by using the xml import feature. We have specified an xml schema, which can be used to create and validate xml files from any VLE data output. To ensure data privacy, we had to make modifications to the data extraction process. Since we have no interest in the identity of each user, but still need to be able to uniquely distinguish their data, we chose to create a hash from the username with a learning environment specific salt, so that the transformation is not easily reversible. The algorithm used is MD5 with a hash size of 128 bits (Rivest, 1992). In future versions, we still expect the need for custom data exports of indicator reports. Some advanced users have already expressed their interest in using the results in Excel or other analytics tools, like Weka (Hall et al., 2009) or Keel (Alcalá-Fdez et al., 2009), to start their own studies. Although we would like to provide as much functionality as possible inside the eLAT toolkit, we have designed interfaces for exporting reports. 
 Evaluation and discussion.
 This section presents our basic evaluation findings and answers the two questions: Does eLAT meet the requirements “usability,” “usefulness,” “interoperability,” “extensibility,” “reusability,” “real-time operation” and “data privacy” sufficiently and what did we learn in relation to those challenges. To evaluate if all requirements are met by our concept of a usable exploratory Learning Analytics toolkit, we have implemented an instantiation of our concept, namely the prototype eLAT, tested its technical functionality with four courses at RWTH Aachen University and conducted several usability evaluation iterations. Results of the first two iterations have been summarized in the section “User interface” by describing the UI and giving insights into indicator usefulness ratings. The final usability study with the software prototype was conducted as a qualitative think aloud study. Four teachers (3 male, 1 female), participants of the former iterations, were asked to perform tasks, which aimed at keeping an eye on the learning progress. At first, the prototype was shown to the participants without giving a detailed explanation. The tasks were designed with increasing difficulty. All participants quickly understood the structure and were able to navigate the user interface. The monitoring and analysis views were well understood. One out of four participants tried to click and investigate the whole interface. During the evaluation, some problems were discovered. One of the participants criticized the font size as being too small, but put into perspective that the font in the web browser could be enlarged by him and thus individually adopted. Furthermore, the mouse over effect, which shows details on the currently regarded information, helped to overcome the drawback of a small font. Two participants did not immediately understand the wording of the indicator “Adoption rate” (figure 8). Regarding the filtering, three participants requested more analysis options than possible. One wished the possibility to get into details by simply clicking on a line of a chart. As an improvement for filtering options, one participant mentioned the possibility of providing templates for common time spans during the lecture period, such as “examination phase” or “examination preparation phase”. Two participants stated that the tool should be extended regarding the correlation of activity and performance. Also the filtering possibilities should be extended and some filters also could be applied in other indicator contexts. Furthermore, filters according to the activity behavior of students could be defined. In addition, advanced users still need more personalization and more analysis functionality, e.g. the possibility to create new indicator templates. However, although the evaluation showed that eLAT is usable, comprehensive field-tests with more courses from different disciplines still have to be carried out to gain more reliable data on pedagogical usefulness of the toolkit. We investigated the usability of several indicators. Yet, we do not have enough data to answer the question which captured variables may be pedagogically meaningful sufficiently. Indicators implemented in eLAT are rather general and focused on simplicity and understandability. For enabling data exploration and visualization manipulation based on individual data analysis interests, more significant indicators that are still easy to use, have to be designed, implemented, and evaluated. Reviewers suggested e.g., considering outliers of student behavior by using a box plot for visualization. May et al. (2010) suggested potentially meaningful indicators, such as, indicators for visualizing different user activities as unique colored spheres on a horizontal bar, or a radar graph for visualizing different aspects of the user’s level of interaction in a communication activity. Zorilla et al. (2010) chose a radar graph as well for presenting student and session profiles. Yet, the authors of both research projects mention evaluated difficulties in the understandability caused by the visualizations (May et al., 2010; Zorilla et al., 2010). These evaluation results conform to findings from our indicator evaluation: visualization types should be well known and simple to interpret. Fancy graphics might cause problems by requiring too much interpretation time, but they have the potential to deliver meaningful information. The most impact has a visualization that may present outliers of data or rather unexpected data about students. Therefore, the design of indicators should be carefully accompanied by user studies and supplemented by adequate interaction options and help facilities as needed. For providing the right information to the right people, the development of more advanced functionality, like the systematic comparison of selected data of different courses should be pushed on. As a further advancement, the tool could analyze indicator usage and offer a sophisticated rating mechanism to recommend indicators depending on teachers’ data analysis goals. In addition, eLAT could expand its target group to learners for the purpose of selfmonitoring. Current Learning Analytics tools should be interoperable with different learning environments and systems. eLAT uses a neutral data model and has been tested with data of three different learning environments, namely the learning and teaching portal L²P, Moodle and Dynexite, an e-learning exercise tool. We are also working on a mapping between our data model and Contextualized Attention Metadata (CAM) (Schmitz et al., 2009) that will be supported in the next versions of eLAT. In the future, Learning Analytics tools may be linked more often to more open, networked, and lifelong learning environments, driven by new learning theories, such as Connectivism (Siemens, 2005) and LaaN (Chatti, 2010). Hence, integratability is an important factor for the sustainability of Learning Analytics tools. Furthermore, integration of diverse data sources may lead to more pedagogically meaningful indicators because a more holistic picture of the learner could be drawn. To support the implementation of new indicators, in terms of extensibility and reusability, indicator implementations make use of expressions that are performance-optimized database queries. Furthermore, visualizers operate on standardized datasets and are therefore generic to the data inside the report. As a result of the semi-structured interview, the possibility to filter data was identified as a substantial requirement. However, when implementing the filtering options, we did not entirely meet our goal of providing real-time operations, which are necessary for a smooth user experience. The reason can be found in the data tables, which are not normalized in order to improve performance. To avoid too long calculations, we implemented on-the-fly generation of lookup tables containing pre-calculated intermediate results, thus, increasing the efficiency of filtering. However, a thorough redesign of the database scheme with data warehouse principles in mind is required to minimize the time delay between the capture and use of data. Regarding data privacy, we chose to pseudonymize student data to protect students’ identities and prevent data misuse. According to prior agreements with the universities data protection officer, creating a hash from the username pseudonymized the collected data for clearly assigning and saving personal data. We used a learning environment specific salt with the username, which is a value that makes it difficult to guess the original value, for security reasons. Data was exported from the VLE to eLAT’s backend on a weekly basis during the semester. For transparency reasons, students of each course were informed verbally and in writing about the data collection and analysis goals. Of course, the more data is collected, the higher is the potential to recognize individual students during the data analysis. Therefore, we prohibit certain parameter selection options, if less than five students with that particular property are registered for a course. This leads to a trade-off of “data privacy” versus “pedagogical useful indicators”. Teachers of small courses with few registered students will not have as much indicator options as large courses with many students. Also, courses with unequally distributed student properties, e.g. with very few female students, cannot be analyzed according to that property, i.e. questions on gender differences cannot not be investigated. 
 Related Work.
 Several researchers are trying to solve some of the EDM and Learning Analytics challenges similar to the ones mentioned above. Krüger et al. (2010) observed that VLEs are usually not designed for data analysis and mining. Therefore they also developed a data model to allow for efficient data mining in a VLE. A prior aim was to automate and alleviate the pre-processing, which is needed to explore, analyze, and mine VLE data. The data model and an implementation are oriented towards the structure of Moodle and have not yet been tested with other VLEs. Our own data model design differs from this work in terms of more detailed logging data concerning content interactions, a property bag mechanism for storing custom data type extensions, the introduction of events as its own data type and an assessment data model. Reasons for these differences are due to our requirements that demand the possibility for integrating custom content and user properties that diverge with different course type instances as well as the need for adding versatile information. An extension model enables instructors to extend the information set of his or her students by using surveys inside the course room, while preserving data privacy, and using the results and user submissions for cross-comparison in certain indicators. Furthermore, our data model was designed to fit with different kinds of VLEs and has been evaluated with L²P as well as Moodle. Pedraza-Perez et al. (2011) have presented a first prototype of a java desktop tool addressed to non-data-mining experts, which supports simple execution of common data mining steps. A simple wizard-based interface helps the user to create, pre-process, visualize, and mine Moodle data files by clicking buttons that are named accordingly, e.g. “create data file” or “pre-process data file”, whereby they choose between pre-defined and recommended data mining options. Even though its usage is simple, the interface is still oriented towards users with background knowledge about the data mining process. Teachers, who have no data mining experience, will first have to learn about the process and understand the wizard’s vocabulary, such as “classification,” “regression,” or “association.” Lately, several research projects similar to eLAT are emerging. These projects show that information visualizations are gaining importance for increasing usability of Learning Analytics tools. Mazza and Dimitrova (2007) studied graphical user interfaces for EDM tools. They presented CourseVis, which has been built as an extension of WebCT. Its design is based on the results of a survey, which revealed that instructors need information on social, cognitive, and behavioral aspects about their students when running distant education courses with a VLE. CourseVis uses web log data similar to eLAT and also renders it graphically. An evaluation has shown that the graphical representations of CourseVis helped instructors to quickly and more accurately grasp information of students (Mazza, 2006). As a follow-up, the successful visualization principles of CourseVis have been implemented with a graphical interactive plug-in for student monitoring in Moodle, called GISMO (Mazza and Botturi, 2007). Another example is EDM Vis, an information visualization tool for exploring students’ data logs that uses a tree structure to provide an overview of class performance, and interface elements to allow easy navigation and exploration of student behavior (Johnson et al., 2011a). The main differences of CourseVis, GISMO, and EDM Vis compared to eLAT are that the three former mentioned provide visualization in a more static way defined by the system developers. In our approach, teachers are able to dynamically choose by themselves which visual indicators are most helpful to answer their individual data analysis questions. They can change parameters of existing visualization and, hence, create sets of relevant indicators composed on dashboards that fit a specific teaching intervention. Based on motivations and goals very similar to those of eLAT, two other tools are taking individual user perspectives into account: Graf et al. (2011) recently introduced the Academic Analytics tool (AAT) and Garciá-Saiz and Zorilla Pantaleón (2011) presented the data mining application, called E-learning Web Miner (EIWM). AAT has been primarily developed for learning designers, but can also be used by teachers. The application allows its users to access and analyze students’ behavior in Moodle online courses by customizing and performing analytical data base queries. This data analysis may be used as additional evidence for the formative evaluation of courses. Combined with students’ evaluations and professor and tutor recommendations for changes, the analytics results are supposed to inform the work of learning designers, who can then adapt, revise and extend resources. Like eLAT, AAT is applicable for different VLEs and aims at being easily extendable, with respect to adding analysis techniques/indicators (Graf et al., 2011). EIWM is supposed to help teachers explore distance students’ behavior in a VLE, e.g., Moodle or Blackboard. It offers a set of templates, which can be compared with eLAT’s indicator approach. Currently these templates visualize data graphically related to three common questions of teachers, concerning resource usage, students’ sessions, and students’ profiles in a selected course. The templates contain all needed input attributes for the execution of the related data mining algorithms, which have been implemented on the basis of the Weka toolkit (Garciá-Saiz & Zorilla Pantaleón, 2011). A challenge for Learning Analytics tools like EIWM or eLAT is not only to provide meaningful templates/indicators, but also to decrease the necessity to input parameters by users. As distinguished from EIWM currently eLAT only implements indicators based on simple statistical methods. In the future, we are going to implement further indicators based on data mining methods and evaluate their usefulness for facilitating analysis and improvement of teaching. It is a complex task for teachers to input data mining parameters. Therefore, parameter-free data mining is a promising approach. Zorilla et al. (2011) compared yacaree, a parameter-free association miner with three wellknown association rule miners. In their study yacaree was well-suited and superior in terms of usefulness to a teacher involved in the evaluation. Research like this will support the further development of usable Learning Analytics and EDM tools that support explorative analytics usage. 
 Conclusion and outlook. 
 In this paper, we presented the theoretical background, requirements, design, implementation, and evaluation details of eLAT; an exploratory Learning Analytics Toolkit that enables teachers to monitor and analyze their teaching activities. The main goal of eLAT is the improvement of teacher support with graphical analytics, which are useful because they allow extending the audience to “normal” instructors without prior knowledge in data mining techniques. With the help of eLAT, teachers are enabled to explore, reflect and evaluate teaching interventions based on their interests. Key EDM and Learning Analytics requirements, such as usability, interoperability, extensibility, reusability, and data privacy, have been tackled with the development of eLAT. Currently, eLAT has been primarily developed with the intention to support teachers in their ongoing reflection, evaluation and improvement of their instructional design. In the future, we plan to enhance eLAT in ways that students can use it as well. Furthermore, eLAT has been successfully tested with data collected from four courses. Future work will include the integration of eLAT in L²P and its field-testing with more courses from different disciplines, based on new indicators. We also plan to enhance eLAT with an intelligent recommendation component and evaluate its usefulness. 
 Acknowledgements.
 This project was partly funded by the Excellence Initiative of the Federal and State Governments.]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Design and Implementation of a Learning Analytics Toolkit for Teachers</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>edm</dc:subject>
		<dc:subject>improving educational software</dc:subject>
		<dc:subject>teacher support</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-lea-dyckhoff"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-lea-dyckhoff"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/dennis-zielke"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/dennis-zielke"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mareike-bultmann"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mareike-bultmann"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mohamed-amine-chatti"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mohamed-amine-chatti"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/ulrik-schroeder"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/ulrik-schroeder"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/73/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-lea-dyckhoff"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/dennis-zielke"/>
		<rdf:_3 rdf:resource="http://data.linkededucation.org/resource/lak/person/mareike-bultmann"/>
		<rdf:_4 rdf:resource="http://data.linkededucation.org/resource/lak/person/mohamed-amine-chatti"/>
		<rdf:_5 rdf:resource="http://data.linkededucation.org/resource/lak/person/ulrik-schroeder"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/74">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>Social Learning Analytics</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/74/authorlist"/>
		<swrc:abstract>We propose that the design and implementation of effective Social Learning Analytics (SLA) present significant challenges and opportunities for both research and enterprise, in three important respects. The first is that the learning landscape is extraordinarily turbulent at present, in no small part due to technological drivers. Online social learning is emerging as a significant phenomenon for a variety of reasons, which we review, in order to motivate the concept of social learning. The second challenge is to identify different types of SLA and their associated technologies and uses. We discuss five categories of analytic in relation to online social learning; these analytics are either inherently social or can be socialised. This sets the scene for a third challenge, that of implementing analytics that have pedagogical and ethical integrity in a context where power and control over data are now of primary importance. We consider some of the concerns that learning analytics provoke, and suggest that Social Learning Analytics may provide ways forward. We conclude by revisiting the drivers and trends, and consider future scenarios that we may see unfold as SLA tools and services mature.</swrc:abstract>
		<led:body><![CDATA[ Introduction. 
 The concept of Learning Analytics is attracting significant attention within several communities with interests at the intersection of learning and information technology, including educational administrators, enterprise computing services, educators and learners. The core proposition is that, as unprecedented amounts of digital data about learners’ activities and interests become available, there is significant potential to make better use of this data to improve learning outcomes. After introducing some of the conceptual roots of Learning Analytics (§2), we propose that the implementation of effective Social Learning Analytics is a distinctive part of this broader design space, and offers a grand challenge for technology-enhanced learning research and enterprise, in three important respects (§3).

 1. The first is that the educational landscape is extraordinarily turbulent at present, in no small part due to technological drivers. The move to a participatory online culture sets a new context for thinking about analytics. Online social learning is emerging as a significant phenomenon for a variety of reasons, which we review (§4) in order to clarify the concept of online social learning (§5) and ways of conceiving social learning environments as distinct from other social platforms. 2. The second challenge is to understand the possibilities offered by different types of Social Learning Analytic, both those that are either inherently social (§6) and those that can be socialised, i.e., usefully applied in social settings (§7). 3. Thirdly, we face the challenge of implementing analytics that satisfy concerns about the limitations and abuses of analytics (§8). We conclude (§9) by considering potential futures for Social Learning Analytics, if the drivers and trends reviewed continue. 
 Learning analytics. 
 Learning analytics has its roots in two computing endeavours not specifically concerned with learning, but rather with strong business imperatives to understand internal organisational data, and external consumer behaviour. - Business Intelligence focuses on computational tools to improve organisational decision-making through effective fusion of data collected via diverse systems. The earliest mention of the term ‘learning analytics’ that we have found relates to business intelligence about e-learning products and services (Mitchell & Costello, 2000). - Data Mining, also called Knowledge Discovery in Databases (KDD), is the field concerned with employing large amounts of data to support the discovery of novel and potentially useful information (Piatetsky-Shapiro, 1995). This field brings together many strands of research in computing, including artificial neural networks, Bayesian learning, decision tree construction, instance-based learning, logic programming, rule induction and statistical algorithms (Romero & Ventura, 2007). 
 From data mining developed the field of: - Educational Data Mining (EDM) “an emerging discipline, concerned with developing methods for exploring the unique types of data that come from educational settings, and using those methods to better understand students, and the settings which they learn in” (Baker & Yacef, 2009). Originally, relatively fine-grained, quantitative data came from private educational software applications—Romero and Ventura (2007) trace the first EDM publications to 1995—but their overview of the field shows that research projects multiplied after widespread adoption of virtual learning environments (VLEs) in the early 21st century. Blackboard and Moodle are well-known examples of VLEs, which are also known as learning management systems (LMSs) and content management systems (CMSs). These tools automatically amass large amounts of log data relating to student activities. They not only record student activities and browse time, but also personal information such as user profiles, academic results, and interaction data. Many of them include student tracking capabilities as generic software features. Dawson (2009) reported that the depth of extraction and aggregation, reporting and visualisation functionality of these built-in analytics was often basic or non-existent, but in the last year, all of the major VLE products now include at least rudimentary analytics “dashboards.” Educational institutions have become increasingly interested in analysing the available datasets in order to support retention of students and to improve student results. This use of academic analytics stretches back for at least 50 years, but has become more significant in the last five years as datasets have grown larger and more easily available for analysis. - Academic Analytics are described by Campbell & Oblinger (2007) as ‘an engine to make decisions or guide actions. That engine consists of five steps: capture, report, predict, act, and refine.’ They note that ‘administrative units, such as admissions and fund raising, remain the most common users of analytics in higher education today.’ - Action Analytics is a related term, proposed by Norris, Baer and Offerman (2009) to emphasise the need for benchmarking both within and across institutions, with particular emphasis on the development of practices that make them effective. The Signals project at Purdue University is currently the field’s flagship example of the successful application of academic analytics, reporting significantly higher grades and retention rates than were observed in control groups (Arnold, 2010; Pistilli & Arnold, 2012). The project mines data from a VLE, and combines this with predictive modelling to provide a real-time red/amber/green traffic-light to students and educators, helping staff intervene in a timely manner where it will be most beneficial, and giving students a sense of their progress. Encouraged by such examples, educational institutions are seeking both to embed academic/action analytics and to develop a culture that values the insights that analytics provide for organisational strategic planning and improved learner outcomes. A growing number of universities are implementing data warehouse infrastructures in readiness for a future in which they see analytics as a key strategic asset (Stiles, Jones, & Paradkar, 2011). These data warehouses store and integrate data from one or more systems, allowing complex queries and analysis to take place without disrupting or slowing production systems. This brings us to the present situation; the first significant academic gathering of the learning analytics community was in 2011 at the 1st International Conference on Learning Analytics & Knowledge, doubling in size to 200 in 2012. The 2011 conference defined the term as follows: Learning analytics is the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs. 
 Clearly, this encapsulates strands from all the above fields, reflecting the topic’s interdisciplinary convergence but, in contrast to more theoretical research or artificial experimentation which might be published in some of the above fields, there is an emphasis on impacting authentic learning from real-world contexts, through the use of practical tools. There is also a shift away from an institutional perspective towards a focus on the concerns of learners and teachers. The main beneficiaries are no longer considered to be administrators, funders, marketing departments and education authorities, but instead are learners, teachers and faculty members (Long & Siemens, 2011). 
 The challenge of social learning analytics. 
 In a literature analysis of the field, we found that in the discourse of academic analytics there is little mention of pedagogy, theory, learning or teaching (Ferguson, 2012). This reflects the roots of these analytics in management information systems and business intelligence, whose mission has been to guide strategic action by senior leaders in organisations, and whose tools deliver abstracted summaries of key performance indicators. In such contexts, senior executives do not have the time to delve into the process details of a particular individual’s or group’s interactions, and similarly, the arguments for academic analytics seem to focus on finding variables that predict positive or negative outcomes for cohorts of learners. Performance indicators in educational settings typically involve outcomes-centric analytics based on learners’ performance on predefined tasks. Within formal education, success is typically defined as the display of expertise through summative assessment tasks (for example, assignments, exams or quizzes) intended to gauge mastery of discipline knowledge. The focus is on individual performance and on what has been achieved. This model is familiar within settings such as schools and universities, but it is less relevant in the context of online social learning, which involves lifelong learners drawing together resources and connections from across the Internet to solve real-life problems, often without access to the support of a skilled teacher or accredited learning institution. Social Learning Analytics (SLA) are strongly grounded in learning theory and focus attention on elements of learning that are relevant when learning in a participatory online culture. They shift attention away from summative assessment of individuals’ past performance in order to render visible, and in some cases potentially actionable, behaviours and patterns in the learning environment that signify effective process. In particular, the focus of social learning analytics is on processes in which learners are not solitary, and are not necessarily doing work to be marked, but are engaged in social activity, either interacting directly with others (for example, messaging, friending or following), or using platforms in which their activity traces will be experienced by others (for example, publishing, searching, tagging or rating). Social Learning Analytics is, we propose, a distinctive subset of learning analytics that draws on the substantial body of work demonstrating that new skills and ideas are not solely individual achievements, but are developed, carried forward, and passed on through interaction and collaboration. A socio-cultural strand of educational research demonstrates that language is one of the primary tools through which learners construct meaning. Its use is influenced by their aims, feelings and relationships, all of which shift according to context (Wells & Claxton, 2002). Another socio-cultural strand of research emphasises that learning cannot be understood by focusing solely on the cognition, development or behaviour of individual learners; neither can it be understood without reference to its situated nature (Gee, 1997; Wertsch, 1991). As groups engage in joint activities, their success is related to a combination of individual knowledge and skills, environment, use of tools, and ability to work together. Understanding learning in these settings requires us to pay attention to group processes of knowledge construction – how sets of people learn together using tools in different settings. The focus must be not only on learners, but also on their tools and contexts. Viewing learning analytics from a social perspective highlights types of analytic that can be employed to make sense of learner activity in a social setting. This gives us a new way to conceive of both current and emerging approaches—as tools to identify social behaviours and as patterns that signify effective process in learning environments. Social Learning Analytics should render learning processes visible and actionable at different scales: from national and international networks to small groups and individual learners. We turn now to review some of the features of the participatory online culture that drives this work. 
 The emergence of open, social learning. 
 In this section, we identify some of the signals that many futures analysts and horizon-scanning reports on learning technology have highlighted as significant. Taken together, these create synergies that establish a radically new context for learning. In such a context, we argue, analytics focused on summative assessment of performance remain important but do not go far enough: we need to develop new sets of analytics that can be used to support learning and teaching in these new conditions. We summarise these phenomena as: - technological drivers - the shift to ‘free’ and ‘open’ - demand for knowledge-age skills - innovation requires social learning - challenges to educational institutions. 
 Technological drivers. 
 A key force shaping the emerging landscape is clearly the digital revolution. Only very recently do we have almost ubiquitous Internet access in wealthy countries and mobile access in many more. In addition, we now have user interfaces that have evolved through intensive use, digital familiarity from an early age, standards enabling interoperability and commerce across diverse platforms, and scalable computing architectures capable of servicing billions of real-time users and of mining the resulting data. With the rise of social websites serving millions of users, such as Facebook, YouTube and Twitter, plus the thousands of smaller versions and niche applications for specific tasks and communities, we have witnessed a revolution in the ways in which people think about online interaction and publishing. Such social media platforms facilitate the publishing, indexing and tracking of user-generated media, provide simple-to-learn collaboration spaces, and enable social networking functions that are becoming ubiquitous: friending, following, messaging and status updates. Standards such as really simple syndication (RSS) allow information to be shared easily using structured data feeds, web services enable more sophisticated machine-machine interaction, and mobile devices expand the availability and localization of these services. Internet services may also begin to apply pressure to one of the slowest evolving elements in educational provision: accreditation. Christensen et al. (2008) argue that the agencies controlling accreditation often stifle innovation and protect the status quo, because new approaches to learning/accreditation struggle to gain credibility unless they are associated with institutions that have the power to award established qualifications. However, as the infrastructure for secure identity management matures, and as the participatory, social culture fostered by Web 2.0 becomes more deeply ingrained in younger generations, initiatives such as OpenBadges may provide new ways to accredit learning outside established institutions. Moreover, as ubiquitous tools for capturing digital material make it easier to evidence learning and practical knowledge in authentic communities of practice, an e-portfolio of evidence might come to have equivalent or greater credibility than formal certificates. However, changes in technology do not necessarily imply changes in pedagogy. Those who view education as information transfer will use interactive media for storage, drilling, testing and accessing information; those who seek conceptual change will seek to make use of their interactive qualities (Salomon, 2000). Technological shifts support analytics that draw on sets of big data—but they do not necessitate a shift towards analytics focused on such issues as conceptual change, distributed expertise, collaboration or innovation. So, if we do not accept simplistically that technology alone determines the future, we need to look elsewhere to understand the move towards online social learning and its associated analytics. 
 The shift to free and open. 
 There has been a huge shift in expectations of access to digital content. The Internet makes possible completely new revenue-generation models due to the radically lower transaction costs incurred (compared to bricks and mortar businesses with physical products) as one scales to hundreds of thousands of users. Andersen (2009) documents many ways in which online companies are able to offer quality services free of charge, producing an increasing expectation on the part of end-users of huge choice between free tools and sources of content hosted ‘in the cloud’. Within education, the Open Education Resource (OER) movement has been a powerful vehicle for making institutions aware of the value of making high quality learning materials available, not only free of charge, but also in formats that promote remixing, in an effort to reap the benefits seen in the open-source software movement. This has not proven to be a simple matter, but OER has made huge progress, and is gaining visibility at the highest levels of educational policy. This is amplified by efforts to make data open to machine processing as well as human interpretation. This requires not only a shift in mindset by data owners but also the construction of technological infrastructure to make it possible to publish data in useful formats. These efforts can be tracked within communities developing Linked Data and the Semantic Web, and their myriad applications communities, for example, Open Government, Open Mapping, Science 2.0 and Health 2.0. Together, these very rapid shifts contribute to a new cultural context for the provision of learning services, in which the industrial-era value chain, previously delivered by a single institution, is disaggregated into smaller and smaller elements. The provision of content, community, tools and basic analytics may increasingly be expected to come free of charge, while learners may still consider paying for other services such as personalised learning journeys, personal tuition, career guidance, accreditation against formal standards and tailored analytics that support them on a variety of sites, not just within one institution. 
 Demand for knowledge-age skills. 
 Technology is always appropriated to serve what people believe to be their needs and values. Since 1991, we have lived in the “knowledge age”—a period in which knowledge, rather than labour, land or capital, has been the key wealth-generating resource (Savage, 1996). This shift has occurred within a period when constant change in society has been the norm, and it is therefore increasingly difficult to tell which specific knowledge and skills will be required in the future (Lyotard, 1979). These changes have prompted an interest in “knowledge-age skills” that will allow learners to become both confident and competent designers of their own learning goals (Claxton, 2002). Accounts of knowledge-age skills vary, but they can be broadly categorized as relating to learning, management, people, information, research/enquiry, citizenship, values/attributes and preparation for the world of work (Futurelab, 2007). From one viewpoint they are important because employers are looking for “problem-solvers, people who take responsibility and make decisions and are flexible, adaptable and willing to learn new skills” (Educational Subject Center, 2007, p. 5). More broadly, knowledge-age skills are related not just to an economic imperative but to a desire and a right to know, an extension of educational opportunities, and a “responsibility to realise a cosmopolitan understanding of universal rights and acting on that understanding to effect a greater sense of community” (Willinsky, 2005, p111). In both cases, there is a perceived need to move away from a curriculum based on a central canon of information towards learning that develops skills and competencies. This implies a need for ongoing analytics that can support the development of dispositions such as creativity and curiosity, collaboration skills and resilience. 
 Innovation requires social learning. 
 The conditions for online social learning are also related to the pressing need for effective innovation strategy. In an accessible introduction to the literature and business trends, Hagel et al. (2010) argue that social learning is the only way in which organizations can cope in today’s fast-changing world. They invoke the concept of ‘pull’ as an umbrella term to signal some fundamental shifts in the ways in which we catalyse learning and innovation. They highlight quality of interpersonal relationships, tacit knowing, discourse and personal passion as key elements. This is a move away from having information pushed to us during spells of formal education towards a more flexible situation in which we pull resources and information to us as we need them. The move from “push” to “pull” motivates analytics that can be accessed by learners at any point, employed in both informal and formal settings, are sensitive to social relationships, and build transferable learning dispositions and skills. 
 Challenges to educational institutions. 
 Together, these forces create pressures on models of educational provision at all stages of education from childhood into workplace learning. Heppell (2007), amongst many, points to the need for an education system that helps people to help each other, rather than one that delivers learning. The barriers between formal and informal learning, and between online and face-to-face learning are currently being broken down, allowing the development of new models that take into account the range of learners’ experience outside formal study, and the affective elements of learning. An example of this is Gee’s “affinity spaces,” which provide a model for online social learning and were first identified in video gaming environments. Affinity spaces are organized around a passion; within them, knowledge is both distributed and dispersed, they are not age graded, experts work alongside newcomers, learning is proactive but aided as people mentor and are themselves mentored, participants are encouraged to produce as well as to consume, smart tools are available to support learning and everyone, no matter what their level of experience or expertise, remains a learner (Gee, 2004, 2009). Other new models for learning are emerging from a variety of digital sources. Some examples amongst many are the learning affordances of the World of Warcraft online game, with its guilds and carefully planned, collectively executed strategies (Thomas & Brown, 2011), learners beginning to access and create knowledge through persistent avatar identities that can move between different environments (Ferguson, Sheehy, & Clough, 2010), and the development of distributed cognition within virtual worlds (Gillen, Ferguson, Peachey, & Twining, 2012). These models suggest new ways of approaching learning analytics. Gee (2003) showed that well-designed video games incorporate analysis of the development of participants’ relevant knowledge and skills, so that their experience is constantly customized to their current level, effort and growing mastery, they are aware of ongoing achievements, and they are provided with information at the point when it can best be understood and used in practice. Having noted some of the features of the emerging landscape for open, social learning, and the implications of these features for analytics, we now consider some of the key features of social learning, and the nature of online social learning environments. 
 Characterising online social learning. 
 Why has someone sawn down half of the beautiful cedar tree outside my office window? I can’t find this out from a book, and I don’t know anyone with the precise knowledge that I am looking for. It is as I engage in conversations with different people that my understanding of what I see outside my window increases, and I learn more about the tree’s history, health, ecosystem and future possibilities. It is not just the social construction of understanding that is important here, since this is a part of most human interactions. My intention to learn is part of what makes this social learning, as are interactions with others. This is not a one-sided engagement with books or online content—it involves social relationships. As such, it has lots of ‘affective’ aspects: people must be motivated to engage with me and I must have the confidence to ask questions in the first place, as well as some way of assessing the expertise of the people I’m talking to. (Ferguson, 2010) Social learning has been conceptualised as societal learning in general, as processes of interaction that lead to concerted action for change, as group learning, and as the learning of individuals within a social context (Blackmore, 2010). Our conception of online social learning takes into account the changing affordances of a world in which social activity increasingly takes place at a distance and in mediated forms. It is succinctly expressed by Seely Brown and Adler (2008) as being “based on the premise that our understanding of content is socially constructed through conversations about that content and through grounded interactions, especially with others, around problems or actions.” Many others have, of course, argued for similar conceptions, unpacking this broad concept in great detail within the constructivist educational literature, and computer-supported collaborative learning (CSCL) research. Social learning adds an important dimension to CSCL, introducing a particular interest in the non-academic contexts in which it may take place (including the home, social network, and workplace) and the use of free, ready-to-hand online tools, with no neatly packaged curriculum or signed-up peer cohort, no formally prescribed way to test one’s understanding and no pre-scheduled activities (Blackmore’s (2010) edited readings remind us how far back everyday, non-digital social learning goes in learning theory, and provide us with foundations for extension into the digital realm). While OERs greatly increase the amount of good quality material available online to learners, another consequence can be that individual learners find themselves adrift in an ocean of information, struggling to solve ill-structured problems, with little clear idea of how to solve them, or how to recognise when they have solved them. At the same time, distributed networks of learners are grappling with ‘wicked problems’ such as climate change, which offer the same challenges on a grander scale. Social learning infrastructure could have a key role to play in these situations, helping learners connect with others who can provide emotional and conceptual support for locating and engaging with resources, just as in our tree story at the start of this section. This forces us to ask whether our current educational and training regimes are fit for purpose in equipping our children, students and workforce with the dispositions and skills needed under conditions of growing uncertainty—a challenge explored in detail by many others, for example in the collection edited by Deakin Crick (2009). The Open University, where we are based, has been seeking to address these issues with its SocialLearn project, aimed at supporting large-scale social learning. In the early days of the project, Weller (2008) identified six broad principles of SocialLearn: Openness, Flexibility, Disruptive, Perpetual beta, Democracy and Pedagogy. Following a series of workshops, Conole (2008) proposed a set of learning principles for the project—thinking & reflection, conversation & interaction, experience & interactivity and evidence & demonstration—and articulated how these could be linked to characteristics of social learning. Distilling this array of perspectives, we have derived a simple working definition focused on three dynamics, which serves to guide us in designing for meaningful interpersonal and conceptual connection: Online social learning can take place when people are able to: - clarify their intention—learning rather than browsing - ground their learning—by defining their question/problem, and experimenting - engage in learning conversations—increasing their understanding. A significant feature of the Web 2.0 paradigm is the degree of personalisation that end-users now expect. However, a me-centred universe has self-evident limitations as a paradigm for holistic development: learning often disorients and reorients one’s personal universe. User-centred is not the same as Learner-centred: what I want is not necessarily what I need, because my grasp of the material, and of myself as a learner, is incomplete. The centrality of good relationships becomes clear when we remind ourselves that a university’s job is to teach people to think, and that deeper learning requires leaving a place of cognitive and emotional safety where assumptions are merely reinforced—see the extensive research on learning dispositions that characterize this readiness (for example, Claxton, 2001; Perkins, Jay, & Tishman, 1993). This implies challenge to stretch learners out of their comfort zones, underlining the importance of affirmation and encouragement that give a learner the security to step out. As Figure 1 shows, the design of a social media space tuned for learning involves many alterations and additions to a generic space for social media. Within an online space tuned for learning, friends can become learning peers and mentors, informal endorsements are developed into verifiable accreditation, information exchanges become learning conversations and, likewise, generic web analytics need to be developed into learning analytics that can be used in such an environment. To summarise: we have outlined what we mean by online social learning, some of the major drivers that help to explain why it is emerging as a phenomenon, and some of the elements that may differentiate a social learning environment from other social media spaces. We have also indicated why these factors require new approaches to learning analytics. Constructivist pedagogies suggest the need for a shift away from a positivist approach to analytics and towards analytics that are concerned with conceptual change, distributed expertise, collaboration and innovation. This ties in with an increasing emphasis on knowledge-age skills and their associations with learning dispositions such as creativity and resilience. Within an open environment, there is a need for a range of analytics that can extend beyond an institutional platform in order to provide support for lifelong learners at all points in their learning journey. These learners may be organised in classes and cohorts, but they may also need analytics that help them to learn together in looser groupings such as communities and networks. These analytics, and their associated recommendations, will be informed by those developed for social media tools and platforms, but they will be tuned for learning, examples being prompting the development of conversations into educational dialogue, recommending resources that challenge learners to leave their comfort zones, or making learners aware that social presence and role are increasingly important to attend to in a complex world. 
 Figure 1. Dimensions of the social learning design space. 
 Together, these motivate a conception of Social Learning Analytics as a distinctive class of analytic. 
 Inherently social learning analytics. 
 Social learning analytics make use of data generated by learners’ online activity in order to identify behaviours and patterns within the learning environment that signify effective process. The intention is to make these visible to learners, to learning groups and to teachers, together with recommendations that spark and support learning. In order to do this, these analytics make use of data generated when learners are socially engaged. This engagement includes both direct interaction—particularly dialogue—and indirect interaction, when learners leave behind ratings, recommendations or other activity traces that can influence the actions of others. Another important source of data consists of users’ responses to these analytics and their associated visualizations and recommendations. We identify two inherently social analytics, and three socialised analytics: Inherently social analytics—only make sense in a collective context: - Social Network Analytics—interpersonal relationships define social platforms and link learners to contacts, resources and ideas. - Discourse Analytics—language is a primary tool for knowledge negotiation and construction. Socialised analytics—although these are relevant as personal analytics, they have important new attributes in a collective context: - Content Analytics—user-generated content is one of the defining characteristics of Web 2.0 - Disposition Analytics—intrinsic motivation to learn lies at the heart of engaged learning and innovation - Context Analytics—mobile computing is transforming access to people, content and both formal and informal learning. 
 We do not present these as an exhaustive “taxonomy,” since this would normally be driven by, for instance, a specific pedagogical theory or technological framework in order to motivate the category distinctions. We are not grounding our work in a single theory of social learning, nor do we think that a techno-centric taxonomy is helpful. These categories of analytics respond to the spectrum of drivers reviewed above, drawing on diverse pedagogical and technological underpinnings as reviewed above, and further cited below as we introduce each category. We summarise the essence of each approach, identify examples of tools, and then consider how these tools are being, or might be, used to support online social learning. In this section, we introduce the two inherently social analytics. 
 Social network analytics. 
 Essence of social network analysis. 
 Networked learning involves the use of ICT to promote connections between one learner and other learners, between learners and tutors, and between learning communities and learning resources (Jones & Steeples, 2003). These networks are made up of actors (both people and resources) and the relations between them. Actors with a relationship between them are said to be tied and these ties can be classified as strong or weak, depending on their frequency, quality or importance (Granovetter, 1973). Social network analysis is a perspective that has been developed to investigate the network processes and properties of ties, relations, roles and network formations, and to understand how people develop and maintain these relations to support learning (Haythornthwaite & de Laat, 2010). Fortunato (2010) describes social networks as “paradigmatic examples of graphs with communities”; social network analysis brings graph theory from the field of mathematics together with work on interpersonal and communal relationships from the fields of sociology and communication. The many uses of social network analysis applicable to social learning include detection of communities within networks (Clauset, Newman, & Moore, 2004; Fortunato, 2010); identification of types of subset within a network where a level of cohesion exists and depends on properties such as proximity, frequency and affinity or other properties (Reffay & Chanier, 2003); investigation of the density of social networks (Borgatti, Mehra, Brass, & Labianca, 2009); and exploration of individuals’ centrality within a network (Wasserman & Faust, 1994). 
 Social network analysis tools. 
 Many tools have been developed to support social network analysis in the context of learning. Commercial products such as Mzinga can be used to identify learners with the highest and most active participation in a network, those who are having the most influence on the activity of others and those who have the potential to make most impact. SNAPP (Social Networks Adapting Pedagogical Practice) is a freely available network visualisation tool that reinterprets discussion forum postings as a network diagram. These diagrams can be used to trace the growth of course communities, to identify disconnected students, to highlight the role of information brokers and to visualise how teacher support is employed within the network (Bakharia & Dawson, 2011; Dawson, Bakharia, & Heathcote, 2010). Gephi is a free, open-source platform that supports visualisation and exploration of all kinds of networks. In an extended series of blog posts, Hirst has explored ways in which this tool can be used to explore the learning networks that develop around shared resources and online course. His work picks out different networks with interconnected interests, identifies the interests that are shared by actors in a network, and highlights not only the role played by information brokers in sharing resources, but also the roles played by resources in connecting networks. Network-focused social learning analytics Social network analysis is a useful tool for examining online learning because of its focus on the development of interpersonal relationships, and its view that technology forms part of this process. It thus offers the potential to identify interventions that are likely to increase the potential of a network to support the learning of its actors by linking them to contacts, resources and ideas. Haythornthwaite and De Laat (2010) approach this form of analysis from two perspectives: egocentric and whole network. Egocentric networks are described from the point of view of the individual, who is set at the centre of an array of relationships both formally and informally connected with learning. Studying networks in this way can help to identify the people from whom an individual learns, where conflicts in understanding may originate, and which contextual factors influence learning. A whole-network view, on the other hand, considers the distribution of information and the development of learning across a set of people. In this case, analysis can characterise the network in terms of its character, interests and practices. This whole-network view is able to take “the results of pairwise connections to describe what holds the network together” (Haythornthwaite & de Laat, 2010, p. 189). Characterising the ties between actors adds a different dimension to this analysis—people rely on weak ties with people they trust when accessing new knowledge or engaging in informal learning, but make use of strong ties with trusted individuals as they deepen and embed their knowledge (Levin & Cross, 2004). Another option is to combine social network analysis with content analysis and context analysis to gain a richer picture of networked learning, investigating not only who is talking to whom, but what they are talking about and why they are talking in this way (De Laat, Lally, Lipponen, & Simons, 2006; Hirst, 2011). As social network analysis is developed and refined, it has the potential to be combined with other social learning analytics in order to define what counts as a learning tie and thus to identify which interactions promote the learning process. It also has the potential to be extended in order to take more account of interactions with resources, identifying indirect relationships between people which are characterised by their interaction with the same resources rather than through direct communication. 
 Social learning discourse analytics. 
 Essence of discourse analysis. 
 Discourse analysis is the collective term for a wide variety of approaches to the analysis of series of communicative events. Some of these approaches cannot easily be employed as online social learning discourse analytics because they focus on face-to-face or spoken interactions and may require intensive examination of semiotic events from a qualitative perspective. Others provide new ways of understanding the large amounts of text generated in online courses and conferences. Schrire (2004) used discourse analysis to understand the relationship between the interactive, cognitive and discourse dimensions of online interaction, examining initiation, response and follow-up (IRF) exchanges. More recently, Lapadat (2007) has applied discourse analysis to asynchronous discussions between students and tutors, showing how groups of learners create and maintain community and coherence through the use of discursive devices. 
 Discourse analysis tools. 
 There are many tools available for the online analysis of text and discourse; the Digital Research Tools Wiki currently lists 55. These range from well-known visualisation tools such as Wordle and Tag Crowd to powerful generic tools such as NVivo, which can be used to support a range of qualitative research methods. A method of discourse analysis that relies heavily on electronic tools and computer processing power is corpus linguistics, the study of language based on examples of real-life use. The corpus of examples is typically in electronic form and may be massive; the European Corpus Initiative Multilingual Corpus includes 98 million words covering most of the major European languages, while the British National Corpus is a 100-million-word sample of a wide range of written and spoken sources. Automated software, such as WMatrix, facilitates quantitative investigation of such corpora (O'Halloran, 2011). 
 A different approach to seeking to extract structure from naturally occurring but relatively unstructured texts is to ask users to add more structure themselves. This is an extension of asking users to enrich resources with metadata, which we see in social tagging. Learners cannot be asked to structure their annotations on documents and contributions to discussion simply to facilitate computational processing, since there would be no value for them in doing so. However, significant research in concept mapping (Novak, 1998) and computer-supported argumentation (Scheuer, Loll, Pinkwart, & McLaren, 2010) has shown that this can be a pedagogically effective discipline to ask of students in a formal academic context, and within organisational contexts, the mapping of conversations can promote quality meetings and shared ownership of outcomes amongst diverse stakeholders (Selvin & Buckingham Shum, 2002). Cohere is a web-based tool that provides a medium not only for engaging in structured online discourse, but also for summarizing or analysing it (Buckingham Shum, 2008). Following the approach of structured deliberation/argument mapping, Cohere renders annotations on the web, or a discussion, as a network of rhetorical moves: users must reflect on, and make explicit, the nature of their contribution to a discussion. This tool can be used to augment online conversation by making explicit information on the rhetorical function and relationship between posts. Users also have the option to browse their online dialogue as a semantic network of posts rather than as a linear text. 
 Discourse-focused social learning analytics. 
 A sociocultural perspective on learning “highlights the possibility that educational success and failure may be explained by the quality of educational dialogue, rather than simply in terms of the capability of individual students or the skill of their teachers” (Mercer, 2004, p. 139). The ways in which learners engage in dialogue are indicators of how they engage with other learners’ ideas, how they compare those ideas with their personal understanding, and how they account for their own point of view, which is an explicit sign of the stance they hold in the conversation. Mercer and his colleagues distinguished three social modes of thinking that are used by groups of learners in face-toface settings: disputational, cumulative and exploratory talk (Mercer, 2000; Mercer & Littleton, 2007). Disputational dialogue is characterised by disagreement and individualised decision-making; in cumulative dialogue speakers build on each other’s contributions but do not critique or challenge these. Exploratory dialogue is typically regarded as the most desirable by educators because speakers share knowledge, challenge ideas, evaluate evidence and consider options together. Learning analytics researchers have built on this work to provide insight into textual discourse in online learning (Ferguson, 2009), providing a bridge to the world of online learning analytics for knowledge building. Initial investigations (Ferguson & Buckingham Shum, 2011) suggest that indicators of exploratory dialogue—challenges, extensions, evaluations and reasoning—can be automatically identified within online discussion. This analysis can be used to provide recommendations about relevant learning discussions, as well as to prompt the development of meaningful learning dialogue. The Cohere structured deliberation platform has been extended by De Liddo and her colleagues (2011) to provide learning analytics that identify: - Learners’ attention—what they focus on, which problems and questions they raise, which comments they make and which viewpoints they express - Learners’ rhetorical attitude to discourse contributions—areas of agreement and disagreement, the ideas supported by learners and the ideas questioned by learners - Distribution of learning topics—the people who propose and discuss the most contentious topics - Learners’ relationships—beyond the undifferentiated ties of social network analysis, Cohere users are tied with semantic relationships (such as supporting or challenging), showing how learners relate to each other and how they act within a discussion group. While informal text chat is difficult to analyse automatically in any detail, due to non-standard use of spelling, punctuation and grammar, more formally structured texts such as a journal article can be analysed using natural language processing technologies. Sándor and her colleagues (Sándor, Kaplan, & Rondeau, 2006; Sándor & Vorndran, 2009) have used the Xerox Incremental Parser (XIP) to highlight key sentences in academic articles in order to focus an evaluator’s attention on the key rhetorical moves within the text which signal claims to contribute to knowledge. Analysis of XIP and human annotation suggests that they are complementary in nature (Sándor, De Liddo, & Buckingham Shum, 2012). Whitelock and Watt analysed discourse using Open Mentor, a tool for teachers to analyse, visualise and compare the quality of their feedback to students (Whitelock & Watt, 2007, 2008). Open Mentor uses a classification system based on that of Bales (1950) in order to investigate the socio-emotive aspects of dialogue as well as the domain level. A standard charting component is then used to provide interactive bar chart views onto tutors’ comments, showing the difference between actual and ideal distributions of different comment types. Tutors can use these analytics to reflect on their feedback, and the analytics can also be used to recommend moves towards the types of feedback that students find most useful. The development of the field of learning analytics has brought approaches to discourse that originated in the social sciences more closely in contact with statistical methods of extracting and representing the contextual usage and meaning of words (Landauer, Foltz, & Laham, 1998). A social learning analytics perspective offers the possibility of harnessing these methods and understandings in order to provide analytics and representations that can help learners to develop their conversations into reasoned arguments and educational dialogue. 
 Socialised learning analytics. 
 Discourse and social network analytics are inherently concerned with social interaction. In the context of learning, they already have a strong focus on the learning group. In this section, we consider three kinds of learning analytic that are more typically viewed from the perspective of the isolated learner who may be making no use of interpersonal connections or social media platforms. We argue that these analytics take on significant new dimensions in the context of online social learning. 
 Social learning disposition analytics. 
 Essence of learning dispositions. 
 The first of these socialised learning analytics is the only one of our five categories that originated in the field of educational research rather than being adapted to apply to the analysis of learning. A well-established research programme has identified, theoretically, empirically and statistically, a seven-dimensional model of learning dispositions (Deakin Crick, 2007). These dispositions can be used to render visible the complex mixture of experience, motivation and intelligences that make up an individual’s capacity for lifelong learning and influence responses to learning opportunities (Deakin Crick, Broadfoot, & Claxton, 2004). They can be used to assess and characterise the complex mixture of experience, motivation and intelligences that a learning opportunity evokes for a specific learner. It is these developing qualities that make up an individual’s capacity for lifelong learning (Deakin Crick, et al., 2004). Learning dispositions are not “learning styles,” a blanket phrase used to refer to a wide variety of frameworks that have been critiqued on a variety of grounds, including lack of contextual awareness (Coffield, Moseley, Hall, & Ecclestone, 2004). By contrast, important characteristics of learning dispositions are that they vary according to context, and that focused interventions have been shown to produce statistically significant improvements in diverse learner groups, ranging in age from primary school to adults, demographically from violent young offenders and disaffected teenagers to high achieving pupils and professionals, and culturally from middle-class Western society to Indigenous communities in Australia (Buckingham Shum & Deakin Crick, 2012). Together, learning dispositions comprise the seven dimensions of “learning power”: changing & learning, critical curiosity, meaning making, dependence & fragility, creativity, relationships/interdependence and strategic awareness (Deakin Crick, 2007). Dynamic assessment of learning power can be used to reflect back to learners what they say about themselves in relation to these dimensions, and to provide teachers with information about individuals and groups that can be used to develop students’ self-awareness as well as their ownership of and responsibility for their learning. 
 Disposition analysis tools. 
 The ELLI (Effective Lifelong Learning Inventory) assessment tool arose from an exploratory factor analytic study involving 2000 learners. Since then, it has been developed in a range of educational settings worldwide as an instrument to help assess capacity for lifelong learning (Deakin Crick, 2007; Deakin Crick, et al., 2004; Small & Deakin Crick, 2008). ELLI is a self-report questionnaire which individuals are asked to answer with a specific piece of recent learning in mind. These responses are used to produce a learning profile, a graphical representation of how the learner has reported themselves in relation to the dimensions of learning power: “very much like me,” “quite like me” or “a little like me.” This diagram is not regarded as a description of fixed attributes but as the basis for a mentored discussion with the potential to spark and encourage changes in the learner’s activities, attitude and approach to learning. In order to gather ELLI data globally, with quality and access controls in place, and to generate analytics fast enough to impact practice in a timely manner, ELLI is hosted within a learning analytics infrastructure called the Learning Warehouse. This supports large-scale analysis of international datasets (e.g., >40,000 ELLI profiles), providing portals to organisations including remote Australian communities, schools in China, Malaysia, Germany, Italy, US, and corporates in the UK (Buckingham Shum & Deakin Crick, 2012). 
 Disposition-focused social learning analytics. 
 Learning dispositions are personal, related to the identity, personhood and desire of the learner (Deakin Crick & Yu, 2008). They can be regarded as socialised learning analytics when the emphasis shifts away from the learner as individual towards the learner in a social setting. From this perspective, two elements of disposition analytics are particularly important—their central role in an extended mentoring relationship, and the importance of relationships / interdependence as one of the seven key learning dispositions. The ELLIment tool provides a collaboration space for a learner and mentor to reflect on a learner’s ELLI profile, and agree on interventions. EnquiryBlogger mines information from a blogging tool set up to support enquiry, providing learners and teachers with visual analytics reflecting student activity and their self-assessment of progress in their enquiry, use of learning dispositions, and overall enjoyment. This then enables appropriate and timely intervention from teachers and, being a blogging environment, comments from peers (Ferguson, Buckingham Shum, & Deakin Crick, 2011). Mentors play an important part in social learning, providing both motivation and opportunities to build knowledge. They may act as role models, encouraging and counselling learners, and can also provide opportunities to rehearse arguments and to increase understanding (Anderson & Shannon, 1995; Ferguson, 2005; Liu, Macintyre, & Ferguson, 2012). People providing these online support relationships may be able to provide more useful assistance if they are aware of the prior knowledge, progress and goals of the person asking a question (Babin, Tricot, & Mariné, 2009). From a social learning perspective, disposition analytics provide ways of stimulating conceptual change, distributed expertise, collaboration and innovation. They tie in with an increasing emphasis on knowledge-age skills, and can be used to encourage learners to reflect on their ways of perceiving, processing and reacting to learning interactions. From the perspective of teachers and mentors, awareness of these elements contributes significantly to their ability to engage groups of learners in meaningful, engaging education. 
 Social learning content analytics. 
 Essence of content analytics. 
 Whereas disposition analytics have been developed within the field of education, content analytics have only recently been associated with education, originating in technical fields concerned with recommender systems and information retrieval (Drachsler, Hummel, & Koper, 2008; Zaïane, 2002). Content analytics is used here as a broad heading for the variety of automated methods that can be used to examine, index and filter online media assets, with the intention of guiding learners through the ocean of potential resources available to them. Note that these analytics are not identical to content analysis, which is concerned with description of the latent and/or manifest elements of communication (Potter & Levine-Donnerstein, 1999). Combined with learning context analytics or with defined search terms, content analytics may be used to provide recommendations of resources that are tailored either to the needs of an individual or to the needs of a group of learners. Research in information retrieval represents the leading edge of techniques for the automated indexing and filtering of content, whether textual, or multimedia (for example, images, video, or music). The state of the art in textual and video information retrieval tools is displayed annually in the competitions hosted at the Text Retrieval Conference (see Little, Llorente, & Rüger, 2010 for a review). Visual similarity search is an example of multimedia content analysis that uses features of images such as colour, texture and shape in order to find material that is visually related. This allows near-duplicate detection, known object identification and general search. Together, these elements can be used to provide novel methods of suggesting, browsing or finding educational media. Other approaches to content analytics are more closely aligned with content analysis. These involve examination of the latent elements that can be identified within transcripts of exchanges between people learning together online. This method has been used to investigate a variety of issues related to online social learning, including collaborative learning, presence and online cooperation (de Wever, Schellens, Vallcke, & van Keer, 2006). These latent elements of interpersonal exchanges can also be used to support sentiment analysis, using the objectivity/subjectivity of messages, and the emotions expressed within them to explore which resources are valued, and the motivations behind recommendations (Fakhraie, 2011). 
 Content analysis tools. 
 Web-based search engines are the default tools to which most learners and educators turn for text search, but multimedia search is becoming increasingly possible. While some approaches exploit the metadata around a multimedia asset, such as the text surrounding a photo, rather than analyse its actual content, true image-based search on the web is now available (for instance, Google Image search allows the filtering of results by colour). Some ecommerce websites enable product filtering by visual similarity, and mobile phone applications are able to parse images such as book covers, in order to retrieve their metadata (e.g., http://www.snaptell.com). Turning to transcript analysis, commonly used tools for content analysis include NVivo and Atlas.ti, both of which are software packages designed to support the analysis of unstructured information and qualitative data. However, these are manual tools for human analysts. Erkens and Janssen (2008) review the challenges of automated analysis, and describe Multiple Episode Protocol Analysis (MEPA), which has been validated against human coders, and used to automatically annotate chat transcripts from learning environment in numerous studies. In the selection of any of these tools, researchers face the bigger challenge of identifying an analytic framework that “emphasizes the criteria of reliability and validity and the counting of instances within a predefined set of mutually exclusive and jointly exhaustive categories” (de Wever et al., 2006). The validity of content analysis of online discussion has been persistently criticised (Pidgeon, 1996, p. 78) and it has proved difficult to identify empirically validated content analysis instruments to use in these contexts (Rourke, Anderson, Garrison, & Archer, 2003). 
 Content-focused social learning analytics. 
 How do these tools take on a new dimension in social learning? Visual Similarity Search can be used to support navigation of educational materials in a variety of ways, including discovering the source of an image, finding items that share visual features and may provide new ways of understanding a concept, or finding other articles, talks or movies in which a given image or movie frame is used (Little, Ferguson, & Rüger, 2011). Content analytics take on a social learning aspect when they draw upon the tags, ratings and additional data supplied by learners. An example is iSpot, which helps learners to identify anything in the natural world (Clow & Makriyannis, 2011). When a user first uploads a photo to the site, it has little to connect it with other information. The addition of a possible identification by another user ties that photo to other sets of data held externally in the Encyclopaedia of Life and within the UK’s National Biodiversity Network. In the case of iSpot, this analysis is not solely based on the by-products of interaction, an individual’s reputation within the network helps to weight the data that is added. The site’s reputation system has been developed with the purpose of magnifying the impact of known experts. Overall, the example of iSpot suggests one way in which content analytics can be combined with social network analytics to support learning. The two forms of analytics can also be used to support the effective distribution of key resources through a learning network. Another approach is to apply content analysis to the interplay of learning activities, learning objects, learning outcomes, and learners themselves, establishing semantic relations between different learning artefacts. This is the approach taken by LOCO-Analyst, which is used to analyse these semantic relations and thus provide feedback for content authors and teachers that can help them to improve their online courses (Jovanović et al., 2008). This type of analysis can draw on the information about user activity and behaviour that is provided by tools such as Google Analytics and userfly.com as well as by the tools built into environments such as Moodle and Blackboard. 
 Social learning context analytics. 
 Essence of context analytics. 
 Overall, social learning analytics can be applied to a wide variety of contexts that extends far beyond institutional systems. They can be used in formal settings such as schools, colleges and universities, in informal contexts in which learners choose both the process and the goal of their learning (Vavoula, 2004) and by mobile learners in a variety of situations (Sharples, Taylor, & Vavoula, 2005). In some cases, learners are in synchronous environments, structured on the basis that participants are co-present in time, and at others they are in asynchronous environments, where the assumption is that they will be participating at different times (Ferguson, 2009). They may be learning alone, in a network, in an affinity group, in communities of inquiry, communities of interest or communities of practice (Ferguson, 2009). Here we are grouping under the heading “context analytics” the various analytic tools that expose, make use of or seek to understand these contexts in relation to learning. Zimmerman and his colleagues (2007) provide a definition of context that allows the definition of the context of an entity (for example, a learner) depending on five distinct categories: - Individuality context includes information about the entity within the context. In the case of learners, this might include their language, their behaviour, their preferences and their goals - Time context includes points in times, ranges and histories so can take into account work flow, long-term courses and interaction histories - Location context can include absolute location, location in relation to people or resources, or virtual location (IP address) - Activity context is concerned with goals, tasks and actions - Relations context captures the relations of an entity with other entities, of example with learners, teachers and resources. Early work in context-aware computing treated the environment as a shell encasing the user and focused on scalar properties such as current time and location, together with a list of available objects and services (see, for example, Abowd, Atkeson, Hong, Long, & Pinkerton, 1997; Want, Hopper, Falcao, & Gibbons, 1992). The focus was on the individual user receiving data from an environment rather than interacting with it. This model did not acknowledge the dynamics of interaction between people and the environment. When considered in the context of learning, it did not provide information that could help people to modify their environment in order to create supportive workspaces or form social networks with those around them or accessible online (Brown et al., 2010). 
 Context analysis tools. 
 The MOBIlearn project took a different view, considering context to be a dynamic process, constructed through learners’ interactions with learning materials and the surrounding world over time (Beale & Lonsdale, 2004; Syvänen, Beale, Sharples, Ahonen, & Lonsdale, 2005). The MOBIlearn context awareness subsystem was developed to allow learners to maintain their attention on the world around them while their device presents content, options and resources that support their learning activities. The developers of the system designed the system to analyse a variety of available data in order to produce learner-focused information and recommendations, taking into account not only the physical environment but also social relationships. Environmental information such as geographical position allows us to provide location-specific information, e.g., for a museum. Other user information such as the identification and presence of another person allows us to create a peer-to-peer network for informal chat. But the combination of the two may allow us to determine that the other user is a curator, and we can provide the mechanisms for one to give a guided tour to the other. (Beale & Lonsdale, 2004) The Active Campus tool was another one developed to prompt connections with learners and resources. The aim was to provide a tool that could analyse people, resources and events in the vicinity and then act like a pair of “x-ray glasses,” providing opportunities for serendipitous learning by letting users see through crowds and buildings to reveal nearby friends, potential colleagues and interesting events (Griswold et al., 2004). 
 Context-focused social learning analytics. 
 The MOBIlearn project produced several recommendations to be considered in the design process of an adaptive and pervasive learning environment. Some of these are focused on the physical design of tools, but others are directly relevant to the development of context-focused social learning analytics, specifically: - Organizing the information provided to the user according to the availability for cooperation (students), advice (experts, instructors) and groups available at a given moment. - Supporting the communication between users by presenting tools, such as news groups and chats, ordered by their current popularity in the learning community (placing first the most popular, or the most relevant to the learner according to the profile, at any given moment). - Encouraging users to cooperate and affiliate by pushing the information when relevant opportunities occur. Actions by the system are guided, for example, by the information related to a group-based modeling that takes into account each user’s evident interest in certain piece(s) of information (Syvänen et al., 2005). These suggest fruitful ways forward in this area. In the case of online learning, context analytics can draw upon readily available data such as profile information, timestamps, operating system and location. Such data mining can support recommendations that are appropriate for learners’ situation, the time they have available, the devices they can access, their current role and their future goals. Context analytics can also be used to highlight the activity of other learners in a community or network, through tag clouds, hash tags, data visualizations, activity streams and emergent folksonomies. In addition to development work in this field, there is also a need for substantial theoretical work that can underpin it. Social network analysts have spent many years identifying elements and structures that have been found to support learning and which can be used to create contexts that promote the development of sophisticated learning networks. There are currently no such sophisticated analytics available to help us develop suitable contexts for other groupings known to support social learning, such as affinity groups and communities of practice. We also lack the long-term analytics of learner behaviour that could help us to analyse context in order to support the development of personal learning narratives, learning trajectories or other understandings of lifelong learning (Gee, 2004; Jones & Preece, 2006; Lipman, 2003; Wenger, 1998). 
 The challenge of powerful analytics. 
 Having explained how we are conceiving social learning analytics, we now consider some of the critiques around the balance of power in learning analytics, in response to which we will conclude by sketching potential future scenarios that may address these concerns. New forms of measurement and classification—for that is essentially what learning analytics are—are rightly exposed to disciplinary and ethical critiques concerning issues such as: who is defining the measures, to what ends, what is being measured, and who has access to what data? In their incisive critique of classification systems, Bowker and Star (2000) demonstrate how these become the mechanisms by which we choose not only how to remember, but also systematically forget, what is known. If a phenomenon is not visible within a classification scheme, it is systematically erased. The issue of power is, therefore, a central one to confront. This dilemma sits at the heart of the controversy around any policy dependent on a predefined performance indicator. Schools, universities, faculties or individuals whose work is invisible within a classification scheme are disenfranchised when defined by powerful stakeholders with associated rewards/sanctions. Whether this is reasonable sparks debate as to whether phenomena are being justifiably ignored because they are not something to be encouraged, or whether it is simply that they are too hard to quantify for automated processing and performance grading. The challenge for learning analytics is more complex still. As described above, at least some forms of learning analytics research have an interest in using data generated by users as a by-product of online activity (for example, asking/answering questions, or recommending resources), rather than as an intentional form of evidence of learning (such as taking a test or submitting an essay). Building on this potentially noisy data, research into recommendation engines goes one step further, exploring the potential to mine such data for patterns that can be acted on by software agents in some way—perhaps in the form of feedback to learners via a personal analytics dashboard or as modifications to the content that is displayed based on the system’s model of the learner. Such research must engage fully with questions around the academic, pedagogical and ethical integrity of the principles for defining such patterns and recommender algorithms, and who is permitted to see them within the set of stakeholders. Important concerns (boyd & Crawford, 2011) are beginning to be expressed about learning analytics, such as the following variants on longstanding debates at the intersection of education, technology and artificial intelligence: - Analytics are dependent on computational platforms that use, re-use and merge learner data, both public and private: institutions should steer clear of open data and minimise the merging of datasets of any sort until there are much clearer ethical and legal guidelines. - Analytics could disempower learners, making them increasingly reliant on institutions providing them with continuous feedback, rather than developing meta-cognitive and learning-to-learn skills and dispositions. - Analytics are a crude way to operationalise proxy measures of teacher effectiveness, and will be used to compare and contrast student outcomes, leading to the gaming of the system: “learning and teaching to the analytic” to maintain performance indicators that do not genuinely promote meaningful learning. In sum, learning analytics and recommendation engines are always designed with a particular conception of “success,” thus defining the patterns deemed to be evidence of progress, and hence, the data that should be captured. A marker of the health of the learning analytics field will be the quality of debate around what the technology renders visible and leaves invisible. Briefly, let us consider how these issues may be seen through a Social Learning Analytics lens, recognising that a more detailed treatment is needed in future work. If the values and practices we see in the open, social web inform the ways in which SLAs are deployed, we may see ways to address these concerns. For example: - If SLA tools and data are placed in the hands of learners, the balance of power shifts significantly. When the exposure of personal data to analytics is voluntary, when a group’s data is collectively owned, and when gaming the system or trying to pretend to be someone you are not incurs social sanctions, the risks of abuse are arguably lower than when a hierarchical institution carries the unrealistic burden of responsibility for controlling a living ecosystem of participants, data and tools. It is realistic to note that the above imply a maturing in technologies, learner literacies, and institutional practices around the management of personal data, compared to the situation we have today. - If analytics are drawing learners’ attention to their development as self-aware, intrinsically motivated learners, they are being moved in the opposite direction to becoming passively dependent on the institution or platform to tell them how they are doing and what to do next. - If analytics are focused on providing formative feedback to improve learning process, rather than making automated judgments about mastery levels in a given subject, there might be fewer concerns around the removal of human mentors from the feedback loop. We also hypothesise that the risks of “gaming the analytic” reduce: SLA activity patterns are by definition hard to fabricate privately, so not only are learners fooling themselves if they fake behaviour (e.g., designed to look like skillful discourse, supportive networking, or self-reflection), they risk making fools of themselves among peers for whom authenticity and trustworthiness are valued personal qualities. 
 Conclusion: SLA future scenarios. 
 Let us conclude by engaging in the early stages of what Miller (2007) terms “futures literacy”—stretching our imaginations in disciplined ways in order to sketch potential futures, were social learning analytics to develop in line with these cultural shifts. Consider the forces identified earlier (§4), and for each, imagine future scenarios in which SLA values, tools and practices have matured beyond today’s nascent state. The digital infrastructure is reaching a state of maturity that enables non-technical people to engage with expertly designed “walk up and use” interfaces on both large-screen and mobile devices, to connect with people and information on a global scale, and to make their contributions via social media platforms. - Potential SLA future: Institutions lacking the infrastructure needed for computationally intensive analytics and recommendation engines will call on SLA services in the computing “cloud,” following the business developments we are now seeing to offer commercial learning analytics cloud services on school/university data. Individual learners or communities who need such services also utilise these services. Some companies and educational institutions will exploit their pedagogical expertise to provide SLA consulting services. As we see the commercialization of the analytics computing space, there is an argument that at this point the field needs a complementary Open Learning Analytics innovation platform (SoLAR, 2011). “Free and Open” is a key expectation and dynamic within online social learning. It highlights the recalibration that is taking place around expectations of freely provided quality services, accompanied by readiness to pay for valueadded services once the free service has proven itself. Data is expected to be accessible, appropriately licensed for remixing and, wherever possible, in machine-readable formats to facilitate interoperability and avoid data or users being locked into a given platform. - Potential SLA future: Many SLA tools become available in open source versions, making them customisable within the myriad unique social contexts in which they may be deployed. It becomes normal that SLA patterns and data are open, shareable resources for reflection, and analysis in alternative tools. In addition to a diverse palette of free SLA tools, an economy grows which helps learners to configure these to create meaningful toolkits that support particular kinds of learning, or work well with particular platforms. Learners are willing to pay for more powerful features, once the most successful tools have earned their right to charge. A key lesson from the social web paradigm, and a long-held aspiration of researchers into end-user customisability, is that when empowered with appropriately flexible tools, an ecosystem grows in which new roles are created for different kinds of user to customise their tools (MacLean, Carter, Lovstrand, & Moran, 1990). Aspirations across cultures have been shifting in empirically verifiable ways towards a growing desire for participation and self-expression. The social web is an expression of this shift, providing a significant medium for many people to construct their identity. - Potential SLA future: The outputs of SLA tools become an important part of individuals’ sense of identity, and their ability to evidence their skills. For example, we might see Badges such as: “I am a good broker between communities,” “I can distill complex debates into their essence,” “I can mentor learners in building their creativity.” Innovation in complex, turbulent environments requires social knowledge-creation and negotiation infrastructures built on quality relationships and conversations—beyond impersonal “transactions”—in order for individuals, groups and organisations to be agile enough to respond to turbulent change and to work together to solve “wicked problems.” - Potential implications for SLA: SLAs become an integral part of the employee’s toolkit, helping to track the swirl of people, conversations and resources by rendering significant changes in coherent ways that keep cognitive load at a manageable level, rather than amplifying demands on attention. 
 The role of educational institutions is changing. They are moving increasingly to provide personalised support for learning how to think deeply, and learning how to be an effective member of the communities that one cares about. - Potential implications for SLA: Educational institutions are no longer the only option for evidencing advanced learning. Analytics become a new form of trusted evidence, being generated from verifiable public datasets, or private datasets that could not have been reasonably fabricated, such as a reputable online community. In sum, if it is the case that these tectonic shifts define a new context for thinking about learning, in particular around questions of power and the central role of interpersonal relationships, by extension they set a new context for thinking about learning analytics. They call into question the assumption inherited from the business intelligence and management information systems orientation, that learning analytics are designed and controlled primarily by institutional educators and administrators in order to optimize learners’ performance, and hence the institution’s performance. This is not at all to argue that academic/action analytics are unimportant—but it now becomes clear that this is only one of a range of possible analytics scenarios. To conclude, we have motivated the concept of Social Learning Analytics as a response to some of the forces reshaping the educational landscape, and our growing understanding that many forms of learning most relevant to becoming a citizen in our complex society are socially grounded and evidenced phenomena. SLAs may be deployed as institutional tools in conventional courses, to yield insight for educators and administrators. Equally, however, they should be seen as tools to be placed in the hands of the very subjects being analysed—the learners—and for the many informal learning contexts that we now see outside the walls of conventional institutions. It would indeed be ironic if the ways in which Social Learning Analytics tools were deployed did not honour and promote the open, democratising, critical dynamics that underpin much of the participatory, social web philosophy—dynamics which SLA tools make visible in new ways. 
 Acknowledgements. 
 We gratefully acknowledge The Open University for resourcing the SocialLearn Project, several anonymous reviewers for their constructive reviews on earlier drafts, and the encouragement from researchers and practitioners who have found these ideas valuable in their own work.]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Social Learning Analytics</rdfs:label>
		<dc:subject>learning analytics</dc:subject>
		<dc:subject>social learning</dc:subject>
		<dc:subject>dispositions</dc:subject>
		<dc:subject>social networks</dc:subject>
		<dc:subject>discourse</dc:subject>
		<dc:subject>informal learning</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/74/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/75">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>A Multidimensional Analysis Tool for Visualizing Online Interactions</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/75/authorlist"/>
		<swrc:abstract>This study proposes and verifies the performance of an analysis tool for visualizing online interactions. A review of the most widely used methods for analyzing online interactions, including quantitative analysis, content analysis, and social network analysis methods, indicates these analysis methods have some limitations resulting from their one-dimensional analysis approach. To overcome such limitations, we developed the Multidimensional Interaction Analysis Tool (MIAT) by considering the advantages of well-known methods and incorporating the concept of the comparative interaction level. To verify the performance of the MIAT as a tool for multidimensional visualization of online interactions, results of the one-dimensional interaction analyses and those of the MIAT were compared. Findings suggest that the MIAT can provide a more in-depth interpretation of online interaction than any one-dimensional analysis method. In addition, the MIAT allows researchers to customize their analysis frameworks based on their own theoretical backgrounds.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 In recent years, problem-solving skills have attracted increasing attention from education researchers. The importance of collaborative learning, which facilitates literacy by enabling learners to cooperate with one another through various interactive activities, has been emphasized (An, Shin, & Lim, 2009; Pozzi, 2010). In particular, collaborative learning has been emphasized for implementation in online learning as well as face-to-face classes because, unlike face-to-face classes, online collaborative learning is available on an anywhere/anytime basis (Edge, 2006). In this regard, an increasing number of studies have focused on online collaborative learning, and some have explored factors that can facilitate online collaborative leaning (Benbunan-Fich & Hiltz, 1999; Sinclair, 2005; Yang, Newby, & Bill, 2008). Some factors that have been found to promote online collaborative learning include individual characteristics of learners, instruction methods, the learning environment, learners’ motives, and type and level of online interaction). Researchers have suggested that, among the factors mentioned above, the type and level of online interaction are most likely to influence online collaborative learning (An et al., 2009; Daradoumis, Martı´nez-Mone, & Xhafa, 2006). For this reason, previous studies have typically focused on dynamic interaction in the online collaborative learning environment. In particular, a number of studies have attempted to find ways to analyze such interactions because different analysis methods tend to provide different information and interpretations. Online interaction has attracted increasing attention from researchers and, thus, a number of studies have attempted to enhance existing analysis methods for measuring online interaction (Marra, 2006), including quantitative analysis, content analysis, and social network analysis (SNA) methods. The quantitative analysis method is used to investigate the level of online interaction by considering the number of posts by users, the number of replies, and the number of logins (Benbunan-Fich & Hiltz, 1999). A major advantage of this method is that it can easily quantify the level of online interaction. On the other hand, the content analysis method allows for an analysis of interaction types and levels by classifying learners’ posts based on certain criteria. Among various relationship analysis methods, the SNA method has recently been used by researchers to analyze the relationship among individuals within a certain group by treating those individuals as nodes and structuralizing message content into links (Hu & Racherla, 2008). Although all of these methods are useful when analyzing online interaction, each of them cannot provide multidimensional aspects of online interaction, and thus each method focuses only on one aspect (e.g., the quantitative analysis method focuses on the number of interactions, the content analysis method on the types and levels of interaction, and the SNA method on the structure of the interaction). Researchers want a method that can provide rich information of online interpretation because they require an in-depth understanding of online interaction. In addition, researchers want to visualize the analysis results of online interaction because visualization is a useful way to interpret complex interaction among group members more clearly (Hirschi, 2010). However, any principle or method analyzing interactions with a multidimensional approach has not been reported as of yet. Therefore, this study intends to develop an instrument that visualizes the results of the analysis on the basis of principles of multidimensional approaches to analyzing online interactions. A second objective of this study is to show how the results of multidimensional analysis and that of existing one-dimensional analyses are different. 
 Analysis methods for interactions in the online collaborative learning environment 
 Online interaction is defined as more than two people are giving and taking information to pursue their common learning goals in an online learning environment (Ma¨kitalo, Ha¨kkinen, Leinonen, & Ja¨rvela, 2002). Online interaction includes relatively intensive information about the process of learners’ thinking and knowledge formation because it mostly happens in an asynchronous environment, which allows enough time for learners’ reflective thinking (An, Shin, & Lim, 2009; Blanchette, 2012; Garrison & Cleveland-Innes, 2005). To understand the online learning process properly, researchers must recognize that analyzing online interaction is an important issue. Depending on the types of analysis enacted, researchers can gather or lose valuable information (Blanchette, 2012). For this reason, many studies have addressed how to analyze online interaction. The following are representative ways to analyze online interaction. 
 Quantitative analysis of online interaction.
 The quantitative analysis method was the first method used for analyzing interactions in the online collaborative learning environment (Marra, 2006). This method considers the number of posts written and read, as well as replies and logins by learners. In addition, the quantitative analysis method also compares the points produced by adding the number of writings and readings for the level of online interaction (Benbunan-Fich & Hiltz, 1999; Gorghiua, Lindforsb, Gorghiuc, & Hämäläinend, 2011; Pozzi, 2010). For other methods, the average found from dividing the number of postings into the number of participants (Hewett, 2000), as well as a level of online interaction, was also analyzed by scoring the values of each message with certain criteria (Brooks & Jeong, 2006; Newman, Webb, & Cochrane, 1996). This method typically employs relatively simple and objective quantitative data. Early studies considered this method to be the most objective for analyzing online interactions among learners since researchers were able to use diverse statistical methods based on quantitative data (Benbunan-Fich & Hiltz, 1999; Marra, 2006; Mason, 1992). However, this method is limited in that it provides only quantities for online interaction without analyzing the type and structure of online interaction or identifying important phenomena in the interaction process (Strijbos, Martens, Prins, & Jochems, 2006) 
 Content analysis of online interaction.
 The content analysis method is frequently used in research on online interaction because it can better analyze the type and level of online interaction than the quantitative analysis method, which allows only for limited information (George, 2008; Strijbos et al., 2006). The content analysis method characterizes the meaning of message content in a systematic and qualitative manner (George, 2008). The unit of analysis and the category of analysis play important roles in content analysis (Strijbos et al., 2006). The content of learners’ interactions is analyzed as messages, and such messages are classified based on the decided unit of analysis. A number of recent studies have used the content analysis method because of its ability to determine type, structure, and level of online interaction (Strijbos et al., 2006). De Weber, Schellens, Valcke, & Van Keer (2006) described the framework of content analysis, and diverse analytical frameworks have been used for content analysis in the context of online collaborative learning. Among such frameworks, Henri’s (1992) framework has widely been used because its categories are clearly distinguished and its analysis method is relatively simple, allowing even non-experts to analyze messages exchanged during the learning process. Henri’s framework is composed of five categories: participative, social, interactive, cognitive, and metacognitive. Other widely-used frameworks are that by Gunawardena, Lowe, and Anderson (1997), which is composed of the following five categories: sharing/comparing information; discovery and exploration of dissonance; negotiation of meaning/co-construction of knowledge; testing and modification of proposed synthesis; and phrasing of agreement, statement, and application of the newly-constructed meaning, and that by Zhu’s (1996), which is composed of the following six categories: answers, information sharing, discussion, comment, reflection, and scaffolding. These content analysis methods enhance the ability of researchers to gather a wide range of information from the interaction process, and thus the methods have been extensively used (Bassani, 2011; Kale, 2008). However, the content analysis method is limited in that, although it provides qualitative information about the types and levels of discourse content, it does not provide structural information of interaction. 
 Social network analysis of online interaction.
 The SNA method focuses on revealing the relationship and structure of online interactions among individuals in a group (Sternitzke, Bartkowski, & Schramm, 2009). The key advantage of the SNA method is its ability to visualize the relationship among individuals and the structure of their online interaction through nodes and links (Medina & Suthers, 2009; Sternitzke, et al., 2009). This method of analysis also provides information on personal contribution to interaction within the group (Contractor, Wasserman, & Faust, 2006), as well as varied information for analyzing interaction, such as its structure, flow, and processes. It is able to present results of the analysis after visualizing them (Bergs, 2006; Wasserman & Faust, 1989). In addition, SNA can visualize learning processes through group members’ interaction (Suthers & Rosen, 2011). Also, SNA provides quantitative data in the form of various indices, including centrality (the degree to which an individual occupies a central position in the network), concentration (the degree to which the entire network is concentrated toward the center), and density (the number of connections between individuals) (An et al., 2009; Heo, Lim, & Kim, 2010). However, although this method can be used to analyze the relationship and structure of online interactions among learners, it is limited in that specific types of messages cannot be analyzed. Given the discussion above, each analysis method has beneficial aspects in terms of analyzing online interaction, but is limited in providing multiple aspects of online interaction due to its pursuit of one-dimensional analysis. In addition, the methods do not help researchers understand the results of interaction analysis more explicitly. Therefore, we suggest a multidimensional analysis method in order to overcome the limitations of one-dimensional analysis approaches. 
 Multidimensional Interaction Analysis Tool (MIAT).
 We developed the Multidimensional Interaction Analysis Tool (MIAT), which can facilitate a multidimensional (quantitative analysis, content analysis, and structural analysis) analysis of online interactions among individuals in a group, as well as conceptualize those interactions in a visual way. In other words, the MIAT can simultaneously analyze the quantitative analysis/content analysis aspects of online interactions and the relationships among individuals in a group. In addition, the most remarkable feature of the MIAT is its ability to visualize all interactions among individuals in a certain group at a specific point in time. The principles of MIAT are as follows: 
 Unit of analysis.
 The unit of analysis is the most critical factor in content analysis (Woo & Reeves, 2007). In the MIAT, the unit of analysis is the message (the entire post under a title) because the MIAT considers the structure of the relationship among group members based on SNA. 
 Multidimensional principles of the MIAT.
 Quantitative analysis used in the MIAT carried two principles. One of them is using the frequency of a message, and the other is giving quantitative scores by evaluating the values of each message. For instance, assuming that there were three messages, and one contained false information whereas others contained exact information, the researcher gave marks of less than three by deducting or subtracting points from the error message rather than giving a quantitative score of three with the number of messages. Thus, more plentiful information on interaction could be obtained if it was analyzed by giving quantitative scores from evaluating values of messages rather than simply using the frequency (Brooks & Jeong, 2006; Newman, et al., 1996). The criteria for assessing the values of each message in the MIAT can be freely selected by researchers. For instance, if a researcher desired to evaluate the values of messages by using the 10 criteria of Newman et al. (1996), he could give scores between -10~10 to each message. Additionally, if he desired to use criteria of Brooks & Jeong (2006), he could give marks between -1~1 by scoring +1 for a message that helped solve the assignment and -1 for a message that was not helpful. A key feature of the MIAT is it allows for criteria that assess values of each message to be adjustable by research goals or frameworks since online interactions can vary with learning contexts. The MIAT used content analysis principles. Content analysis is a method that distinguishes message content by a certain category. The method can analyze the learning process shown during interactions and can achieve accurate, objective, consistent information on types and structures of interactions (De Weber, et al., 2006; Strijbos et al., 2006). The MIAT was developed for researchers to create their own frameworks for content analysis, in accordance with research purposes. For example, researchers can use Henri’s (1992) analyzing category or that of Zhu (1996) for MIAT analysis, with a category of content analysis according to the intention of the researcher. The MIAT also offers researchers the flexibility to input a self-generated category or use various other categories (see Figure 1). A lot of flexibility was given to selecting a category of content analysis because online interaction occurs in varied learning contexts, and the research frameworks of researchers desiring to analyze such interactions are also diverse. 
 Figure.

 1. Creating the researcher’s analysis frameworks (e.g., using Henri’s (1992) categories (left) and Zhu’s (1996) categories (right)). 
 Figure 2. Basic model for analyzing the relationships among individuals. 
 The MIAT used a principle of online network analysis to examine a relationship between learners. In other words, if A posted something on a bulletin board and B made a B, and B influenced C. Wiki pages can be analyzed by using the history function as an editing method, which was different from the editing method of a bulletin board. Assuming that B modified A’s posting, and C either added details to this or asked a question about B’s modification, one could say that A influenced B, and B influenced C. Lastly, in the case of live chat, if A wrote a message and B wrote a message after A, and C wrote a message after B, one could say that that A influenced B, and B influenced C (see Figure 2). The direction of arrows in the relationship model is the direction of message influence. Because B’s message comes after A’s, A’s message is expected to influence B, and thus the direction of the arrow goes from A to B. Therefore, the individual with the thickest outgoing arrows is expected to be the most influential group member, and the one with the thickest incoming arrows is most likely to be influenced by his interaction within the group. As such, network analysis can clearly show the relationships among individuals in a group as well as the structure of their interactions. In this regard, the MIAT uses basic network analysis principles to analyze relationships among learners. 
 Calculation of the level of online interaction.
 To explain the principles for calculating a level of online interactions in the MIAT, we will use example data from a bulletin board system. First, criteria for assigning quantitative scores and categories for content analyses are required for explanations. Then, suppose the following four criteria of newness, importance, relevance, and accuracy are applied as a framework of quantitative analysis among 10 criteria used in research performed by Newman et al. (1996). Scores given ranged between 0 and 4 points. The remaining six criteria are excluded from this scoring since these are considered inappropriate because they are generally used to sort message content. 
 Table 1. Criteria for Scoring Values of Each Message. 
 Now, suppose Henri’s (1992) framework is used in categories for content analysis. The original analysis categories of Henri include five types, but, for this example, only three categories are used, social, cognitive, and metacognitive, because a category analyzing the types of messages is required. Participative and interactive are the categories used to analyze a level and structure of interaction rather than the types of messages. Therefore, social, cognitive, and metacognitive were chosen for the analysis. Principles that calculate a level of online interaction using the MIAT could be explained as follows. In order to calculate the level of interaction, the MIAT first requires the input of the analyzed data (the type of and score for each message) (see Figure 3). Then, the MIAT automatically provides a matrix of interaction scores (Figure 4). 
 Figure 3. A sample screen showing input data for the MIAT. 
 The MIAT uses an interaction matrix to calculate two types of interaction levels: the total interaction level and the comparative interaction level. The total number of messages, average score, and standard deviation express the total interaction level. As shown in Figure 4, the total interaction level can be summarized as follows: the total number of messages=24, the total sum of scores per message=55, the average score=2.29, and the standard deviation=0.93. The MIAT uses the average score and the standard deviation to calculate the T-score for the comparative interaction level. When using the sum or average of raw scores, it is difficult to identify comparative levels of interaction in a given group. Therefore, the MIAT uses the T–score (a standard score) to identify comparative scores for the group. For example, the cognitive interaction level between C and A is 3, which becomes 53.43 through the MIAT’s indexation (which uses the T–score). As a result, the cognitive interaction level between C and A is slightly higher than the average. The comparative interaction level is calculated as follows: 
 Figure 4. Matrices of interaction scores. 
 Interpretation of outputs. 
 We analyzed the data in 24 messages using the MIAT, and Figure 5 shows the results. In terms of the total interaction level, the total number of messages was 24, the average score was 2.29, and the standard deviation was 0.93. The comparative interaction level is indicated by the number next to each arrow; the larger the number, the higher the comparative interaction level. Further, the thicker the arrow, the larger the number of messages. The direction of the arrow indicates the direction of the interaction, and interaction types are classified into cognitive, metacognitive, and social categories (indicated by the style of the arrow). As shown in Figure 5, Student A and Student C had a large number of links and thick arrows, which implies the interaction between these two students was the most active. Student A and Student B had fewer links than the other student pairs, and their arrows were thinner, indicating that the interaction between Student A and Student B was passive. However, the arrow for cognitive interaction (the solid arrow) of Student A and Student B was thicker than those of the other student pairs. In addition, the active interaction between Student A and Student C was mainly social. 
 Figure 5. The MIAT results. 
 MIAT Implementation.
 In this section, we will explain how the results of the MIAT analysis are different from those of other onedimensional analysis methods through an example study. Since the level of participation in interaction is one of important indicators of successful online collaborative learning, we conducted a study aiming to recognize group members’ interaction levels. Specifically, we wanted to identify the most active participant in a collaborative work. Through the study, we will try to explain how the results of the MIAT analysis are different from others. 
 Participants and the task.
 We conducted the MIAT analysis by considering a sample of 30 students taking an online course in education technology in the spring semester of 2011 at D University. The average age of these students was 21.7, and 67% were female. They had diverse majors, including human studies, social sciences, curriculum studies, engineering science, and art. The online collaborative task assigned to the class was based on instructional design. The students were randomly assigned to one of six groups (five students per group). Each group was expected to determine the theme of the instructional design through asynchronized interactions on an online bulletin board and to follow the instructional design for two weeks. Only the team showing the most active interaction was selected for this case analysis. 
 Comparison analysis methods.
 We compared the results from the MIAT with those from existing one-dimensional analysis methods (i.e., quantitative analysis, content analysis, and SNA methods). - For the quantitative analysis method, we used the method of Gorghiua et al. (2011), which is one of the most commonly used quantitative analysis methods of interaction levels. We counted the number of messages and the hit number of those messages. - For the content analysis method, we only used three categories (social, cognitive, and metacognitive) among Henri’s five categories (1992) because the other two categories (participative and interactive) do not pertain to the type of interaction. We classified each message based on these three categories and analyzed the interaction level based on the number of messages in each category. - We used NetMiner 2.4 for SNA. We analyzed the relationship among students by considering centrality, cohesion, and the number of messages sent and received. 
 Data analysis.
 The unit of analysis was the message. For the quantitative analysis method, we calculated the total number of posts as well as the number of hits. For the content analysis method, we analyzed the content of messages after their classification. Cohen’s kappa for the inter-rater reliability was 0.90. For SNA, we decided the direction of messages and calculated the number of messages. Cohen’s kappa for the inter-rater reliability was 0.92. For the MIAT, we scored each message’s value based on the criteria in Table 1. Then, we categorized the message and decided its direction. Cohen’s kappa for the inter-rater reliability was 0.84 for scoring values, 0.93 for categorizing messages, and 0.92 for direction of messages. For inconsistent results, we reached an agreement through face-to-face discussions. 
 Analysis results of one-dimensional approaches.
 Table 2 shows the results of the quantitative analysis method. According to results from the quantitative analysis method, Student D produced the most posts and Student C yielded the most hits. Therefore, we can infer Student C and Student D were the most active participants in the group work. 
 Table 2. Results from the Quantitative Analysis Method. 
 According to results obtained using Henri’s (1992) framework, the most active participant is Student D with the largest sum. Among the Student D’s messages, the most common category was social. The next active participants were Student C and Student A, but they show different participation patterns. Student C’s messages are evenly distributed across all categories, but Student A’s messages were concentrated on social messages. Thus, content analysis gives information on the types of interaction as well as similar results of quantitative analysis. 
 Table 4. Results from the Content Analysis Method. 
 According to the results of the SNA, the total number of nodes was five and the total number of links was 20. The betweenness centrality of all nodes was 0 (see Figure 6), and the betweenness centrality stood for a degree of a node mediating the connection of other nodes. These results indicated that flow and exchange of information was even without a special focus on a certain student (Cho, Gay, Davidson, & Ingraffea, 2007). In terms of the cohesion analysis which is about an attractive force between nodes, there were five nodes and five clusters. This implies that no specific nodes gathered to form a cluster. Indeed, if nodes gather to form a cluster, the nodes in the cluster interact only with one another and not with nodes outside of the cluster. Therefore, the equal number of nodes and clusters indicated students engaged in balanced interactions. In terms of messages sent and received between nodes, Student C sent the highest number of messages, whereas Student D received the highest number of messages. This indicated that Student C interacted with other students most actively. 
 Figure 6. SNA results. 
 The results of MIAT method.
 Figure 7 shows the results from the MIAT. Student C and Student E engaged in all three types of interactions (cognitive, metacognitive, and social). The comparative interaction level for cognitive interactions was 163.75 for Student C to Student E, and 79.96 for Student E to Student C. The comparative interaction level for metacognitive interactions was 121.36 for Student C to Student E, and 90.43 for Student E to Student C. The comparative interaction levels for cognitive and metacognitive interactions exceeded the average of 50, indicating the interaction between Student C and Student E contributed to their collaborative task. 
 Comparison of analysis results.
 The results of one-dimensional analyses (quantitative analysis, content analysis, and SNA) indicated that Student C and Student D are most active participants in collaborative group work. However, the MIAT showed slightly different results. According to the MIAT analysis, the most active participants were Student C and Student E. Student C was commonly identified as one of the most active participants, whereas Student E only appeared active in the results of the MIAT analysis. Therefore, who is the more active participant among collaborative work between Student D and Student E? According to the results of the one-dimensional analysis, Student D is a more active participant than Student E. Also, the results of quantitative analysis and SNA indicated Student D’s number of interactions is more than that of Student E. Additionally, the content analysis also indicated Student D’s number of cognitive messages exceeded those of Student E. However, the results of the MIAT analysis indicated Student D’s Tscore of cognitive messages was 598.58, which is lesser than Student E’s T-score of 622.77. In addition, Student D’s T-score of meta-cognitive messages was 107.61, which was also below Student E’s T-score of 382.27. 
 Figure 7. The MIAT results. 
 The variability in the results is due to differences between one-dimensional analyses and the MIAT analysis. Onedimensional analyses simply count the number of messages or the types of messages, whereas the MIAT considers the type of interaction and the comparative interaction level. For example, there was almost no difference in the total number of messages between Student A and Student C, and between Student C and Student E. In this case, results of the SNA indicated balanced interactions among group members. However, results from the MIAT convey a different story. Student A and Student C were most likely to engage in social interactions, whereas Student C and Student E were most likely to engage in cognitive interactions. These results indicated interactions between Students C and E were more likely to contribute to collaborative work than were the interactions between Students A and C. Previous studies have found that cognitive interactions directly influence the problem–solving activity of learners (Veerman & Veldhuis-Diermanse, 2001). In sum, the MIAT considers both the type of interaction and the comparative interaction level in order to provide more specific and in-depth interpretation information on interactions among group members. However, SNA considers only the number of messages and the direction of messages. The results obtained through an analysis were differentiated by whether the approach was one-dimensional or multidimensional. Results showed the multidimensional approach can provide more in-depth information about the learning process and the structure of online interactions. This is because interactions could be understood more deeply when taking the multidimensional approach into account, compared to working only with a one-dimensional approach (Tomsic & Suthers, 2005). 
 Conclusion and implications.
 Existing quantitative or content analysis methods for analyzing interactions among group members are limited in that they have difficulty providing rich information on such interactions. This is because such methods take a onedimensional approach. For instance, Driver (2002) and Chiu and Hsiao (2010) examined the effects of group size in online collaborative learning but provided different findings. Driver (2002) found no differences in interactions among group members when comparing large and small groups, whereas Chiu and Hsiao (2010) concluded that interactions among small-group members were more effective than those among large-group members. The researchers obtained different results because they used different methods. Driver used a self-reported questionnaire to measure interactions among students, whereas Chiu and Hsiao conducted a content analysis. This example suggests a need for caution when interpreting interaction results obtained using a method based on a one-dimensional approach because the results can vary according to the method used. Methods based on a one-dimensional approach have difficulty providing sufficient information for an in-depth analysis of the interactions among group members. Thus, this research was performed based upon the premise that multidimensional analysis was pivotal for an in-depth understanding on interactions. Looking into the methods for analyzing interactions of online cooperative activities used until now, we came across a case using either one of quantitative analysis, content analysis, and relational methods, as well as a case using two analyses in tandem (e.g., Newman et al., 1996; Tomsic & Suthers, 2005). However, a multidimensional analysis considering all types of analyses simultaneously has rarely been performed. Multidimensional analysis could provide information on teaching and learning to instructors conducting online lessons since it analyzes the details of interactions and provides visual information on a learner’s structural relationships in an online cooperative study. Therefore, this study introduced the principles of developing the MIAT, a multidimensional instrument analyzing online interactions and considering quantitative analysis, content analysis, and relational analyses simultaneously. We also explained its advantages in analyzing online interactions by comparing its performance with existing analysis methods. Utilities in the MIAT, in comparison with existing methods for analyzing online interactions, could be arranged as follows. The MIAT could provide the results of quantitative analysis, content analysis, and relational analyses simultaneously since the functions of one-dimensional analytical methods were integrated for the MIAT. It also provided visual results of an analysis so researchers could see the flow and processes of interactions as well as the visual, relational structure between learners. Numerous scholars have acknowledged that visualization is an effective way to support a deep understanding of interactions (Medina & Suthers, 2009; Saltz, Roxanne, & Turoff, 2004). Also, the MIAT provided flexibility, which could modify a specific analytical framework of existing analyzing methods into various forms. For instance, some researchers might desire to use Henri’s framework for a content analytical framework for interaction, while other researchers might wish to use the framework of Gunawardena and his colleagues (1997). The MIAT uses a content analytical framework, where an appropriate framework could be entered directly in accordance with the researcher’s purpose or background of studies. Moreover, quantitative scoring criteria for assessing the values of each message could be entered by a researcher’s intention or framework of studies (Newman and his colleagues’ standard was used as a criterion for assigning quantitative scores and Henri’s framework was used for content analysis in an example analysis in this study). It is expected that the MIAT can provide meaningful information for instructors or researchers due to its characteristics of flexible analysis framework and visualization. The information provided by the MIAT would vary and be more robust than information provided by one-dimensional analysis methods of interactions. De Weber et al. (2006) indicated that coding categories for interaction are developed to analyze the process of knowledge acquisition, sharing, and formation. This means the information from interaction analysis pertains to the learning process. Thus, the MIAT would provide some useful information about the learning process by providing multidimensional information. 
 Limitations and future study.
 The purpose of this study was to provide instructors and researchers with more teaching-learning information through analyzing interactions. This was accomplished by developing an instrument that suggested principles for the multidimensional approach to online interaction analysis. However, there were a few limitations to the study. First, the MIAT confined the scope of analysis in analyzing online interactions. Relational analysis was one of the most important analytic functions for the MIAT, so the context must be definite for the utterance. In this case, it is considered as interaction where an individual wrote after another individual. Although the writing targeted unspecified individuals, it is simply considered an interaction between previous writer and next writer. So in this case, the MIAT cannot analyze the interaction completely. Second, the MIAT focused on quantitative analysis, content analysis, and relational analysis. However, the MIAT did not provide information on a change of interactions according to time. The MIAT would function as a more powerful analytic instrument if it also considered a change of learners, in accordance with time, while learning for a certain period of time. Third, the MIAT allowed for an analytic framework in which a researcher was interested in using a standard for quantitative and content analyses. The MIAT also included flexibility, as it could select the number of people for a study team according to the intentions of the researcher. Nevertheless, the researcher must enter scores assigned to messages for content analysis and evaluation during this process. Therefore, the MIAT’s capabilities could be optimized to the researcher’s purpose. However, this requires the user’s efforts and the MIAT is not completely automated. Finally, a few follow-up studies are suggested to improve meaningful use of the MIAT. This study focused on the necessity of multidimensional analysis for online interactions, as well as developing an instrument for such a purpose. However, a study that applies the developed MIAT in a research context, with a researcher’s theoretical framework, should be performed as well. Also, this study analyzed interactions on ordinary bulletin boards in order to examine the relative advantages and disadvantages offered by the MIAT over existing analyzing methods. It would be beneficial to investigate further what kind of analytic information can be provided by the MIAT in various cooperative activities online (e.g., wiki, live chat). Furthermore, this study focused on comparing the MIAT with existing analysis methods to verify whether the actual results of the MIAT analysis were appropriate and not supplemented with content analysis research. This concern must also be addressed by additional studies. The process requires future work to ensure the results of the MIAT analysis are valid. This should involve additional information, such as learner interviews, to analyze online interactions more precisely and make appropriate conclusions. 
 Acknowledgements.
 The present research was conducted by the research fund of Dankook University in 2011.]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>A Multidimensional Analysis Tool for Visualizing Online Interactions</rdfs:label>
		<dc:subject>online interaction</dc:subject>
		<dc:subject>visualization</dc:subject>
		<dc:subject>multidimensional analysis tool</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/minjeong-kim"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/minjeong-kim"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/eunchul-lee"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/eunchul-lee"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/75/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/minjeong-kim"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/eunchul-lee"/>
	</rdf:Description>
	<swrc:InProceedings rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/76">
		<swc:isPartOf rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/proceedings"/>
		<dc:title>Teaching Analytics: A Clustering and Triangulation Study of Digital Library User Data</dc:title>
		<bibo:authorList rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/76/authorlist"/>
		<swrc:abstract>Teachers and students increasingly enjoy unprecedented access to abundant web resources and digital libraries to enhance and enrich their classroom experiences. However, due to the distributed nature of such systems, conventional educational research methods, such as surveys and observations, provide only limited snapshots. In addition, educational data mining, as an emergent research approach, has seldom been used to explore teachers’ online behaviors when using digital libraries. Building upon results from a preliminary study, this article presents results from a clustering study of teachers’ usage patterns while using an educational digital library tool, called the Instructional Architect. The clustering approach employed a robust statistical model called latent class analysis. In addition, frequent itemsets mining was used to clean and extract common patterns from the clusters initially generated. The final clusters identified three groups of teachers in the IA: key brokers, insular classroom practitioners, and inactive islanders. Identified clusters were triangulated with data collected in teachers’ registration profiles. Results showed that increased teaching experience and comfort with technology were related to teachers’ effectiveness in using the IA.</swrc:abstract>
		<led:body><![CDATA[ Introduction.
 Increasingly, education and training are delivered beyond the constraints of the classroom environment, and the increasingly widespread availability of online repositories, educational digital libraries, and their associated tools are major catalysts for these changes (Borgman et al., 2008; Choudhury, Hobbs, & Lorie, 2002). Teachers, of course, are a primary intended audience of educational digital libraries. Studies have shown that teachers use digital libraries and web resources in many ways, including lesson planning, curriculum planning (Carlson & Reidy, 2004; Perrault, 2007; Sumner & CCS Team, 2010), and looking for examples, activities as well as illustrations to complement textbook materials (Barker, 2009; Sumner & CCS Team, 2010; Tanni, 2008). Less frequently mentioned ways are learning about teaching areas (Sumner & CCS Team, 2010; Tanni, 2008), networking to find out what other teachers do (Recker, 2006), and conducting research (Recker et al., 2007). These studies, however, were generally conducted in laboratory-like settings, using traditional research methods, such as interview, survey, and observation. Due to the distributed nature of the Web, traditional research methods and data sources do not support a thorough understanding of teachers’ online behaviors in large online repositories. In response, web-based educational applications are increasingly engineered to capture users’ fine-grained behaviors in real-time, and thus provide an exciting opportunity for researchers to analyze these massive datasets, and hence better understand online users (Romero & Ventura, 2007). These records of access patterns can provide an overall picture of digital library users and their usage behaviors. With the help of modern data mining techniques—the discovery and extraction of implicit knowledge from one or more large databases (Han & Kamber, 2006; Pahl & Donnellan, 2002; Romero & Ventura, 2007)—the data can further be analyzed to gain an even deeper understanding of users. Yet, despite the wealth of fine-grained usage data, data mining has seldom been applied to digital library user datasets, especially when studying teacher users. The study reported in this article used a particular digital library tool, called the Instructional Architect (IA.usu.edu), which supports teachers in authoring and sharing instructional activities using online resources (Recker, 2006). The IA was used as a test bed for investigating how the data mining process in general, and clustering methods in particular, can help identify the different and diverse teacher groups based on their online usage patterns. This study built substantially on results from a preliminary study that also used a clustering approach (Xu & Recker, in press). In particular, both studies relied on a clustering approach that used a robust statistical model, latent class analysis (LCA). In addition, this study used more refined user feature space, and frequent itemsets mining was used to clean and extract common patterns from the clusters initially generated. Lastly, as a means of validation the clustering results, we explored the relationship between teachers’ characteristics (comfort level with technology and teaching experience) and the teacher clusters that emerged from the study. This article is organized as follows. The literature review first describes the Knowledge Discovery and Data Mining (KDD) process, and several clustering studies conducted with educational datasets. This is followed by a brief introduction to the Instructional Architect tool. We then describe our data mining approach, starting from data collection and selection, through data analysis, interpretation, and inference. Finally, as part of the interpretation process, we triangulated data from teachers’ registration profiles to validate the clustering results. We conclude with the implications, contributions, and limitations of this work. This section describes the general data mining approach, and reviews several clustering studies set within educational contexts. 
 Educational data mining.
 There is increasing interest in applying data mining (DM) to the evaluation of web-based educational systems, making educational data mining (EDM) a rising and promising research field (Romero & Ventura, 2007). Data mining is the discovery and extraction of implicit knowledge from one or more large databases, data warehouses, and other massive information repositories (Han & Kamber, 2006; Pahl & Donnellan, 2002; Romero & Ventura, 2007). When the context is the Web, it is sometimes explicitly termed web mining (Cooley, Mobasher, & Srivastava, 1997). Educational data mining, as an emerging discipline, is concerned with applying data mining methods for exploring unique types of data that come from educational settings (Baker & Yacef, 2009). As web-based educational applications are able to record users’ fine-grained behaviors in real-time, a massive amount of data becomes available for researchers to analyze in order to better understand an application’s impact, usage, and users (Romero & Ventura, 2007). The knowledge discovery and data mining (KDD) process typically consists of three phases: 1) preprocessing datasets, 2) applying data mining algorithms to analyze the data, and 3) post-processing results (Cooley et al., 1997; Romero & Ventura, 2007). Data preprocessing refers to all the steps necessary to convert a raw dataset to a form that can be ingested into a data mining algorithm. It may include any of the following tasks: data cleaning, missing value imputation, data transformation, and data integration. The application of data mining algorithms usually has one of two purposes: description and prediction. Description aims at finding human-interpretable patterns to describe the data; prediction attempts to discover relationships between variables, in order to predict the unknown or future values of similar variables. Currently, there is no universal standard for post-processing and evaluating data mining results. Typical interpretation techniques draw from a number of fields such as statistics, data visualization, and usability studies. 
 Clustering studies in educational settings.
 The increasing availability of educational datasets and the evolution of data mining algorithms have made educational data mining a major interdisciplinary area, lying between the fields of education and information/computer sciences. Based on Romero and Ventura’s (2007) educational data mining survey, most commonly used data mining techniques include statistical data mining, classification, clustering, association rule mining, and sequential pattern mining. This study focused on using clustering approach to analyze teachers’ online behaviors when using a digital library tool. As such, several clustering studies using in educational datasets are reviewed. Hübscher, Puntambekar, & Nye (2007) used K-means and hierarchical clustering techniques to group students who used CoMPASS, an educational hypermedia system that helps students understand relationships between science concepts and principles. K-means is a clustering analysis method that aims to partition n data points into k clusters in which each data point belongs to the cluster with the nearest cluster center. Hierarchical clustering is a clustering analysis method that seeks to build a hierarchy of clusters. In CoMPASS, navigation data was collected in the form of navigation events, where each event consisted of a timestamp, a student name, and a science concept. After preprocessing, K-means and hierarchical clustering algorithms were used to find student clusters based on the structural similarity between navigation matrices. Durfee, Schneberger, & Amoroso (2007) analyzed the relationship between student characteristics and their adoption and use of particular computer-based training software, using factor analysis and self-organizing map (SOM) techniques. Survey responses to questions regarding user demographics, computer skills, and experience with the software were collected from over 40 undergraduate students. They used SOM to cluster and visualize the dataset. By visually analyzing the similarity and difference of the shades and borders, four resulting student clusters were identified. Finally, a t test on performance scores supported the clustering decisions. Wang, Weng, Su, & Tseng (2004) combined sequential pattern mining with a clustering algorithm to study students’ learning portfolios. The authors first defined each student’s sequence of learning activities as a learning sequences, LS = <s1s2…sn>, where si was a content block. They then applied a sequential pattern mining algorithm to find the set of maximal frequent learning patterns from learning sequences. The discovered patterns were considered as variables in a feature vector. For each learner, the value of bit i was set as 1 if the pattern i was a subsequent of the original learning sequence, 0 otherwise. After the feature vectors were extracted, a clustering algorithm called ISODATA was used to group users into four clusters. The literature review only identified one clustering study investigating teachers’ use of an educational digital library tool. In this study, a clustering approach was applied to model and discover patterns in teachers’ using an online curriculum planner (Maull, Saldivar, & Sumner, 2010). In this study, user sessions were first abstracted, and 27 features were selected for clustering experiments. The study then used K-means and expectation-maximum (EM) likelihood to cluster the user sessions. The two algorithms identified very similar patterns in the largest clusters, such as clicking on instructional support materials, embedded assessments, and answers and teaching tips. However, the authors acknowledged that their study was preliminary, in that there was not complete agreement between the different algorithms on top cluster features or cluster sizes. There are other clustering studies documented in the literature on educational web mining, however, the above examples are sufficient in revealing some major considerations in discovering user groups in the context of online environments, as follows: - A user-model must be carefully defined that accounts for the task and domain. Navigational paths, online performance, user characteristics, and a user’s prior knowledge are all good candidates for user features. - Clustering is a generic definition for a certain type of data mining method. Researchers must select the clustering algorithm appropriate for their studies; however, different approaches may produce different results. - Other data mining methods such as rule discovery, dimensionality reduction, and filling in missing values can be used with clustering algorithms to achieve a better grouping effect. - To better understand online user behaviors and produce more useful information, the data mining results should be used in conjunction with other data. - As an indispensible component of the KDD process, evaluation of the clustering results should be conducted if at all possible. 
 Teachers’ use of digital libraries.
 As noted, the research context is teachers’ use of digital libraries, an area that is seeing explosive growth in educational settings (Borgman et al., 2008). While prior work has examined the influence of teacher characteristics (such as teaching experience, information literacy skills, and usage patterns), little work has identified quantitative evidence linking these. For example, prior work has noted that teachers often lack the necessary information seeking and integration skills to effectively use online resources (Perrault, 2007; Tanni, 2008). In a nation-wide survey on teachers’ perceived value of the Internet, Barker (2009) found a positive correlation between teacher self-reports of the perceived value of the Internet in teaching, and use of hardware/electronic media. However, this work failed to find any correlation between teachers’ perceptions and years of teaching experience. To examine usage, researchers are increasingly turning to web metrics, a close kin to the EDM family. In a review of four educational digital libraries projects, Khoo et al. (2008) reviewed the use and utility of web metrics. Others have examined such metrics in conjunction with other sources of data, thereby seeking triangulation and complementarity in findings (Greene, Caracelli, & Graham, 1989). In an evaluation of a digital library service, the Curriculum Customization Service (CCS), Sumner & CCS Team (2010) reported interview data of middle and high school science teachers, and examined how their experiences were supported and clarified by usage log data. However, web metrics do not always agree with teachers’ own stories. For example, in Shreeves and Kirkham’s (2004) usability testing of a search portal, 65% of the users reported using the advanced search features; however, transaction log analyses did not support these claims. As such, these studies raise important questions. Since every research method has limitations, which should be trusted when there are discrepancies? Can data triangulation be conducted to help resolve these discrepancies? 
 Technology context: The instructional architect.
 This research is set within the context of the Instructional Architect (IA.usu.edu), a lightweight, web-based tool developed for supporting authoring of simple instructional activities using online learning resources in the National Science Digital Library (NSDL.org) and on the Web (Recker, 2006). With the IA, teachers are able to search for, select, sequence, annotate, and reuse online learning resources to create instructional web pages, called IA projects. These IA projects (or, projects, for short) can be kept private (private-view), made available to only students (student-view), or to the wider Web (public-view). Anyone can visit a public-view IA project, students can access their teachers’ student-view IA projects through their student accounts, and private IA projects are only viewable by the author. Any registered teacher can make a duplicate of any public IA project by clicking the copy button at the bottom of the project. In this way, the IA provides a service level for supporting a teacher community around creating and sharing instructional resources and activities. To date, the IA has over 7,000 registered users who have created over 16,000 IA projects. To use the IA, a teacher must first register by creating a free IA account, which provides exclusive access to his/her saved resources and projects. As part of the registration process, teachers were asked two optional profile questions: years of teaching experience and comfort level with technology. After logging in, the IA offers two major usage modes: resource management and project management. In the resource management mode, teachers can search for and store links to NSDL resources, web resources, as well as to other users’ IA projects. These links are added to teachers’ personal collections within the IA. Within the IA’s project management interface, teachers only need to enter the IA project’s title, overview, and content for the IA system to dynamically generate a webpage which can then be published. Figure 1 shows an example of a teachercreated IA project. 
 Purpose and research questions.
 As noted above, this study relied on results from a preliminary study organized around the KDD process and using latent class analysis (described below) as the clustering algorithm with the same usage data (Xu & Recker, in press). Preliminary results demonstrated LCA’s utility by clustering teachers into seven groups based on thirteen features drawn from teachers’ online behaviors. Results, however, also suggested the following improvements: 1) a more parsimonious user feature space, 2) inclusion of a clustering pruning process to make the clustering results less ambiguous, and 3) validation of clustering results by triangulating with teacher profile data. As such, the purpose of this study is to build upon results from the preliminary study to better understand teachers’ use of the IA. In particular, by implementing the suggested improvements, what usage patterns and clusters emerge when mining teacher usage data? What inferences can be made about teachers’ behaviors from the discovered usage patterns? Finally, how can user patterns be combined with more traditional user data for triangulation purposes? 
 Figure.

 1. Screenshot of a teacher-created IA project. 
 Results.
 Phase 1 -- Data preprocessing: Generating the user feature space.
 The dataset included usage data from 661 teachers who registered in the IA in 2009 and had created either publicview or student-view project(s) (57% of the 1,164 teachers who registered during that period). As outlined above, a teacher can assume three general roles in the IA environment: project authoring, project usage, and navigation. In the preliminary study, we generated an initial list of 13 indicators based on teachers’ possible behaviors in each of these three roles (Xu & Recker, in press). Clustering results from this preliminary study were used to inform how we reduced the complexity of the feature space, by fine-tuning or removing some indicators (see Table 1). Note that the number of student visits referred to the number of times a teacher’s project was viewed by his/her students. The number of peer visits referred to the number of times a teacher’s projects was viewed by other IA users. Our dataset also contained variables that were rather skewed or had outliers. The presence of outliers can lead to inflated variance and error rate, as well as distorted estimation of parameters in statistical models (Zimmerman, 1994). For example, 98% users’ projects had less than 150 maximum number of student visits; the inclusion of the 2% users with more than 150 maximum number of student visits increased the mean value by 2.5 times (from 4.29 to 10.96) and the standard deviation by almost 4.5 times (from 12.58 to 56.48). Thus, eight features in the original dataset were scaled into three levels using ordinal variables. The remaining feature, number of projects, was segmented into two levels. Generally, equal intervals were used to discretize a continuous variable, except for those features with extremely skewed distributions. Then, professional opinion influenced the segmentation process. 
 Phase 2 -- Applying data mining algorithms.
 This study also used Latent Class Analysis (LCA) (Magidson & Vermunt, 2004) to classify registered teacher users into groups. LCA is a model-based cluster analysis technique in that a statistical model (a mixture of probability distributions) is postulated for the population based on a set of sample data. LCA offers several advantages over traditional clustering approaches such as K-means: 1) for each data point, it assigns a probability to the cluster membership, instead of relying on the distances to cluster means; 2) it provides various diagnostics such as common statistics, Log-likelihood (LL), Bayesian information criterion (BIC) and p-value to determine the number of clusters and the significance of variables’ effects; 3) it accepts variables of mixed types without the need to standardize or normalize them; and 4) it allows for the inclusion of demographic and other exogenous variables either as active or inactive factors (Magidson & Vermunt, 2004). The traditional LCA (Goodman, 1974) assumes that each observation belongs to only one of the K latent classes, and that all the manifest variables are locally independent of each other. Local dependence means that all associations among the variables are solely explained by the latent classes; there are no external associations between any pair of input variables. An example of an external association is having two survey items with similar wording in the questions (Magidson & Vermunt, 2004). LCA uses the maximum likelihood method for parameter estimation. It starts with an expectation-maximization (EM) algorithm and then switches to the Newton-Raphson algorithm when it is close enough to the final solution. In this way, the advantages of both algorithms, the stability of EM and the speed of Newton-Raphson when it is close to the optimum solution, are exploited. 
 Table 1. User feature space. 
 
 The next section describes how LCA was applied to the user feature space, and how the final user clusters were selected. The user feature space (consisting of nine features in three roles) was used as the input for the LCA. Due to the unsupervised nature of clustering studies, it is hard to determine the number of clusters without any predefined guidelines. Therefore, we explored the clustering problem with different k’s, and then observed the common patterns emerging from different settings. By doing this, the clustering results as defined by common patterns were robust and not contingent on a particular setting. The data analysis consisted of four steps: (1) generating preliminary clusters, (2) deriving user patterns, (3) mining frequent user patterns, and finally (4) selecting the final user clusters. Step 1 was used to generate preliminary LCA models. Steps 2 through 4 were used to extract the common patterns, in other words, the final user clusters. Step	 1:	 Generating	 preliminary	 clusters. All LCA models were generated starting from the number of clusters k = 3 to k = 15. With all models, we monitored three criteria (R2, BVR, BIC) to ensure that the optimal model could be achieved. R2, also called the coefficient of determination, is the proportion of the total variation of scores from the grand mean that is accounted for by group membership (Aron, Aron, & Coups, 2009; Howell, 2007). In terms of the LCA, it means how much of the variance of each indicator is explained by an LCA model (Statistical Innovations, 2005). If an indicator has a very small R2 value, then it is making little contribution to current latent class analysis model, and the current model needs to be adjusted. Bivariate residual (BVR) in an LCA model is a local measure of model fit by assessing the extent to which the observed association between any pair of indicators is explained by a model (Statistical Innovation, 2005). If we encountered a BVR greater than 1 for any pair of indicators, we manually forced a correlation between them. BIC is a posterior estimation of model fit based on comparing probabilities that each of the models under consideration is the true model that generates the observed data (Kuha, 2004). A model with a lower BIC value is preferred over a model with a higher value. The BIC measure is widely used to help in LCA model selection. The best LCA models under different number of clusters k were selected using the three measures described above. We found that some resulting clusters were too small to demonstrate a reliable pattern. For instance, some clusters only had 10 users, with several of their indicators distributed across all segmentation levels. This means that after filtering out the outliers, the few users left did not demonstrate a distinctive cluster-wise pattern. In order to obtain representative user patterns, these kinds of small-sized clusters were excluded and only clusters greater than a certain threshold, α, were used. α was defined as the smaller of the two: 1) 10% of the total number of users, or 2) N / k, where N was the total number of users and k was the cluster size. In the end, 59 clusters from models of different k were above their respective thresholds. Step	 2:	 Deriving	 user	 patterns. A valid cluster was then converted to a piece of user pattern, which was a conjunction of the themes of individual features within a cluster. As noted in Table 1, each feature was segmented to two or three levels. When deriving user patterns, an individual feature’s theme for a given cluster referred to how users within this cluster distributed among the levels of this particular feature. For example, the number of projects was the only two-level indicator, and it had two themes (one project, and more than one projects). All other indicators had three levels, and thus, in theory, could produce five themes: 1) the lowest level is dominant, 2) the lowest two levels are dominant, 3) the middle level is dominant, 4) the highest two levels are dominant, and 5) the highest level is dominant. To be a dominant level (e.g., the lowest level is dominant) or dominant adjacent levels (e.g., the lowest two levels are dominant), more than 70% users must fall into such level(s). For instance, when the number of clusters k = 3, 84.6% teachers in the 2nd cluster had only a few words (the lowest level) for their project content, thus, this cluster was labeled as the “lowest level is dominant” theme for the number of project content words feature. The goal of step 2 was to deriving user patterns through the observed dominant themes. The 70% rule was reached based on several trials of experiments. Setting a higher percentage bar left fewer dominant themes for us to make inferences, while a lower percentage bar was too lenient and hardly produced distinctive traces for each cluster. Thus, we settled on 70%. It is worth noting that although we had one 2-level feature and eight 3-level features, which in theory should produce 42 features in total, only 30 dominant themes emerged from this study. If a feature under a certain setting did not display a dominant theme, it was dropped from that particular cluster. Lastly, the dominant themes for each cluster were combined together to represent a usage pattern. Again taking the 2nd cluster when k = 3 as an example, its final usage pattern was: {the number of projects = more than one AND the number of words in project overview = none or a few AND the number of words in project content = none or a few AND the number of resources in project = none or a few AND the number of student visits = a few or many AND the number of projects being copied = none}. Step 3. Mining of frequent user patterns. Frequent itemsets mining (Han & Kamber, 2006) was used to find the user patterns that most often occurred together, in particular identifying the itemsets that exist in more than a certain proportion of the entire dataset. In data mining language, this proportion threshold is called support. In this study, we set the minimum support at 10%. This means that in order to be considered as a frequent user pattern, a combination of feature themes needed to appear six times or more in the 59 usage patterns generated in Step 2. An Open Source data mining tool, Weka, was then used for frequent itemsets mining, and identified 24 1-itemsets, 110 2-itemsets, 190 3-itemsets, 182 4-itemsets, 102 5-itemsets, 31 6-itemsets, and four 7-itemsets frequent user patterns. For example {number of projects = one AND number of words in project overview = high AND number of words in project content = high AND number of project resources = high AND number of student visits = zero} is one of the discovered 5-item frequent user patterns. Step 4. Selecting final user clusters. The final user clusters were selected among the frequent itemsets. Selecting meaningful and useful patterns from the large number of frequent itemsets can be a difficult and subjective process. In this study, four principles were used to guide the selection process: 1. Mutual exclusiveness. The selected frequent itemsets should not overlap in any of its individual feature’s theme. This guaranteed that the final user clusters had no conflicting patterns and thus any user would belong to only one final cluster. 2. Balance. Balanced cluster size (N) was preferred; a cluster that was too small (N < 100) or too large (N > 200) was not selected even if it met all the other principles. 3. Comprehensiveness. Recall that the user feature space allowed for three roles: project authoring, resource usage, and navigation. Ideally, the final selected frequent itemsets should exhibit distinctive themes in all three aspects of the feature space. If it cannot be met, a frequent itemset covering more roles was preferred. 4. Maximum. Given two similar user patterns that both meet the other three principles, the pattern containing more items (pairs of features and themes) in it was preferred. If the two patterns contained the same number of items, then the one with more users in it was preferred. The four clustering and cluster pruning steps produced three user clusters, as shown in Table 2. Each user cluster represented a distinctive user pattern and the defining indicators are noted with asterisks. Those indicators are the dominant themes of each cluster. As part of the data post-processing phase, the next section provides an interpretation of and labels for the three clusters, based on their overarching characteristics. 
 Phase 3 -- Data post-processing I: Interpreting the clustering result 
 Cluster 1: Key brokers (N = 108). Teachers in this group were frequent browsers, had verbose projects, and created projects that attracted visits from other people. Of all three groups, this group scored relatively high on other measures, except for the maximum number of student projects, which was lower than cluster 2. This group did not necessarily share every single project with the public, but was careful in selecting what to share, suggesting that teachers in this group gave serious thought to their IA projects. If the IA is viewed as a learning community, teachers in Cluster 1 were the stickiest and key brokers because they appeared to be willing to observe and learn from others and also give back to the community. Cluster 2: Insular classroom practitioners (N=114). This group of teachers did not create high-quality projects, as they were characterized by few resource links, limited overview, and little content. Meanwhile they did not visit the IA or browse others IA projects as often as teachers in cluster 1, nor did they copy other teachers’ projects for their own use. In spite of the lack of enthusiasm for creating IA projects, they appeared to implement their IA projects in classroom teaching. Students viewed their projects at least once; 50% of the teachers in this group had projects viewed by the students five times or more, and in addition, 30% had projects viewed by the students 10 times or more. Given their behaviors, this group is dubbed the insular classroom practitioners. Cluster 3: Inactive islanders (N=126). This group of teachers only published one IA project each. These published projects were apparently good when judged by three project authoring measures: a medium amount of text in the overview, relatively verbose project body, and a reasonable number of resource links. In terms of navigation, this group appeared to be relatively inactive, as it was low in all three navigation measures. We speculate that the fact that users did not explore the IA as much as those in cluster 1 may have affected their knowledge of using the IA as well as their skills in creating quality IA projects. The IA was designed to allow teachers to collect and reuse web resources, and borrow curricular ideas from each other. Since this group was isolated from others and showed little navigation, it was dubbed inactive islanders. 
 Table 2. Final User Clusters. 
 Phase 3 -- Data post-processing II: Triangulating the clustering results 
 This section describes our second data post-processing efforts, in which we validated cluster interpretations with an additional triangulation study. When teachers first register for their free IA account, they are asked to optionally answer two user profile questions: years of teaching experience (0 ~ 3, 4+), and comfort level with technology (on a scale of 0 “low” to 4 “high”). For the three clusters of teachers (N=348), 116 reported their years of teaching, and 292 reported their comfort level with technology. Tables 3 and 4 show how these profile items are distributed in the three user clusters. The tables show that the key brokers cluster had a larger proportion of tech-savvy teachers than the other two groups, and the insular classroom practitioners group mostly consisted of novice teachers. To test whether this is a random effect, a chi-square test and an exact test were used as preliminary analyses to evaluate the frequency distributions of the demographic profile across the different clusters. The chi-square test is used when the sample size is large, while the exact test is used when any cell has small (< 5) or 0 counts. 
 Table 3. Teacher clusters by teaching experience. 
 Table 4. Teacher clusters by comfort level with technology. 
 An exact test showed that the probability distribution of the teaching experience was significantly different among the three groups (p < .01). A chi-square test showed that the probability distribution of the comfort level with technology was also significantly different among the three groups (	chi‐square=10.42, p < .05). Given these results, we fitted a multinomial logistic regression model to further explore how teachers’ teaching experience and technology comfort level were related to their online behaviors, with teachers’ cluster labels set as the response variable, and their profile data as the explanatory variable. In Table 5, B is the estimated coefficient relative to the reference cluster (key brokers), while Exp(B) is the exponentiation of B, or the odds ratio of being in this group relative to the reference cluster, in this case, the key brokers. Finally, the percentage is calculated from Exp(B), and indicates the change of predicted odds in percentages as compared to the reference group. Positive numbers are increases, while negative numbers are decreases. 
 Table 5. Multinomial logistic regression analysis of the impact of teaching experience and comfort level with technology on users’ online behaviors. 
 As shown in Table 5, for teaching experience, the coefficient for the insular classroom practitioners relative to key brokers is -2.08, in other words, the predicted odds of being categorized as an insular classroom practitioner rather than a key broker would decrease by 87% (p < .01). The coefficient for inactive islanders relative to key brokers is 1.02, thus the predicted odds of being in the inactive islanders cluster rather than the key brokers cluster would decrease by 64% (p < .05). In sum, an experienced teacher is expected to have a higher chance being in the key brokers cluster than in the other two types. As shown in Table 5, for comfort level with technology, for every one unit increase (from low to medium, or from medium to high), the coefficient for being an insular classroom practitioners relative to key brokers is -.45, thus the predicted odds of being in the insular classroom practitioners cluster rather than the key brokers cluster would decrease by 36% (p < .05). For every one unit increase in technology comfort level, the coefficient of being in the inactive islanders cluster relative to the key brokers cluster would be expected to -.15, and the predicted odds of being in the inactive islanders cluster rather than the key brokers cluster would decrease by14%, but this difference failed to achieve statistical significance. In sum, the multinomial logistic regression showed strong relationships between teachers’ characteristics and their online behaviors as described by user clusters. Specifically, teachers with more teaching experience were more likely to be key brokers, and those with less teaching experience were more likely to be inactive islanders. Teachers who were more comfortable with technology were more likely to be key brokers and were least likely to be insular classroom practitioners. 
 Conclusions, limitations, and future work.
 This research examined and analyzed teachers’ online behaviors in the context of a digital library tool, the Instructional Architect. First, an educational data mining approach, clustering, was applied to identify different groups of IA teacher users according to their diverse online behaviors. A user model consisting of nine features was identified and fed into a LCA model, clustering IA teacher users into three groups, labeled key brokers, insular classroom practitioners, and inactive islanders. Second, a triangulation study examined relationships between teachers’ profile data and their usage patterns. This analysis showed strong relationships between teachers’ characteristics and their online behaviors as described by user clusters. Specifically, teachers with more teaching experience were more likely to be key brokers, and those with less teaching experience were more likely to demonstrate ineffective use of the IA. Teachers who were more comfortable with technology were more likely to be key brokers and were least likely to be insular classroom practitioners. Such results show that effective usage of the Instructional Architect requires both pedagogical knowledge (gained through experience teaching) and technological knowledge. This finding helps to predict which kinds of teachers are more likely to adapt technology tools such as digital libraries, and more importantly, how to help teachers become more effective digital libraries users. Three areas are proposed for future work. First, although LCA is alleged to outperform K-means, no competing clustering algorithm has been implemented to justify this choice. Secondly, previous work showed that greater use of the IA occurs in geographical areas where teacher professional development workshops using the IA have been conducted (Khoo et al., 2008; Xu, Recker, & Hsi, 2010). This suggests that workshop participants have a higher chance of becoming sticky users. Therefore, teachers who participated in such workshop can be singled out for detailed analysis, as their distribution among clusters is predicted to be different. Finally, the third stage of KDD, evaluation and interpretation, could be conducted in a more comprehensive fashion. For example, the survey information filled out by workshop participants could be used to triangulate the clustering results, providing evidence for why and how the teachers like and dislike the IA. Despite the current challenges, the field of educational data mining is making progress towards standardizing its procedures for tackling educational problems. This research shows that teachers’ use of online resources can be studied by productively using web usage data and employing data mining approaches to investigate digital library problems in innovative ways. 
 Acknowledgements.
 This material is based upon work supported by the National Science Foundation under grant #0840745. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. We thank the teacher users of the IA.]]></led:body>
		<swrc:month>July</swrc:month>
		<swrc:year>2012</swrc:year>
		<rdfs:label>Teaching Analytics: A Clustering and Triangulation Study of Digital Library User Data</rdfs:label>
		<dc:subject>educational data mining</dc:subject>
		<dc:subject>latent class analysis</dc:subject>
		<dc:subject>teacher usage patterns</dc:subject>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<dc:creator rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
		<foaf:maker rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
	</swrc:InProceedings>
	<rdf:Description rdf:about="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/76/authorlist">
		<rdf:_1 rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<rdf:_2 rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
	</rdf:Description>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/beijing-normal-university">
		<rdfs:label>Beijing Normal University</rdfs:label>
		<foaf:name>Beijing Normal University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/lanqin-zheng"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kaicheng-yang"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ronghuai-huang"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/boise-state-university">
		<rdfs:label>Boise State University</rdfs:label>
		<foaf:name>Boise State University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jui-long-hung"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/yu-chang-hsu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/kerry-rice"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/dankook-university">
		<rdfs:label>Dankook University</rdfs:label>
		<foaf:name>Dankook University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/minjeong-kim"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/eunchul-lee"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/dominion-university">
		<rdfs:label>Dominion University</rdfs:label>
		<foaf:name>Dominion University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/m-hammed-abdous"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/wu-he"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/cherng-jyh-yen"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/greek-research-and-technology-network--grnet-">
		<rdfs:label>Greek Research and Technology Network (GRNET)</rdfs:label>
		<foaf:name>Greek Research and Technology Network (GRNET)</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-manouselis"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/nikos-palavitsinis"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/vassilios-protonotarios"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/kuleuven">
		<rdfs:label>K.U.Leuven</rdfs:label>
		<foaf:name>K.U.Leuven</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/jose-luis-santos"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sten-govaerts"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/katrien-verbert"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/erik-duval"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherland">
		<rdfs:label>Open University of the Netherland</rdfs:label>
		<foaf:name>Open University of the Netherland</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherlands">
		<rdfs:label>Open University of the Netherlands</rdfs:label>
		<foaf:name>Open University of the Netherlands</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/wolfgang-greller"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/rwth-aachen-university">
		<rdfs:label>RWTH Aachen University</rdfs:label>
		<foaf:name>RWTH Aachen University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-lea-dyckhoff"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/dennis-zielke"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mareike-bultmann"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mohamed-amine-chatti"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/ulrik-schroeder"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/the-open-university">
		<rdfs:label>The Open University</rdfs:label>
		<foaf:name>The Open University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/doug-clow"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/haiming-liu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/sharon-slade"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/fenella-galpin"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/anna-de-liddo"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/michelle-bachler"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/elpida-makriyannis"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/the-university-of-british-columbia">
		<rdfs:label>The University of British Columbia</rdfs:label>
		<foaf:name>The University of British Columbia</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/university-of-british-columbia">
		<rdfs:label>University of British Columbia</rdfs:label>
		<foaf:name>University of British Columbia</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/shane-dawson"/>
	</foaf:Organization>
	<foaf:Organization rdf:about="http://data.linkededucation.org/resource/lak/organization/utah-state-university">
		<rdfs:label>Utah State University</rdfs:label>
		<foaf:name>Utah State University</foaf:name>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/beijie-xu"/>
		<foaf:member rdf:resource="http://data.linkededucation.org/resource/lak/person/mimi-m-recker"/>
	</foaf:Organization>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ronghuai-huang">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/beijing-normal-university"/>
		<rdfs:label>Ronghuai Huang</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/China"/>
		<foaf:firstName>Ronghuai</foaf:firstName>
		<foaf:lastName>Huang</foaf:lastName>
		<foaf:mbox_sha1sum>ef2703a60f8dabad9a7bc5426a6c83cacbf0d602</foaf:mbox_sha1sum>
		<foaf:name>Ronghuai Huang</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/68"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/lanqin-zheng">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/beijing-normal-university"/>
		<rdfs:label>Lanqin Zheng</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/China"/>
		<foaf:firstName>Lanqin</foaf:firstName>
		<foaf:lastName>Zheng</foaf:lastName>
		<foaf:mbox_sha1sum>635a3712366a60bf820f6e44c72677060c1eba44</foaf:mbox_sha1sum>
		<foaf:name>Lanqin Zheng</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/68"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kaicheng-yang">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/beijing-normal-university"/>
		<rdfs:label>Kaicheng Yang</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/China"/>
		<foaf:firstName>Kaicheng</foaf:firstName>
		<foaf:lastName>Yang</foaf:lastName>
		<foaf:mbox_sha1sum>0d590cbb5a4dc98c4acd48539e869b911fc0e057</foaf:mbox_sha1sum>
		<foaf:name>Kaicheng Yang</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/68"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/yu-chang-hsu">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/boise-state-university"/>
		<rdfs:label>Yu-Chang Hsu</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Yu-Chang</foaf:firstName>
		<foaf:lastName>Hsu</foaf:lastName>
		<foaf:mbox_sha1sum>72257fc1ba13dae9d2fc1411435a0bff436869ae</foaf:mbox_sha1sum>
		<foaf:name>Yu-Chang Hsu</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/70"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/kerry-rice">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/boise-state-university"/>
		<rdfs:label>Kerry Rice</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Kerry</foaf:firstName>
		<foaf:lastName>Rice</foaf:lastName>
		<foaf:mbox_sha1sum>48de03d991fdf672cd6ddb262195c57a861adce3</foaf:mbox_sha1sum>
		<foaf:name>Kerry Rice</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/70"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/jui-long-hung">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/boise-state-university"/>
		<rdfs:label>Jui-Long Hung</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Jui-Long</foaf:firstName>
		<foaf:lastName>Hung</foaf:lastName>
		<foaf:mbox_sha1sum>f47d54fb0c504faa84e39cafd73c22e7140e419e</foaf:mbox_sha1sum>
		<foaf:name>Jui-Long Hung</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/70"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/minjeong-kim">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/dankook-university"/>
		<rdfs:label>Minjeong Kim</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/South_Korea"/>
		<foaf:firstName>Minjeong</foaf:firstName>
		<foaf:lastName>Kim</foaf:lastName>
		<foaf:mbox_sha1sum>397ba48693033028d9dc1e31e162d0facd2e5597</foaf:mbox_sha1sum>
		<foaf:name>Minjeong Kim</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/75"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/eunchul-lee">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/dankook-university"/>
		<rdfs:label>Eunchul Lee</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/South_Korea"/>
		<foaf:firstName>Eunchul</foaf:firstName>
		<foaf:lastName>Lee</foaf:lastName>
		<foaf:mbox_sha1sum>b8eb90ee21589600bac0ed34dfb0ca9824fdd50d</foaf:mbox_sha1sum>
		<foaf:name>Eunchul Lee</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/75"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/m-hammed-abdous">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/dominion-university"/>
		<rdfs:label>M'hammed Abdous</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>M'hammed</foaf:firstName>
		<foaf:lastName>Abdous</foaf:lastName>
		<foaf:mbox_sha1sum>672980eeb0d9ebe9e06670e62da46d30432e5990</foaf:mbox_sha1sum>
		<foaf:name>M'hammed Abdous</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/71"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/wu-he">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/dominion-university"/>
		<rdfs:label>Wu He</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Wu</foaf:firstName>
		<foaf:lastName>He</foaf:lastName>
		<foaf:mbox_sha1sum>8c5262e50cbe5798cf11e9cc023b696f616d5a3f</foaf:mbox_sha1sum>
		<foaf:name>Wu He</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/71"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/cherng-jyh-yen">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/dominion-university"/>
		<rdfs:label>Cherng-Jyh Yen</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Cherng-Jyh</foaf:firstName>
		<foaf:lastName>Yen</foaf:lastName>
		<foaf:mbox_sha1sum>8bfbf88e323f3864384322d8b6077233cde1a3e6</foaf:mbox_sha1sum>
		<foaf:name>Cherng-Jyh Yen</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/71"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/nikos-manouselis">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/greek-research-and-technology-network--grnet-"/>
		<rdfs:label>Nikos Manouselis</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Greece"/>
		<foaf:firstName>Nikos</foaf:firstName>
		<foaf:lastName>Manouselis</foaf:lastName>
		<foaf:mbox_sha1sum>e4749e14bcd5f2219506274ea7c7f7ce76c51dd6</foaf:mbox_sha1sum>
		<foaf:name>Nikos Manouselis</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/72"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/katrien-verbert">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/kuleuven"/>
		<rdfs:label>Katrien Verbert</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Belgium"/>
		<foaf:firstName>Katrien</foaf:firstName>
		<foaf:lastName>Verbert</foaf:lastName>
		<foaf:mbox_sha1sum>610730a9705def3b48bc4b17449cc5ac317d7bf8</foaf:mbox_sha1sum>
		<foaf:name>Katrien Verbert</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/72"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/erik-duval">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/kuleuven"/>
		<rdfs:label>Erik Duval</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Belgium"/>
		<foaf:firstName>Erik</foaf:firstName>
		<foaf:lastName>Duval</foaf:lastName>
		<foaf:mbox_sha1sum>33b04e59663928cd5da8044a9475463e09adfa3c</foaf:mbox_sha1sum>
		<foaf:name>Erik Duval</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/72"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherland"/>
		<rdfs:label>Hendrik Drachsler</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Hendrik</foaf:firstName>
		<foaf:lastName>Drachsler</foaf:lastName>
		<foaf:mbox_sha1sum>a0f12a89fc8bf9cfbd44b49a9e1bbcec40106def</foaf:mbox_sha1sum>
		<foaf:name>Hendrik Drachsler</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/67"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/72"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/hendrik-drachsler">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherland"/>
		<rdfs:label>Hendrik Drachsler</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Hendrik</foaf:firstName>
		<foaf:lastName>Drachsler</foaf:lastName>
		<foaf:mbox_sha1sum>a0f12a89fc8bf9cfbd44b49a9e1bbcec40106def</foaf:mbox_sha1sum>
		<foaf:name>Hendrik Drachsler</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/67"/>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/72"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/wolfgang-greller">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/open-university-of-the-netherlands"/>
		<rdfs:label>Wolfgang Greller</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Netherlands"/>
		<foaf:firstName>Wolfgang</foaf:firstName>
		<foaf:lastName>Greller</foaf:lastName>
		<foaf:mbox_sha1sum>b76d0dbeb6f156274d5086116ff55a44083d86ef</foaf:mbox_sha1sum>
		<foaf:name>Wolfgang Greller</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/67"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/dennis-zielke">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/rwth-aachen-university"/>
		<rdfs:label>Dennis Zielke</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Dennis</foaf:firstName>
		<foaf:lastName>Zielke</foaf:lastName>
		<foaf:mbox_sha1sum>d21ae341790d6498890ebef784208848a2fd87ac</foaf:mbox_sha1sum>
		<foaf:name>Dennis Zielke</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/73"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mareike-bultmann">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/rwth-aachen-university"/>
		<rdfs:label>Mareike Bultmann</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Mareike</foaf:firstName>
		<foaf:lastName>Bultmann</foaf:lastName>
		<foaf:mbox_sha1sum>b6c624aef3797e957fa1e1040640af36d9772564</foaf:mbox_sha1sum>
		<foaf:name>Mareike Bultmann</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/73"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mohamed-amine-chatti">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/rwth-aachen-university"/>
		<rdfs:label>Mohamed Amine Chatti</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Mohamed</foaf:firstName>
		<foaf:lastName>Amine Chatti</foaf:lastName>
		<foaf:mbox_sha1sum>b46a4480b006262e0d0670dab372092f5730e6cf</foaf:mbox_sha1sum>
		<foaf:name>Mohamed Amine Chatti</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/73"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/ulrik-schroeder">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/rwth-aachen-university"/>
		<rdfs:label>Ulrik Schroeder</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Ulrik</foaf:firstName>
		<foaf:lastName>Schroeder</foaf:lastName>
		<foaf:mbox_sha1sum>edb9e1d4a321aef586fc7203a74b67e5a10ba3da</foaf:mbox_sha1sum>
		<foaf:name>Ulrik Schroeder</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/73"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/anna-lea-dyckhoff">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/rwth-aachen-university"/>
		<rdfs:label>Anna Lea Dyckhoff</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Germany"/>
		<foaf:firstName>Anna</foaf:firstName>
		<foaf:lastName>Lea Dyckhoff</foaf:lastName>
		<foaf:mbox_sha1sum>dcd74aaafc09bc2a6ea1687d26fd68c41d31302a</foaf:mbox_sha1sum>
		<foaf:name>Anna Lea Dyckhoff</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/73"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/simon-buckingham-shum">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Simon Buckingham Shum</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Simon</foaf:firstName>
		<foaf:lastName>Buckingham Shum</foaf:lastName>
		<foaf:mbox_sha1sum>bbaf2c40d77ba4a21d7c6f19c0b973ed3624cdc5</foaf:mbox_sha1sum>
		<foaf:name>Simon Buckingham Shum</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/74"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/rebecca-ferguson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-open-university"/>
		<rdfs:label>Rebecca Ferguson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/UK"/>
		<foaf:firstName>Rebecca</foaf:firstName>
		<foaf:lastName>Ferguson</foaf:lastName>
		<foaf:mbox_sha1sum>9a2a32c74e31c9e3a338d550c2619bae7273ec6d</foaf:mbox_sha1sum>
		<foaf:name>Rebecca Ferguson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/74"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/leah-p-macfadyen">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/the-university-of-british-columbia"/>
		<rdfs:label>Leah P. Macfadyen</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Leah</foaf:firstName>
		<foaf:lastName>P. Macfadyen</foaf:lastName>
		<foaf:mbox_sha1sum>63662c7bcb45a03680406a2b156674c63dc52d20</foaf:mbox_sha1sum>
		<foaf:name>Leah P. Macfadyen</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/69"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/shane-dawson">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/university-of-british-columbia"/>
		<rdfs:label>Shane Dawson</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/Canada"/>
		<foaf:firstName>Shane</foaf:firstName>
		<foaf:lastName>Dawson</foaf:lastName>
		<foaf:mbox_sha1sum>ba08b6fe9285cb6790847f0b206fcb04bf4620f3</foaf:mbox_sha1sum>
		<foaf:name>Shane Dawson</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/69"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/mimi-m-recker">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utah-state-university"/>
		<rdfs:label>Mimi M. Recker</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Mimi</foaf:firstName>
		<foaf:lastName>M. Recker</foaf:lastName>
		<foaf:mbox_sha1sum>11bf17dad9deec0b8157d8bbbcd850140ba8a9e4</foaf:mbox_sha1sum>
		<foaf:name>Mimi M. Recker</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/76"/>
	</foaf:Person>
	<foaf:Person rdf:about="http://data.linkededucation.org/resource/lak/person/beijie-xu">
		<swrc:affiliation rdf:resource="http://data.linkededucation.org/resource/lak/organization/utah-state-university"/>
		<rdfs:label>Beijie Xu</rdfs:label>
		<foaf:based_near rdf:resource="http://dbpedia.org/resource/USA"/>
		<foaf:firstName>Beijie</foaf:firstName>
		<foaf:lastName>Xu</foaf:lastName>
		<foaf:mbox_sha1sum>95764d5b6e21f4f1c2e2aac06b865a0a56a56984</foaf:mbox_sha1sum>
		<foaf:name>Beijie Xu</foaf:name>
		<foaf:made rdf:resource="http://data.linkededucation.org/resource/lak/specialissue/jets12/paper/76"/>
	</foaf:Person>
</rdf:RDF>