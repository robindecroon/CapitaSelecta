1. INTRODUCTION.
 Today, it is not uncommon practice for faculty to deploy an online instructional environment in order to promote student conversations over course materials and ideally to heighten comprehension and retention of course readings. Co-blogging is an example of a technology-mediated learning activity of this sort. The “blogging” part requires students to explain course material in their own words. The “co-” part makes the students exchange ideas and interact regarding the course material by discussing the alternate viewpoints that emerge within the blogosphere [26]. One of the trends in the American university system over a number of years has been the migration towards larger so-called “gateway” courses [28]. These introductory courses are typically a student’s ﬁrst exposure to collegiate work. However, these large classes have a negative impact on the student’s learning process. Large lectures are useful for certain things, but less useful for fostering higher order thinking [31, 10]. Technology like co-blogging, however, can be used to make the class seem smaller, and provide the foundation for students to learn more and better. For most if not all learning activities, a substantial amount of an instructor’s time and eﬀort is devoted to evaluating and monitoring the quality of students’ work, and thus, hopefully, the depth of their learning [11]. The purpose of this monitoring, however, is not merely the determination of grades; part of the instructor’s work is entirely self-reﬂective, enabling the instructor to concurrently, or ideally even preemptively, intervene to make adjustments to course pedagogy based on students’ engagement or understanding [29]. While assigning grades might be facile, some diﬃculties complicate this second objective: how might an instructor intuit when, precisely, students have understood the material sufﬁciently? Making this determination manually, speciﬁcally in larger gateway courses, would prove an intensely laborious and time-consuming process, far more complicated than simple reading and re-reading of any single student’s work. When supporting learning using technology, however, a positive by-product is that students produce their work in an electronic form, which enables the creation of computerassisted instructional aids [22]. This paper describes an automated solution that can be used by educators to help resolve these tensions. Through the application of lexical analysis to student writing, we have implemented an analysis tool that allows an instructor to track how a student’s written language migrates from mere paraphrase to mastery, isolating the moment when the student’s understanding of core concepts best demonstrates an ability to place that concept in his or her own words, a moment that we’ve chosen to call the “point of originality.” This process recreates the same cognitive activity that educators might ordinarily undergo, yet in an automatic manner that is less labor-intensive. Ultimately, the resulting data is presented to the instructor by way of a custom visualization, which allows for continuous self-monitoring with minimally expended effort. In order to demonstrate the utility and validity of the analysis method, this paper explores the potential of the point of originality to preemptively predict likely student success or failure. The remainder of the paper is organized as follows. Section II explores the beneﬁts of co-blogging in education. Section III and IV provide background on the particular problems of, and potential solutions for, evaluation in larger gateway courses. Section V and VI introduce the theory behind, and the speciﬁcs of, the point of originality analysis method. Section VII and VIII report on a case study where the point of originality method was used to evaluate student co-blogging data. The paper ends by discussing future work. 
 2. CO-BLOGGING AND EDUCATION.
 Co-blogging is an example of a social computing activity that can be very conducive to learning [13, 26]. Overall, blogging provides a platform that promotes individual expression, enables students to establish their own “voice” and yields a richer conversational interactivity within a community [48, 47]. Each student has a blog, composed of multiple blog posts. Students can read each other’s blog posts and comment on them. Because blogs are easy to use, they can promote students’ digital ﬂuency [23] and encourage students to explore and publish their own nascent ideas under less pressure than in the rough-and-tumble of in-class discussions [3]. Writing a blog forces students to become analytic and critical as they contemplate how their ideas may be perceived by others [47]. Being able to review older contributions affords reﬂection and enables students to revisit and revise their artifacts, further developing their own viewpoints in the context of each other’s writing as they sense how others understand the material similarly or diﬀerently [37]. Conversations emerge when students read, and then comment on, each other’s blog posts, thus enabling them to exchange, explore, and present alternate viewpoints on the course material [17]. This type of social explanatory discussion can beneﬁt learning [12, 9]. Alternative spaces such as asynchronous discussion forums are another example of a technology that is sometimes used to mediate online discussions between students. However, as predominantly shared community spaces, forums give students voices that are heard but are without a distinct, individual identity [14]. Critical thinking may emerge for individuals, but the organization does not promote the coherent and interactive dialogue necessary for conversational modes of learning in the same way that co-blogging does [44]. A blogosphere can function as a repository of information, opinions, monologues and dialogues about course content, where students participate, and leverage each other’s contributions in other educational activities (e.g., when writing term papers) [2, 1]. Blogging enables students to gather their thoughts and come better prepared for class [24, 12] and can be predictive of student performance in a course [13]. Even non-active student bloggers can beneﬁt from the blog’s educational value as it exposes them to diﬀerent views of the material without necessarily participating directly [47]. Overall, having students discuss and/or “argue” about course readings has signiﬁcant educational utility [5, 4, 38]. Some discussion might take place during class, however, class time is a limited resource. This is particularly true for larger classes or so-called “gateway courses” that typically enroll large numbers of undergraduates where there is simply not time for everyone to speak up. Using co-blogging, students can both express individual voices and continue conversing with their peers outside the conﬁnes of the physical classroom. Unfortunately, the sheer size of these courses presents several challenges. 
 3. PROBLEMS WITH LARGER GATEWAY COURSES.
 The ability to monitor and respond to student progress is ever more imperative given the realities of the modern classroom. As noted even a decade ago, the political economies of American universities increasingly mandate large class sizes, particularly in the introductory or “gateway” courses that are typically a student’s ﬁrst exposure to collegiate work [28]. These large classes have a negative impact on students and instructors alike. There is, for example, an abidingly inverse correlation between class size and student achievement [20, 42]. The large lecture, while useful for reinforcing rote facts, is less successful in fostering higher-order thinking [31, 10], or in encouraging students to construct their own understanding of core concepts [28]. Such a sizable student population further constrains instructors’ abilities to familiarize themselves with students’ individual learning styles [28], thereby forcing instructors to assume that their audience consists of uniform types of learners [10]. Although the extent of feedback that students receive is one of the most powerful predictors of student achievement [46, 40], instructor feedback in large lecture courses is often slow and sporadic; students typically need to wait weeks - from, for example, one midterm assessment to the next - to put their course-related skills into practice, and even longer than that to have their assignments evaluated by an instructor [10]. Pedagogical adjustments, in other words, become both more unwieldy and more unlikely in the precise environment where they would be most necessary. Given the problems inherent to large lectures classes, but given also their entrenched status within the American university system, it would thus logically be prudent to ﬁnd a way to minimize their most pernicious consequences. Any broader attempt to remedy the problems of larger gateway courses should thus aspire to ﬁrst, foster higher-order thinking; second, to suit multiple types of learning styles; and third, to provide students with feedback as rapidly as possible. These ﬁrst two objectives are inherent virtues of the co-blogging process (e.g. [24, 12, 37, 9, 5, 4, 38]); the ﬁnal objective is the focus of this paper. 
 4. PRIOR EFFORTS.
 Several attempts to minimize the unintended consequences of large gateway courses exist. Almost all efforts call for resizing the large class group, either by literally subdividing the class or else by designing activities to make the large class “seem” small. This latter method, it might be argued, is the one already pursued by student participation in a coblogging environment, where conversations take place in an ad hoc and freeform manner. Known interventions can be roughly classiﬁed into two major groups: those interventions that are speciﬁcally meant for in-class use, and those interventions that are intended to take place between classes. Those activities that take place during class typically interrupt the lecture itself [32, 35, 8], asking students, for instance, to respond to a series of prompts which they answer through remote devices. These same activities, however beneﬁcial, generally disrupt the actual process of knowledge transmission and tend to reward rote memorization rather than higher-order thinking; what feedback students receive reﬂects only whether or not they got a prompt right or wrong, and not how well or how comprehensively they understood the material. Since the activities take place in the classroom, and in front of the entire student population, the activities themselves moreover treat all students in exactly the same manner regardless of learning style. Interventions intended for use between classes are roundly invested in providing instructors with observable statistical modeling in near real-time [39, 8, 19], which can then be referred to before the next session. These activities typically attempt to breed higher-order thinking by forcing students to reﬂect on their own learning, asking, for instance, that students rate their level of conﬁdence before responding to prompts [8], or that they engage in a collaborative peer review of one another’s written work [39, 19, 21]. The beneﬁts of this type of activity are directly analogous to the beneﬁts of co-blogging. 
 5. ORIGINALITY IN STUDENT WRITING.
 When students engage in a writing activity, the ﬁnal evaluation of their work cannot only assess whether or not the student has provided the most closely correct answer. Process is just as relevant to student writing as content [43]. Student writing that exhibits exceptional higher-order thinking is generally seen as that which demonstrates a mastery of the course material in new, profound or statistically unusual ways [33]. The ideal is not only for students to conﬁrm that they’ve understood lectures, but to do so in ways that even the educator might not have thought of. This process of mastery need not take place all at once. As a student is continually exposed to the same material, or is given the independent opportunity to rethink, reframe, or revisit that material [45], their writing on the subject has the chance to evolve, from rote regurgitation to wholly original expression [34]. At the level of language, this evolution is reﬂected through recasting. Recasting is the learning process whereby a student reﬁnes his or her understanding of a concept found in course lectures or readings by putting that concept into his or her own words [41]. In the acquisition of new languages especially, this process can be useful, because it allows students to acquire new vocabulary using the assortment of words already available to them [41, 30]. Even where the student’s understanding of a language is not an explicit concern, recasting can mark a student’s attempts to graduate to more sophisticated or professionalized terminology, or, inversely but to the same end, to place new concepts into terms that are nearer to what the student would naturally be more likely to say [15]. “Originality,” fully deﬁned, can of course take numerous forms. The concept of recasting, however, spans a number of theoretical orientations, with an inﬂuence on theories of schema formulation [25], the sensemaking process known as “scaﬀolding” [18], as well as the express principles of educational constructivism [27]. “Originality,” as deployed here, does not therefore strictly refer to a student’s creativity or capacity for non-conformity as might be suggested by more colloquial uses of the term. Rather, what the Point of Originality tool seeks to gauge is students’ ability to interpret, to place core concepts into new and diﬀuse usages. This deﬁnition of originality straddles the tiers of learning that Bloom’s taxonomy [6] associates with “understanding” and “application.” By interpreting core concepts and extrapolating to diﬀerent terms, students demonstrate their understanding of the material, and when putting those concepts into play through iterative exercises like co-blogging, they begin to apply that knowledge is newer and more diversedomains. For an instructor, the simple identiﬁcation of recast terminology within a student’s written work can provide an eﬀective barometer for pedagogical self-reﬂection. If a subset of terms or concepts are deemed vital to the syllabus, repetitions and recast iterations of those same terms will at least suggest that those terms are being acknowledged and reﬂected upon. Although the presence of recast terminology is not the only metric representative of a student’s mastery, the central role that recasting plays in a host of pedagogies (e.g. [25, 18, 27]) suggests that writing demonstrating high or low levels of recasting will reﬂect other aspects of performance within the course. Yet if the instructor hopes not only to identify instances where key concepts are deployed, but to determine how comprehensively the concepts are being internalized, it is ﬁrst necessary to possess a method of scoring how original any given recast might be. In order to do this, we have developed a metric for isolating a speciﬁc point of originality within student writing. 
 6. EVALUATION OF CO-BLOGGING.
 The process of computer-assisted evaluation of student writing is primarily composed of two parts: the analysis method, and a custom-made visualization depicting each student’s “originality” at any given time throughout the duration of the semester. 
 6.1 Analysis Method: Theoretical Background.
 WordNet is a lexical database that arranges nouns, verbs, adjectives, and adverbs by their conceptual-semantic and lexical relationships [16]. Whereas a simple thesaurus would be able to identify any two words as synonyms or antonyms of one another, WordNet is able to note the similarity between two words that don’t have literally identical meanings. These relationships are ideally meant to mirror the same lexical associations made by human cognition. WordNet’s arrangement is hierarchical, which is to say that certain terms are more closely related than others. Within WordNet, these relationships are displayed as “synsets,” clusters of terms that fork, like neurons or tree branches, from more speciﬁc to more and more diﬀuse associations (see Figure 1). If two words are found within one another’s synset tree, it stands to reason that these terms are, in some way, related, be it closely or distantly. As discussed in the next sub-section, these distances between two terms can be calculated, and assigned a value commensurate with their degree of semantic relatedness [7]. 
 Figure 1: Model synset tree (by hyponym relation). 
 The hierarchical arrangement inherent to WordNet provides one method of determining the relationship between two terms. If the synset tree of one term encompasses another term, it is simple enough to note how many synset jumps it takes to move from one to another. In Figure 1, a “Dalmatian” is a type of “dog,” which itself belongs to the subcategory of “domestic animals;” thus there are two tiers of associations between the concepts of “Dalmation” and “domestic animals.” Unfortunately, however, just how closely any two terms might be related is not a purely linear relationship. WordNet organizes related terms by their precise lexical entailment, such that nouns might be categorized as synonyms, hypernyms, hyponyms, holonyms and meronyms, as seen in Table 1. These possible entailments provide a rudimentary roadmap for all the ways in which two words might be related. Since WordNet attempts to map the cognitive associations automatically formed between words [16], a student’s evocation [36] of the holonym or hypernym of a given noun instead of the noun itself is more likely to form an associative recast of the original term. To put these abstract concepts into the same terms used to describe the original problem, if an instructor in a large lecture course in animal biology wanted to monitor how students had grappled with the (admittedly basic but obviously fundamental) concept “animal,” he or she would want to know not only that the students had deployed the literal term “animal,” but were also conversant in the other associated concepts found (in immensely abbreviated form) in Figure 1. This association is consistent with the pedagogical principle of recasting, and with the concept of “original” expression as deﬁned here. Where an instructor wants students to know one thing - in this case, about “animals” that those students can deploy more diﬀuse and more concrete examples of animals would be the readiest evidence that they actually understand. Yet while this simple index displays just how any two terms might be related, all the possible relationships noted are not necessarily equal. Some relationships, like that between synonyms smile and grin, are obviously bound to be more strongly associated than that between mammal and dog. Following a method ﬁrst noted by Yang & Powers [49], it is possible to install a series of weights that can best calculate the semantic distance between any two terms. This method in particular is useful because of all known methods, it bears the highest correspondence between its own distance calculations and the intuitions of actual human respondents (at 92.1 percent accuracy). 
 Table 1: Possible lexical entailments for nouns in WordNet. 
 6.2 Analysis Method: Implemention.
 Determining the point of originality of a student’s blog post depends upon the manual input of a speciﬁc query term by the instructor. The term relates to a key course topic and manual input of the topic reinforces the pedagogical utility of the process. For the query term, the process generates a WordNet synset tree. Words within the tree are then compared to the body of words extracted from a student’s blog post. Where matches are found, a summation of distance calculations between the original query term and the matches is performed as follows: Let q be a query term supplied by the instructor. Then, let W = {w0 , w1 , ..., wn } be a set containing all synset word matches (w ) from the WordNet database for q. Let B = {b0 , b1 , ..., bn } be a set of all words composing a blog post by a particular student and let S = {s0 , s1 , ..., sn } be a set of stopwords, a list of common words in English usage (like “the” or “and”), to be omitted to speed up processing time. Then, M = {m0 , m1 , ..., mn }, the set of synset term matches found in a blog post for query term q can be deﬁned as: 
 (FORMULA_1).
 WordNet stores synset matches in a tree structure with q as the root node. Then, δ, the distance (depth) for any given synset match (m ∈ M ) from the root node (query term q) is deﬁned as: 
 (FORMULA_2).
 WordNet also supplies the lexical entailment of each synset term. Thus, t, the “word type” of any given synset term match m ∈ M , is deﬁned as: 
 (FORMULA_3).
 Then α, the weight of any given synset term match is calculated as: 
 (FORMULA_4).
 The depth for any given synset term is multiplied by a constant value of 0.7, which reﬂects the diminished associations between two terms the farther separated they are along the synset tree. This value is selected because it corresponds with the calculation of distance between terms that yields the nearest match with human intuition [49]. Then, C, the cumulative originality score for a given query term q in a student’s blog post, can be deﬁned as: 
 (FORMULA_5).
 The point of originality for a particular course topic is in many cases deﬁned by the presence of several related query terms, or in other words, the synset matches for those terms. By deﬁning Q = {q0 , q1 , ..., qn } as the set of query terms supplied by the instructor at any one time, then P, the overall point of originality of a given student’s blog post for a particular course topic (deﬁned by Q), is: 
 (FORMULA_6).
 Finally, repeating the point of originality calculation (Equation 6) for each blog post written by a particular student, and plotting all instances of originality on a horizontal timeline, allows for an optimal instruction comprehension whereas the instructor can see recasts of a particular course topic (deﬁned by Q) across the entire body of a student’s writing throughout a single course. Although this paper focuses on the analysis of blog posts as students’ writing examples, given some additional programming work, any electronic form of student writing could be made compatible with the tool for subsequent analysis. 
 6.3 Visualization for the Point of Originality.
 The timeline visualization, as seen in Figure 2, displays a horizontal timeline that represents the time interval for the writing activity of any student for the duration of a particular semester. The numbered components of Figure 2 correspond to the following features. 1. This drop-down menu allows the instructor to select which student’s writing samples are currently being displayed. 2. This is where query terms (Q) are input by the instructor. 3. This timeline displays the date/times of each of the students’ writing samples. Each marker is color-coded, from colder to warmer colors along the ROYGBIV spectrum, the higher the value of the point of originality (P ) score for any given writing sample. These color assignments present an intuitive way for the instructor to quickly recognize that the sample has been assigned a higher originality value. 4. If a writing sample marker is selected in the timeline window (see inset 3), the text of that writing sample is displayed here. This assortment of visualization options allows the point of originality calculation to be displayed in a number of intuitive ways: both within chronology (inset 3) and in context (inset 5). 
 7. CASE STUDY.
 This section reports on a case study that explores the capability of using the Point of Originality tool to assess the originality of student writing in a semester-long co-blogging activity. More speciﬁcally, the study focuses on correlating originality scores assigned to students’ blog posts to their activities in the blogosphere during the semester and the ﬁnal grades assigned to a term paper covering the same topics. Although primarily aimed at testing the validity of the point of originality method, this study models a likely use case. By demonstrating how low point of originality values correspond to poor performance in other aspects of the course, the Point of Originality tool could provide instructors with an early, near-instantaneous diagnostic of which students might require additional help. The tool might thus ideally streamline the process of conducting targeted pedagogical adjustments or interventions. The co-blogging data was collected from a course taught in the Fall of 2008 in the Computer Science Department at Brandeis University. The course is an introductory course, an elective, focused on exposing students to topics such as the social life of information, virtual communities, privacy, intellectual property and peer-to-peer computing. In the co-blogging activity, each student has a blog where he or she writes opinions on the course readings. Students can read each other’s posts and comment on the posts of their peers. The blogosphere, provides several features focused on increasing students’ awareness of recent activity, and enabling them to ﬁnd interesting blog posts to read and conversations in which to participate. 
 7.1 Participants.
 There were 8 female and 17 male students, all undergraduates, enrolled in the class. There were 3 science majors and 1 science minor in the class. There were 12 students majoring in the social sciences and 8 minoring in the social sciences. The remainder of the class was either in the humanities or ﬁne arts. Three students were omitted from the data set because they did not begin blogging until the end of the semester following a warning from the instructor. As an introductory course, open to non-majors, the technical requirements for enrollment were few. No formal evaluations were done to assess the students’ computer literacy or prior domain knowledge. In class discussions, most of the students expressed moderate or advanced technical skills. The instructor and teaching assistant did not design, or implement, the co-blogging activity in such a way that it pre-assigned students into particular authoring roles in the online blogosphere, thus potentially inﬂuencing the students choice of writing topics or styles. 
 Figure 2: The Point of Originality timeline visualization. 
 7.2 Procedure.
 At the beginning of the semester, an in-class tour and exercise introduced the students to the important features of the co-blogging environment. The students were required to blog at the pace of one post per lecture: there were two lectures per week. A typical post was 1 or 2 paragraphs in length. The students were also required to read and comment on other contributions to the blogosphere. The coblogging work of each student counted for 35% of the ﬁnal grade. During the semester, the students read four books and wrote a paper on one of these books. The focus of the analysis presented in this paper is on the co-blogging work that the students did during the time the class read the book for which they wrote their papers. 
 7.3 Metrics.
 Lectures were presented using slides that summarized the key points of the presentation. At the beginning of each lecture, hard copies of the slides were handed out to support student note taking. We used the lecture slides as a basis for identifying the inputs to the blogosphere. For each set of slides, a set of key topics that were covered by the lecture ultimately became the query terms used for analysis. 
 7.4 Method.
 All of the students’ online work was automatically recorded in a transcript and analyzed using the Point Originality tool. Originality scores were generated for all blog posts and papers, which were then correlated to students’ ﬁnal paper grades and to statistical data summarizing their reading and writing activities during the co-blogging part of the semester. 
 8. RESULTS.
 The analysis was composed of two principle parts. The ﬁrst part compared the degree to which the tool indicated the originality of the students’ blog posts and how well the originality scores related to the grades that the instructor assigned their papers. In the ideal situation, given that the instructor graded their papers based on how well the students expressed higher order understanding of the course material, or in other words their writing reﬂected original thought, the tool should provide scores where higher originality values would correspond to higher paper grades. The second part sought to explore to what degree the students’ interactivity in the blogosphere inﬂuenced their understanding of the course readings, and in what way their immersion in the co-blogging community positively or negatively impacted their levels of originality when writing papers. Ideally, students would ﬁnd suﬃcient impetus to become deeply involved in the co-blogging learning community and their exposure to alternate or similar viewpoints of the same materials would help them develop their own viewpoints or to strengthen existing ones, thus leading to more original thought and better papers. Since the analysis was primarily concerned with ensuring that the tool could be used during a course to preemptively diagnose likely student success, the blog post dataset was ﬁltered to only include blog posts written in what was deﬁned as the “lead-in” period of co-blogging. During this period, the students were writing blog posts and comments on the topics that they eventually wrote their papers on, but at the time were unaware which speciﬁc topics they would have to address in those papers. The paper grades were assigned during the fall of 2008, roughly two years prior to the study described in this paper. Furthermore, grading was done by the course instructor, who is not a participant in the Point of Originality project. 
 8.1 Originality in the lead-in period.
 We began by collecting the originality scores calculated by our system for the blog posts written by each student on the paper topics and the actual grades that each student received for his or her paper. The average grade for student papers was 80.00 with a standard deviation of 16.83. The highest grade assigned was 95 and lowest was 40 on a scale from 0 to 100. The students’ blog posts received on average an originality score of 10.61 with a standard deviation of 4.29. The highest originality score assigned by our system was 18.30 whereas the lowest score was 3.92. Soon, a pattern emerged indicating that the more original the students’ co-blogging work, the higher the paper grades assigned by the instructor. While this is to be expected, the importance here is that the Point of Originality tool is automatically producing results that potentially correlate to standard approaches to pedagogy. A chi-square distribution test conﬁrmed that there was indeed a positive correlation between the two factors. As students’ blog post originality scores increased, their ﬁnal paper grades covering the same topics increased as well. In other words, as their blogging activity became more original, the students wrote better papers: 
 (FORMULA_7).
 To further conﬁrm the potential relationship between originality while initially learning the course materials (during the lead-in period) and how well that work transformed into mastery of course content as reﬂected by paper writing, students were divided into two groups based on their paper grades. Students whose paper received a grade above the average (80.00) were assigned to one group, the upper group, whereas students who scored below the average were assigned to the lower group. 
 Table 2: Originality variance and paper grades for two different groups of students. 
 As shown in Table 2, the students in the upper group received an average grade of 90.63 on their papers whereas the students in the lower group received an average grade of 66.79. What is more interesting, however, is what can be deﬁned as the originality variance: the diﬀerence between how original the students’ blog posts were compared to their ﬁnal papers. While the lower student group had an originality variance of 21.49, the variance for the students in the upper group was -6.10. Because the variance for the upper group is negative, those students’ blog posts, written during the lead-in period, were on average more original than their ﬁnal papers. It might seem then that those students were not necessarily more original than the students in the lower group, however, that is not the case. The fact that the variance is negative for the upper group is indicative of the fact that those students were at the height of their understanding of the materials even during the lead-in period. These students had mastered the materials in such a way that they had an easier time of writing their papers, whereas the students in the lower group were only ﬁrst beginning to wrestle with this content after the papers were assigned. This is suggested by the fact that the originality variance for the lower group was a positive value of 21.49, a value more than twice as great as the students’ average originality score during the entire period. A t-test of independent samples conﬁrmed that the originality variance between the upper and lower groups was indeed statistically signiﬁcant. Students who had received higher grades for papers wrote blog posts that were more original in the lead-in period: 
 (FORMULA_8).
 Similarly, a t-test also conﬁrmed that the students’ distribution of grades was by itself signiﬁcant: 
 (FORMULA_9).
 The key observation is whether or not students’ retention of course materials was equal for both groups. Students that master materials when taking exams don’t necessarily have the ability of applying that knowledge after the course ends because their “grasping” of the content was short lived. These students knew the material well enough to pass the exam but not necessarily well enough to be able to easily apply that knowledge later on. If students can get “into the game” earlier in the semester, they have greater opportunities to participate in discussions, reﬁne their understanding and “lock it down deep” so that they leave the course with a higher degree of mastery. In a large reading- and writing-intensive course, where a bulk of the work towards mastery might take place in machine-readable form, it goes without saying that it would be advantageous for the instructor to be able to use technology to monitor each student’s progress. Speciﬁcally in larger gateway courses, where the odds are already stacked against student achievement and the need for interventions is more diﬃcult to spot, students who fail to integrate completely with the class community - either because their experience comes from another discipline, or because they simply aren’t accustomed to the speciﬁc class environment - are likely to suﬀer poor performance. Having the ability to assess students’ mastery of the material, however, would enable the instructor to identify those students who are perhaps struggling or only falling behind, and to intervene to correct the students’ performance. 
 8.2 Interactivity in the blogosphere.
 In an online technology-mediated community like the one described in this paper, students beneﬁt from the exposure to both similar and contrasting viewpoints of the same course material. If the students’ deep emersion in the coblogging activity has a positive impact on their learning, one can assume that the originality score would correlate with the degree to which each student participates online. In other words, for those students that take advantage of the technology-mediated activity, frequently reading other students’ viewpoints and partaking in thoughtful conversations about the course readings, then originality scores should correlate with positive student outcomes. To assess student participation in the blogosphere, each student’s exposure (reading blog posts and comments by others) and contributions (writing blog posts and comments oneself) were measured. These activities were then correlated with the originality scores assigned to each student’s paper. Table 3 summarizes these metrics. Overall, the student papers received an average originality score of 53.49, with a standard deviation of 14.53. The highest originality score was 93.76, whereas the lowest score was 31.66. In terms of exposure in the blogosphere, the average number of times that a student was exposed to other students’ contributions was 4.36, with a standard deviation of 3.93. 
 Table 3: Originality and interactivity in the blogosphere. 
 The highest number of contributions read by a student in the blogosphere was 14, whereas one student read no contributions by the class at all. A chi-square test was used to explore the potential correlation between the originality of student papers and the degree of each student’s exposure in the blogosphere. As shown in Equation 10, there is a statistically signiﬁcant positive correlation between the two factors. In other words, higher exposure in the blogosphere led to more original papers. 
 (FORMULA_10).
 In terms of contributing in the blogosphere, each student made on average 4.18 contributions during the lead-in period, with a standard deviation of 2.17. The highest number of blog posts and comments written by a student was 9, whereas the lowest number of contributions was 1. As before, a chi-square test conﬁrmed that there was a statistically signiﬁcant positive correlation between the number of contributions a student makes in the blogosphere and the eventual originality of his or her paper. 
 (FORMULA_11).
 9. CONCLUSION.
 Integrating technology into higher education curricula to extend the physical boundaries of the classroom can be of signiﬁcant value, as it enables students to interact and learn outside of class time. This is particularly true in larger gateway courses, where there are fewer opportunities for students to engage in higher order thinking and to construct their own understanding of core concepts. While the introduction of technology like co-blogging can create a successful learning experience, the large number of students creates additional noise that makes it harder for instructors to isolate the students most in need of help. This paper described a method and tool by which student writing can be automatically analyzed to determine whether or not students have reached a point of originality in their writing, reﬂecting mastery of the course content. The paper presented a case study where the tool was used to analyze co-blogging data collected from an interdisciplinary reading- and writing-intensive course. The evidence showed that the tool was generating originality scores for students’ blog posts that correlated both with the degree to which they participated in the online activity as well as the ﬁnal grades that they received for their term papers. In other words, students who were more original during their co-blogging wrote better papers, and students who took advantage of the technology were more original. Although the paper was primarily aimed at conﬁrming the validity of the Point of Originality method, these ﬁndings suggest a likely use case for the technology. In a large class, where students engage both in iterative writing assignments like co-blogging and in summative writing assignments like midterm essays, an instructor might employ the Point of Originality tool at regular intervals throughout the semester to see which students are utilizing recast terminology in their work. Given the correlation seen here between a student’s ability to recast key concepts and their eventual performance on an assessment of those concepts (see Equation 7), the instructor could essentially use the tool to identify students with low point of originality values, and thus those most likely to do poorly on the assessment. Especially in large gateway courses, with potentially hundreds of students producing iterative assignments during the lead-in period, the process of identifying student learning styles and responding to student work is unwieldy [28, 10]. This strategy would allow an instructor to identify problems before it is too late: to determine which students might be struggling, to begin to isolate why, and to implement adjustments to pedagogy accordingly. It goes without saying that any tool of this sort might give a skewed measure for some students. For example, in any given class, some students simply learn best through faceto-face participation in class discussions while others accrue the highest learning beneﬁt through solitary reﬂective writing outside lecture hours. However, this does not reduce the merit or applicability of learning analytics tools such as the Point of Originality. On the contrary, if the “intangibility” of a technology-mediated learning activity is what makes any kind of evaluation or monitoring diﬃcult, even in smaller classes, then the production of tools that can assist the teacher in performing these activities would be a signiﬁcant boon. It is perhaps better to consider tools of this sort to be part of a larger “arsenal of assistive devices”, where one can pick-and-choose the tools most appropriate for the needs of a particular instructor, student, learning activity or course, whether it be for exploring textual content, activity logs or other types of data. It is the ability to be able to conduct any diagnoses at all, with a tailor-made analytics set, which is the primary beneﬁt. 
 10. FUTURE WORK.
 The tool is currently being used to analyze even larger gateway courses that typically enroll over 90 students. Additional features are also being developed to combat two not necessarily common but plausible anomalies where the composition of the writing examples themselves can produce false positives of originality. First, within the same writing sample, a student might (inadvertently) repeatedly use a word that not only is a synset match, but a match that yields a particularly high α value (see Equation 4). Therefore, an unreasonable degree of originality might be suggested for a particular writing sample. Currently, development is under way for a “decay factor,” that once enabled will gradually decrease the weight of the α value for a particular synset match, given how many times it has appeared before in the sample. The ﬁrst mention gets the maximum weight, where the nth mention receives the relative lowest possible weight. Second, where the evaluation of “originality” of a particular course topic depends on the presence of synset matches of multiple query terms within the same writing sample, the set Q (see Equation 6), then the distance between those matches within the text may also be signiﬁcant. For example, in response to a query for the compound term “color blindness,” the occurrence of a synset match for the word “color” in the ﬁrst paragraph of a writing sample may be otherwise unrelated to a synset match for “blindness” four paragraphs later. By implementing a “distance factor,” it will be possible for the instructor to specify a maximum distance (in terms of character, word, or paragraph count) between any two related synset matches in order for their α values to be included in the ﬁnal originality calculations for a given writing sample. 
 11. ACKNOWLEDGMENTS.
 Special thanks to Thanya Rajkobal for her contributions to the Point of Originality analysis tool and to the students in the course for providing data for this research project.