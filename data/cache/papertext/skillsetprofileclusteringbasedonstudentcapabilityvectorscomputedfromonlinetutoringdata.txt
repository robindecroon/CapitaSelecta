1. Set the starting cluster centers mg to the corners of the K-dim hyper-cube (2K centers). 2. Create the cluster assignment vector A by assigning each Bi to the closest mg. 3. For all clusters g, if no Bi is assigned to mg, i.e. 4. Alternate between 2) and 3) until the cluster assignment vector A does not change. 
 This flexible k-Means variation allows for empty clusters or fewer clusters than origi- nally requested and removes the constraint that there be one cluster per skill set profile. 
 3.2 Model-based Clustering.
 Model-based clustering [3,9] is a parametric statistical approach that assumes: the data X = {x1, x2, ..., xn}, xi ∈ <K are an independently and identically distributed sample from some unknown population density p(x); each population group g is represented by a (often Gaussian) density pg(x); and p(x) is a weighted mixture of these density components, i.e. 
 FORMULA_3.
 where ∑ pig = 1, 0 < pig ≤ 1 for g = 1, 2, ...,G, and θg = (µg,Σg) for Gaussian compo- nents. The method finds estimates for the number of clusters G as well as their centers and variances (µg,Σg) that maximize a chosen information criterion. Essentially, it finds the weighted combination of Gaussian densities that “best fits” the data. While it may require the groups to have Gaussian densities, it is very flexible (unlike k-Means) on the shape, volume, and orientation of the densities. This freedom allows model-based clustering to fit a wide array of student groups of different shapes and sizes. Both methods return a set of cluster centers and variances and an assignment vector mapping each Bi to a cluster. They do not, however, automatically assign a natural skill set profile (hyper-cube corner) to each cluster. Ideally, we have 2K clusters, each closest to a unique corner. In reality, some corners will have no students nearby. The k-Means algo- rithm has been altered to allow for this option; model-based clustering estimates centers in high-frequency areas and should not put a center near an empty corner. We do not advocate a one-to-one mapping of clusters to corners; clusters near areas of uncertainty in the hyper- cube should be identified as such. If a cluster of students is centered at {0.12, 0.88, 0.55}, they should be labeled as likely not having skill 1, likely having skill 2, and uncertain on skill 3. This conservative classification will help teachers avoid misclassifying students. To classify a new student, we calculate the capability vector and assign to the nearest cluster. 
 3.3 Subspace Clustering.
 If few items require skill k, Bik only take a few unique values. For example, if three items need skill k, Bik ∈ {0, 13 , 12 , 23 , 1}. Clustering on the K-dimensional hyper-cube may not perform well as students will map to only a few (K-1)-dimensional hyper-cubes. Instead we recommend conditioning on the coarsely gridded dimension (skill k, where students are already well-separated) and clustering on the (K-1)-dimensional conditional subspaces (repeating as needed). 
 4 Examples.
 For our simulated data, we use the deterministic inputs, noisy “and” gate model (DINA; [8]) a conjunctive cognitive diagnosis model. The DINA model item response form is 
 FORMULA_4.
 where αik = I{Student i has skill k} indicates if student i possesses skill indicates if student i has all skills needed for item j, for item j, s j = P(Yi j = 0 | ηi j = 1) is the slip parameter and g j = P(Yi j = 1 | ηi j = 0) is the guess parameter. If a student is missing any of the required skills, the probability that they will answer an item correctly drops due to the conjunctive assumption. When simulating data from the DINA model, we first fix skill difficulties and inter-skill correlation and generate true skill set profiles Ci for each student. If skills are of equal diffi- culty with little or no inter-skill correlation, students are evenly spread among the 2K natural skill set profiles. If skill difficulty varies, skill set profiles with only “easy” skills will have more students than those including the “hard” skills. High inter-skill correlation pulls stu- dents toward the no mastered skills and all mastered skills corners (Ci = {0}, {1}). Next we draw slip and guess parameters from a random uniform distribution (s j ∼Unif(0,0.30); g j ∼ Unif(0,0.15)). Given profiles and slip/guess parameters, we generate the student response matrix Y . Prior to clustering, we remove 10% of the responses completely at random. For these examples we know the true underlying skill set profiles Ci and can calculate their agreement with the clustering partitions using the Adjusted Rand Index (ARI; [7]), a common measure of agreement between two partitions. The expected value of the ARI is zero and the maximum value is one, with larger values indicating better agreement. 
 4.1 Simulated DINA Data.
 In Example 1, we generated response data for N = 250 students for J = 30 items, K = 2 skills. The Q-matrix contains only single skill items, 15 items per skill. The skills are equal difficulty with an inter-skill correlation of 0.25. Figure 2(a) shows the results. Clusters are number/color coded with triangle centers. We asked k-Means for 2K = 4 clusters; all students were clustered correctly (ARI = 1). Model-based clustering chooses five clusters (ARI = 0.926). The “extra” high frequency area near {1, 1} results from the close proximity or identical locations of the 19 students in Cluster 5. Teachers could interpret these results as two groups with similar skill 2 mastery but different skill 1 mastery. 
 Figure 2: Simulated data examples for K = 2, 3 skills, single skill and multiple skill items, 10% missing responses. Clusters are color/number coded, centers denoted by triangles. 
 In Example 2, we simulated as in Example 1 but increased the number of skills to K = 3. Again the Q-matrix was designed to only include single skill items, 10 items per skill. Here, both k-Means and model-based clustering recovered the true skill set profiles (ARI=1). Figure 2(b) shows the clustering results for both methods. For Example 3, we simulated as in Example 2 but used a balanced design Q-matrix including multiple skill items where each skill appeared by itself in four items, in four double skill items with each of the other two skills, and in three triple skill items. Results are in Figure 2(c). Both methods find clusters of students showing mastery of all three skills in the back upper right corner near the {1, 1, 1} skill set profile. However, the remaining students are pulled toward the front lower left corner (the {0, 0, 0} skill set profile), a direct result of the combination skill items. If a student incorrectly answers a multiple skill item, all skills required by that item are penalized (not just the unmastered skills). We have seen that a balanced design negates the penalty effect (ARI = 0.837, 0.829); the remaining clusters are effectively scaled and maintain their separation. The datasets presented are missing 10% of the responses; we compare their results to those for only students not missing any responses. In educational data mining, we com- monly use case-wise deletion of students to generate a complete dataset. This method is impractical here as it leaves us with 11, 10, and 15 students respectively. Instead we use the original generated response matrices prior to removing responses at random. The B- matrices are re-calculated and clustered. Only Example 3 had different ARIs. When using the complete data set, the ARI for k-Means increases from 0.837 to 0.880, for model-based clustering, 0.829 to 0.946. These jumps are expected as the lack of missingness increases the number of items seen (and the fineness of the grid) and decreases the relative effect of the penalty associated with incorrectly answering a multiple skill item; the resulting clusters are less removed from the corners. A higher dimensional example with N = 1000 students, J = 80 items, and K = 20 skills was also explored. In this case there were 425 unique latent classes used to generate the data. Model-based clustering found 424 clusters and had an ARI of 0.99. Giving k-means 220 starting centers is unreasonable; we’re currently developing methods to systematically and appropriately choose a smaller set of starting centers. 
 4.2 Assistment Data.
 Figure 3: Assistment System example of conditional k-Means clustering on the B-matrix; clusters are color/number coded. The table shows the cluster centers. 
 For our real data, we use a subset of 26 items requiring three skills (for easy visual- ization) from the Assistment System online mathematics tutor [5]. The Q-matrix is unbal- anced; Skill 1 (Evaluating Functions) appears in eight items, Skill 2 (Multiplication) in 20 items, and Skill 3 (Unit Conversion) in two items. Overall, 551 students answered at least one item, however there is a large amount of missing data (57%). Recall, if student i did not see any items requiring skill k, Bik = 0.5. Since Unit Conversion appears in only two items, BiUC ∈ {0, 12 , 1}. The three corresponding planes are visible in Figure 3. We condition the unique BiUC values and apply our k-Means variation (Section 3.1) to each plane. The final cluster centers are in the table in Figure 3. k-Means is preferable here because the limited number of unique values in the Evaluate Functions skill dimension leads to instability in the more flexible model-based clustering models. The planes corresponding to BiUC = 0 and 0.5 each have four clusters; the plane for BiUC = 1 has two. There are natural interpre- tations for each of the clusters. For example, a teacher might interpret Cluster 9 as students who know Unit Conversion and Multiplication, but are uncertain on Evaluating Functions. Cluster 10 could be interpreted as the students who have mastered all three skills. 
 5 Conclusions and Future Work.
 We derived a capability matrix to summarize student skill mastery for use in clustering algorithms. In simulated datasets, the method performed well (i.e., high values of ARI). In the Assistments data the method responded well to missing data, allowing us to draw conclusions for the skills that students have seen and distinguish the skills that require more assessment. Early results suggest that the Q-matrix design plays a large role in the location and interpretation of the clusters. Finally, we visually presented examples with K = 2 and K = 3 skills and showed the method scales to a larger number of skills. Currently, we are comparing our results to other student skill knowledge estimates. For example, using WinBUGS [11], the DINA model estimates produce essentially the same profile clusters for the simulated datasets; however, it runs around 700 times more slowly.