1. INTRODUCTION.
 An classic tension between the ﬁelds of adaptive systems and human computer interaction centres on the question of the locus of control : should the system adapt to perceived user needs, or should the system be adaptable by the end user at their demand? There is no clear right answer, nor is it a binary choice that must be made. Instead, a variety of successful systems have made choices between the poles of the adapt/adaptable continuum, taking into account the users, tasks, and domains at play. The goal of learning analytics is to provide insight into learners based upon their activity in e-learning systems. How this insight is used is up to the administrators, instructors, and instructional designers that have access to it. Remediation of known suboptimal behaviours is perhaps the principle interest of the area, and a number of diﬀerent techniques ranging from automated (as in intelligent tutoring systems), semi-automated (as in nudge analytics [3], perhaps exempliﬁed in the signals project [5]), and non-automated (through instructor or peer help interaction) remediation have been explored. Similar to the issue of the locus of control, there is no clear correct choice between these three levels of automation; all of the previously described techniques have shown that they can increase learner knowledge and satisfaction, and choosing one over others depends largely on the capabilities and quality of data you have available. In this work we address the question of the locus of control in adaptive e-learning systems in light of having instructional experts empowered with analytics knowledge. Instead of either automatically adapting to or being adaptable by learners, we aim to consider how instructors or instructional designers can gain insights from usage data which they then use to parametrize the way in which adaptation in the system is to take place. Thus the tension of the locus of control is mediated by a pedagogue who acts as a guiding hand, quietly informing how the system both adapts and presents cues to the learner for adaptation. 
 2. MOTIVATION & CONTEXT.
 In previous work we have shown how lecture video content can be automatically segmented into meaningful pieces using a combination of expert data and image recognition [1]. Through interviews with a group of learners, it became clear that diﬀerent learners approached the issue of how to segmenting video diﬀerently, and this diﬀerence was largely the result of the perceived usefulness of segments for a given learning task. Using the same system we have shown that learners, based on their usage in the system, can be automatically clustered into diﬀerent groups [2]. These groups appear to be indicative of end-user task; some learners would watch lecture videos every week, some only during the early or late portions of the course, while still others would watch video only when assessment drew near. Since clustering is an unsupervised technique, the groupings of students found in this second investigation aren’t meaningful until an instructional expert has labelled them. Without knowing this label, the system is unable to provide diﬀerent segmentations in a principled manner. It is possible to provide this labelling once for the application as a whole, which can then use these labels in choosing an appropriate segmentation algorithm for a given learner. But we would be remiss to do so without ﬁrst verifying that the clusters discovered are true for all domains, instructors, and circumstances that the system might be used in – a signiﬁcant endeavour indeed! Further, even if it were shown that clusters are stable across domains, and clusters were validated with respect to educational tasks, video segmentation is but one piece of an adaptive e-learning environment; this process would need to be repeated for each element in the system that is to be made adaptive. 
 Figure 1: A mockup of a lecture video playback system. 
 3. DESIGNING INSTRUCTIONAL EXPERTS INTO THE PROCESS.
 Our solution to this issue is to not design the system as an adaptive system per se, but to design it as an adaptable system where an instructional expert chooses how and when the system should present itself to the end user. In short, the system monitors learner usage, presents analytics information to an instructor or instructional designer who then labels meaningful patterns and parametrizes how adaption within the system should occur when these patterns are found. Consider the case of the video lecture system described in [2] a mockup1 of which is shown in ﬁgure 1. In the system there are multiple videos show to users depending on the capabilities of the classroom. Data projector video is segmented, and a list of segments is shown to the user on the left hand side. Clicking on a segment navigates the user to the corresponding portion of the video., and traditional video scrubber tools as well as a note taking widget are available. In this system the note taking widget contains both a private note-taking space, as well as the combined outputs of all students who have taken notes (a shared space). As students use the system they leave behind traces of what they have done; segments they have clicked on; pieces of video they have watched, paused, and rewatched; notes that they have made; etc. An ongoing challenge is how to present this information to instructional experts who may not understand statistical clustering techniques. We are considering a ‘learner-ﬁrst’ approach, in which visualizations of the results of clustering are shown using treemaps, where the top level treemap describes all learners who are registered in the course2 (ﬁgure 2). The expert can then modify the criteria by which learners are clustered using attributes available to them on the left hand side, and explore the results of the clustering process on the right hand side. Key to this method is that the clusters have no meaning to the system until they are labelled. The instructional expert does this by selecting a cluster (a rectangle in the treemap), inspecting the data using traditional charting tools (shown at the bottom), and editing the label ﬁeld. Each cluster is hierarchical, allowing the expert to recursively inspect and label sub-clusters of the data by double clicking. Clustering is static process based upon the attributes which the pedagogue has identiﬁed (in the left hand window). Membership of learners in clusters will change over time as more user data is collected, but the deﬁnitions of each cluster (the centroid ) will not change until the expert chooses to delete labels. A learner may be in multiple clusters at once. The instructional expert may choose to cluster data around some attribute set and provide labels for those clusters, then cluster around another attribute set for other purpose and come up with diﬀerent labels. The eﬀect of being in multiple clusters is that the system may be able to adapt the user interface in multiple ways. For example, a learner who is reviewing content for an exam and is a social constructivist learner may be recognized as such, and the system may adapt lecture video segmenting to provide overviews of relevant material while at the same time making available social tools such as chatrooms or shared notetaking features. Or, a learner who regularly returns to content and is a non-native language speaker may be shown closed captioning tools and more detailed segments to aid in navigation, while learners who had been shown to navigate quickly between segments may be provided video in high speed playback. Once clustering data has been labelled, the instructional expert can make these kinds of parametrizations to describe how adaptation takes place. We envision this using an interface similar to that which the student sees, where the pedagogue can add, remove, or characterize widgets based on the clusters a learner may belong to (ﬁgure 3). Parameters are widget-speciﬁc, and a default application view exists for those learners who are not in a labelled cluster. 
 Figure 2: Data exploration page; a list of the possible attributes to cluster by are shown on the left hand side. The treemap at the top right shows the clusters found, as well as the number of learners in each cluster and the expert-provided label. In this example, the expert has labelled the smallest cluster ‘reviewers’, and is exploring the data through traditional charts and graphs at the bottom right. 
 Figure 3: Parametrization of the segmentation widget. Note that each widget (in background) has a drop down allowing the pedagogue to delete the widget, add a new widget, or parameterize the widget that already exists. The parameters are supplied for each cluster label in the system; in this case there are three labels (Reviewing, Social, and First Time). Widget parameters, such as ‘1 every 30s’, are speciﬁc to the widget being customized, and we envision the use of controlled vocabularies and interface mechanisms to make this natural. 
 4. CONCLUSIONS.
 This work is proposing that instructors and instructional designers be included as mediating agents with respect to the locus of control for adaptable systems where learning analytics data is available. By having instructional experts parametrize how adaptation happens, the burden of validating the educational eﬀectiveness of a given adaptation by system developers is lessened. Further, this approach provides an inclusive method of customizing an adaptive elearning system for diﬀerent educational domains, tasks, and contexts. As a design, this work leaves us with unanswered questions of end-user perceptions of such a system, some of which we elaborate on here: - Will instructors, content experts, and instructional designers see value in attaining the insights and providing methods of adaptation? - Can the system be written such that it is accessible to these experts, and uses language and terms they understand? - Does this approach force on already burdened educators the need (either explicitly or implicitly) to micromanage the adaptive systems that support their courses? - Will adaptations be natural for learners, or does more of the adaptation process need to be opened up to them (for instance, through scrutable modelling [4])? - Are adaptations reusable enough to be shared such that they can serve as a starting point for new instructors and instructional designers who want to partake in this kind of endeavour? The areas of educational data mining, adaptive hypermedia, artiﬁcial intelligence in education, and intelligent tutoring systems are largely void of researchers situated in traditional education departments. With this work, we’re hoping broadening the dialogue around adaptive e-learning systems to include these experts of instruction directly. We do so by proposing that the starting point for adaptation sit in the hands of instructors and instructional designers, and that they determine, based on learning analytics, what actions should be taken. In designing the parameters for these environments, we believe instructional experts will reason more deeply about the patterns found in their classroom data. We aim to capitalize on this insight, and hope that not only will those experts see pedagogical gains in their daily activities, but that education researchers will use these methods to contribute to the growth of the ﬁeld of e-learning.