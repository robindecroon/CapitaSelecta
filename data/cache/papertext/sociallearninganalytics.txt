Introduction. 
 The concept of Learning Analytics is attracting significant attention within several communities with interests at the intersection of learning and information technology, including educational administrators, enterprise computing services, educators and learners. The core proposition is that, as unprecedented amounts of digital data about learners’ activities and interests become available, there is significant potential to make better use of this data to improve learning outcomes. After introducing some of the conceptual roots of Learning Analytics (§2), we propose that the implementation of effective Social Learning Analytics is a distinctive part of this broader design space, and offers a grand challenge for technology-enhanced learning research and enterprise, in three important respects (§3).

 1. The first is that the educational landscape is extraordinarily turbulent at present, in no small part due to technological drivers. The move to a participatory online culture sets a new context for thinking about analytics. Online social learning is emerging as a significant phenomenon for a variety of reasons, which we review (§4) in order to clarify the concept of online social learning (§5) and ways of conceiving social learning environments as distinct from other social platforms. 2. The second challenge is to understand the possibilities offered by different types of Social Learning Analytic, both those that are either inherently social (§6) and those that can be socialised, i.e., usefully applied in social settings (§7). 3. Thirdly, we face the challenge of implementing analytics that satisfy concerns about the limitations and abuses of analytics (§8). We conclude (§9) by considering potential futures for Social Learning Analytics, if the drivers and trends reviewed continue. 
 Learning analytics. 
 Learning analytics has its roots in two computing endeavours not specifically concerned with learning, but rather with strong business imperatives to understand internal organisational data, and external consumer behaviour. - Business Intelligence focuses on computational tools to improve organisational decision-making through effective fusion of data collected via diverse systems. The earliest mention of the term ‘learning analytics’ that we have found relates to business intelligence about e-learning products and services (Mitchell & Costello, 2000). - Data Mining, also called Knowledge Discovery in Databases (KDD), is the field concerned with employing large amounts of data to support the discovery of novel and potentially useful information (Piatetsky-Shapiro, 1995). This field brings together many strands of research in computing, including artificial neural networks, Bayesian learning, decision tree construction, instance-based learning, logic programming, rule induction and statistical algorithms (Romero & Ventura, 2007). 
 From data mining developed the field of: - Educational Data Mining (EDM) “an emerging discipline, concerned with developing methods for exploring the unique types of data that come from educational settings, and using those methods to better understand students, and the settings which they learn in” (Baker & Yacef, 2009). Originally, relatively fine-grained, quantitative data came from private educational software applications—Romero and Ventura (2007) trace the first EDM publications to 1995—but their overview of the field shows that research projects multiplied after widespread adoption of virtual learning environments (VLEs) in the early 21st century. Blackboard and Moodle are well-known examples of VLEs, which are also known as learning management systems (LMSs) and content management systems (CMSs). These tools automatically amass large amounts of log data relating to student activities. They not only record student activities and browse time, but also personal information such as user profiles, academic results, and interaction data. Many of them include student tracking capabilities as generic software features. Dawson (2009) reported that the depth of extraction and aggregation, reporting and visualisation functionality of these built-in analytics was often basic or non-existent, but in the last year, all of the major VLE products now include at least rudimentary analytics “dashboards.” Educational institutions have become increasingly interested in analysing the available datasets in order to support retention of students and to improve student results. This use of academic analytics stretches back for at least 50 years, but has become more significant in the last five years as datasets have grown larger and more easily available for analysis. - Academic Analytics are described by Campbell & Oblinger (2007) as ‘an engine to make decisions or guide actions. That engine consists of five steps: capture, report, predict, act, and refine.’ They note that ‘administrative units, such as admissions and fund raising, remain the most common users of analytics in higher education today.’ - Action Analytics is a related term, proposed by Norris, Baer and Offerman (2009) to emphasise the need for benchmarking both within and across institutions, with particular emphasis on the development of practices that make them effective. The Signals project at Purdue University is currently the field’s flagship example of the successful application of academic analytics, reporting significantly higher grades and retention rates than were observed in control groups (Arnold, 2010; Pistilli & Arnold, 2012). The project mines data from a VLE, and combines this with predictive modelling to provide a real-time red/amber/green traffic-light to students and educators, helping staff intervene in a timely manner where it will be most beneficial, and giving students a sense of their progress. Encouraged by such examples, educational institutions are seeking both to embed academic/action analytics and to develop a culture that values the insights that analytics provide for organisational strategic planning and improved learner outcomes. A growing number of universities are implementing data warehouse infrastructures in readiness for a future in which they see analytics as a key strategic asset (Stiles, Jones, & Paradkar, 2011). These data warehouses store and integrate data from one or more systems, allowing complex queries and analysis to take place without disrupting or slowing production systems. This brings us to the present situation; the first significant academic gathering of the learning analytics community was in 2011 at the 1st International Conference on Learning Analytics & Knowledge, doubling in size to 200 in 2012. The 2011 conference defined the term as follows: Learning analytics is the measurement, collection, analysis and reporting of data about learners and their contexts, for purposes of understanding and optimising learning and the environments in which it occurs. 
 Clearly, this encapsulates strands from all the above fields, reflecting the topic’s interdisciplinary convergence but, in contrast to more theoretical research or artificial experimentation which might be published in some of the above fields, there is an emphasis on impacting authentic learning from real-world contexts, through the use of practical tools. There is also a shift away from an institutional perspective towards a focus on the concerns of learners and teachers. The main beneficiaries are no longer considered to be administrators, funders, marketing departments and education authorities, but instead are learners, teachers and faculty members (Long & Siemens, 2011). 
 The challenge of social learning analytics. 
 In a literature analysis of the field, we found that in the discourse of academic analytics there is little mention of pedagogy, theory, learning or teaching (Ferguson, 2012). This reflects the roots of these analytics in management information systems and business intelligence, whose mission has been to guide strategic action by senior leaders in organisations, and whose tools deliver abstracted summaries of key performance indicators. In such contexts, senior executives do not have the time to delve into the process details of a particular individual’s or group’s interactions, and similarly, the arguments for academic analytics seem to focus on finding variables that predict positive or negative outcomes for cohorts of learners. Performance indicators in educational settings typically involve outcomes-centric analytics based on learners’ performance on predefined tasks. Within formal education, success is typically defined as the display of expertise through summative assessment tasks (for example, assignments, exams or quizzes) intended to gauge mastery of discipline knowledge. The focus is on individual performance and on what has been achieved. This model is familiar within settings such as schools and universities, but it is less relevant in the context of online social learning, which involves lifelong learners drawing together resources and connections from across the Internet to solve real-life problems, often without access to the support of a skilled teacher or accredited learning institution. Social Learning Analytics (SLA) are strongly grounded in learning theory and focus attention on elements of learning that are relevant when learning in a participatory online culture. They shift attention away from summative assessment of individuals’ past performance in order to render visible, and in some cases potentially actionable, behaviours and patterns in the learning environment that signify effective process. In particular, the focus of social learning analytics is on processes in which learners are not solitary, and are not necessarily doing work to be marked, but are engaged in social activity, either interacting directly with others (for example, messaging, friending or following), or using platforms in which their activity traces will be experienced by others (for example, publishing, searching, tagging or rating). Social Learning Analytics is, we propose, a distinctive subset of learning analytics that draws on the substantial body of work demonstrating that new skills and ideas are not solely individual achievements, but are developed, carried forward, and passed on through interaction and collaboration. A socio-cultural strand of educational research demonstrates that language is one of the primary tools through which learners construct meaning. Its use is influenced by their aims, feelings and relationships, all of which shift according to context (Wells & Claxton, 2002). Another socio-cultural strand of research emphasises that learning cannot be understood by focusing solely on the cognition, development or behaviour of individual learners; neither can it be understood without reference to its situated nature (Gee, 1997; Wertsch, 1991). As groups engage in joint activities, their success is related to a combination of individual knowledge and skills, environment, use of tools, and ability to work together. Understanding learning in these settings requires us to pay attention to group processes of knowledge construction – how sets of people learn together using tools in different settings. The focus must be not only on learners, but also on their tools and contexts. Viewing learning analytics from a social perspective highlights types of analytic that can be employed to make sense of learner activity in a social setting. This gives us a new way to conceive of both current and emerging approaches—as tools to identify social behaviours and as patterns that signify effective process in learning environments. Social Learning Analytics should render learning processes visible and actionable at different scales: from national and international networks to small groups and individual learners. We turn now to review some of the features of the participatory online culture that drives this work. 
 The emergence of open, social learning. 
 In this section, we identify some of the signals that many futures analysts and horizon-scanning reports on learning technology have highlighted as significant. Taken together, these create synergies that establish a radically new context for learning. In such a context, we argue, analytics focused on summative assessment of performance remain important but do not go far enough: we need to develop new sets of analytics that can be used to support learning and teaching in these new conditions. We summarise these phenomena as: - technological drivers - the shift to ‘free’ and ‘open’ - demand for knowledge-age skills - innovation requires social learning - challenges to educational institutions. 
 Technological drivers. 
 A key force shaping the emerging landscape is clearly the digital revolution. Only very recently do we have almost ubiquitous Internet access in wealthy countries and mobile access in many more. In addition, we now have user interfaces that have evolved through intensive use, digital familiarity from an early age, standards enabling interoperability and commerce across diverse platforms, and scalable computing architectures capable of servicing billions of real-time users and of mining the resulting data. With the rise of social websites serving millions of users, such as Facebook, YouTube and Twitter, plus the thousands of smaller versions and niche applications for specific tasks and communities, we have witnessed a revolution in the ways in which people think about online interaction and publishing. Such social media platforms facilitate the publishing, indexing and tracking of user-generated media, provide simple-to-learn collaboration spaces, and enable social networking functions that are becoming ubiquitous: friending, following, messaging and status updates. Standards such as really simple syndication (RSS) allow information to be shared easily using structured data feeds, web services enable more sophisticated machine-machine interaction, and mobile devices expand the availability and localization of these services. Internet services may also begin to apply pressure to one of the slowest evolving elements in educational provision: accreditation. Christensen et al. (2008) argue that the agencies controlling accreditation often stifle innovation and protect the status quo, because new approaches to learning/accreditation struggle to gain credibility unless they are associated with institutions that have the power to award established qualifications. However, as the infrastructure for secure identity management matures, and as the participatory, social culture fostered by Web 2.0 becomes more deeply ingrained in younger generations, initiatives such as OpenBadges may provide new ways to accredit learning outside established institutions. Moreover, as ubiquitous tools for capturing digital material make it easier to evidence learning and practical knowledge in authentic communities of practice, an e-portfolio of evidence might come to have equivalent or greater credibility than formal certificates. However, changes in technology do not necessarily imply changes in pedagogy. Those who view education as information transfer will use interactive media for storage, drilling, testing and accessing information; those who seek conceptual change will seek to make use of their interactive qualities (Salomon, 2000). Technological shifts support analytics that draw on sets of big data—but they do not necessitate a shift towards analytics focused on such issues as conceptual change, distributed expertise, collaboration or innovation. So, if we do not accept simplistically that technology alone determines the future, we need to look elsewhere to understand the move towards online social learning and its associated analytics. 
 The shift to free and open. 
 There has been a huge shift in expectations of access to digital content. The Internet makes possible completely new revenue-generation models due to the radically lower transaction costs incurred (compared to bricks and mortar businesses with physical products) as one scales to hundreds of thousands of users. Andersen (2009) documents many ways in which online companies are able to offer quality services free of charge, producing an increasing expectation on the part of end-users of huge choice between free tools and sources of content hosted ‘in the cloud’. Within education, the Open Education Resource (OER) movement has been a powerful vehicle for making institutions aware of the value of making high quality learning materials available, not only free of charge, but also in formats that promote remixing, in an effort to reap the benefits seen in the open-source software movement. This has not proven to be a simple matter, but OER has made huge progress, and is gaining visibility at the highest levels of educational policy. This is amplified by efforts to make data open to machine processing as well as human interpretation. This requires not only a shift in mindset by data owners but also the construction of technological infrastructure to make it possible to publish data in useful formats. These efforts can be tracked within communities developing Linked Data and the Semantic Web, and their myriad applications communities, for example, Open Government, Open Mapping, Science 2.0 and Health 2.0. Together, these very rapid shifts contribute to a new cultural context for the provision of learning services, in which the industrial-era value chain, previously delivered by a single institution, is disaggregated into smaller and smaller elements. The provision of content, community, tools and basic analytics may increasingly be expected to come free of charge, while learners may still consider paying for other services such as personalised learning journeys, personal tuition, career guidance, accreditation against formal standards and tailored analytics that support them on a variety of sites, not just within one institution. 
 Demand for knowledge-age skills. 
 Technology is always appropriated to serve what people believe to be their needs and values. Since 1991, we have lived in the “knowledge age”—a period in which knowledge, rather than labour, land or capital, has been the key wealth-generating resource (Savage, 1996). This shift has occurred within a period when constant change in society has been the norm, and it is therefore increasingly difficult to tell which specific knowledge and skills will be required in the future (Lyotard, 1979). These changes have prompted an interest in “knowledge-age skills” that will allow learners to become both confident and competent designers of their own learning goals (Claxton, 2002). Accounts of knowledge-age skills vary, but they can be broadly categorized as relating to learning, management, people, information, research/enquiry, citizenship, values/attributes and preparation for the world of work (Futurelab, 2007). From one viewpoint they are important because employers are looking for “problem-solvers, people who take responsibility and make decisions and are flexible, adaptable and willing to learn new skills” (Educational Subject Center, 2007, p. 5). More broadly, knowledge-age skills are related not just to an economic imperative but to a desire and a right to know, an extension of educational opportunities, and a “responsibility to realise a cosmopolitan understanding of universal rights and acting on that understanding to effect a greater sense of community” (Willinsky, 2005, p111). In both cases, there is a perceived need to move away from a curriculum based on a central canon of information towards learning that develops skills and competencies. This implies a need for ongoing analytics that can support the development of dispositions such as creativity and curiosity, collaboration skills and resilience. 
 Innovation requires social learning. 
 The conditions for online social learning are also related to the pressing need for effective innovation strategy. In an accessible introduction to the literature and business trends, Hagel et al. (2010) argue that social learning is the only way in which organizations can cope in today’s fast-changing world. They invoke the concept of ‘pull’ as an umbrella term to signal some fundamental shifts in the ways in which we catalyse learning and innovation. They highlight quality of interpersonal relationships, tacit knowing, discourse and personal passion as key elements. This is a move away from having information pushed to us during spells of formal education towards a more flexible situation in which we pull resources and information to us as we need them. The move from “push” to “pull” motivates analytics that can be accessed by learners at any point, employed in both informal and formal settings, are sensitive to social relationships, and build transferable learning dispositions and skills. 
 Challenges to educational institutions. 
 Together, these forces create pressures on models of educational provision at all stages of education from childhood into workplace learning. Heppell (2007), amongst many, points to the need for an education system that helps people to help each other, rather than one that delivers learning. The barriers between formal and informal learning, and between online and face-to-face learning are currently being broken down, allowing the development of new models that take into account the range of learners’ experience outside formal study, and the affective elements of learning. An example of this is Gee’s “affinity spaces,” which provide a model for online social learning and were first identified in video gaming environments. Affinity spaces are organized around a passion; within them, knowledge is both distributed and dispersed, they are not age graded, experts work alongside newcomers, learning is proactive but aided as people mentor and are themselves mentored, participants are encouraged to produce as well as to consume, smart tools are available to support learning and everyone, no matter what their level of experience or expertise, remains a learner (Gee, 2004, 2009). Other new models for learning are emerging from a variety of digital sources. Some examples amongst many are the learning affordances of the World of Warcraft online game, with its guilds and carefully planned, collectively executed strategies (Thomas & Brown, 2011), learners beginning to access and create knowledge through persistent avatar identities that can move between different environments (Ferguson, Sheehy, & Clough, 2010), and the development of distributed cognition within virtual worlds (Gillen, Ferguson, Peachey, & Twining, 2012). These models suggest new ways of approaching learning analytics. Gee (2003) showed that well-designed video games incorporate analysis of the development of participants’ relevant knowledge and skills, so that their experience is constantly customized to their current level, effort and growing mastery, they are aware of ongoing achievements, and they are provided with information at the point when it can best be understood and used in practice. Having noted some of the features of the emerging landscape for open, social learning, and the implications of these features for analytics, we now consider some of the key features of social learning, and the nature of online social learning environments. 
 Characterising online social learning. 
 Why has someone sawn down half of the beautiful cedar tree outside my office window? I can’t find this out from a book, and I don’t know anyone with the precise knowledge that I am looking for. It is as I engage in conversations with different people that my understanding of what I see outside my window increases, and I learn more about the tree’s history, health, ecosystem and future possibilities. It is not just the social construction of understanding that is important here, since this is a part of most human interactions. My intention to learn is part of what makes this social learning, as are interactions with others. This is not a one-sided engagement with books or online content—it involves social relationships. As such, it has lots of ‘affective’ aspects: people must be motivated to engage with me and I must have the confidence to ask questions in the first place, as well as some way of assessing the expertise of the people I’m talking to. (Ferguson, 2010) Social learning has been conceptualised as societal learning in general, as processes of interaction that lead to concerted action for change, as group learning, and as the learning of individuals within a social context (Blackmore, 2010). Our conception of online social learning takes into account the changing affordances of a world in which social activity increasingly takes place at a distance and in mediated forms. It is succinctly expressed by Seely Brown and Adler (2008) as being “based on the premise that our understanding of content is socially constructed through conversations about that content and through grounded interactions, especially with others, around problems or actions.” Many others have, of course, argued for similar conceptions, unpacking this broad concept in great detail within the constructivist educational literature, and computer-supported collaborative learning (CSCL) research. Social learning adds an important dimension to CSCL, introducing a particular interest in the non-academic contexts in which it may take place (including the home, social network, and workplace) and the use of free, ready-to-hand online tools, with no neatly packaged curriculum or signed-up peer cohort, no formally prescribed way to test one’s understanding and no pre-scheduled activities (Blackmore’s (2010) edited readings remind us how far back everyday, non-digital social learning goes in learning theory, and provide us with foundations for extension into the digital realm). While OERs greatly increase the amount of good quality material available online to learners, another consequence can be that individual learners find themselves adrift in an ocean of information, struggling to solve ill-structured problems, with little clear idea of how to solve them, or how to recognise when they have solved them. At the same time, distributed networks of learners are grappling with ‘wicked problems’ such as climate change, which offer the same challenges on a grander scale. Social learning infrastructure could have a key role to play in these situations, helping learners connect with others who can provide emotional and conceptual support for locating and engaging with resources, just as in our tree story at the start of this section. This forces us to ask whether our current educational and training regimes are fit for purpose in equipping our children, students and workforce with the dispositions and skills needed under conditions of growing uncertainty—a challenge explored in detail by many others, for example in the collection edited by Deakin Crick (2009). The Open University, where we are based, has been seeking to address these issues with its SocialLearn project, aimed at supporting large-scale social learning. In the early days of the project, Weller (2008) identified six broad principles of SocialLearn: Openness, Flexibility, Disruptive, Perpetual beta, Democracy and Pedagogy. Following a series of workshops, Conole (2008) proposed a set of learning principles for the project—thinking & reflection, conversation & interaction, experience & interactivity and evidence & demonstration—and articulated how these could be linked to characteristics of social learning. Distilling this array of perspectives, we have derived a simple working definition focused on three dynamics, which serves to guide us in designing for meaningful interpersonal and conceptual connection: Online social learning can take place when people are able to: - clarify their intention—learning rather than browsing - ground their learning—by defining their question/problem, and experimenting - engage in learning conversations—increasing their understanding. A significant feature of the Web 2.0 paradigm is the degree of personalisation that end-users now expect. However, a me-centred universe has self-evident limitations as a paradigm for holistic development: learning often disorients and reorients one’s personal universe. User-centred is not the same as Learner-centred: what I want is not necessarily what I need, because my grasp of the material, and of myself as a learner, is incomplete. The centrality of good relationships becomes clear when we remind ourselves that a university’s job is to teach people to think, and that deeper learning requires leaving a place of cognitive and emotional safety where assumptions are merely reinforced—see the extensive research on learning dispositions that characterize this readiness (for example, Claxton, 2001; Perkins, Jay, & Tishman, 1993). This implies challenge to stretch learners out of their comfort zones, underlining the importance of affirmation and encouragement that give a learner the security to step out. As Figure 1 shows, the design of a social media space tuned for learning involves many alterations and additions to a generic space for social media. Within an online space tuned for learning, friends can become learning peers and mentors, informal endorsements are developed into verifiable accreditation, information exchanges become learning conversations and, likewise, generic web analytics need to be developed into learning analytics that can be used in such an environment. To summarise: we have outlined what we mean by online social learning, some of the major drivers that help to explain why it is emerging as a phenomenon, and some of the elements that may differentiate a social learning environment from other social media spaces. We have also indicated why these factors require new approaches to learning analytics. Constructivist pedagogies suggest the need for a shift away from a positivist approach to analytics and towards analytics that are concerned with conceptual change, distributed expertise, collaboration and innovation. This ties in with an increasing emphasis on knowledge-age skills and their associations with learning dispositions such as creativity and resilience. Within an open environment, there is a need for a range of analytics that can extend beyond an institutional platform in order to provide support for lifelong learners at all points in their learning journey. These learners may be organised in classes and cohorts, but they may also need analytics that help them to learn together in looser groupings such as communities and networks. These analytics, and their associated recommendations, will be informed by those developed for social media tools and platforms, but they will be tuned for learning, examples being prompting the development of conversations into educational dialogue, recommending resources that challenge learners to leave their comfort zones, or making learners aware that social presence and role are increasingly important to attend to in a complex world. 
 Figure 1. Dimensions of the social learning design space. 
 Together, these motivate a conception of Social Learning Analytics as a distinctive class of analytic. 
 Inherently social learning analytics. 
 Social learning analytics make use of data generated by learners’ online activity in order to identify behaviours and patterns within the learning environment that signify effective process. The intention is to make these visible to learners, to learning groups and to teachers, together with recommendations that spark and support learning. In order to do this, these analytics make use of data generated when learners are socially engaged. This engagement includes both direct interaction—particularly dialogue—and indirect interaction, when learners leave behind ratings, recommendations or other activity traces that can influence the actions of others. Another important source of data consists of users’ responses to these analytics and their associated visualizations and recommendations. We identify two inherently social analytics, and three socialised analytics: Inherently social analytics—only make sense in a collective context: - Social Network Analytics—interpersonal relationships define social platforms and link learners to contacts, resources and ideas. - Discourse Analytics—language is a primary tool for knowledge negotiation and construction. Socialised analytics—although these are relevant as personal analytics, they have important new attributes in a collective context: - Content Analytics—user-generated content is one of the defining characteristics of Web 2.0 - Disposition Analytics—intrinsic motivation to learn lies at the heart of engaged learning and innovation - Context Analytics—mobile computing is transforming access to people, content and both formal and informal learning. 
 We do not present these as an exhaustive “taxonomy,” since this would normally be driven by, for instance, a specific pedagogical theory or technological framework in order to motivate the category distinctions. We are not grounding our work in a single theory of social learning, nor do we think that a techno-centric taxonomy is helpful. These categories of analytics respond to the spectrum of drivers reviewed above, drawing on diverse pedagogical and technological underpinnings as reviewed above, and further cited below as we introduce each category. We summarise the essence of each approach, identify examples of tools, and then consider how these tools are being, or might be, used to support online social learning. In this section, we introduce the two inherently social analytics. 
 Social network analytics. 
 Essence of social network analysis. 
 Networked learning involves the use of ICT to promote connections between one learner and other learners, between learners and tutors, and between learning communities and learning resources (Jones & Steeples, 2003). These networks are made up of actors (both people and resources) and the relations between them. Actors with a relationship between them are said to be tied and these ties can be classified as strong or weak, depending on their frequency, quality or importance (Granovetter, 1973). Social network analysis is a perspective that has been developed to investigate the network processes and properties of ties, relations, roles and network formations, and to understand how people develop and maintain these relations to support learning (Haythornthwaite & de Laat, 2010). Fortunato (2010) describes social networks as “paradigmatic examples of graphs with communities”; social network analysis brings graph theory from the field of mathematics together with work on interpersonal and communal relationships from the fields of sociology and communication. The many uses of social network analysis applicable to social learning include detection of communities within networks (Clauset, Newman, & Moore, 2004; Fortunato, 2010); identification of types of subset within a network where a level of cohesion exists and depends on properties such as proximity, frequency and affinity or other properties (Reffay & Chanier, 2003); investigation of the density of social networks (Borgatti, Mehra, Brass, & Labianca, 2009); and exploration of individuals’ centrality within a network (Wasserman & Faust, 1994). 
 Social network analysis tools. 
 Many tools have been developed to support social network analysis in the context of learning. Commercial products such as Mzinga can be used to identify learners with the highest and most active participation in a network, those who are having the most influence on the activity of others and those who have the potential to make most impact. SNAPP (Social Networks Adapting Pedagogical Practice) is a freely available network visualisation tool that reinterprets discussion forum postings as a network diagram. These diagrams can be used to trace the growth of course communities, to identify disconnected students, to highlight the role of information brokers and to visualise how teacher support is employed within the network (Bakharia & Dawson, 2011; Dawson, Bakharia, & Heathcote, 2010). Gephi is a free, open-source platform that supports visualisation and exploration of all kinds of networks. In an extended series of blog posts, Hirst has explored ways in which this tool can be used to explore the learning networks that develop around shared resources and online course. His work picks out different networks with interconnected interests, identifies the interests that are shared by actors in a network, and highlights not only the role played by information brokers in sharing resources, but also the roles played by resources in connecting networks. Network-focused social learning analytics Social network analysis is a useful tool for examining online learning because of its focus on the development of interpersonal relationships, and its view that technology forms part of this process. It thus offers the potential to identify interventions that are likely to increase the potential of a network to support the learning of its actors by linking them to contacts, resources and ideas. Haythornthwaite and De Laat (2010) approach this form of analysis from two perspectives: egocentric and whole network. Egocentric networks are described from the point of view of the individual, who is set at the centre of an array of relationships both formally and informally connected with learning. Studying networks in this way can help to identify the people from whom an individual learns, where conflicts in understanding may originate, and which contextual factors influence learning. A whole-network view, on the other hand, considers the distribution of information and the development of learning across a set of people. In this case, analysis can characterise the network in terms of its character, interests and practices. This whole-network view is able to take “the results of pairwise connections to describe what holds the network together” (Haythornthwaite & de Laat, 2010, p. 189). Characterising the ties between actors adds a different dimension to this analysis—people rely on weak ties with people they trust when accessing new knowledge or engaging in informal learning, but make use of strong ties with trusted individuals as they deepen and embed their knowledge (Levin & Cross, 2004). Another option is to combine social network analysis with content analysis and context analysis to gain a richer picture of networked learning, investigating not only who is talking to whom, but what they are talking about and why they are talking in this way (De Laat, Lally, Lipponen, & Simons, 2006; Hirst, 2011). As social network analysis is developed and refined, it has the potential to be combined with other social learning analytics in order to define what counts as a learning tie and thus to identify which interactions promote the learning process. It also has the potential to be extended in order to take more account of interactions with resources, identifying indirect relationships between people which are characterised by their interaction with the same resources rather than through direct communication. 
 Social learning discourse analytics. 
 Essence of discourse analysis. 
 Discourse analysis is the collective term for a wide variety of approaches to the analysis of series of communicative events. Some of these approaches cannot easily be employed as online social learning discourse analytics because they focus on face-to-face or spoken interactions and may require intensive examination of semiotic events from a qualitative perspective. Others provide new ways of understanding the large amounts of text generated in online courses and conferences. Schrire (2004) used discourse analysis to understand the relationship between the interactive, cognitive and discourse dimensions of online interaction, examining initiation, response and follow-up (IRF) exchanges. More recently, Lapadat (2007) has applied discourse analysis to asynchronous discussions between students and tutors, showing how groups of learners create and maintain community and coherence through the use of discursive devices. 
 Discourse analysis tools. 
 There are many tools available for the online analysis of text and discourse; the Digital Research Tools Wiki currently lists 55. These range from well-known visualisation tools such as Wordle and Tag Crowd to powerful generic tools such as NVivo, which can be used to support a range of qualitative research methods. A method of discourse analysis that relies heavily on electronic tools and computer processing power is corpus linguistics, the study of language based on examples of real-life use. The corpus of examples is typically in electronic form and may be massive; the European Corpus Initiative Multilingual Corpus includes 98 million words covering most of the major European languages, while the British National Corpus is a 100-million-word sample of a wide range of written and spoken sources. Automated software, such as WMatrix, facilitates quantitative investigation of such corpora (O'Halloran, 2011). 
 A different approach to seeking to extract structure from naturally occurring but relatively unstructured texts is to ask users to add more structure themselves. This is an extension of asking users to enrich resources with metadata, which we see in social tagging. Learners cannot be asked to structure their annotations on documents and contributions to discussion simply to facilitate computational processing, since there would be no value for them in doing so. However, significant research in concept mapping (Novak, 1998) and computer-supported argumentation (Scheuer, Loll, Pinkwart, & McLaren, 2010) has shown that this can be a pedagogically effective discipline to ask of students in a formal academic context, and within organisational contexts, the mapping of conversations can promote quality meetings and shared ownership of outcomes amongst diverse stakeholders (Selvin & Buckingham Shum, 2002). Cohere is a web-based tool that provides a medium not only for engaging in structured online discourse, but also for summarizing or analysing it (Buckingham Shum, 2008). Following the approach of structured deliberation/argument mapping, Cohere renders annotations on the web, or a discussion, as a network of rhetorical moves: users must reflect on, and make explicit, the nature of their contribution to a discussion. This tool can be used to augment online conversation by making explicit information on the rhetorical function and relationship between posts. Users also have the option to browse their online dialogue as a semantic network of posts rather than as a linear text. 
 Discourse-focused social learning analytics. 
 A sociocultural perspective on learning “highlights the possibility that educational success and failure may be explained by the quality of educational dialogue, rather than simply in terms of the capability of individual students or the skill of their teachers” (Mercer, 2004, p. 139). The ways in which learners engage in dialogue are indicators of how they engage with other learners’ ideas, how they compare those ideas with their personal understanding, and how they account for their own point of view, which is an explicit sign of the stance they hold in the conversation. Mercer and his colleagues distinguished three social modes of thinking that are used by groups of learners in face-toface settings: disputational, cumulative and exploratory talk (Mercer, 2000; Mercer & Littleton, 2007). Disputational dialogue is characterised by disagreement and individualised decision-making; in cumulative dialogue speakers build on each other’s contributions but do not critique or challenge these. Exploratory dialogue is typically regarded as the most desirable by educators because speakers share knowledge, challenge ideas, evaluate evidence and consider options together. Learning analytics researchers have built on this work to provide insight into textual discourse in online learning (Ferguson, 2009), providing a bridge to the world of online learning analytics for knowledge building. Initial investigations (Ferguson & Buckingham Shum, 2011) suggest that indicators of exploratory dialogue—challenges, extensions, evaluations and reasoning—can be automatically identified within online discussion. This analysis can be used to provide recommendations about relevant learning discussions, as well as to prompt the development of meaningful learning dialogue. The Cohere structured deliberation platform has been extended by De Liddo and her colleagues (2011) to provide learning analytics that identify: - Learners’ attention—what they focus on, which problems and questions they raise, which comments they make and which viewpoints they express - Learners’ rhetorical attitude to discourse contributions—areas of agreement and disagreement, the ideas supported by learners and the ideas questioned by learners - Distribution of learning topics—the people who propose and discuss the most contentious topics - Learners’ relationships—beyond the undifferentiated ties of social network analysis, Cohere users are tied with semantic relationships (such as supporting or challenging), showing how learners relate to each other and how they act within a discussion group. While informal text chat is difficult to analyse automatically in any detail, due to non-standard use of spelling, punctuation and grammar, more formally structured texts such as a journal article can be analysed using natural language processing technologies. Sándor and her colleagues (Sándor, Kaplan, & Rondeau, 2006; Sándor & Vorndran, 2009) have used the Xerox Incremental Parser (XIP) to highlight key sentences in academic articles in order to focus an evaluator’s attention on the key rhetorical moves within the text which signal claims to contribute to knowledge. Analysis of XIP and human annotation suggests that they are complementary in nature (Sándor, De Liddo, & Buckingham Shum, 2012). Whitelock and Watt analysed discourse using Open Mentor, a tool for teachers to analyse, visualise and compare the quality of their feedback to students (Whitelock & Watt, 2007, 2008). Open Mentor uses a classification system based on that of Bales (1950) in order to investigate the socio-emotive aspects of dialogue as well as the domain level. A standard charting component is then used to provide interactive bar chart views onto tutors’ comments, showing the difference between actual and ideal distributions of different comment types. Tutors can use these analytics to reflect on their feedback, and the analytics can also be used to recommend moves towards the types of feedback that students find most useful. The development of the field of learning analytics has brought approaches to discourse that originated in the social sciences more closely in contact with statistical methods of extracting and representing the contextual usage and meaning of words (Landauer, Foltz, & Laham, 1998). A social learning analytics perspective offers the possibility of harnessing these methods and understandings in order to provide analytics and representations that can help learners to develop their conversations into reasoned arguments and educational dialogue. 
 Socialised learning analytics. 
 Discourse and social network analytics are inherently concerned with social interaction. In the context of learning, they already have a strong focus on the learning group. In this section, we consider three kinds of learning analytic that are more typically viewed from the perspective of the isolated learner who may be making no use of interpersonal connections or social media platforms. We argue that these analytics take on significant new dimensions in the context of online social learning. 
 Social learning disposition analytics. 
 Essence of learning dispositions. 
 The first of these socialised learning analytics is the only one of our five categories that originated in the field of educational research rather than being adapted to apply to the analysis of learning. A well-established research programme has identified, theoretically, empirically and statistically, a seven-dimensional model of learning dispositions (Deakin Crick, 2007). These dispositions can be used to render visible the complex mixture of experience, motivation and intelligences that make up an individual’s capacity for lifelong learning and influence responses to learning opportunities (Deakin Crick, Broadfoot, & Claxton, 2004). They can be used to assess and characterise the complex mixture of experience, motivation and intelligences that a learning opportunity evokes for a specific learner. It is these developing qualities that make up an individual’s capacity for lifelong learning (Deakin Crick, et al., 2004). Learning dispositions are not “learning styles,” a blanket phrase used to refer to a wide variety of frameworks that have been critiqued on a variety of grounds, including lack of contextual awareness (Coffield, Moseley, Hall, & Ecclestone, 2004). By contrast, important characteristics of learning dispositions are that they vary according to context, and that focused interventions have been shown to produce statistically significant improvements in diverse learner groups, ranging in age from primary school to adults, demographically from violent young offenders and disaffected teenagers to high achieving pupils and professionals, and culturally from middle-class Western society to Indigenous communities in Australia (Buckingham Shum & Deakin Crick, 2012). Together, learning dispositions comprise the seven dimensions of “learning power”: changing & learning, critical curiosity, meaning making, dependence & fragility, creativity, relationships/interdependence and strategic awareness (Deakin Crick, 2007). Dynamic assessment of learning power can be used to reflect back to learners what they say about themselves in relation to these dimensions, and to provide teachers with information about individuals and groups that can be used to develop students’ self-awareness as well as their ownership of and responsibility for their learning. 
 Disposition analysis tools. 
 The ELLI (Effective Lifelong Learning Inventory) assessment tool arose from an exploratory factor analytic study involving 2000 learners. Since then, it has been developed in a range of educational settings worldwide as an instrument to help assess capacity for lifelong learning (Deakin Crick, 2007; Deakin Crick, et al., 2004; Small & Deakin Crick, 2008). ELLI is a self-report questionnaire which individuals are asked to answer with a specific piece of recent learning in mind. These responses are used to produce a learning profile, a graphical representation of how the learner has reported themselves in relation to the dimensions of learning power: “very much like me,” “quite like me” or “a little like me.” This diagram is not regarded as a description of fixed attributes but as the basis for a mentored discussion with the potential to spark and encourage changes in the learner’s activities, attitude and approach to learning. In order to gather ELLI data globally, with quality and access controls in place, and to generate analytics fast enough to impact practice in a timely manner, ELLI is hosted within a learning analytics infrastructure called the Learning Warehouse. This supports large-scale analysis of international datasets (e.g., >40,000 ELLI profiles), providing portals to organisations including remote Australian communities, schools in China, Malaysia, Germany, Italy, US, and corporates in the UK (Buckingham Shum & Deakin Crick, 2012). 
 Disposition-focused social learning analytics. 
 Learning dispositions are personal, related to the identity, personhood and desire of the learner (Deakin Crick & Yu, 2008). They can be regarded as socialised learning analytics when the emphasis shifts away from the learner as individual towards the learner in a social setting. From this perspective, two elements of disposition analytics are particularly important—their central role in an extended mentoring relationship, and the importance of relationships / interdependence as one of the seven key learning dispositions. The ELLIment tool provides a collaboration space for a learner and mentor to reflect on a learner’s ELLI profile, and agree on interventions. EnquiryBlogger mines information from a blogging tool set up to support enquiry, providing learners and teachers with visual analytics reflecting student activity and their self-assessment of progress in their enquiry, use of learning dispositions, and overall enjoyment. This then enables appropriate and timely intervention from teachers and, being a blogging environment, comments from peers (Ferguson, Buckingham Shum, & Deakin Crick, 2011). Mentors play an important part in social learning, providing both motivation and opportunities to build knowledge. They may act as role models, encouraging and counselling learners, and can also provide opportunities to rehearse arguments and to increase understanding (Anderson & Shannon, 1995; Ferguson, 2005; Liu, Macintyre, & Ferguson, 2012). People providing these online support relationships may be able to provide more useful assistance if they are aware of the prior knowledge, progress and goals of the person asking a question (Babin, Tricot, & Mariné, 2009). From a social learning perspective, disposition analytics provide ways of stimulating conceptual change, distributed expertise, collaboration and innovation. They tie in with an increasing emphasis on knowledge-age skills, and can be used to encourage learners to reflect on their ways of perceiving, processing and reacting to learning interactions. From the perspective of teachers and mentors, awareness of these elements contributes significantly to their ability to engage groups of learners in meaningful, engaging education. 
 Social learning content analytics. 
 Essence of content analytics. 
 Whereas disposition analytics have been developed within the field of education, content analytics have only recently been associated with education, originating in technical fields concerned with recommender systems and information retrieval (Drachsler, Hummel, & Koper, 2008; Zaïane, 2002). Content analytics is used here as a broad heading for the variety of automated methods that can be used to examine, index and filter online media assets, with the intention of guiding learners through the ocean of potential resources available to them. Note that these analytics are not identical to content analysis, which is concerned with description of the latent and/or manifest elements of communication (Potter & Levine-Donnerstein, 1999). Combined with learning context analytics or with defined search terms, content analytics may be used to provide recommendations of resources that are tailored either to the needs of an individual or to the needs of a group of learners. Research in information retrieval represents the leading edge of techniques for the automated indexing and filtering of content, whether textual, or multimedia (for example, images, video, or music). The state of the art in textual and video information retrieval tools is displayed annually in the competitions hosted at the Text Retrieval Conference (see Little, Llorente, & Rüger, 2010 for a review). Visual similarity search is an example of multimedia content analysis that uses features of images such as colour, texture and shape in order to find material that is visually related. This allows near-duplicate detection, known object identification and general search. Together, these elements can be used to provide novel methods of suggesting, browsing or finding educational media. Other approaches to content analytics are more closely aligned with content analysis. These involve examination of the latent elements that can be identified within transcripts of exchanges between people learning together online. This method has been used to investigate a variety of issues related to online social learning, including collaborative learning, presence and online cooperation (de Wever, Schellens, Vallcke, & van Keer, 2006). These latent elements of interpersonal exchanges can also be used to support sentiment analysis, using the objectivity/subjectivity of messages, and the emotions expressed within them to explore which resources are valued, and the motivations behind recommendations (Fakhraie, 2011). 
 Content analysis tools. 
 Web-based search engines are the default tools to which most learners and educators turn for text search, but multimedia search is becoming increasingly possible. While some approaches exploit the metadata around a multimedia asset, such as the text surrounding a photo, rather than analyse its actual content, true image-based search on the web is now available (for instance, Google Image search allows the filtering of results by colour). Some ecommerce websites enable product filtering by visual similarity, and mobile phone applications are able to parse images such as book covers, in order to retrieve their metadata (e.g., http://www.snaptell.com). Turning to transcript analysis, commonly used tools for content analysis include NVivo and Atlas.ti, both of which are software packages designed to support the analysis of unstructured information and qualitative data. However, these are manual tools for human analysts. Erkens and Janssen (2008) review the challenges of automated analysis, and describe Multiple Episode Protocol Analysis (MEPA), which has been validated against human coders, and used to automatically annotate chat transcripts from learning environment in numerous studies. In the selection of any of these tools, researchers face the bigger challenge of identifying an analytic framework that “emphasizes the criteria of reliability and validity and the counting of instances within a predefined set of mutually exclusive and jointly exhaustive categories” (de Wever et al., 2006). The validity of content analysis of online discussion has been persistently criticised (Pidgeon, 1996, p. 78) and it has proved difficult to identify empirically validated content analysis instruments to use in these contexts (Rourke, Anderson, Garrison, & Archer, 2003). 
 Content-focused social learning analytics. 
 How do these tools take on a new dimension in social learning? Visual Similarity Search can be used to support navigation of educational materials in a variety of ways, including discovering the source of an image, finding items that share visual features and may provide new ways of understanding a concept, or finding other articles, talks or movies in which a given image or movie frame is used (Little, Ferguson, & Rüger, 2011). Content analytics take on a social learning aspect when they draw upon the tags, ratings and additional data supplied by learners. An example is iSpot, which helps learners to identify anything in the natural world (Clow & Makriyannis, 2011). When a user first uploads a photo to the site, it has little to connect it with other information. The addition of a possible identification by another user ties that photo to other sets of data held externally in the Encyclopaedia of Life and within the UK’s National Biodiversity Network. In the case of iSpot, this analysis is not solely based on the by-products of interaction, an individual’s reputation within the network helps to weight the data that is added. The site’s reputation system has been developed with the purpose of magnifying the impact of known experts. Overall, the example of iSpot suggests one way in which content analytics can be combined with social network analytics to support learning. The two forms of analytics can also be used to support the effective distribution of key resources through a learning network. Another approach is to apply content analysis to the interplay of learning activities, learning objects, learning outcomes, and learners themselves, establishing semantic relations between different learning artefacts. This is the approach taken by LOCO-Analyst, which is used to analyse these semantic relations and thus provide feedback for content authors and teachers that can help them to improve their online courses (Jovanović et al., 2008). This type of analysis can draw on the information about user activity and behaviour that is provided by tools such as Google Analytics and userfly.com as well as by the tools built into environments such as Moodle and Blackboard. 
 Social learning context analytics. 
 Essence of context analytics. 
 Overall, social learning analytics can be applied to a wide variety of contexts that extends far beyond institutional systems. They can be used in formal settings such as schools, colleges and universities, in informal contexts in which learners choose both the process and the goal of their learning (Vavoula, 2004) and by mobile learners in a variety of situations (Sharples, Taylor, & Vavoula, 2005). In some cases, learners are in synchronous environments, structured on the basis that participants are co-present in time, and at others they are in asynchronous environments, where the assumption is that they will be participating at different times (Ferguson, 2009). They may be learning alone, in a network, in an affinity group, in communities of inquiry, communities of interest or communities of practice (Ferguson, 2009). Here we are grouping under the heading “context analytics” the various analytic tools that expose, make use of or seek to understand these contexts in relation to learning. Zimmerman and his colleagues (2007) provide a definition of context that allows the definition of the context of an entity (for example, a learner) depending on five distinct categories: - Individuality context includes information about the entity within the context. In the case of learners, this might include their language, their behaviour, their preferences and their goals - Time context includes points in times, ranges and histories so can take into account work flow, long-term courses and interaction histories - Location context can include absolute location, location in relation to people or resources, or virtual location (IP address) - Activity context is concerned with goals, tasks and actions - Relations context captures the relations of an entity with other entities, of example with learners, teachers and resources. Early work in context-aware computing treated the environment as a shell encasing the user and focused on scalar properties such as current time and location, together with a list of available objects and services (see, for example, Abowd, Atkeson, Hong, Long, & Pinkerton, 1997; Want, Hopper, Falcao, & Gibbons, 1992). The focus was on the individual user receiving data from an environment rather than interacting with it. This model did not acknowledge the dynamics of interaction between people and the environment. When considered in the context of learning, it did not provide information that could help people to modify their environment in order to create supportive workspaces or form social networks with those around them or accessible online (Brown et al., 2010). 
 Context analysis tools. 
 The MOBIlearn project took a different view, considering context to be a dynamic process, constructed through learners’ interactions with learning materials and the surrounding world over time (Beale & Lonsdale, 2004; Syvänen, Beale, Sharples, Ahonen, & Lonsdale, 2005). The MOBIlearn context awareness subsystem was developed to allow learners to maintain their attention on the world around them while their device presents content, options and resources that support their learning activities. The developers of the system designed the system to analyse a variety of available data in order to produce learner-focused information and recommendations, taking into account not only the physical environment but also social relationships. Environmental information such as geographical position allows us to provide location-specific information, e.g., for a museum. Other user information such as the identification and presence of another person allows us to create a peer-to-peer network for informal chat. But the combination of the two may allow us to determine that the other user is a curator, and we can provide the mechanisms for one to give a guided tour to the other. (Beale & Lonsdale, 2004) The Active Campus tool was another one developed to prompt connections with learners and resources. The aim was to provide a tool that could analyse people, resources and events in the vicinity and then act like a pair of “x-ray glasses,” providing opportunities for serendipitous learning by letting users see through crowds and buildings to reveal nearby friends, potential colleagues and interesting events (Griswold et al., 2004). 
 Context-focused social learning analytics. 
 The MOBIlearn project produced several recommendations to be considered in the design process of an adaptive and pervasive learning environment. Some of these are focused on the physical design of tools, but others are directly relevant to the development of context-focused social learning analytics, specifically: - Organizing the information provided to the user according to the availability for cooperation (students), advice (experts, instructors) and groups available at a given moment. - Supporting the communication between users by presenting tools, such as news groups and chats, ordered by their current popularity in the learning community (placing first the most popular, or the most relevant to the learner according to the profile, at any given moment). - Encouraging users to cooperate and affiliate by pushing the information when relevant opportunities occur. Actions by the system are guided, for example, by the information related to a group-based modeling that takes into account each user’s evident interest in certain piece(s) of information (Syvänen et al., 2005). These suggest fruitful ways forward in this area. In the case of online learning, context analytics can draw upon readily available data such as profile information, timestamps, operating system and location. Such data mining can support recommendations that are appropriate for learners’ situation, the time they have available, the devices they can access, their current role and their future goals. Context analytics can also be used to highlight the activity of other learners in a community or network, through tag clouds, hash tags, data visualizations, activity streams and emergent folksonomies. In addition to development work in this field, there is also a need for substantial theoretical work that can underpin it. Social network analysts have spent many years identifying elements and structures that have been found to support learning and which can be used to create contexts that promote the development of sophisticated learning networks. There are currently no such sophisticated analytics available to help us develop suitable contexts for other groupings known to support social learning, such as affinity groups and communities of practice. We also lack the long-term analytics of learner behaviour that could help us to analyse context in order to support the development of personal learning narratives, learning trajectories or other understandings of lifelong learning (Gee, 2004; Jones & Preece, 2006; Lipman, 2003; Wenger, 1998). 
 The challenge of powerful analytics. 
 Having explained how we are conceiving social learning analytics, we now consider some of the critiques around the balance of power in learning analytics, in response to which we will conclude by sketching potential future scenarios that may address these concerns. New forms of measurement and classification—for that is essentially what learning analytics are—are rightly exposed to disciplinary and ethical critiques concerning issues such as: who is defining the measures, to what ends, what is being measured, and who has access to what data? In their incisive critique of classification systems, Bowker and Star (2000) demonstrate how these become the mechanisms by which we choose not only how to remember, but also systematically forget, what is known. If a phenomenon is not visible within a classification scheme, it is systematically erased. The issue of power is, therefore, a central one to confront. This dilemma sits at the heart of the controversy around any policy dependent on a predefined performance indicator. Schools, universities, faculties or individuals whose work is invisible within a classification scheme are disenfranchised when defined by powerful stakeholders with associated rewards/sanctions. Whether this is reasonable sparks debate as to whether phenomena are being justifiably ignored because they are not something to be encouraged, or whether it is simply that they are too hard to quantify for automated processing and performance grading. The challenge for learning analytics is more complex still. As described above, at least some forms of learning analytics research have an interest in using data generated by users as a by-product of online activity (for example, asking/answering questions, or recommending resources), rather than as an intentional form of evidence of learning (such as taking a test or submitting an essay). Building on this potentially noisy data, research into recommendation engines goes one step further, exploring the potential to mine such data for patterns that can be acted on by software agents in some way—perhaps in the form of feedback to learners via a personal analytics dashboard or as modifications to the content that is displayed based on the system’s model of the learner. Such research must engage fully with questions around the academic, pedagogical and ethical integrity of the principles for defining such patterns and recommender algorithms, and who is permitted to see them within the set of stakeholders. Important concerns (boyd & Crawford, 2011) are beginning to be expressed about learning analytics, such as the following variants on longstanding debates at the intersection of education, technology and artificial intelligence: - Analytics are dependent on computational platforms that use, re-use and merge learner data, both public and private: institutions should steer clear of open data and minimise the merging of datasets of any sort until there are much clearer ethical and legal guidelines. - Analytics could disempower learners, making them increasingly reliant on institutions providing them with continuous feedback, rather than developing meta-cognitive and learning-to-learn skills and dispositions. - Analytics are a crude way to operationalise proxy measures of teacher effectiveness, and will be used to compare and contrast student outcomes, leading to the gaming of the system: “learning and teaching to the analytic” to maintain performance indicators that do not genuinely promote meaningful learning. In sum, learning analytics and recommendation engines are always designed with a particular conception of “success,” thus defining the patterns deemed to be evidence of progress, and hence, the data that should be captured. A marker of the health of the learning analytics field will be the quality of debate around what the technology renders visible and leaves invisible. Briefly, let us consider how these issues may be seen through a Social Learning Analytics lens, recognising that a more detailed treatment is needed in future work. If the values and practices we see in the open, social web inform the ways in which SLAs are deployed, we may see ways to address these concerns. For example: - If SLA tools and data are placed in the hands of learners, the balance of power shifts significantly. When the exposure of personal data to analytics is voluntary, when a group’s data is collectively owned, and when gaming the system or trying to pretend to be someone you are not incurs social sanctions, the risks of abuse are arguably lower than when a hierarchical institution carries the unrealistic burden of responsibility for controlling a living ecosystem of participants, data and tools. It is realistic to note that the above imply a maturing in technologies, learner literacies, and institutional practices around the management of personal data, compared to the situation we have today. - If analytics are drawing learners’ attention to their development as self-aware, intrinsically motivated learners, they are being moved in the opposite direction to becoming passively dependent on the institution or platform to tell them how they are doing and what to do next. - If analytics are focused on providing formative feedback to improve learning process, rather than making automated judgments about mastery levels in a given subject, there might be fewer concerns around the removal of human mentors from the feedback loop. We also hypothesise that the risks of “gaming the analytic” reduce: SLA activity patterns are by definition hard to fabricate privately, so not only are learners fooling themselves if they fake behaviour (e.g., designed to look like skillful discourse, supportive networking, or self-reflection), they risk making fools of themselves among peers for whom authenticity and trustworthiness are valued personal qualities. 
 Conclusion: SLA future scenarios. 
 Let us conclude by engaging in the early stages of what Miller (2007) terms “futures literacy”—stretching our imaginations in disciplined ways in order to sketch potential futures, were social learning analytics to develop in line with these cultural shifts. Consider the forces identified earlier (§4), and for each, imagine future scenarios in which SLA values, tools and practices have matured beyond today’s nascent state. The digital infrastructure is reaching a state of maturity that enables non-technical people to engage with expertly designed “walk up and use” interfaces on both large-screen and mobile devices, to connect with people and information on a global scale, and to make their contributions via social media platforms. - Potential SLA future: Institutions lacking the infrastructure needed for computationally intensive analytics and recommendation engines will call on SLA services in the computing “cloud,” following the business developments we are now seeing to offer commercial learning analytics cloud services on school/university data. Individual learners or communities who need such services also utilise these services. Some companies and educational institutions will exploit their pedagogical expertise to provide SLA consulting services. As we see the commercialization of the analytics computing space, there is an argument that at this point the field needs a complementary Open Learning Analytics innovation platform (SoLAR, 2011). “Free and Open” is a key expectation and dynamic within online social learning. It highlights the recalibration that is taking place around expectations of freely provided quality services, accompanied by readiness to pay for valueadded services once the free service has proven itself. Data is expected to be accessible, appropriately licensed for remixing and, wherever possible, in machine-readable formats to facilitate interoperability and avoid data or users being locked into a given platform. - Potential SLA future: Many SLA tools become available in open source versions, making them customisable within the myriad unique social contexts in which they may be deployed. It becomes normal that SLA patterns and data are open, shareable resources for reflection, and analysis in alternative tools. In addition to a diverse palette of free SLA tools, an economy grows which helps learners to configure these to create meaningful toolkits that support particular kinds of learning, or work well with particular platforms. Learners are willing to pay for more powerful features, once the most successful tools have earned their right to charge. A key lesson from the social web paradigm, and a long-held aspiration of researchers into end-user customisability, is that when empowered with appropriately flexible tools, an ecosystem grows in which new roles are created for different kinds of user to customise their tools (MacLean, Carter, Lovstrand, & Moran, 1990). Aspirations across cultures have been shifting in empirically verifiable ways towards a growing desire for participation and self-expression. The social web is an expression of this shift, providing a significant medium for many people to construct their identity. - Potential SLA future: The outputs of SLA tools become an important part of individuals’ sense of identity, and their ability to evidence their skills. For example, we might see Badges such as: “I am a good broker between communities,” “I can distill complex debates into their essence,” “I can mentor learners in building their creativity.” Innovation in complex, turbulent environments requires social knowledge-creation and negotiation infrastructures built on quality relationships and conversations—beyond impersonal “transactions”—in order for individuals, groups and organisations to be agile enough to respond to turbulent change and to work together to solve “wicked problems.” - Potential implications for SLA: SLAs become an integral part of the employee’s toolkit, helping to track the swirl of people, conversations and resources by rendering significant changes in coherent ways that keep cognitive load at a manageable level, rather than amplifying demands on attention. 
 The role of educational institutions is changing. They are moving increasingly to provide personalised support for learning how to think deeply, and learning how to be an effective member of the communities that one cares about. - Potential implications for SLA: Educational institutions are no longer the only option for evidencing advanced learning. Analytics become a new form of trusted evidence, being generated from verifiable public datasets, or private datasets that could not have been reasonably fabricated, such as a reputable online community. In sum, if it is the case that these tectonic shifts define a new context for thinking about learning, in particular around questions of power and the central role of interpersonal relationships, by extension they set a new context for thinking about learning analytics. They call into question the assumption inherited from the business intelligence and management information systems orientation, that learning analytics are designed and controlled primarily by institutional educators and administrators in order to optimize learners’ performance, and hence the institution’s performance. This is not at all to argue that academic/action analytics are unimportant—but it now becomes clear that this is only one of a range of possible analytics scenarios. To conclude, we have motivated the concept of Social Learning Analytics as a response to some of the forces reshaping the educational landscape, and our growing understanding that many forms of learning most relevant to becoming a citizen in our complex society are socially grounded and evidenced phenomena. SLAs may be deployed as institutional tools in conventional courses, to yield insight for educators and administrators. Equally, however, they should be seen as tools to be placed in the hands of the very subjects being analysed—the learners—and for the many informal learning contexts that we now see outside the walls of conventional institutions. It would indeed be ironic if the ways in which Social Learning Analytics tools were deployed did not honour and promote the open, democratising, critical dynamics that underpin much of the participatory, social web philosophy—dynamics which SLA tools make visible in new ways. 
 Acknowledgements. 
 We gratefully acknowledge The Open University for resourcing the SocialLearn Project, several anonymous reviewers for their constructive reviews on earlier drafts, and the encouragement from researchers and practitioners who have found these ideas valuable in their own work.